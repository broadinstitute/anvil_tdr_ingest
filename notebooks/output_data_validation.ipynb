{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version\n",
    "#print('Version 1.0.0: 05/23/2022 09:59am')\n",
    "#print('Version 2.0.0: 08/25/2022 08:49pm -- Upgraded to generic version of data validation (DI-78)')\n",
    "#print('Version 2.0.1: 09/01/2022 02:11pm -- Fixed a bug in the format of the output file')\n",
    "#print('Version 2.0.2: 09/14/2022 10:23am -- Made output directory and validation schema more configurable')\n",
    "#print('Version 2.0.3: 10/05/2022 12:39pm -- Returned file path to results for easier access')\n",
    "#print('Version 2.0.4: 10/19/2022 9:49am -- Updated schema processing to support schemas not sourced from API')\n",
    "#print('Version 2.0.5: 10/25/2022 9:38am -- Removed auto-flagging of fileref columns with nulls')\n",
    "#print('Version 2.0.6: 2/28/2023 11:33am -- Updated notebook to be usable in dev (removed TDR host hardcoding)')\n",
    "print('Version 2.0.7: 12/13/2023 1:13pm -- Replaced deprecated df append with pd.concat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import import_ipynb\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import data_repo_client\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from io import StringIO\n",
    "import uuid\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import json\n",
    "import ingest_pipeline_utilities as utils\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate UUID provided is value\n",
    "def is_valid_uuid(value):\n",
    "    try:\n",
    "        uuid.UUID(str(value))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Function to validate a specified file exists\n",
    "def file_exists(filename):\n",
    "    try:\n",
    "        f = open(os.path.expanduser(filename))\n",
    "        f.close()\n",
    "        return True\n",
    "    except IOError:\n",
    "        return False\n",
    "\n",
    "# Function to retrieve the TDR schema definition for the data\n",
    "def retrieve_tdr_schema(uuid, storage_type, api_client):\n",
    "    # Retrieve TDR schema definition from a dataset or snapshot\n",
    "    tdr_schema_dict = {}\n",
    "    bq_project = \"\"\n",
    "    bq_schema = \"\"\n",
    "    skip_bq_queries = False\n",
    "    try:\n",
    "        if storage_type == \"dataset\":\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            response = datasets_api.retrieve_dataset(id=uuid, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "            tdr_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "            tdr_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "            try:\n",
    "                bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "                bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "            except:\n",
    "                logging.error(\"Error retrieving BigQuery Project and Dataset information. Skipping BQ-based data profiling checks. Confirm this is a BigQuery hosted dataset and try again to run these checks.\")\n",
    "                skip_bq_queries = True\n",
    "        else:\n",
    "            snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "            response = snapshots_api.retrieve_snapshot(id=uuid, include=[\"TABLES\", \"RELATIONSHIPS\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "            tdr_schema_dict[\"tables\"] = response[\"tables\"]\n",
    "            tdr_schema_dict[\"relationships\"] = response[\"relationships\"]\n",
    "            try:\n",
    "                bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "                bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "            except:\n",
    "                logging.error(\"Error retrieving BigQuery Project and Dataset information. Skipping BQ-based data profiling  checks. Confirm this is a BigQuery hosted snapshot and try again to run these checks.\")\n",
    "                skip_bq_queries = True\n",
    "    except Exception as e:\n",
    "        logging.error(\"TDR error on retrieving specified dataset: {}\".format(str(e)))\n",
    "        logging.error(\"Exiting script.\")\n",
    "        sys.exit(1)\n",
    "    return tdr_schema_dict, bq_project, bq_schema, skip_bq_queries\n",
    "\n",
    "# Function to retrieve a TDR schema definition and parse it into more useful objects\n",
    "def process_tdr_schema(tdr_schema_dict, schema_src):\n",
    "    # Parse TDR schema into a table set, array field set, and field list for use in query construction\n",
    "    table_set = set()\n",
    "    array_field_set = set()\n",
    "    field_list = []\n",
    "    relationship_count = len(tdr_schema_dict[\"relationships\"])\n",
    "    for table_entry in tdr_schema_dict[\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])\n",
    "        if schema_src == \"api\":\n",
    "            pk_prop = \"primary_key\"\n",
    "            from_prop = \"_from\"\n",
    "        else:\n",
    "            pk_prop = \"primaryKey\"\n",
    "            from_prop = \"from\"\n",
    "        for column_entry in table_entry[\"columns\"]:\n",
    "            field_dict = {}\n",
    "            field_dict[\"table\"] = table_entry[\"name\"]\n",
    "            field_dict[\"column\"] = column_entry[\"name\"]\n",
    "            field_dict[\"datatype\"] = column_entry[\"datatype\"]\n",
    "            field_dict[\"is_array\"] = column_entry[\"array_of\"]\n",
    "            field_dict[\"required\"] = column_entry[\"required\"]\n",
    "            if column_entry[\"name\"] in table_entry[pk_prop]:\n",
    "                field_dict[\"is_primary_key\"] = True\n",
    "            else:\n",
    "                field_dict[\"is_primary_key\"] = False\n",
    "            joins_to_list = []\n",
    "            for relation_entry in tdr_schema_dict[\"relationships\"]:\n",
    "                joins_to_dict = {}\n",
    "                if relation_entry[from_prop][\"table\"] == table_entry[\"name\"] and relation_entry[from_prop][\"column\"] == column_entry[\"name\"]:\n",
    "                    joins_to_dict[\"table\"] = relation_entry[\"to\"][\"table\"]\n",
    "                    joins_to_dict[\"column\"] = relation_entry[\"to\"][\"column\"]\n",
    "                    joins_to_list.append(joins_to_dict)\n",
    "            field_dict[\"joins_to\"] = joins_to_list\n",
    "            joins_from_list = []\n",
    "            for relation_entry in tdr_schema_dict[\"relationships\"]:\n",
    "                joins_from_dict = {}\n",
    "                if relation_entry[\"to\"][\"table\"] == table_entry[\"name\"] and relation_entry[\"to\"][\"column\"] == column_entry[\"name\"]:\n",
    "                    joins_from_dict[\"table\"] = relation_entry[from_prop][\"table\"]\n",
    "                    joins_from_dict[\"column\"] = relation_entry[from_prop][\"column\"]\n",
    "                    joins_from_list.append(joins_from_dict)\n",
    "            field_dict[\"joins_from\"] = joins_from_list\n",
    "            field_list.append(field_dict)\n",
    "            if column_entry[\"array_of\"] == True:\n",
    "                array_field_set.add(table_entry[\"name\"] + \".\" + column_entry[\"name\"])\n",
    "    return table_set, array_field_set, field_list, relationship_count \n",
    "\n",
    "# Function to collect table level statistics: row counts\n",
    "def run_table_profiling_checks(client, df, bq_project, bq_schema, table_set, field_list):\n",
    "    logging.info(\"Building and executing table-level queries...\")\n",
    "    # Loop through tables in the table set and pull record counts (and record empty tables for use in column-level queries)\n",
    "    empty_table_list = []\n",
    "    query_count = 0\n",
    "    for table_entry in table_set:\n",
    "\n",
    "        # Construct the table record count query\n",
    "        row_count_query = \"\"\"SELECT 'Summary Stats' AS metric_type, '{table}' AS source_table, 'All' AS source_column, \n",
    "                   'Count of records in table' AS metric, \n",
    "                   COUNT(*) AS n, null AS d, null AS r, \n",
    "                   CASE WHEN COUNT(*) = 0 THEN 1 END AS flag \n",
    "                   FROM `{project}.{schema}.{table}`\"\"\".format(project = bq_project, schema = bq_schema, table = table_entry)\n",
    "\n",
    "        # Execute the query and append results to dataframe\n",
    "        query_count += 1\n",
    "        #print(row_count_query)\n",
    "        try:\n",
    "            df_temp = client.query(row_count_query).result().to_dataframe()\n",
    "            df = pd.concat([df, df_temp])\n",
    "            if df_temp[\"n\"].values[0] == 0:\n",
    "                empty_table_list.append(table_entry)\n",
    "            else:\n",
    "\n",
    "                # Identify all fileref columns\n",
    "                fileref_col_list = []\n",
    "                for column_entry in field_list:\n",
    "                    if column_entry[\"table\"] == table_entry and column_entry[\"datatype\"] == \"fileref\":\n",
    "                        fileref_col_list.append(\"'\" + column_entry[\"column\"] + \"'\")\n",
    "                if len(fileref_col_list) > 0:\n",
    "                    fileref_col_str = \", \".join(fileref_col_list)\n",
    "                else:\n",
    "                    fileref_col_str = \"''\"\n",
    "\n",
    "                # Construct the null column count query\n",
    "                null_query = \"\"\"WITH null_counts AS\n",
    "                        (\n",
    "                          SELECT column_name, COUNT(1) AS cnt\n",
    "                          FROM `{project}.{schema}.{table}`, \n",
    "                          UNNEST(REGEXP_EXTRACT_ALL(TO_JSON_STRING(`{project}.{schema}.{table}`), r'\"(\\w+)\":(?:null|\\[\\])')) column_name\n",
    "                          GROUP BY column_name\n",
    "                        ),\n",
    "                        table_count AS\n",
    "                        (\n",
    "                          SELECT COUNT(*) AS cnt FROM `{project}.{schema}.{table}`\n",
    "                        )\n",
    "                        SELECT 'Summary Stats' AS metric_type, src.table_name AS source_table, src.column_name AS source_column, \n",
    "                        'Count of nulls or empty lists in column'||CASE WHEN src.column_name IN ({fileref_list}) THEN ' (fileref)' ELSE '' END AS metric,\n",
    "                        COALESCE(tar.cnt, 0) AS n, \n",
    "                        table_count.cnt AS d,\n",
    "                        CASE WHEN table_count.cnt > 0 THEN COALESCE(tar.cnt, 0)/table_count.cnt END AS r,\n",
    "                        null AS flag\n",
    "                        FROM `{project}.{schema}.INFORMATION_SCHEMA.COLUMNS` src\n",
    "                          LEFT JOIN null_counts tar ON src.column_name = tar.column_name\n",
    "                          CROSS JOIN table_count\n",
    "                        WHERE src.table_name = '{table}'\n",
    "                        AND src.column_name NOT IN ('datarepo_row_id', 'datarepo_ingest_date')\"\"\".format(project = bq_project, schema = bq_schema, table = table_entry, fileref_list = fileref_col_str)\n",
    "\n",
    "                # Execute the null count query and append results to dataframe\n",
    "                query_count += 1\n",
    "                #print(null_query)\n",
    "                try:\n",
    "                    df_result = client.query(null_query).result().to_dataframe()\n",
    "                    df = pd.concat([df, df_result])\n",
    "                except Exception as e:\n",
    "                    logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "                \n",
    "                # Construct the distinct column value query\n",
    "                distinct_query = \"\"\"WITH distinct_counts AS\n",
    "                        (\n",
    "                          SELECT column_name, APPROX_COUNT_DISTINCT(CASE WHEN column_value NOT IN ('null', '[]') THEN column_value END) AS cnt\n",
    "                          FROM `{project}.{schema}.{table}`,\n",
    "                          UNNEST(REGEXP_EXTRACT_ALL(TO_JSON_STRING(`{project}.{schema}.{table}`), r'\"(\\w+)\":')) AS column_name WITH OFFSET pos1,\n",
    "                          UNNEST(REGEXP_EXTRACT_ALL(TO_JSON_STRING(`{project}.{schema}.{table}`), r':(.+?),')) AS column_value WITH OFFSET pos2\n",
    "                          WHERE pos1 = pos2\n",
    "                          GROUP BY column_name\n",
    "                        ),\n",
    "                        table_count AS\n",
    "                        (\n",
    "                          SELECT COUNT(*) AS cnt FROM `{project}.{schema}.{table}`\n",
    "                        )\n",
    "                        SELECT 'Summary Stats' AS metric_type, src.table_name AS source_table, src.column_name AS source_column, \n",
    "                        'Count of distinct values in column' AS metric,\n",
    "                        COALESCE(tar.cnt, 0) AS n, \n",
    "                        table_count.cnt AS d,\n",
    "                        CASE WHEN table_count.cnt > 0 THEN COALESCE(tar.cnt, 0)/table_count.cnt END AS r,\n",
    "                        null AS flag\n",
    "                        FROM `{project}.{schema}.INFORMATION_SCHEMA.COLUMNS` src\n",
    "                          LEFT JOIN distinct_counts tar ON src.column_name = tar.column_name\n",
    "                          CROSS JOIN table_count\n",
    "                        WHERE src.table_name = '{table}'\n",
    "                        AND src.column_name NOT IN ('datarepo_row_id', 'datarepo_ingest_date')\"\"\".format(project = bq_project, schema = bq_schema, table = table_entry)\n",
    "                                        \n",
    "                # Execute the distinct count query and append results to dataframe\n",
    "                query_count += 1\n",
    "                #print(distinct_query)  \n",
    "                try:\n",
    "                    df_result = client.query(distinct_query).result().to_dataframe()\n",
    "                    df = pd.concat([df, df_result])\n",
    "                except Exception as e:\n",
    "                    logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "\n",
    "    logging.info(\"Table-level queries complete. {0} queries executed.\".format(query_count))\n",
    "    return df, empty_table_list\n",
    "\n",
    "# Function to collect column level statistics: null counts, unique counts, linkage counts (counts of records where foreign key doesn\"t join to a primary key), and reverse linkage counts (counts of records where a primary key isn\"t reference by any foriegn key)\n",
    "def run_column_profiling_checks(client, df, bq_project, bq_schema, field_list, array_field_set, empty_table_list):\n",
    "    logging.info(\"Building and executing column-level queries...\")\n",
    "    # Loop through columns and pull null counts and distinct value counts\n",
    "    query_count = 0\n",
    "    for column_entry in field_list:\n",
    "    \n",
    "        # Skip column-level queries for tables that don't have records (to save processing time)\n",
    "        if column_entry[\"table\"] not in empty_table_list:\n",
    "    \n",
    "            # Loop through join_to fields (if any) and build linkage queries\n",
    "            table_name = column_entry[\"table\"]\n",
    "            col_name = column_entry[\"column\"]\n",
    "            for join_entry in column_entry[\"joins_to\"]:\n",
    "\n",
    "                # Set parameters for linkage queries\n",
    "                target_table = join_entry[\"table\"]\n",
    "                target_col = join_entry[\"column\"]\n",
    "                target_table_col = target_table + \".\" + target_col\n",
    "                if column_entry[\"is_array\"] == True:\n",
    "                    src_col_name = \"{col}_unnest\".format(col = col_name)\n",
    "                    from_statement = \"(select * from `{project}.{schema}.{table}` t left join unnest(t.{col}) as {unnest_col}) src\".format(project = bq_project, schema = bq_schema, table = table_name, col = col_name, unnest_col = src_col_name)\n",
    "                    where_statement = \"array_length(src.{col}) > 0\".format(col = col_name)\n",
    "                else:\n",
    "                    src_col_name = col_name\n",
    "                    from_statement = \"`{project}.{schema}.{table}` src\".format(project = bq_project, schema = bq_schema, table = table_name)\n",
    "                    where_statement = \"src.{col} is not null\".format(col = col_name)\n",
    "                if target_table_col in array_field_set:\n",
    "                    tar_col_name = \"{col}_unnest\".format(col = target_col)\n",
    "                    join_statement = \"(select * from `{project}.{schema}.{table}` t left join unnest(t.{col}) as {unnest_col}) tar\".format(project = bq_project, schema = bq_schema, table = target_table, col = target_col, unnest_col = tar_col_name)\n",
    "                else:\n",
    "                    tar_col_name = target_col\n",
    "                    join_statement = \"`{project}.{schema}.{table}` tar\".format(project = bq_project, schema = bq_schema, table = target_table)\n",
    "\n",
    "                # Construct the linkage query\n",
    "                linkage_query = \"\"\"SELECT 'Referential Integrity' AS metric_type, '{table}' AS source_table, '{col}' AS source_column, \n",
    "                       'Count of non-null rows that do not fully join to {target}' AS metric, \n",
    "                       COUNT(DISTINCT CASE WHEN tar.datarepo_row_id IS NULL THEN src.datarepo_row_id END) AS n, \n",
    "                       COUNT(DISTINCT src.datarepo_row_id) AS d, \n",
    "                       CASE WHEN COUNT(DISTINCT src.datarepo_row_id) > 0 THEN COUNT(DISTINCT CASE WHEN tar.datarepo_row_id IS NULL THEN src.datarepo_row_id END)/COUNT(DISTINCT src.datarepo_row_id) END AS r, \n",
    "                       CASE WHEN COUNT(DISTINCT CASE WHEN tar.datarepo_row_id IS NULL THEN src.datarepo_row_id END) > 0 THEN 1 END AS flag\n",
    "                       FROM {frm}\n",
    "                       LEFT JOIN {join}\n",
    "                       ON CAST(src.{src_col} AS STRING) = CAST(tar.{tar_col} AS STRING)\n",
    "                       WHERE {where}\"\"\".format(project = bq_project, schema = bq_schema, table = table_name, col = col_name, target = target_table_col, frm = from_statement, join = join_statement, src_col = src_col_name, tar_col = tar_col_name, where = where_statement)\n",
    "\n",
    "                # Execute the referential integrity query and append results to dataframe\n",
    "                query_count += 1\n",
    "                #print(linkage_query)\n",
    "                try:\n",
    "                    df_result = client.query(linkage_query).result().to_dataframe()\n",
    "                    df = pd.concat([df, df_result])\n",
    "                except Exception as e:\n",
    "                    logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "\n",
    "            # For primary key fields, loop through join_from fields and build reverse linkage checks\n",
    "            if column_entry[\"is_primary_key\"] == True and len(column_entry[\"joins_from\"]) > 0:\n",
    "\n",
    "                # Construct CTE that includes all foreign keys that reference the primary key\n",
    "                counter = 0\n",
    "                cte_query = \"WITH temp_fks AS (\"\n",
    "                source_col_list = []\n",
    "                for entry in column_entry[\"joins_from\"]:\n",
    "                    cte_query_segment = \"\"\n",
    "                    counter += 1\n",
    "                    source_table = entry[\"table\"]\n",
    "                    source_column = entry[\"column\"]\n",
    "                    source_table_col = entry[\"table\"] + \".\" + entry[\"column\"]\n",
    "                    source_col_list.append(source_table_col)\n",
    "                    if counter > 1:\n",
    "                        cte_query_segment = \"UNION ALL \"\n",
    "                    if source_table_col in array_field_set:\n",
    "                        cte_query_segment += \"SELECT DISTINCT {tar_col} FROM `{project}.{schema}.{table}` CROSS JOIN UNNEST({src_col}) AS {tar_col}\".format(project = bq_project, schema = bq_schema, table = source_table, src_col = source_column, tar_col = col_name)\n",
    "                    else:\n",
    "                        cte_query_segment += \"SELECT DISTINCT {src_col} as {tar_col}  FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = source_table, src_col = source_column, tar_col = col_name)\n",
    "                    cte_query = cte_query + cte_query_segment + \" \"\n",
    "                cte_query = cte_query + \")\"\n",
    "                source_col_list_string = \", \".join(source_col_list)\n",
    "\n",
    "                # Construct the reverse linkage query\n",
    "                reverse_linkage_query = \"\"\"{cte} SELECT 'Referential Integrity' As metric_type, '{table}' AS source_table, '{col}' AS source_column, \n",
    "                                      'Count of rows where primary key is not referenced by foreign key fields ({fk_list})' AS metric,\n",
    "                                      COUNT(DISTINCT CASE WHEN tar.{col} IS NULL THEN src.{col} END) AS n,\n",
    "                                      COUNT(DISTINCT src.{col}) AS d,\n",
    "                                      CASE WHEN COUNT(DISTINCT src.{col}) > 0 THEN COUNT(DISTINCT CASE WHEN tar.{col} IS NULL THEN src.{col} END)/COUNT(DISTINCT src.{col}) END AS r, \n",
    "                                      CASE WHEN COUNT(DISTINCT CASE WHEN tar.{col} IS NULL THEN src.{col} END) > 0 THEN 1 END AS flag\n",
    "                                      FROM `{project}.{schema}.{table}` src LEFT JOIN temp_fks tar ON src.{col} = tar.{col}\"\"\".format(cte = cte_query, project = bq_project, schema = bq_schema, table = table_name, col = col_name, fk_list = source_col_list_string)\n",
    "\n",
    "                # Execute the reverse linkage query and append results to dataframe\n",
    "                query_count += 1\n",
    "                #print(reverse_linkage_query)\n",
    "                try:\n",
    "                    df_result = client.query(reverse_linkage_query).result().to_dataframe()\n",
    "                    df = pd.concat([df, df_result])\n",
    "                except Exception as e:\n",
    "                    logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "                \n",
    "    logging.info(\"Column-level queries complete. {0} queries executed.\".format(query_count))\n",
    "    return df\n",
    "\n",
    "# Function to collect the files in TDR that aren't referenced in the table data\n",
    "def run_orphan_file_checks(client, df, bq_project, bq_schema, field_list, array_field_set):\n",
    "    logging.info(\"Building and executing orphaned files query...\")\n",
    "    # Collect file reference fields\n",
    "    file_ref_list = []\n",
    "    for column_entry in field_list:\n",
    "        column_dict = {}\n",
    "        if column_entry[\"datatype\"] == \"fileref\":\n",
    "            column_dict[\"table\"] = column_entry[\"table\"]\n",
    "            column_dict[\"column\"] = column_entry[\"column\"]\n",
    "            file_ref_list.append(column_dict)\n",
    "\n",
    "    # Construct CTE that includes all fileref fields\n",
    "    counter = 0\n",
    "    orphan_count = 0\n",
    "    cte_query = \"WITH temp_fks AS (\"\n",
    "    source_col_list = []\n",
    "    if len(file_ref_list) > 0:\n",
    "        for entry in file_ref_list:\n",
    "            cte_query_segment = \"\"\n",
    "            counter += 1\n",
    "            source_table = entry[\"table\"]\n",
    "            source_column = entry[\"column\"]\n",
    "            source_table_col = entry[\"table\"] + \".\" + entry[\"column\"]\n",
    "            source_col_list.append(source_table_col)\n",
    "            if counter > 1:\n",
    "                cte_query_segment = \"UNION ALL \"\n",
    "            if source_table_col in array_field_set:\n",
    "                cte_query_segment += \"SELECT DISTINCT file_id FROM `{project}.{schema}.{table}` CROSS JOIN UNNEST({src_col}) AS file_id\".format(project = bq_project, schema = bq_schema, table = source_table, src_col = source_column)\n",
    "            else:\n",
    "                cte_query_segment += \"SELECT DISTINCT {src_col} AS file_id  FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = source_table, src_col = source_column)\n",
    "            cte_query = cte_query + cte_query_segment + \" \"\n",
    "            source_col_list_string = \", \".join(source_col_list)\n",
    "    else:\n",
    "        cte_query += \"SELECT '1' AS file_id\" \n",
    "        source_col_list_string = \"\"\n",
    "    cte_query += \")\"\n",
    "\n",
    "    # Construct the orphaned files query\n",
    "    orphaned_file_query = \"\"\"{cte} SELECT 'Orphaned Files' As metric_type, 'datarepo_load_history' AS source_table, 'file_id' AS source_column, \n",
    "                          'Count of file_ids not referenced by a fileref field ({fk_list})' AS metric,\n",
    "                          COUNT(DISTINCT CASE WHEN tar.file_id IS NULL THEN src.file_id END) AS n,\n",
    "                          COUNT(DISTINCT src.file_id) AS d,\n",
    "                          CASE WHEN COUNT(DISTINCT src.file_id) > 0 THEN COUNT(DISTINCT CASE WHEN tar.file_id IS NULL THEN src.file_id END)/COUNT(DISTINCT src.file_id) END AS r, \n",
    "                          CASE WHEN CASE WHEN COUNT(DISTINCT src.file_id) > 0 THEN COUNT(DISTINCT CASE WHEN tar.file_id IS NULL THEN src.file_id END)/COUNT(DISTINCT src.file_id) END > 0 THEN 1 END AS flag\n",
    "                          FROM `{project}.{schema}.datarepo_load_history` src LEFT JOIN temp_fks tar ON src.file_id = tar.file_id\n",
    "                          WHERE state = 'succeeded'\"\"\".format(cte = cte_query, project = bq_project, schema = bq_schema, fk_list = source_col_list_string)\n",
    "\n",
    "    # Execute the orphaned files query and append results to dataframe\n",
    "    #print(orphaned_file_query)\n",
    "    try:\n",
    "        df_temp = client.query(orphaned_file_query).result().to_dataframe()\n",
    "        df = pd.concat([df, df_temp])\n",
    "        orphan_count = df_temp[\"n\"].values[0]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "    \n",
    "    logging.info(\"Orphaned file query complete. {0} orphaned files found.\".format(orphan_count))\n",
    "    return df\n",
    "\n",
    "# Function to compare the TDR schema definition with the referenced schema definition\n",
    "def run_schema_comparison_checks(df, tdr_schema_dict, comparison_schema):\n",
    "    logging.info(\"Executing schema comparison checks...\")\n",
    "    result_list = []\n",
    "    # Table existence comparison\n",
    "    tdr_table_set = set()\n",
    "    comp_table_set = set()\n",
    "    for table_entry in tdr_schema_dict[\"tables\"]:\n",
    "        tdr_table_set.add(table_entry[\"name\"])\n",
    "    try:\n",
    "        for table_entry in comparison_schema[\"tables\"]:\n",
    "            comp_table_set.add(table_entry[\"name\"])\n",
    "    except KeyError:\n",
    "        logging.error(\"Comparison schema file 'tables' property is missing or malformed. Will skip remaining schema comparison checks.\")\n",
    "        return\n",
    "    in_tdr_not_comp = tdr_table_set.difference(comp_table_set)\n",
    "    for item in in_tdr_not_comp:\n",
    "        result_list.append([\"Schema Comparison\", item, \"All\", \"In TDR schema but not comparison schema\", 0, 0, 0, 0])\n",
    "    in_comp_not_tdr = comp_table_set.difference(tdr_table_set)\n",
    "    for item in in_comp_not_tdr:\n",
    "        result_list.append([\"Schema Comparison\", item, \"All\", \"In comparison schema but not TDR schema\", 0, 0, 0, 1])\n",
    "    disjunctive_table_set = in_tdr_not_comp.union(in_comp_not_tdr)\n",
    "    logging.info(\"Table comparison results: \\n Count tables present in TDR schema but not comparison schema file: {0} \\n Count tables present in comparison schema file but not TDR schema: {1}\".format(len(in_tdr_not_comp), len(in_comp_not_tdr)))\n",
    "    \n",
    "    # Column existence comparison\n",
    "    tdr_column_set = set()\n",
    "    comp_column_set = set()\n",
    "    for table_entry in tdr_schema_dict[\"tables\"]:\n",
    "        if table_entry[\"name\"] not in disjunctive_table_set:\n",
    "            for column_entry in table_entry[\"columns\"]:\n",
    "                tdr_column_set.add(table_entry[\"name\"] + \" - \" + column_entry[\"name\"])\n",
    "    try:\n",
    "        for table_entry in comparison_schema[\"tables\"]:\n",
    "            if table_entry[\"name\"] not in disjunctive_table_set:\n",
    "                for column_entry in table_entry[\"columns\"]:\n",
    "                    comp_column_set.add(table_entry[\"name\"] + \" - \" + column_entry[\"name\"]) \n",
    "    except KeyError:\n",
    "        logging.error(\"Comparison schema file 'tables' property is missing or malformed. Will skip remaining schema comparison checks.\")\n",
    "        return\n",
    "    in_tdr_not_comp = tdr_column_set.difference(comp_column_set)\n",
    "    for item in in_tdr_not_comp:\n",
    "        result_list.append([\"Schema Comparison\", item.split(\" - \")[0], item.split(\" - \")[1], \"In TDR schema but not comparison schema\", 0, 0, 0, 0])\n",
    "    in_comp_not_tdr = comp_column_set.difference(tdr_column_set)\n",
    "    for item in in_comp_not_tdr:\n",
    "        result_list.append([\"Schema Comparison\", item.split(\" - \")[0], item.split(\" - \")[1], \"In comparison schema but not TDR schema\", 0, 0, 0, 1])  \n",
    "    logging.info(\"Column comparison results for tables present in both schemas: \\n Count columns present in TDR schema but not comparison schema file: {0} \\n Count columns present in comparison schema file but not TDR schema: {1}\".format(len(in_tdr_not_comp), len(in_comp_not_tdr)))\n",
    "    \n",
    "    # Column attribute differences\n",
    "    column_diff_set = set()\n",
    "    try:\n",
    "        for table_entry in comparison_schema[\"tables\"]:\n",
    "            for column_entry in table_entry[\"columns\"]:\n",
    "                for tdr_table_entry in tdr_schema_dict[\"tables\"]:\n",
    "                    if tdr_table_entry[\"name\"] == table_entry[\"name\"]:\n",
    "                        for tdr_column_entry in tdr_table_entry[\"columns\"]:\n",
    "                            if tdr_column_entry[\"name\"] == column_entry[\"name\"]:\n",
    "                                diff_attr_list = []\n",
    "                                # Compare \"datatype\" attribute\n",
    "                                if tdr_column_entry[\"datatype\"] != column_entry[\"datatype\"]:\n",
    "                                    diff_attr_list.append(\"datatype\")\n",
    "                                \n",
    "                                # Compare \"array_of\" attribute\n",
    "                                try:\n",
    "                                    comp_array_of = column_entry[\"array_of\"]\n",
    "                                except KeyError:\n",
    "                                    comp_array_of = False\n",
    "                                try:\n",
    "                                    tdr_array_of = tdr_column_entry[\"array_of\"]\n",
    "                                except KeyError:\n",
    "                                    tdr_array_of = False\n",
    "                                if comp_array_of != tdr_array_of:\n",
    "                                    diff_attr_list.append(\"array_of\")\n",
    "                                \n",
    "                                # Compare \"required\" attribute\n",
    "                                try:\n",
    "                                    comp_required = column_entry[\"required\"]\n",
    "                                except KeyError:\n",
    "                                    comp_required = False\n",
    "                                try:\n",
    "                                    tdr_required = tdr_column_entry[\"required\"]\n",
    "                                except KeyError:\n",
    "                                    tdr_required = False\n",
    "                                if comp_required != tdr_required:\n",
    "                                    diff_attr_list.append(\"required\")\n",
    "                                \n",
    "                                # Add column differences to column_diff_set\n",
    "                                if len(diff_attr_list) > 0:\n",
    "                                    diff_attr_str = ','.join(diff_attr_list)\n",
    "                                    column_diff_set.add(table_entry[\"name\"] + \" - \" + column_entry[\"name\"] + \" - \" + diff_attr_str)\n",
    "    except KeyError:\n",
    "        logging.error(\"Comparison schema file 'tables' property is missing or malformed. Will skip remaining schema comparison checks.\")\n",
    "        return\n",
    "    for item in column_diff_set:\n",
    "        result_list.append([\"Schema Comparison\", item.split(\" - \")[0], item.split(\" - \")[1], \"Difference in attributes of shared column (\" + item.split(\" - \")[2] + \")\", 0, 0, 0, 1])  \n",
    "    logging.info(\"Column attribute comparison results for columns present in both schemas: \\n Count columns with differing attributes between TDR schema and comparison schema file: {0}\".format(len(column_diff_set)))\n",
    "    \n",
    "    # Relationship existence comparison\n",
    "    tdr_relationship_set = set()\n",
    "    comp_relationship_set = set()\n",
    "    for rel_entry in tdr_schema_dict[\"relationships\"]:\n",
    "        tdr_relationship_set.add(rel_entry[\"_from\"][\"table\"] + \" - \" + rel_entry[\"_from\"][\"column\"] + \" - \" + rel_entry[\"to\"][\"table\"] + \" - \" + rel_entry[\"to\"][\"table\"])\n",
    "    try:\n",
    "        for rel_entry in comparison_schema[\"relationships\"]:\n",
    "            comp_relationship_set.add(rel_entry[\"from\"][\"table\"] + \" - \" + rel_entry[\"from\"][\"column\"] + \" - \" + rel_entry[\"to\"][\"table\"] + \" - \" + rel_entry[\"to\"][\"table\"])\n",
    "    except KeyError:\n",
    "        logging.warning(\"Comparison schema file 'relationships' property is missing or malformed. Will continue schema comparison checks as if the schema has no relationships recorded.\")\n",
    "    in_tdr_not_comp = tdr_relationship_set.difference(comp_relationship_set)\n",
    "    for item in in_tdr_not_comp:\n",
    "        result_list.append([\"Schema Comparison\", item.split(\" - \")[0], item.split(\" - \")[1], \"Relationship in TDR schema but not comparison schema (to \" + item.split(\" - \")[2] + \".\" + item.split(\" - \")[3] + \")\", 0, 0, 0, 0])\n",
    "    in_comp_not_tdr = comp_relationship_set.difference(tdr_relationship_set)\n",
    "    for item in in_comp_not_tdr:\n",
    "        result_list.append([\"Schema Comparison\", item.split(\" - \")[0], item.split(\" - \")[1], \"Relationship in comparison schema but not TDR schema (to \" + item.split(\" - \")[2] + \".\" + item.split(\" - \")[3] + \")\", 0, 0, 0, 1])\n",
    "    logging.info(\"Relationship comparison results: \\n Count relationships present in TDR schema but not comparison schema file: {0} \\n Count relationships present in comparison schema file but not TDR schema: {1}\".format(len(in_tdr_not_comp), len(in_comp_not_tdr)))\n",
    "\n",
    "    # Write out and append results to dataframe\n",
    "    df_results = pd.DataFrame(result_list, columns = [\"metric_type\", \"source_table\", \"source_column\", \"metric\", \"n\", \"d\", \"r\", \"flag\"])\n",
    "    df = pd.concat([df, df_results])\n",
    "    return df\n",
    "\n",
    "# Main function\n",
    "def profile_data(uuid, storage_type, output_dir, validation_schema_path):\n",
    "    \n",
    "    # Collect and validate input parameters\n",
    "    logging.info(\"Starting TDR data validation...\")\n",
    "    logging.info(\"Collecting and validating input parameters...\")\n",
    "    if not is_valid_uuid(uuid):\n",
    "        logging.error(\"Invalid uuid parameter passed. Please pass a valid UUID value. Exiting script.\")\n",
    "        return\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "    output_file_path = \"results_{0}_{1}.tsv\".format(uuid, current_datetime_string)\n",
    "    if storage_type not in [\"dataset\", \"snapshot\"]:\n",
    "        storage_type = \"dataset\" \n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    env = api_client.configuration.host\n",
    "    schema_file_path = validation_schema_path\n",
    "    logging.info(\"Input parameters collected: \\n uuid: {0} \\n storage_type: {1} \\n env: {2} \\n schema_file_path: {3} \\n output_file_path: {4}\".format(uuid, storage_type, env, schema_file_path, output_file_path))\n",
    "\n",
    "    # Collect comparison schema\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "        dataset_blob = bucket.blob(schema_file_path)\n",
    "        comparison_schema = json.loads(dataset_blob.download_as_string(client=None))\n",
    "        run_schema_compare = True\n",
    "    except:\n",
    "        comparison_schema = None\n",
    "        run_schema_compare = False\n",
    "        logging.error(\"Error retrieving the comparison schema. Will skip schema comparison checks.\")\n",
    "    \n",
    "    # Retrieve the schema for the data in TDR and parse into something more useful for building queries\n",
    "    logging.info(\"Attempting to identify the TDR object, and collect and parse its schema...\")\n",
    "    tdr_schema_dict, bq_project, bq_schema, skip_bq_queries = retrieve_tdr_schema(uuid, storage_type, api_client)\n",
    "    table_set, array_field_set, field_list, relationship_count = process_tdr_schema(tdr_schema_dict, \"api\")\n",
    "    logging.info(\"TDR object identified and schema parsed: \\n BQ project id: {0} \\n BQ dataset name: {1} \\n table count: {2} \\n field count: {3} \\n array field count: {4} \\n relationships count: {5}\".format(bq_project, bq_schema, len(table_set), len(field_list), len(array_field_set), relationship_count))\n",
    "\n",
    "    # Initialize metric collect from BigQuery and create dataframe to store results \n",
    "    client = bigquery.Client()\n",
    "    df = pd.DataFrame(columns = [\"metric_type\", \"source_table\", \"source_column\", \"metric\", \"n\", \"d\", \"r\", \"flag\"])\n",
    "\n",
    "    # Run validation checks\n",
    "    if run_schema_compare == True:\n",
    "        df = run_schema_comparison_checks(df, tdr_schema_dict, comparison_schema) \n",
    "    if not skip_bq_queries == True:\n",
    "        df, empty_table_list = run_table_profiling_checks(client, df, bq_project, bq_schema, table_set, field_list)\n",
    "        df = run_column_profiling_checks(client, df, bq_project, bq_schema, field_list, array_field_set, empty_table_list)\n",
    "        if storage_type == \"dataset\":\n",
    "            df = run_orphan_file_checks(client, df, bq_project, bq_schema, field_list, array_field_set)\n",
    "\n",
    "    ## Write metrics results dataframe out to CSV\n",
    "    logging.info(f\"Writing out results to {output_file_path}.\")\n",
    "    df_final = df.fillna(0)\n",
    "    df_final.sort_values(by=[\"metric_type\", \"source_table\", \"source_column\", \"metric\"], inplace=True, ignore_index=True)\n",
    "    destination_dir = output_dir\n",
    "    df_final.to_csv(output_file_path, index=False, sep=\"\\t\")\n",
    "\n",
    "    # Copy file to workspace bucket\n",
    "    !gsutil cp $output_file_path $ws_bucket/$destination_dir/ 2> stdout\n",
    "\n",
    "    # Remove file from notebook environment\n",
    "    !rm $output_file_path\n",
    "    \n",
    "    logging.info(\"TDR data validation complete!\")\n",
    "    full_output_file_path = \"{}/{}/{}\".format(ws_bucket, destination_dir, output_file_path)\n",
    "    return full_output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "# dataset_id = \"bad1fb5c-d263-48d7-8e4c-fa873a17d707\"\n",
    "# profile_data(dataset_id, \"dataset\", \"ingest_pipeline/output/transformed/anvil/{}/validation\".format(dataset_id), \"ingest_pipeline/output/transformed/anvil/{}/schema/mapping_schema_object.json\".format(dataset_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
