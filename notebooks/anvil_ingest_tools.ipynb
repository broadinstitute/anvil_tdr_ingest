{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "# 05/16/2023 9:48am - Nate Calvanese - First version created and released\n",
    "# 06/1/2023 11:02am - Nate Calvanese - Updated Bulk Manage User Access to Workspaces to allow the user to remove themselves from workspaces\n",
    "# 06/12/2023 12:18pm - Rachel Kutner - Add File Management section with gsutils script to sync files between folders\n",
    "# 06/21/2023 11:30am - Nate Calvanese - Fixed a bug in the workspace access reports and added more dbGaP groups\n",
    "# 07/13/2023 2:08pm - Nate Calvanese - Added a sub-section for locking workspaces\n",
    "# 07/17/2023 4:35pm - Nate Calvanese - Updated other_workspace_tags column to all_workspace_tags in workspace access report \n",
    "# 12/20/2023 11:44am - Nate Calvanese - Added section 1.7 to help bulk update workspace permissions for release\n",
    "# 3/11/2024 12:52pm - Data Ops - Updated workspace creation to work for Azure workspaces as well\n",
    "# 4/16/2024 9:59am - Nate Calvanese - Added section 2.2 for bulk workspace deletion\n",
    "# 4/17/2024 8:38pm - Nate Calvanese - Added Azure support to section 1.1\n",
    "# 4/19/2024 9:45pm - Nate Calvanese - Added section 1.8 to remove workspace permissions prior to deletion\n",
    "# 4/23/2024 8:23pm - Nate Calvanese - Added section 2.3 to unpublish workspaces prior to deletion (plus some bug fixes)\n",
    "# 5/3/2024 11:18am - Nate Calvanese - Updated workspace access reports in 1.4 to include Azure workspaces\n",
    "# 6/11/2024 1:27pm - Nate Calvanese - Updated workspace creation to include data access controls (effectively \"auth domains\") for Azure workspaces\n",
    "# 6/14/2024 1:26pm - Nate Calvanese - Added section 1.9 to allow addition of \"auth domains\" to existing Azure workspaces\n",
    "# 9/6/2024 10:47am - Nate Calvanese - Updated section 1.5 to include DAC information from dbGaP\n",
    "# 9/9/2024 10:20pm - Nate Calvanese - Added section 1.10 to create snapshot access report to mirror workspace report in 1.4\n",
    "# 10/15/2024 8:56am - Nate Calvanese - Moved content of section 1.10 into 1.4 and expanded data access reports produced in 1.4\n",
    "# 10/17/2024 3:32pm - Nate Calvanese - Significant changes to add support for snapshots (see ANVIL-716 in Jira for details)\n",
    "# 10/29/2024 8:34pm - Nate Calvanese - Updated section 1.6 (Release permissions) to turn on requester pays for GCP workspaces programmatically\n",
    "# 10/29/2024 10:30pm - Nate Calvanese - Addition of a new section 1.3 (bumping all other 1.x numbers down) to bulk create and assign groups\n",
    "# 10/30/2024 9:46am - Nate Calvanese - Updated section 1.4 to include an All DUOS Registrations Report\n",
    "# 1/6/2025 12:43pm - Nate Calvanese - Updated section 1.4 to pull additional information for snapshots\n",
    "# 1/15/2025 9:46am - Nate Calvanese - Updated section 1.4 to pull requester pays information for workspaces\n",
    "# 2/13/2025 9:07am - Nate Calvanese - Updated section 2.1 to return the workspace bucket as part of the output\n",
    "# 2/19/2025 8:39am - Nate Calvanese - Updated section 1.1 to support access for TDR datasets\n",
    "# 2/20/2025 1:15pm - Nate Calvanese - Updated sections 1.1 and 1.2 to support a buffer time between requests\n",
    "# 2/23/2025 9:42pm - Nate Calvanese - Updated DUOS and snapshot reports in section 1.4 to pull additional information, updated section 5.1 to allow consent_name to be updated\n",
    "# 2/26/2025 2:38pm - Nate Calvanese - Tweaked elevated permissions flagging to ignore \"public-workspace-creators@firecloud.org\"\n",
    "# 2/27/2025 11:17am - Nate Calvanese - Updated section 1.4 to explicitly capture anvil-admins role on workspaces and snapshots\n",
    "# 2/28/2025 12:11pm - Nate Calvanese - Updated section 1.1 to not inadvertently remove anvil-admins from its roles; updated section 5.1 to include a dataset_ticket property\n",
    "# 3/3/2025 12:20pm - Nate Calvanese - Fixed bug in section 5.1's output logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Install necessary modules (one time per cloud environment)\n",
    "#!pip install --upgrade xmltodict jira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json \n",
    "import re\n",
    "import xmltodict\n",
    "import datetime\n",
    "from jira import JIRA\n",
    "from time import sleep\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE:__ \n",
    "See the [anvil_ingest_tools Config Builder sheet](https://docs.google.com/spreadsheets/d/1s32qKSuPZHuwpdQZNfcg-k7yFv87GWhyhrZ98AT5ecQ/edit?gid=0#gid=0) for help in structuring configurations for the below tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnVIL Resource Access Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bulk Manage User Access to AnVIL Resources (and associated Auth Domains)\n",
    "List the users, the role they should have, and the resources (type and identifier) those user-role combinations should apply to. The roles specified will have the following effects:\n",
    "\n",
    "For workspaces:\n",
    "* __READER__ will add the user as a reader on the workspace and a member on any associated authorization domains\n",
    "* __WRITER__ will add the user as a writer on the workspace and a member on any associated authorization domains\n",
    "* __OWNER__ will add the user as an owner on the workspace and a member on any associated authorization domains\n",
    "* __NO ACCESS__ will remove the user from both the workspace and any associated authorization domains\n",
    "\n",
    "For snapshots:\n",
    "* __READER__ will add the user as a reader on the snapshot and a member on any associated authorization domains\n",
    "* __STEWARD__ will add the user as a steward on the snapshot and a member on any associated authorization domains\n",
    "* __NO ACCESS__ will remove the user from both the snapshot and any associated authorization domains\n",
    "\n",
    "For datasets:\n",
    "* __READER__ will add the user as a snapshot creator on the dataset and a member on any associated authorization domains\n",
    "* __CUSTODIAN__ will add the user as a custodian on the dataset and a member on any associated authorization domains\n",
    "* __STEWARD__ will add the user as a steward on the dataset and a member on any associated authorization domains\n",
    "* __NO ACCESS__ will remove the user from both the dataset and any associated authorization domains\n",
    "\n",
    "In all cases, when \"__NO ACCESS__\" is utilized for user \"anvil-admins@firecloud.org\", this user will __NOT__ be removed from its OWNER/STEWARD role on the various cloud resources, nor will it be removed from the associated authorization domains, as this user needs to be in place to manage resources for AnVIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def manage_user_access(user_resource_role_list, request_buffer):\n",
    "    \n",
    "    # Set validation values\n",
    "    acceptable_resource_types = [\"workspace\", \"snapshot\", \"dataset\"]\n",
    "    acceptable_cloud_types = [\"gcp\", \"azure\"]\n",
    "    acceptable_workspace_roles = [\"READER\", \"WRITER\", \"OWNER\", \"NO ACCESS\"]\n",
    "    acceptable_snapshot_roles = [\"READER\", \"STEWARD\", \"NO ACCESS\"]\n",
    "    acceptable_dataset_roles = [\"READER\", \"STEWARD\", \"CUSTODIAN\", \"NO ACCESS\"]\n",
    "    \n",
    "    # Loop through and process user access\n",
    "    results = []\n",
    "    total_entry_count = len(user_resource_role_list)\n",
    "    for idx, user_resource_role in enumerate(user_resource_role_list):\n",
    "        \n",
    "        # Set and validate variables\n",
    "        entry_number = idx + 1\n",
    "        resource = user_resource_role[\"resource\"]\n",
    "        resource_type = user_resource_role[\"resource_type\"]\n",
    "        cloud_provider = user_resource_role[\"cloud_provider\"]\n",
    "        user = user_resource_role[\"user\"]\n",
    "        role = user_resource_role[\"role\"]\n",
    "        billing_object = \"\"\n",
    "        error_list = []\n",
    "        print(f\"Processing user access updates for {resource_type} {resource} - {user} ({role}). Entry {entry_number} of {total_entry_count}.\")\n",
    "        if resource_type not in acceptable_resource_types:\n",
    "            resource_types = \", \".join(acceptable_resource_types)\n",
    "            results.append([resource, resource_type, cloud_provider, user, role, \"Failure\", f\"Resource type must be one of {resource_types}. Skipping user access update.\"])\n",
    "            continue\n",
    "        elif resource_type == \"workspace\" and role not in acceptable_workspace_roles:\n",
    "            workspace_roles = \", \".join(acceptable_workspace_roles)\n",
    "            results.append([resource, resource_type, cloud_provider, user, role, \"Failure\", f\"Role for workspace access update must be one of {workspace_roles}. Skipping user access update.\"])\n",
    "            continue\n",
    "        elif resource_type == \"snapshot\" and role not in acceptable_snapshot_roles:\n",
    "            snapshot_roles = \", \".join(acceptable_snapshot_roles)\n",
    "            results.append([resource, resource_type, cloud_provider, user, role, \"Failure\", f\"Role for snapshot access update must be one of {snapshot_roles}. Skipping user access update.\"])\n",
    "            continue\n",
    "        elif resource_type == \"dataset\" and role not in acceptable_dataset_roles:\n",
    "            dataset_roles = \", \".join(acceptable_dataset_roles)\n",
    "            results.append([resource, resource_type, cloud_provider, user, role, \"Failure\", f\"Role for dataset access update must be one of {dataset_roles}. Skipping user access update.\"])\n",
    "            continue\n",
    "        if cloud_provider not in acceptable_cloud_types:\n",
    "            cloud_types = \", \".join(acceptable_cloud_types)\n",
    "            results.append([resource, resource_type, cloud_provider, user, role, \"Failure\", f\"Cloud provider must be one of {cloud_types}. Skipping user access update.\"])\n",
    "            continue\n",
    "        else:\n",
    "            if cloud_provider == \"gcp\":\n",
    "                if resource_type == \"workspace\":\n",
    "                    billing_object = \"anvil-datastorage\"\n",
    "                elif resource_type in [\"snapshot\", \"dataset\"]:\n",
    "                    billing_object = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "            elif cloud_provider == \"azure\":\n",
    "                if resource_type == \"workspace\":\n",
    "                    billing_object = \"AnVILDataStorage_Azure\"\n",
    "                elif resource_type in [\"snapshot\", \"dataset\"]:\n",
    "                    billing_object = \"9ee23bed-b46c-4561-9103-d2a723113f7f\"\n",
    "        \n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "        \n",
    "        # Workspace access logic\n",
    "        if resource_type == \"workspace\":\n",
    "\n",
    "            # Pull auth domains and workspace resource ID from workspace attributes\n",
    "            ad_list = []\n",
    "            resource_id = \"\"\n",
    "            ws_attributes = requests.get(\n",
    "                url=f\"https://api.firecloud.org/api/workspaces/{billing_object}/{resource}?fields=workspace.attributes,workspace.authorizationDomain,workspace.googleProject,workspace.bucketName,workspace.workspaceId\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            ).json()\n",
    "            try:\n",
    "                resource_id = ws_attributes[\"workspace\"][\"workspaceId\"]\n",
    "                for ad in ws_attributes[\"workspace\"][\"authorizationDomain\"]:\n",
    "                    ad_list.append(ad[\"membersGroupName\"])\n",
    "            except:\n",
    "                error_list.append(f\"Error accessing workspace.\")\n",
    "\n",
    "            # For each auth domain, add/remove the user as necessary\n",
    "            if role in [\"READER\", \"WRITER\", \"OWNER\"]:\n",
    "                for auth_domain in ad_list:\n",
    "                    response = requests.put(\n",
    "                        url=f\"https://api.firecloud.org/api/groups/{auth_domain}/member/{user}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 204:\n",
    "                        error_list.append(f\"Error adding to auth domain ({auth_domain})\")\n",
    "            elif role == \"NO ACCESS\":\n",
    "                if user in [\"anvil-admins@firecloud.org\"]:\n",
    "                    print(f\"User {user} in exception list. NOT removing from auth domain.\")\n",
    "                else:\n",
    "                    for auth_domain in ad_list:\n",
    "                        try:\n",
    "                            ad_membership = requests.get(\n",
    "                                url=f\"https://api.firecloud.org/api/groups/{auth_domain}\",\n",
    "                                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                            ).json()\n",
    "                            role_list = []\n",
    "                            if user in ad_membership[\"adminsEmails\"]:\n",
    "                                role_list.append(\"admin\")\n",
    "                            if user in ad_membership[\"membersEmails\"]:\n",
    "                                role_list.append(\"member\")\n",
    "                            for ad_role in role_list:\n",
    "                                response = requests.delete(\n",
    "                                    url=f\"https://api.firecloud.org/api/groups/{auth_domain}/{ad_role}/{user}\",\n",
    "                                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                                )\n",
    "                                if response.status_code != 204:\n",
    "                                    error_list.append(f\"Error removing from auth domain ({auth_domain})\")\n",
    "                        except:\n",
    "                            error_list.append(f\"Error removing from auth domain ({auth_domain})\")\n",
    "                        \n",
    "            # Pull existing workspace ACLs\n",
    "            response = requests.get(\n",
    "                url=f\"https://api.firecloud.org/api/workspaces/{billing_object}/{resource}/acl\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                error_list.append(\"Error retrieving workspace ACL\")\n",
    "                ws_acl = {\"acl\": {}}\n",
    "            else:\n",
    "                ws_acl = response.json()\n",
    "\n",
    "            # Determine if auth domain membership covers the necessary workspace access\n",
    "            ad_covers_ws_acl = False\n",
    "            for auth_domain in ad_list: \n",
    "                for key, val in ws_acl[\"acl\"].items():\n",
    "                    if auth_domain in key:\n",
    "                        if (role == \"READER\" and val[\"accessLevel\"] in [\"READER\", \"WRITER\", \"OWNER\"]) or (role == \"WRITER\" and val[\"accessLevel\"] in [\"WRITER\", \"OWNER\"]) or (role == \"OWNER\" and val[\"accessLevel\"] in [\"OWNER\"]):\n",
    "                            ad_covers_ws_acl = True\n",
    "                            break\n",
    "                if ad_covers_ws_acl == True:\n",
    "                    break\n",
    "\n",
    "            # If auth domain membership doesn't cover needed workspace access, add user role to workspace\n",
    "            if role != \"NO ACCESS\" and ad_covers_ws_acl == False:\n",
    "                if user in [\"anvil-admins@firecloud.org\"] and role not in [\"OWNER\"]:\n",
    "                    print(f\"User {user} in exception list. NOT reducing permissions.\")\n",
    "                else:\n",
    "                    payload = [{\n",
    "                            \"email\": user,\n",
    "                            \"accessLevel\": role,\n",
    "                            \"canShare\": False,\n",
    "                            \"canCompute\": False\n",
    "                        }]\n",
    "                    response = requests.patch(\n",
    "                        url=f\"https://api.firecloud.org/api/workspaces/{billing_object}/{resource}/acl\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                        json=payload\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(\"Error updating workspace ACL\")\n",
    "            \n",
    "            # Removed users from workspace where role is \"NO ACCESS\"\n",
    "            elif role == \"NO ACCESS\":\n",
    "                # Determine if user is even on the workspace\n",
    "                if ws_acl[\"acl\"].get(user):\n",
    "                    # Determine if the user is yourself, and remove yourself if so\n",
    "                    response = requests.get(\n",
    "                        url=f\"https://sam.dsde-prod.broadinstitute.org/register/user/v2/self/info\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    ).json()\n",
    "                    if response[\"userEmail\"] == user:\n",
    "                        response = requests.delete(\n",
    "                            url=f\"https://sam.dsde-prod.broadinstitute.org/api/resources/v2/workspace/{resource_id}/leave\",\n",
    "                            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                        )\n",
    "                        if response.status_code != 204:\n",
    "                            error_list.append(\"Error updating workspace ACL\")\n",
    "                    # If user is not yourself, remove from workspace\n",
    "                    else:\n",
    "                        payload = [{\n",
    "                            \"email\": user,\n",
    "                            \"accessLevel\": role,\n",
    "                            \"canShare\": False,\n",
    "                            \"canCompute\": False\n",
    "                        }]\n",
    "                        response = requests.patch(\n",
    "                            url=f\"https://api.firecloud.org/api/workspaces/{billing_object}/{resource}/acl\",\n",
    "                            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                            json=payload\n",
    "                        )\n",
    "                        if response.status_code != 200:\n",
    "                            error_list.append(\"Error updating workspace ACL\")\n",
    "                    \n",
    "        # Snapshot logic\n",
    "        elif resource_type == \"snapshot\":\n",
    "\n",
    "            # Pull snapshot auth domains and policies\n",
    "            ad_list = []\n",
    "            ss_steward_list = []\n",
    "            ss_reader_list = []\n",
    "            snapshot_policies = requests.get(\n",
    "                url=f\"https://data.terra.bio/api/repository/v1/snapshots/{resource}/policies\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            ).json()\n",
    "            try:\n",
    "                ad_list = snapshot_policies[\"authDomain\"]\n",
    "                for policy in snapshot_policies[\"policies\"]:\n",
    "                    if policy[\"name\"] == \"steward\":\n",
    "                        for member in policy[\"members\"]:\n",
    "                            ss_steward_list.append(member)\n",
    "                    elif policy[\"name\"] == \"reader\":\n",
    "                        for member in policy[\"members\"]:\n",
    "                            ss_reader_list.append(member)\n",
    "            except:\n",
    "                error_list.append(f\"Error accessing snapshot policies.\")\n",
    "\n",
    "            # For each auth domain, add/remove the user as necessary\n",
    "            if role in [\"READER\", \"STEWARD\"]:\n",
    "                for auth_domain in ad_list:\n",
    "                    response = requests.put(\n",
    "                        url=f\"https://api.firecloud.org/api/groups/{auth_domain}/member/{user}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 204:\n",
    "                        error_list.append(f\"Error adding to auth domain ({auth_domain})\")\n",
    "            elif role == \"NO ACCESS\":\n",
    "                if user in [\"anvil-admins@firecloud.org\"]:\n",
    "                    print(f\"User {user} in exception list. NOT removing from auth domain.\")\n",
    "                else:\n",
    "                    for auth_domain in ad_list:\n",
    "                        try:\n",
    "                            ad_membership = requests.get(\n",
    "                                url=f\"https://api.firecloud.org/api/groups/{auth_domain}\",\n",
    "                                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                            ).json()\n",
    "                            role_list = []\n",
    "                            if user in ad_membership[\"adminsEmails\"]:\n",
    "                                role_list.append(\"admin\")\n",
    "                            if user in ad_membership[\"membersEmails\"]:\n",
    "                                role_list.append(\"member\")\n",
    "                            for ad_role in role_list:\n",
    "                                response = requests.delete(\n",
    "                                    url=f\"https://api.firecloud.org/api/groups/{auth_domain}/{ad_role}/{user}\",\n",
    "                                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                                )\n",
    "                                if response.status_code != 204:\n",
    "                                    error_list.append(f\"Error removing from auth domain ({auth_domain})\")\n",
    "                        except:\n",
    "                            error_list.append(f\"Error removing from auth domain ({auth_domain})\")\n",
    "            \n",
    "            # Determine if auth domain membership covers the necessary snapshot access\n",
    "            ad_covers_ss_acl = False\n",
    "            for auth_domain in ad_list:\n",
    "                if role == \"READER\":\n",
    "                    for ss_reader in ss_reader_list:\n",
    "                        if auth_domain in ss_reader:\n",
    "                            ad_covers_ss_acl = True\n",
    "                            break\n",
    "                    for ss_steward in ss_steward_list:\n",
    "                        if auth_domain in ss_steward:\n",
    "                            ad_covers_ss_acl = True\n",
    "                            break\n",
    "                elif role == \"STEWARD\":\n",
    "                    for ss_steward in ss_steward_list:\n",
    "                        if auth_domain in ss_steward:\n",
    "                            ad_covers_ss_acl = True\n",
    "                            break\n",
    "                if ad_covers_ss_acl == True:\n",
    "                    break\n",
    "\n",
    "            # If auth domain membership doesn't cover needed snapshot access, add user role to snapshot\n",
    "            if role != \"NO ACCESS\" and ad_covers_ss_acl == False:\n",
    "                policy = role.lower()\n",
    "                payload = {\n",
    "                        \"email\": user\n",
    "                    }\n",
    "                response = requests.post(\n",
    "                    url=f\"https://data.terra.bio/api/repository/v1/snapshots/{resource}/policies/{policy}/members\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                    json=payload\n",
    "                )\n",
    "                if response.status_code != 200:\n",
    "                    error_list.append(\"Error updating snapshot policies\")\n",
    "            \n",
    "            # Removed users from snapshot where role is \"NO ACCESS\"\n",
    "            elif role == \"NO ACCESS\":\n",
    "                # Determine if user is on the snapshot, and remove if so\n",
    "                if user in ss_reader_list:\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/snapshots/{resource}/policies/reader/members/{user}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(\"Error updating snapshot policies\")\n",
    "                elif user in ss_steward_list and user not in [\"anvil-admins@firecloud.org\"]:\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/snapshots/{resource}/policies/steward/members/{user}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(\"Error updating snapshot policies\")\n",
    "\n",
    "        # Dataset logic\n",
    "        elif resource_type == \"dataset\":\n",
    "            \n",
    "            # Get existing dataset policies\n",
    "            ds_custodian_list = []\n",
    "            ds_steward_list = []\n",
    "            ds_reader_list = []\n",
    "            dataset_policies = requests.get(\n",
    "                url=f\"https://data.terra.bio/api/repository/v1/datasets/{resource}/policies\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            ).json()\n",
    "            try:\n",
    "                for policy in dataset_policies[\"policies\"]:\n",
    "                    if policy[\"name\"] == \"custodian\":\n",
    "                        for member in policy[\"members\"]:\n",
    "                            ds_custodian_list.append(member)\n",
    "                    elif policy[\"name\"] == \"steward\":\n",
    "                        for member in policy[\"members\"]:\n",
    "                            ds_steward_list.append(member)\n",
    "                    elif policy[\"name\"] == \"snapshot_creator\":\n",
    "                        for member in policy[\"members\"]:\n",
    "                            ds_reader_list.append(member)\n",
    "            except:\n",
    "                error_list.append(f\"Error accessing dataset policies.\")\n",
    "            \n",
    "            # Add user role to dataset\n",
    "            if role != \"NO ACCESS\":\n",
    "                policy = role.lower()\n",
    "                policy = \"snapshot_creator\" if policy == \"reader\" else policy\n",
    "                payload = {\n",
    "                        \"email\": user\n",
    "                    }\n",
    "                response = requests.post(\n",
    "                    url=f\"https://data.terra.bio/api/repository/v1/datasets/{resource}/policies/{policy}/members\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                    json=payload\n",
    "                )\n",
    "                if response.status_code != 200:\n",
    "                    error_list.append(\"Error updating dataset policies\")\n",
    "\n",
    "            # Removed users from dataset where role is \"NO ACCESS\"\n",
    "            elif role == \"NO ACCESS\":\n",
    "                # Determine if user is on the dataset, and remove if so\n",
    "                if user in ds_reader_list:\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/datasets/{resource}/policies/snapshot_creator/members/{user}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(\"Error updating dataset policies\")\n",
    "                elif user in ds_custodian_list:\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/datasets/{resource}/policies/custodian/members/{user}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(\"Error updating dataset policies\")\n",
    "                elif user in ds_steward_list and user not in [\"anvil-admins@firecloud.org\"]:\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/datasets/{resource}/policies/steward/members/{user}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(\"Error updating dataset policies\")\n",
    "        \n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([resource, resource_type, cloud_provider, user, role, status, error_str])\n",
    "        \n",
    "        # If request buffer time specified, sleep that amount of time\n",
    "        if request_buffer and request_buffer > 0 and entry_number < total_entry_count:\n",
    "            print(f\"Sleeping for {str(request_buffer)} seconds...\")\n",
    "            sleep(request_buffer)\n",
    "        \n",
    "    # Display results\n",
    "    print(f\"\\nResults: \")\n",
    "    results_df = pd.DataFrame(results, columns = [\"resource\", \"resource_type\", \"cloud_provider\", \"user\", \"role\", \"update_status\", \"errors\"])\n",
    "    display(results_df)\n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the user-resource-role changes that should be made\n",
    "user_resource_role_list = [\n",
    "#     {\"resource\": \"workspace_name, snapshot_id, dataset_id\", \"resource_type\": \"workspace/snapshot/dataset\", \"cloud_provider\": \"gcp/azure\", \"user\": \"ncalvane@broadinstitute.org\", \"role\": \"READER/WRITER/OWNER/CUSTODIAN/STEWARD/NO ACCESS\"},\n",
    "    {'resource': 'cba804c9-0bdd-4219-a53e-98c8db6334a0', 'resource_type': 'dataset', 'cloud_provider': 'gcp', 'user': 'anvil-admins@firecloud.org', 'role': 'NO ACCESS'},\n",
    "    {'resource': '5c6a1c4f-ccd3-48a8-ac00-e18e5ecaa0bb', 'resource_type': 'dataset', 'cloud_provider': 'gcp', 'user': 'anvil-admins@firecloud.org', 'role': 'NO ACCESS'},\n",
    "]\n",
    "\n",
    "# Specify the time buffer that should be added between requests in seconds (if any)\n",
    "request_buffer = 0\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "manage_user_access(user_resource_role_list, request_buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bulk Manage User Access to Terra Groups\n",
    "List the users, target Terra groups, and roles the users should have on those target Terra groups. The roles specified will have the following effects:\n",
    "* __ADMIN__ will add the user as an admin on the group\n",
    "* __MEMBER__ will add the user as a member on the group\n",
    "* __NO ACCESS__ will remove the user from the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def manage_user_group_role(user_group_role_list, request_buffer):\n",
    "    \n",
    "    # Loop through and process user group roles\n",
    "    results = []\n",
    "    total_entry_count = len(user_group_role_list) \n",
    "    for idx, user_group_role in enumerate(user_group_role_list):\n",
    "        \n",
    "        # Initialize\n",
    "        entry_number = idx + 1\n",
    "        user = user_group_role[0]\n",
    "        group = user_group_role[1]\n",
    "        role = user_group_role[2]\n",
    "        print(f\"Processing user group role updates for: {user} - {group} - {role}. Entry {entry_number} of {total_entry_count}.\")\n",
    "        error_list = []\n",
    "        \n",
    "        # Validate specified role\n",
    "        if role not in [\"MEMBER\", \"ADMIN\", \"NO ACCESS\"]:\n",
    "            error_list.append(\"Unknown role specified. Role must be MEMBER, ADMIN or NO ACCESS.\")\n",
    "        else:\n",
    "\n",
    "            # Establish credentials\n",
    "            creds, project = google.auth.default()\n",
    "            auth_req = google.auth.transport.requests.Request()\n",
    "            creds.refresh(auth_req)\n",
    "\n",
    "            # Get existing group membership\n",
    "            response = requests.get(\n",
    "                url=f\"https://api.firecloud.org/api/groups/{group}\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                error_list.append(\"Error retrieving existing group membership.\")\n",
    "                curr_role = \"UNKNOWN\"\n",
    "            else:\n",
    "                response_json = response.json()\n",
    "                curr_role = \"NONE\"\n",
    "                for member in response_json[\"membersEmails\"]:\n",
    "                    if member == user:\n",
    "                        curr_role = \"MEMBER\"\n",
    "                        break\n",
    "                for admin in response_json[\"adminsEmails\"]:\n",
    "                    if admin == user:\n",
    "                        curr_role = \"ADMIN\"\n",
    "                        break \n",
    "\n",
    "            # Process user group role updates \n",
    "            if role == \"ADMIN\":\n",
    "                response = requests.put(\n",
    "                    url=f\"https://api.firecloud.org/api/groups/{group}/{role.lower()}/{user}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                )\n",
    "                if response.status_code != 204:\n",
    "                    error_list.append(\"Error updating user group role.\")\n",
    "            elif role == \"MEMBER\":\n",
    "                # If necessary, remove user's admin role from group\n",
    "                if curr_role == \"ADMIN\":\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://api.firecloud.org/api/groups/{group}/{curr_role.lower()}/{user}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 204:\n",
    "                        error_list.append(\"Error updating user group role.\") \n",
    "                # Add user as a member to group\n",
    "                response = requests.put(\n",
    "                    url=f\"https://api.firecloud.org/api/groups/{group}/{role.lower()}/{user}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                )\n",
    "                if response.status_code != 204:\n",
    "                    error_list.append(\"Error updating user group role.\")\n",
    "            elif role == \"NO ACCESS\":\n",
    "                response = requests.delete(\n",
    "                    url=f\"https://api.firecloud.org/api/groups/{group}/{curr_role.lower()}/{user}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                )\n",
    "                if response.status_code != 204:\n",
    "                    error_list.append(\"Error updating user group role.\")\n",
    "\n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([user, group, role, status, error_str])\n",
    "        \n",
    "        # If request buffer time specified, sleep that amount of time\n",
    "        if request_buffer and request_buffer > 0 and entry_number < total_entry_count:\n",
    "            print(f\"Sleeping for {str(request_buffer)} seconds...\")\n",
    "            sleep(request_buffer)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"user\", \"target_group\", \"target_role\", \"update_status\", \"errors\"])\n",
    "    display(results_df)\n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the user, target group, and the role the user should have on the group:\n",
    "user_group_role_list = [\n",
    "    #[\"user\", \"terra_group_name\", \"role - ADMIN, MEMBER, NO ACCESS\"]\n",
    "    ['tdr-ingest-sa@datarepo-00aaa29b.iam.gserviceaccount.com', 'anvil_tdr_ingest', 'NO ACCESS'],\n",
    "    ['tdr-ingest-sa@datarepo-0138cae6.iam.gserviceaccount.com', 'anvil_tdr_ingest', 'NO ACCESS'],\n",
    "]\n",
    "\n",
    "# Specify the time buffer that should be added between requests in seconds (if any)\n",
    "request_buffer = 0\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "manage_user_group_role(user_group_role_list, request_buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bulk Create and Assign Terra Groups\n",
    "List the new Terra groups to create, and optionally the users that should be admins of that group, the users that should be a members of that group, and the other existing Terra groups the new Terra group should be added to as a member. This functionality is limited to new groups only. To update the membership of existing Terra groups, use the \"Bulk Manage User Access to Terra Groups\" section of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def bulk_create_user_group(user_group_list):\n",
    "    results = []\n",
    "    # Loop through and process user group list\n",
    "    for user_group in user_group_list:\n",
    "        \n",
    "        # Initialize\n",
    "        group = user_group[\"new_group_name\"]\n",
    "        group_admins_list = user_group[\"group_owners\"]\n",
    "        group_members_list = user_group[\"group_members\"]\n",
    "        group_to_add_to_list = user_group[\"groups_to_add_to\"]\n",
    "        print(f\"Processing new user group: {group}\")\n",
    "        error_list = []\n",
    "        \n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Attempt to create the group\n",
    "        group_already_exists = False\n",
    "        response = requests.post(\n",
    "            url=f\"https://api.firecloud.org/api/groups/{group}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code == 409:\n",
    "            error_list.append(\"Group already exists.\")\n",
    "        elif response.status_code != 201:\n",
    "            error_list.append(f\"Error creating group: {response.text}\")\n",
    "            \n",
    "        # If group is created, process group membership\n",
    "        if not error_list:\n",
    "            # Add members\n",
    "            member_err_list = []\n",
    "            for user in group_members_list:\n",
    "                response = requests.put(\n",
    "                    url=f\"https://api.firecloud.org/api/groups/{group}/member/{user}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                )\n",
    "                if response.status_code != 204:\n",
    "                    member_err_list.append(user)\n",
    "            if member_err_list:\n",
    "                member_err_str = \", \".join(member_err_list)\n",
    "                error_list.append(f\"Error adding members to group: {member_err_str}\")\n",
    "            \n",
    "            # Add admins\n",
    "            admin_err_list = []\n",
    "            for user in group_admins_list:\n",
    "                response = requests.put(\n",
    "                    url=f\"https://api.firecloud.org/api/groups/{group}/admin/{user}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                )\n",
    "                if response.status_code != 204:\n",
    "                    admin_err_list.append(user)\n",
    "            if admin_err_list:\n",
    "                admin_err_str = \", \".join(admin_err_list)\n",
    "                error_list.append(f\"Error adding admins to group: {admin_err_str}\")\n",
    "            \n",
    "            # Add group as member of other groups\n",
    "            group_add_err_list = []\n",
    "            group_email = group + \"@firecloud.org\"\n",
    "            for group_to_add_to in group_to_add_to_list:\n",
    "                response = requests.put(\n",
    "                    url=f\"https://api.firecloud.org/api/groups/{group_to_add_to}/member/{group_email}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                )\n",
    "                if response.status_code != 204:\n",
    "                    group_add_err_list.append(group_to_add_to)\n",
    "            if group_add_err_list:\n",
    "                group_add_err_str = \", \".join(group_add_err_list)\n",
    "                error_list.append(f\"Error adding new group to groups: {group_add_err_str}\")\n",
    "            \n",
    "            status = \"Success\" if not error_list else \"Warning\"\n",
    "            error_str = \"; \".join(error_list) \n",
    "        else:\n",
    "            status = \"Failure\"\n",
    "            error_str = \"; \".join(error_list)\n",
    "\n",
    "        # Record status\n",
    "        results.append([group, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"new_group_name\", \"status\", \"errors\"])\n",
    "    display(results_df)\n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the user, target group, and the role the user should have on the group:\n",
    "user_group_list = [\n",
    "#     {\"new_group_name\": \"group_name\", \"group_owners\": [\"<user1>\", \"<user2>\"], \"group_members\": [\"<user1>\", \"<user2>\"], \"groups_to_add_to\": [\"<group1>\", \"<group2>\"]},\n",
    "    {'new_group_name': 'test_group_1', 'group_owners': [], 'group_members': ['ncalvane@broadinstitute.org','kbalacon@broadinstitute.org'], 'groups_to_add_to': []},\n",
    "    {'new_group_name': 'test_group_2', 'group_owners': [], 'group_members': [], 'groups_to_add_to': ['test_group_1']},\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "bulk_create_user_group(user_group_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## AnVIL Resource Access Reports\n",
    "Optionally run five reports related to AnVIL cloud resource access:\n",
    "* The __All Workspaces Report__ pulls all of the AnVIL workspaces specified (or all the workspaces you have access to in the AnVIL billing projects if the workspace_list parameter is left empty), and records any auth domains, any roles the auth domains have on the workspace, the roles the anvil-admins and AnVIL_Devs groups have within the auth domain, the list of dbgap groups that are part of the auth domain, the users in the auth domain, the users and their roles on the workspace, whether or not the workspace is public, and whether or not the workspace is locked. \n",
    "* The __All Snapshots Report__ pulls all of the AnVIL snapshots specified (or all the snapshots you have access to in the AnVIL billing profiles if the snapshot_list parameter is left empty), and records any duos identifiers, any auth domains, any roles the auth domains have on the snapshots, the roles the anvil-admins and AnVIL_Devs groups have within the auth domain, the list of dbgap groups that are part of the auth domain, the users in the auth domain, the users and their roles on the snapshot, whether or not the snapshot is public, and whether or not the snapshot is locked.\n",
    "* The __All dbGaP Telemetry Groups Report__ takes a list of user-specified dbGaP Telemetry Groups and returns whether that group exists as a user in Terra, and if so, which auth domains it is currently in. Note that if ONLY the dbGaP Terra Group Report is run, then the report will be unable to determine which auth domains the existing dbGaP Telemetry Groups are in.\n",
    "* The __All DUOS Registrations Report__ returns information on all AnVIL dataset registrations in the DUOS production environment.\n",
    "* The __All Dataset Tickets Report__ returns information on all ANVIL Jira tickets with an issue type of \"Dataset\", for use in connecting the workspace and snapshot information together at a per-dataset level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     227,
     457,
     508,
     570,
     651
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def gen_workspace_report(workspace_list):\n",
    "    \n",
    "    print(\"Generating All Workspaces Report:\")\n",
    "    # Establish credentials\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "\n",
    "    # Collect list of AnVIL workspaces to process and their auth domains\n",
    "    anvil_ws_list = []\n",
    "    workspaces = requests.get(\n",
    "        url=f\"https://api.firecloud.org/api/workspaces?fields=workspace.authorizationDomain,public,workspace.namespace,workspace.name,workspace.bucketName,workspace.googleProject,workspace.isLocked,workspace.workspaceId\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "    ).json()\n",
    "    for workspace in workspaces:\n",
    "        include_ws = False\n",
    "        if workspace_list:\n",
    "            if workspace[\"workspace\"][\"name\"] in workspace_list:\n",
    "                include_ws = True \n",
    "        else:\n",
    "            if workspace[\"workspace\"][\"namespace\"] in [\"anvil-datastorage\", \"AnVILDataStorage_Azure\"]:\n",
    "                include_ws = True  \n",
    "        if include_ws:\n",
    "            if workspace[\"workspace\"][\"authorizationDomain\"]:\n",
    "                for ad in workspace[\"workspace\"][\"authorizationDomain\"]:\n",
    "                    anvil_ws_list.append([workspace[\"workspace\"][\"name\"], workspace[\"workspace\"][\"namespace\"], workspace[\"workspace\"][\"googleProject\"], workspace[\"workspace\"][\"bucketName\"], ad[\"membersGroupName\"], workspace[\"public\"], workspace[\"workspace\"][\"isLocked\"], workspace[\"workspace\"][\"workspaceId\"]])\n",
    "            else:\n",
    "                anvil_ws_list.append([workspace[\"workspace\"][\"name\"], workspace[\"workspace\"][\"namespace\"], workspace[\"workspace\"][\"googleProject\"], workspace[\"workspace\"][\"bucketName\"], None, workspace[\"public\"], workspace[\"workspace\"][\"isLocked\"], workspace[\"workspace\"][\"workspaceId\"]])\n",
    "\n",
    "    # Loop through AnVIL workspaces and collect additional report information\n",
    "    results = []\n",
    "    for idx, workspace in enumerate(anvil_ws_list):\n",
    "\n",
    "        # Initialize\n",
    "        workspace_id = workspace[7]\n",
    "        name = workspace[0]\n",
    "        namespace = workspace[1]\n",
    "        project = workspace[2]\n",
    "        bucket = workspace[3]\n",
    "        ad = workspace[4]\n",
    "        ad_ws_role = \"NO ACCESS\"\n",
    "        anv_admins_role = \"No Role\"\n",
    "        anv_devs_role = \"No Role\"\n",
    "        anv_admins_ws_role = \"No Role\"\n",
    "        duos_group_list = []\n",
    "        dbgap_group_list = []\n",
    "        other_ad_admin_list = []\n",
    "        other_ad_member_list = []\n",
    "        public = workspace[5]\n",
    "        locked = workspace[6]\n",
    "        ws_owner_list = []\n",
    "        ws_writer_list = []\n",
    "        ws_reader_list = []\n",
    "        error_list = []\n",
    "        ad_str = f\" - {ad}\" if ad else \"\"\n",
    "        print(f\"Processing {name}{ad_str}: {str(idx+1)} of {str(len(anvil_ws_list))}\")\n",
    "\n",
    "        # If AD on workspace, pull AD group membership\n",
    "        if ad:\n",
    "            response = requests.get(\n",
    "                url=f\"https://api.firecloud.org/api/groups/{ad}\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                error_list.append(\"Error retrieving auth domain membership\")\n",
    "                anv_admins_role = \"Unknown\"\n",
    "                anv_devs_role = \"Unknown\"\n",
    "            else:\n",
    "                group_acl = response.json()\n",
    "                # Determine roles/presence of anvil-admins, anvil_devs, and dbgap groups\n",
    "                for admin in group_acl[\"adminsEmails\"]:\n",
    "                    if admin == \"anvil-admins@firecloud.org\":\n",
    "                        anv_admins_role = \"Admin\"\n",
    "                    elif admin == \"AnVIL_Devs@firecloud.org\":\n",
    "                        anv_devs_role = \"Admin\"\n",
    "                    elif \"dbgap_anvil\" in admin.lower() or \"dbgap-authorized\" in admin.lower():\n",
    "                        dbgap_group_list.append(admin)\n",
    "                    elif \"duos-\" in admin.lower():\n",
    "                        duos_group_list.append(admin)\n",
    "                    else:\n",
    "                        other_ad_admin_list.append(admin)\n",
    "                for member in group_acl[\"membersEmails\"]:\n",
    "                    if member == \"anvil-admins@firecloud.org\" and anv_admins_role != \"Admin\":\n",
    "                        anv_admins_role = \"Member\"\n",
    "                    elif member == \"AnVIL_Devs@firecloud.org\" and anv_devs_role != \"Admin\":\n",
    "                        anv_devs_role = \"Member\"\n",
    "                    elif \"dbgap_anvil\" in member.lower() or \"dbgap-authorized\" in member.lower():\n",
    "                        dbgap_group_list.append(member)\n",
    "                    elif \"duos-\" in member.lower():\n",
    "                        duos_group_list.append(member)\n",
    "                    else:\n",
    "                        other_ad_member_list.append(member)\n",
    "        else:\n",
    "            ad_ws_role = \"N/A\"\n",
    "            anv_admins_role = \"N/A\"\n",
    "            anv_devs_role = \"N/A\"\n",
    "        duos_group_str = \", \".join(duos_group_list) \n",
    "        dbgap_group_str = \", \".join(dbgap_group_list)\n",
    "        ad_elevated_permissions = False\n",
    "        other_ad_str = \"\"\n",
    "        if other_ad_admin_list:\n",
    "            other_ad_str += \"ADMIN - \" + \", \".join(other_ad_admin_list) + \"; \"\n",
    "            ad_elevated_permissions = True\n",
    "        if other_ad_member_list:\n",
    "            other_ad_str += \"MEMBER - \" + \", \".join(other_ad_member_list) + \"; \"\n",
    "\n",
    "        # Pull workspace ACLs\n",
    "        elevated_permissions = False\n",
    "        response = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{namespace}/{name}/acl\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(\"Error retrieving workspace ACL\")\n",
    "            ad_ws_role = \"Unknown\"\n",
    "            anv_admins_ws_role = \"Unknown\"\n",
    "            ws_member_str = \"\"\n",
    "        else:\n",
    "            ws_acl = response.json()\n",
    "            for key, val in ws_acl[\"acl\"].items():\n",
    "                if ad and ad in key:\n",
    "                    ad_ws_role = val[\"accessLevel\"]\n",
    "                else:\n",
    "                    if val[\"accessLevel\"] == \"OWNER\":\n",
    "                        ws_owner_list.append(key)\n",
    "                    elif val[\"accessLevel\"] == \"WRITER\":\n",
    "                        ws_writer_list.append(key)\n",
    "                    elif val[\"accessLevel\"] == \"READER\":\n",
    "                        ws_reader_list.append(key)\n",
    "            ws_member_str = \"\"\n",
    "            if ws_owner_list:\n",
    "                ws_member_str += \"OWNER - \" + \", \".join(ws_owner_list) + \"; \"\n",
    "                for ws_owner in ws_owner_list:\n",
    "                    if ws_owner not in [\"anvil-admins@firecloud.org\", \"public-workspace-creators@firecloud.org\"]:\n",
    "                        elevated_permissions = True\n",
    "                        break\n",
    "            if ws_writer_list:\n",
    "                ws_member_str += \"WRITER - \" + \", \".join(ws_writer_list) + \"; \"\n",
    "                elevated_permissions = True\n",
    "            if ws_reader_list:\n",
    "                ws_member_str += \"READER - \" + \", \".join(ws_reader_list) + \"; \"\n",
    "            if \"anvil-admins@firecloud.org\" in ws_owner_list:\n",
    "                anv_admins_ws_role = \"OWNER\"\n",
    "            elif \"anvil-admins@firecloud.org\" in ws_writer_list:\n",
    "                anv_admins_ws_role = \"WRITER\"\n",
    "            elif \"anvil-admins@firecloud.org\" in ws_reader_list:\n",
    "                anv_admins_ws_role = \"READER\"            \n",
    "\n",
    "        # Pull requester pays status\n",
    "        requester_pays = False\n",
    "        response = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{namespace}/{name}?fields=bucketOptions\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(\"Error retrieving requester pays status\")\n",
    "        else:\n",
    "            ws_details = response.json()\n",
    "            if ws_details.get(\"bucketOptions\"):\n",
    "                requester_pays = ws_details[\"bucketOptions\"][\"requesterPays\"]\n",
    "        \n",
    "        # Pull workspace tags\n",
    "        released = False\n",
    "        consortium = \"\"\n",
    "        phs_id = \"\"\n",
    "        consent = \"\"\n",
    "        other_tags = \"\"\n",
    "        response = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{namespace}/{name}/tags\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(\"Error retrieving workspace tags.\")\n",
    "            released = None\n",
    "        else:\n",
    "            try:\n",
    "                for item in response.json():\n",
    "                    if item == \"AnVIL Status: Released\":\n",
    "                        released = True\n",
    "                    elif \"consortium:\" in item.lower():\n",
    "                        consortium = item.split(\":\")[1].strip()\n",
    "                    elif \"dbgap:\" in item.lower():\n",
    "                        phs_id = item.split(\":\")[1].strip()\n",
    "                    elif \"consent_code:\" in item.lower():\n",
    "                        consent = item.split(\":\")[1].strip()\n",
    "                    other_tags += f\"'{item}', \"\n",
    "            except:\n",
    "                error_list.append(\"Error parsing workspace tags.\")\n",
    "        \n",
    "        # If Azure workspace, pull additional details\n",
    "        if namespace == \"AnVILDataStorage_Azure\":\n",
    "            platform = \"azure\"\n",
    "            project = \"https://lzb34bb58bfb122730765416.blob.core.windows.net/\"\n",
    "            ws_resources = requests.get(\n",
    "                url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{workspace_id}/resources?offset=0&limit=10&resource=AZURE_STORAGE_CONTAINER\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            ).json()\n",
    "            for resource_entry in ws_resources[\"resources\"]:\n",
    "                if resource_entry[\"resourceAttributes\"][\"azureStorageContainer\"][\"storageContainerName\"][0:3] == \"sc-\":\n",
    "                    bucket = resource_entry[\"resourceAttributes\"][\"azureStorageContainer\"][\"storageContainerName\"]\n",
    "                    break\n",
    "        else:\n",
    "            platform = \"gcp\"\n",
    "        \n",
    "        # Record results\n",
    "        status = \"Success\" if not error_list else \"Errors\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([name, workspace_id, namespace, platform, project, bucket, ad, ad_ws_role, anv_admins_role, anv_devs_role, duos_group_str, dbgap_group_str, other_ad_str, ad_elevated_permissions, public, locked, ws_member_str, anv_admins_ws_role, elevated_permissions, phs_id, consent, consortium, other_tags, requester_pays, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nAll Workspaces Report Results: \")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace\", \"workspace_id\", \"billing_project\", \"cloud_platform\", \"cloud_project\", \"bucket\", \"auth_domain\", \"ad_workspace_role\", \"anvil_adms_ad_role\", \"anvil_devs_ad_role\", \"duos_groups_in_ad\", \"dbgap_groups_in_ad\", \"other_ad_members\", \"ad_elevated_permissions_flag\", \"workspace_public\", \"workspaced_locked\", \"workspace_members\", \"anvil_adms_ws_role\", \"elevated_permissions_flag\", \"phs_id\", \"consent_name\", \"consortium\", \"all_workspace_tags\", \"requester_pays\", \"status\", \"errors\"])\n",
    "    results_df_sorted = results_df.sort_values(by=[\"workspace\"], ignore_index=True)\n",
    "    display(results_df_sorted)\n",
    "    \n",
    "    # Output \n",
    "    output = []\n",
    "    for entry in results:\n",
    "        for grp in entry[11].split(\",\"):\n",
    "            if grp:\n",
    "                output.append([grp, entry[6]])\n",
    "    return output\n",
    "\n",
    "def gen_snapshot_report(snapshot_id_list):\n",
    "    \n",
    "    print(\"Generating All Snapshots Report:\")\n",
    "    # Establish credentials\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "\n",
    "    # Collect list of AnVIL snapshots to process and their auth domains\n",
    "    billing_profile_list = [\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", \"9ee23bed-b46c-4561-9103-d2a723113f7f\"]\n",
    "    snapshot_list = []\n",
    "    snapshots = requests.get(\n",
    "        url=f\"https://data.terra.bio/api/repository/v1/snapshots?offset=0&limit=5000\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "    ).json()\n",
    "    for snapshot in snapshots[\"items\"]:\n",
    "        include_ss = False\n",
    "        if snapshot_id_list:\n",
    "            if snapshot[\"id\"] in snapshot_id_list:\n",
    "                include_ss = True\n",
    "        else:\n",
    "            if snapshot[\"profileId\"] in billing_profile_list:\n",
    "                include_ss = True\n",
    "        if include_ss:\n",
    "            snapshot_list.append([snapshot[\"id\"], snapshot[\"name\"], snapshot[\"profileId\"], snapshot[\"cloudPlatform\"], snapshot[\"resourceLocks\"][\"exclusive\"], snapshot[\"duosId\"], snapshot[\"phsId\"], snapshot[\"consentCode\"], snapshot[\"dataProject\"]])\n",
    "\n",
    "    # Loop through AnVIL snapshots and collect additional report information\n",
    "    results = []\n",
    "    for idx, snapshot in enumerate(snapshot_list):\n",
    "\n",
    "        # Initialize\n",
    "        snapshot_id = snapshot[0]\n",
    "        name = snapshot[1]\n",
    "        billing_profile = snapshot[2]\n",
    "        cloud_platform = snapshot[3]\n",
    "        cloud_project = snapshot[8]\n",
    "        duos_id = snapshot[5]\n",
    "        phs_id = snapshot[6]\n",
    "        consent = snapshot[7]\n",
    "        snapshot_locked = True if snapshot[4] else False\n",
    "        snapshot_public = False\n",
    "        ad_list = []\n",
    "        ad = None\n",
    "        ad_ss_role = \"NO ACCESS\"\n",
    "        anv_admins_role = \"No Role\"\n",
    "        anv_devs_role = \"No Role\"\n",
    "        anv_admins_ds_role = \"No Role\"\n",
    "        anv_admins_ss_role = \"No Role\"\n",
    "        dbgap_group_list = []\n",
    "        duos_group_list = []\n",
    "        other_ad_admin_list = []\n",
    "        other_ad_member_list = []\n",
    "        ss_full_steward_list = []\n",
    "        ss_steward_list = []\n",
    "        ss_full_reader_list = []\n",
    "        ss_reader_list = []\n",
    "        snapshot_dataset_id = \"\"\n",
    "        snapshot_dataset_project = \"\"\n",
    "        snapshot_dataset_sa = \"\"\n",
    "        ds_custodian_list = []\n",
    "        ds_steward_list = []\n",
    "        ds_snapshotter_list = []\n",
    "        error_list = []\n",
    "        print(f\"Processing {name}: {str(idx+1)} of {str(len(snapshot_list))}\")\n",
    "\n",
    "        # Pull snapshot dataset details and policies\n",
    "        snapshot_details = requests.get(\n",
    "            url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        snapshot_dataset_id = snapshot_details[\"source\"][0][\"dataset\"][\"id\"]\n",
    "        snapshot_dataset_project = snapshot_details[\"source\"][0][\"dataset\"][\"dataProject\"] \n",
    "        if snapshot_dataset_project and cloud_platform == \"gcp\":\n",
    "            snapshot_dataset_sa = f\"tdr-ingest-sa@{snapshot_dataset_project}.iam.gserviceaccount.com\" \n",
    "        dataset_policies = requests.get(\n",
    "            url=f\"https://data.terra.bio/api/repository/v1/datasets/{snapshot_dataset_id}/policies\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json() \n",
    "        \n",
    "        for policy in dataset_policies[\"policies\"]:\n",
    "            if policy[\"name\"] == \"custodian\":\n",
    "                ds_custodian_list = policy[\"members\"]\n",
    "            elif policy[\"name\"] == \"steward\":\n",
    "                ds_steward_list = policy[\"members\"]\n",
    "            elif policy[\"name\"] == \"snapshot_creator\":\n",
    "                ds_snapshotter_list = policy[\"members\"]\n",
    "        ds_member_str = \"\"\n",
    "        if ds_steward_list:\n",
    "            ds_member_str += \"STEWARD - \" + \", \".join(ds_steward_list) + \"; \"\n",
    "        if ds_custodian_list:\n",
    "            ds_member_str += \"CUSTODIAN - \" + \", \".join(ds_custodian_list) + \"; \"\n",
    "        if ds_snapshotter_list:\n",
    "            ds_member_str += \"SNAPSHOT_CREATOR - \" + \", \".join(ds_snapshotter_list) + \"; \"\n",
    "        if \"anvil-admins@firecloud.org\" in ds_steward_list:\n",
    "            anv_admins_ds_role = \"STEWARD\"\n",
    "        elif \"anvil-admins@firecloud.org\" in ds_custodian_list:\n",
    "            anv_admins_ds_role = \"CUSTODIAN\"\n",
    "        elif \"anvil-admins@firecloud.org\" in ds_snapshotter_list:\n",
    "            anv_admins_ds_role = \"SNAPSHOT_CREATOR\" \n",
    "        \n",
    "        # Pull snapshot auth domains (and other policies)\n",
    "        elevated_permissions = False\n",
    "        snapshot_policies = requests.get(\n",
    "            url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/policies\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        ad_list = snapshot_policies[\"authDomain\"]\n",
    "        ad_full_email_list = []\n",
    "        for ad in ad_list:\n",
    "            ad_full_email_list.append(ad + \"@firecloud.org\")\n",
    "        for policy in snapshot_policies[\"policies\"]:\n",
    "            if policy[\"name\"] == \"steward\":\n",
    "                for member in policy[\"members\"]:\n",
    "                    ss_full_steward_list.append(member)\n",
    "                    if \"DUOS\" not in member and \"policy-\" not in member and member not in ad_full_email_list:\n",
    "                        ss_steward_list.append(member)\n",
    "            elif policy[\"name\"] == \"reader\":\n",
    "                for member in policy[\"members\"]:\n",
    "                    ss_full_reader_list.append(member)\n",
    "                    if \"DUOS\" not in member and \"policy-\" not in member and member not in ad_full_email_list:\n",
    "                        ss_reader_list.append(member)\n",
    "        ss_member_str = \"\"\n",
    "        if ss_steward_list:\n",
    "            ss_member_str += \"STEWARD - \" + \", \".join(ss_steward_list) + \"; \"\n",
    "            for ss_steward in ss_steward_list:\n",
    "                if ss_steward not in [\"anvil-admins@firecloud.org\"]:\n",
    "                    elevated_permissions = True\n",
    "                    break\n",
    "        if ss_reader_list:\n",
    "            ss_member_str += \"READER - \" + \", \".join(ss_reader_list) + \"; \"\n",
    "        if \"anvil-admins@firecloud.org\" in ss_steward_list:\n",
    "            anv_admins_ss_role = \"STEWARD\"\n",
    "        elif \"anvil-admins@firecloud.org\" in ss_reader_list:\n",
    "            anv_admins_ss_role = \"READER\"\n",
    "\n",
    "        # Pull snapshot public status\n",
    "        public_response = requests.get(\n",
    "            url=f\"https://sam.dsde-prod.broadinstitute.org/api/resources/v2/datasnapshot/{snapshot_id}/policies/reader/public\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "        )\n",
    "        if public_response.text == \"true\":\n",
    "            snapshot_public = True\n",
    "\n",
    "        # Process each AD on snapshot\n",
    "        if ad_list:\n",
    "            for ad in ad_list:\n",
    "                # Initialize AD specific variables\n",
    "                ad_ss_role = \"NO ACCESS\"\n",
    "                anv_admins_role = \"No Role\"\n",
    "                anv_devs_role = \"No Role\"\n",
    "                dbgap_group_list = []\n",
    "                duos_group_list = []\n",
    "                other_ad_admin_list = []\n",
    "                other_ad_member_list = []\n",
    "                ad_error_list = error_list.copy()\n",
    "                ad_email = ad + \"@firecloud.org\" \n",
    "                if ad_email in ss_full_steward_list:\n",
    "                    ad_ss_role = \"STEWARD\"\n",
    "                elif ad_email in ss_full_reader_list:\n",
    "                    ad_ss_role = \"READER\"\n",
    "                # Pull AD group members    \n",
    "                response = requests.get(\n",
    "                    url=f\"https://api.firecloud.org/api/groups/{ad}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                )\n",
    "                if response.status_code != 200:\n",
    "                    ad_error_list.append(\"Error retrieving auth domain membership\")\n",
    "                    anv_admins_role = \"Unknown\"\n",
    "                    anv_devs_role = \"Unknown\"\n",
    "                else:\n",
    "                    group_acl = response.json()\n",
    "                    # Determine roles/presence of anvil-admins, anvil_devs, and dbgap groups\n",
    "                    for admin in group_acl[\"adminsEmails\"]:\n",
    "                        if admin == \"anvil-admins@firecloud.org\":\n",
    "                            anv_admins_role = \"Admin\"\n",
    "                        elif admin == \"AnVIL_Devs@firecloud.org\":\n",
    "                            anv_devs_role = \"Admin\"\n",
    "                        elif \"dbgap_anvil\" in admin.lower() or \"dbgap-authorized\" in admin.lower():\n",
    "                            dbgap_group_list.append(admin)\n",
    "                        elif \"duos-\" in admin.lower():\n",
    "                            duos_group_list.append(admin)\n",
    "                        else:\n",
    "                            other_ad_admin_list.append(admin)\n",
    "                    for member in group_acl[\"membersEmails\"]:\n",
    "                        if member == \"anvil-admins@firecloud.org\" and anv_admins_role != \"Admin\":\n",
    "                            anv_admins_role = \"Member\"\n",
    "                        elif member == \"AnVIL_Devs@firecloud.org\" and anv_devs_role != \"Admin\":\n",
    "                            anv_devs_role = \"Member\"\n",
    "                        elif \"dbgap_anvil\" in member.lower() or \"dbgap-authorized\" in member.lower():\n",
    "                            dbgap_group_list.append(member)\n",
    "                        elif \"duos-\" in member.lower():\n",
    "                            duos_group_list.append(member)\n",
    "                        else:\n",
    "                            other_ad_member_list.append(member)\n",
    "                dbgap_group_str = \", \".join(dbgap_group_list)\n",
    "                duos_group_str = \", \".join(duos_group_list) \n",
    "                ad_elevated_permissions = False\n",
    "                other_ad_str = \"\"\n",
    "                if other_ad_admin_list:\n",
    "                    other_ad_str += \"ADMIN - \" + \", \".join(other_ad_admin_list) + \"; \"\n",
    "                    ad_elevated_permissions = True\n",
    "                if other_ad_member_list:\n",
    "                    other_ad_str += \"MEMBER - \" + \", \".join(other_ad_member_list) + \"; \"\n",
    "                # Record AD Results\n",
    "                status = \"Success\" if not ad_error_list else \"Errors\"\n",
    "                error_str = \"; \".join(ad_error_list)\n",
    "                results.append([name, snapshot_id, billing_profile, cloud_platform, cloud_project, duos_id, ad, ad_ss_role, anv_admins_role, anv_devs_role, duos_group_str, dbgap_group_str, other_ad_str, ad_elevated_permissions, snapshot_public, snapshot_locked, ss_member_str, anv_admins_ss_role, elevated_permissions, phs_id, consent, snapshot_dataset_id, snapshot_dataset_sa, ds_member_str, anv_admins_ds_role, status, error_str])\n",
    "        else:\n",
    "            # Record non-AD results\n",
    "            ad_ss_role = \"N/A\"\n",
    "            anv_admins_role = \"N/A\"\n",
    "            anv_devs_role = \"N/A\"\n",
    "            status = \"Success\" if not error_list else \"Errors\"\n",
    "            error_str = \"; \".join(error_list)\n",
    "            results.append([name, snapshot_id, billing_profile, cloud_platform, cloud_project, duos_id, ad, ad_ss_role, anv_admins_role, anv_devs_role, \"\", \"\", \"\", False, snapshot_public, snapshot_locked, ss_member_str, anv_admins_ss_role, elevated_permissions, phs_id, consent, snapshot_dataset_id, snapshot_dataset_sa, ds_member_str, anv_admins_ds_role, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nAll Snapshots Report Results: \")\n",
    "    results_df = pd.DataFrame(results, columns = [\"snapshot\", \"snapshot_id\", \"billing_profile\", \"cloud_platform\", \"cloud_project\", \"duos_id\", \"auth_domain\", \"ad_snapshot_role\", \"anvil_adms_ad_role\", \"anvil_devs_ad_role\", \"duos_groups_in_ad\", \"dbgap_groups_in_ad\", \"other_ad_members\", \"ad_elevated_permissions_flag\", \"snapshot_public\", \"snapshot_locked\", \"snapshot_members\", \"anvil_adms_ss_role\", \"elevated_permissions_flag\", \"phs_id\", \"consent_name\", \"dataset_id\", \"dataset_service_account\", \"dataset_members\", \"anvil_adms_ds_role\", \"status\", \"errors\"])\n",
    "    results_df_sorted = results_df.sort_values(by=[\"snapshot\"], ignore_index=True)\n",
    "    display(results_df_sorted)\n",
    "    \n",
    "    # Output \n",
    "    output = []\n",
    "    for entry in results:\n",
    "        for grp in entry[11].split(\",\"):\n",
    "            if grp:\n",
    "                output.append([grp, entry[6]])\n",
    "    return output\n",
    "\n",
    "def gen_dbgap_report(dbgap_telemetry_group_list, tele_ad_map):\n",
    "    \n",
    "    print(\"Generating All dbGaP Telemetry Groups Report:\")\n",
    "    # Establish credentials\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    \n",
    "    # Augment user-supplied dbgap telemetry groups with those present in the tele_ad_map\n",
    "    for entry in tele_ad_map:\n",
    "        grp_name = entry[0].replace(\"@firecloud.org\", \"\")\n",
    "        grp_found = False\n",
    "        for existing_grp in dbgap_telemetry_group_list:\n",
    "            if grp_name.lower() == existing_grp.lower():\n",
    "                grp_found = True\n",
    "                break\n",
    "        if not grp_found:\n",
    "            dbgap_telemetry_group_list.append(grp_name)\n",
    "\n",
    "    # Loop through dbgap telemetry groups\n",
    "    dbgap_results = []\n",
    "    for grp in dbgap_telemetry_group_list:\n",
    "\n",
    "        group_exists = False\n",
    "        group_in_ads_list = []\n",
    "\n",
    "        # Confirm group exists\n",
    "        response = requests.get(\n",
    "            url=f\"https://sam.dsde-prod.broadinstitute.org/api/groups/v1/{grp}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            group_exists = True\n",
    "\n",
    "        # Loop through workspace access results to determine which ADs the group is in\n",
    "        ad_set = set()\n",
    "        if group_exists:\n",
    "            if tele_ad_map:\n",
    "                for map_entry in tele_ad_map:\n",
    "                    if grp.lower() in map_entry[0].lower():\n",
    "                        ad_set.add(map_entry[1])\n",
    "            else:\n",
    "                group_in_ads_list.append(\"Unknown - WS and SS Reports Not Run\")\n",
    "\n",
    "        # Record results\n",
    "        dbgap_results.append([grp, group_exists, list(ad_set)])\n",
    "\n",
    "    print(\"\\nAll dbGaP Telemetry Groups Report Results: \")\n",
    "    dbgap_results_df = pd.DataFrame(dbgap_results, columns = [\"group\", \"group_exists\", \"group_in_auth_domains\"])\n",
    "    display(dbgap_results_df)   \n",
    "    \n",
    "def gen_duos_report(duos_token):\n",
    "    \n",
    "    print(\"Generating All DUOS Registrations Report:\")\n",
    "    # Pull a list of existing AnVIL studies and datasets from DUOS\n",
    "    url = \"https://consent.dsde-prod.broadinstitute.org\"\n",
    "    results = []\n",
    "    datasets = requests.get(\n",
    "        url=f\"{url}/api/dataset/v3\",\n",
    "        headers={\"Authorization\": f\"Bearer {duos_token}\"}\n",
    "    ).json()\n",
    "    datasets_to_process = len(datasets)\n",
    "    datasets_processed = 0\n",
    "    for dataset_entry in datasets:\n",
    "        datasets_processed += 1\n",
    "        print(f\"Processing dataset {datasets_processed} of {datasets_to_process}...\")\n",
    "        dataset_id = dataset_entry[\"dataset_id\"]\n",
    "        dataset_details = requests.get(\n",
    "            url=f\"{url}/api/dataset/v2/{dataset_id}\",\n",
    "            headers={\"Authorization\": f\"Bearer {duos_token}\"}\n",
    "        ).json() \n",
    "        if dataset_details.get(\"study\"):\n",
    "            study_id = dataset_details[\"study\"][\"studyId\"]\n",
    "            if dataset_details[\"study\"].get(\"description\") and \"Platform: AnVIL\" in dataset_details[\"study\"][\"description\"]: \n",
    "                study_name = dataset_details[\"study\"][\"name\"]\n",
    "                study_phs = \"\"\n",
    "                for prop_entry in dataset_details[\"study\"][\"properties\"]:\n",
    "                    if prop_entry[\"key\"] == \"dbGaPPhsID\":\n",
    "                        study_phs = prop_entry[\"value\"]\n",
    "                        break\n",
    "                dataset_name = dataset_details[\"name\"]\n",
    "                dataset_identifier = dataset_details[\"datasetIdentifier\"]\n",
    "                dac_id = dataset_details.get(\"dacId\") if dataset_details.get(\"dacId\") else \"\"\n",
    "                data_use = dataset_details.get(\"dataUse\")\n",
    "                du_gru = data_use.get(\"generalUse\") if data_use.get(\"generalUse\") else False\n",
    "                du_hmb = data_use.get(\"hmbResearch\") if data_use.get(\"hmbResearch\") else False\n",
    "                du_disease = data_use.get(\"diseaseRestrictions\") if data_use.get(\"diseaseRestrictions\") else []\n",
    "                du_poa = data_use.get(\"populationOriginsAncestry\") if data_use.get(\"populationOriginsAncestry\") else False\n",
    "                du_ethics = data_use.get(\"ethicsApprovalRequired\") if data_use.get(\"ethicsApprovalRequired\") else False\n",
    "                du_collab = data_use.get(\"collaboratorRequired\") if data_use.get(\"collaboratorRequired\") else False\n",
    "                du_geog = data_use.get(\"geographicalRestrictions\") if data_use.get(\"geographicalRestrictions\") else \"\"\n",
    "                du_genetic = data_use.get(\"geneticStudiesOnly\") if data_use.get(\"geneticStudiesOnly\") else False\n",
    "                du_pub = data_use.get(\"publicationResults\") if data_use.get(\"publicationResults\") else False\n",
    "                du_nmds = data_use.get(\"methodsResearch\") if data_use.get(\"methodsResearch\") else False\n",
    "                du_npu = data_use.get(\"nonProfitUse\") if data_use.get(\"nonProfitUse\") else False\n",
    "                du_other = data_use.get(\"other\") if data_use.get(\"other\") else \"\"\n",
    "                access_management = \"\"\n",
    "                snapshot_id = \"\"\n",
    "                for prop_entry in dataset_details[\"properties\"]:\n",
    "                    if prop_entry[\"propertyName\"] == \"URL\":\n",
    "                        snapshot_url = prop_entry[\"propertyValue\"]\n",
    "                        if snapshot_url and \"https://data.terra.bio/snapshots/\" in snapshot_url:\n",
    "                                snapshot_id = snapshot_url.replace(\"https://data.terra.bio/snapshots/\", \"\")   \n",
    "                    elif prop_entry[\"propertyName\"] == \"Access Management\":\n",
    "                        access_management = prop_entry[\"propertyValue\"]\n",
    "                results.append([study_id, study_name, study_phs, dataset_id, dataset_identifier, dataset_name, dac_id, access_management, du_gru, du_hmb, du_disease, du_poa, du_ethics, du_collab, du_geog, du_genetic, du_pub, du_nmds, du_npu, du_other, snapshot_id])\n",
    "\n",
    "    # Display results\n",
    "    df_results = pd.DataFrame(results, columns = [\"Study ID\", \"Study Name\", \"Study PHS\", \"Dataset ID\", \"Dataset Identifier\", \"Dataset Name\", \"DAC ID\", \"Access\", \"GRU\", \"HMB\", \"DS\", \"POA\", \"IRB\", \"COL\", \"GS\", \"GSO\", \"PUB\", \"NMDS\", \"NPU\", \"OTHER\", \"Snapshot ID\"])\n",
    "    df_results_sorted = df_results.sort_values(by=[\"Study ID\", \"Dataset ID\"], ascending=[True, True], ignore_index=True)\n",
    "    print(\"\\nDUOS AnVIL Datasets: \")\n",
    "    display(df_results)\n",
    "    \n",
    "def gen_jira_datasets_report(user, api_key):\n",
    "    \n",
    "    print(\"Generating All Dataset Tickets Report:\")\n",
    "    # Establish Jira instance and collect results\n",
    "    jira = JIRA(\"https://broadworkbench.atlassian.net\", basic_auth=(user, api_key))\n",
    "    active_anvil_datasets = []\n",
    "    inactive_anvil_datasets = []\n",
    "    start_at_param = 0\n",
    "    while True:\n",
    "        search_results = jira.search_issues(\"project = ANVIL and issuetype = Dataset order by summary\", maxResults=100, startAt=start_at_param)\n",
    "        if len(search_results) == 0:\n",
    "            break\n",
    "        else:\n",
    "            for issue in search_results:\n",
    "                key = issue.key\n",
    "                summary = issue.fields.summary\n",
    "                status = issue.fields.status\n",
    "                access_management = issue.fields.customfield_10578.value if issue.fields.customfield_10578 else None\n",
    "                duos_identifier = issue.fields.customfield_10585 \n",
    "                released_in_duos = issue.fields.customfield_10597.value if issue.fields.customfield_10597 else None\n",
    "                released_in_tdr = issue.fields.customfield_10593.value if issue.fields.customfield_10593 else None\n",
    "                released_in_explorer = issue.fields.customfield_10592.value if issue.fields.customfield_10592 else None\n",
    "                data_access_control_group = issue.fields.customfield_10584\n",
    "                dbgap_accession = issue.fields.customfield_10579\n",
    "                dbgap_consent_group = issue.fields.customfield_10615\n",
    "                dbgap_consent_code = issue.fields.customfield_10576\n",
    "                dbgap_dac = issue.fields.customfield_10601 \n",
    "                dbgap_terra_group = issue.fields.customfield_10598\n",
    "                staging_workspace = issue.fields.customfield_10582\n",
    "                staging_buckets = issue.fields.customfield_10617\n",
    "                tdr_gcp_dataset = issue.fields.customfield_10586\n",
    "                tdr_gcp_snapshot = issue.fields.customfield_10587\n",
    "                tdr_az_dataset = issue.fields.customfield_10588\n",
    "                tdr_az_snapshot = issue.fields.customfield_10589\n",
    "                size = issue.fields.customfield_10599\n",
    "                map_spec = issue.fields.customfield_10614\n",
    "                data_on_clouds = issue.fields.customfield_10602\n",
    "                data_on_clouds_list = []\n",
    "                if data_on_clouds:\n",
    "                    for doc_value in data_on_clouds:\n",
    "                         data_on_clouds_list.append(str(doc_value.value))\n",
    "                data_on_clouds_str = \", \".join(data_on_clouds_list)\n",
    "                if str(status) != \"Wont Do\":\n",
    "                    active_anvil_datasets.append([key, summary, status, access_management, duos_identifier, released_in_duos, released_in_tdr, released_in_explorer, data_access_control_group, dbgap_accession, dbgap_consent_group, dbgap_consent_code, dbgap_dac, dbgap_terra_group, staging_workspace, tdr_gcp_dataset, tdr_gcp_snapshot, tdr_az_dataset, tdr_az_snapshot, size, map_spec, data_on_clouds_str, staging_buckets])\n",
    "                else:\n",
    "                    inactive_anvil_datasets.append([key, summary, status, access_management, duos_identifier, released_in_duos, released_in_tdr, released_in_explorer, data_access_control_group, dbgap_accession, dbgap_consent_group, dbgap_consent_code, dbgap_dac, dbgap_terra_group, staging_workspace, tdr_gcp_dataset, tdr_gcp_snapshot, tdr_az_dataset, tdr_az_snapshot, size, map_spec, data_on_clouds_str, staging_buckets])\n",
    "        start_at_param += len(search_results)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nANVIL Jira Datasets: \")\n",
    "    all_anvil_datasets = []\n",
    "    all_anvil_datasets.extend(active_anvil_datasets)\n",
    "    all_anvil_datasets.extend(inactive_anvil_datasets)\n",
    "    all_datasets_df = pd.DataFrame(all_anvil_datasets, columns = [\"key\", \"summary\", \"status\", \"access_management\", \"duos_identifier\", \"released_in_duos\", \"released_in_tdr\", \"released_in_explorer\", \"data_access_control_group\", \"dbgap_accession\", \"dbgap_consent_group\", \"dbgap_consent_code\", \"dbgap_dac\", \"dbgap_terra_group\", \"staging_workspace\", \"tdr_gcp_dataset\", \"tdr_gcp_snapshot\", \"tdr_az_dataset\", \"tdr_az_snapshot\", \"size_tb\", \"mapping_spec\", \"data_on_clouds\", \"staging_buckets\"])  \n",
    "    display(all_datasets_df)\n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify whether the All Workspaces Report should be run:\n",
    "run_ws_access_rpt = True\n",
    "\n",
    "# Specify the workspaces that should be included in the All Workspaces Report, or leave empty to pull all workspaces you have \n",
    "# access to in the anvil-datastorage or AnVILDataStorage_Azure billing projects.\n",
    "workspace_list = [\n",
    "]\n",
    "\n",
    "# Specify whether the All Snapshots Report should be run:\n",
    "run_ss_access_rpt = True\n",
    "\n",
    "# Specify the snapshots that should be included in the All Snapshots Report, or leave empty to pull all snapshots you have \n",
    "# access to in the e0e03e48-5b96-45ec-baa4-8cc1ebf74c61 or 9ee23bed-b46c-4561-9103-d2a723113f7f billing profiles.\n",
    "snapshot_id_list = [\n",
    "]\n",
    "\n",
    "# Specify whether the All dbGaP Telemetry Groups Report should be run:\n",
    "run_dbgap_telemetry_group_rpt = True\n",
    "\n",
    "# Specify the groups to include in the All dbGaP Telemetry Groups Report, or leave empty to only pull the groups\n",
    "# that are currently in use on a workspace and/or snapshot (depending on which of the above reports you run):\n",
    "dbgap_telemetry_group_list = [\n",
    "    'dbGaP_AnVIL_phs000298_c1_Autism_HighSeq_DS-ASD',\n",
    "    'dbGaP_AnVIL_phs000298_c2_Autism_HighSeq_GRU',\n",
    "    'dbGaP_AnVIL_phs000298_c3_Autism_HighSeq_DS-AOND-MDS',\n",
    "    'dbGaP_AnVIL_phs000298_c4_Autism_HighSeq_HMB-MDS',\n",
    "    'dbGaP_AnVIL_phs000711_c1_MendelianGenomics_Ba_GRU',\n",
    "    'dbGaP_AnVIL_phs000711_c2_MendelianGenomics_Ba_HMB-NPU',\n",
    "    'dbGaP_AnVIL_phs000711_c3_MendelianGenomics_Ba_HMB-IRB-NPU',\n",
    "    'dbGaP_AnVIL_phs000693_c1_MendelianGenomics_UW_GRU',\n",
    "    'dbGaP_AnVIL_phs000693_c2_MendelianGenomics_UW_DS-EP',\n",
    "    'dbGaP_AnVIL_phs000693_c3_MendelianGenomics_UW_HMB-GSO',\n",
    "    'dbGaP_AnVIL_phs000693_c4_MendelianGenomics_UW_DS-SL_AN-NPU',\n",
    "    'dbGaP_AnVIL_phs000693_c5_MendelianGenomics_UW_DS-SASC',\n",
    "    'dbGaP_AnVIL_phs000693_c6_MendelianGenomics_UW_HMB-NPU',\n",
    "    'dbGaP_AnVIL_phs000693_c7_MendelianGenomics_UW_DS-SKDYS',\n",
    "    'dbGaP_AnVIL_phs000693_c8_MendelianGenomics_UW_DS-BAV-IRB-PU',\n",
    "    'dbGaP_AnVIL_phs000693_c9_MendelianGenomics_UW_HMB',\n",
    "    'dbGaP_AnVIL_phs000693_c10_MendelianGenomics_UW_DS-BAB',\n",
    "    'dbGaP_AnVIL_phs000693_c11_MendelianGenomics_UW_DS-SKDYS-RD',\n",
    "    'dbGaP_AnVIL_phs000693_c12_MendelianGenomics_UW_DS-HSS',\n",
    "    'dbGaP_AnVIL_phs000693_c13_MendelianGenomics_UW_DS-NDRD',\n",
    "    'dbGaP_AnVIL_phs000693_c14_MendelianGenomics_UW_DS-PRO',\n",
    "    'dbGaP_AnVIL_phs000693_c15_MendelianGenomics_UW_DS-GIDIS',\n",
    "    'dbGaP_AnVIL_phs000693_c16_MendelianGenomics_UW_DS-RCTD',\n",
    "    'dbGaP_AnVIL_phs000693_c17_MendelianGenomics_UW_DS-HECD',\n",
    "    'dbGaP_AnVIL_phs000693_c18_MendelianGenomics_UW_DS-LPARS-IRB',\n",
    "    'dbGaP_AnVIL_phs000693_c19_MendelianGenomics_UW_DS-IPF',\n",
    "    'dbGaP_AnVIL_phs000693_c20_MendelianGenomics_UW_DS-LIVD',\n",
    "    'dbGaP_AnVIL_phs000693_c21_MendelianGenomics_UW_DS-HFA',\n",
    "    'dbGaP_AnVIL_phs000693_c22_MendelianGenomics_UW_DS-IF',\n",
    "    'dbGaP_AnVIL_phs000693_c23_MendelianGenomics_UW_DS-TC-IRB',\n",
    "    'dbGaP_AnVIL_phs000693_c24_MendelianGenomics_UW_DS-PLGD-NPU',\n",
    "    'dbGaP_AnVIL_phs000693_c25_MendelianGenomics_UW_DS-LD',\n",
    "    'dbGaP_AnVIL_phs000693_c26_MendelianGenomics_UW_DS-TRIC',\n",
    "    'dbGaP_AnVIL_phs000693_c27_MendelianGenomics_UW_DS-NBIA',\n",
    "    'dbGaP_AnVIL_phs000693_c28_MendelianGenomics_UW_HMB-IRB',\n",
    "    'dbGaP_AnVIL_phs000693_c29_MendelianGenomics_UW_DS-CHD-IRB',\n",
    "    'dbGaP_AnVIL_phs000693_c30_MendelianGenomics_UW_GRU-IRB',\n",
    "    'dbGaP_AnVIL_phs000693_c31_MendelianGenomics_UW_DS-CHC',\n",
    "    'dbGaP_AnVIL_phs000693_c32_MendelianGenomics_UW_DS-SHFM',\n",
    "    'dbGaP_AnVIL_phs000693_c33_MendelianGenomics_UW_DS-CERD',\n",
    "    'dbGaP_AnVIL_phs000693_c34_MendelianGenomics_UW_DS-BDIS',\n",
    "    'dbGaP_AnVIL_phs000693_c35_MendelianGenomics_UW_DS-CHDEF',\n",
    "    'dbGaP_AnVIL_phs000693_c36_MendelianGenomics_UW_DS-LYD',\n",
    "    'dbGaP_AnVIL_phs000744_c1_MendelianGenomics_Ya_GRU',\n",
    "    'dbGaP_AnVIL_phs001487_c1_CCDG_TAICHI_DS-MULTIPLE_D',\n",
    "    'dbGaP_AnVIL_phs001272_c1_BroadCMG_GRU',\n",
    "    'dbGaP_AnVIL_phs001272_c2_BroadCMG_DS-KRD-RD',\n",
    "    'dbGaP_AnVIL_phs001272_c3_BroadCMG_HMB-MDS',\n",
    "    'dbGaP_AnVIL_phs001272_c4_BroadCMG_DS-NIC-EMP-LE',\n",
    "    'dbGaP_AnVIL_phs001259_c1_VIRGO_AMI_DS-CARD-MDS-G',\n",
    "    'dbGaP_AnVIL_phs001740_c2_CCDG_SAGE_DS-ASD-RD-IRB',\n",
    "    'dbGaP_AnVIL_phs001227_c1_CAD_WU_DS-ATHSCL-IRB',\n",
    "    'dbGaP_AnVIL_phs001227_c2_CAD_WU_GRU-IRB',\n",
    "    'dbGaP_AnVIL_phs001880_c1_CCDG_Emory_GRU-NPU',\n",
    "    'dbGaP_AnVIL_phs001871_c1_CCDG_CCC_DS-CAD-IRB',\n",
    "    'dbGaP_AnVIL_phs001741_c2_CCDG_TASC_DS-ASD-IRB',\n",
    "    'dbGaP_AnVIL_phs001894_c2_CCDG_HMCA_DS-EAC-PUB-GS',\n",
    "    'dbGaP_AnVIL_phs002004_c1_CCDG_CAG_DS-AUT',\n",
    "    'dbGaP_AnVIL_phs001676_c2_CCDG_Autism_SSC_DS-AONDD-IRB',\n",
    "    'dbGaP_AnVIL_phs001766_c1_CCDG_AGRE_DS-ASD-IRB',\n",
    "    'dbGaP_AnVIL_phs002042_c1_CCDG_ACEII_GRU',\n",
    "    'dbGaP_AnVIL_phs002042_c2_CCDG_ACEII_DS-ASD',\n",
    "    'dbGaP_AnVIL_phs002043_c1_CCDG_NeurogeneticsFe_GRU',\n",
    "    'dbGaP_AnVIL_phs002043_c2_CCDG_NeurogeneticsFe_DS-AASD',\n",
    "    'dbGaP_AnVIL_phs002044_c1_CCDG_CATS_DS-ASD-IRB',\n",
    "    'dbGaP_AnVIL_phs001222_c1_CCDG_T1DGC_DS-DRC-IRB-NP',\n",
    "    'dbGaP_AnVIL_phs001642_c1_CCDG_IBD_GRU',\n",
    "    'dbGaP_AnVIL_phs001642_c2_CCDG_IBD_HMB',\n",
    "    'dbGaP_AnVIL_phs001642_c3_CCDG_IBD_DS-IBD',\n",
    "    'dbGaP_AnVIL_phs001642_c4_CCDG_IBD_DS-GID',\n",
    "    'dbGaP_AnVIL_phs001913_c1_CCDG_eMERGE_Northwes_GRU-IRB',\n",
    "    'dbGaP_AnVIL_phs000235_c1_NCI_CGCI_CMR',\n",
    "    'dbGaP_AnVIL_phs000235_c2_NCI_CGCI_PC',\n",
    "    'dbGaP_AnVIL_phs000235_c1_CGCI_DS-CA-MDS',\n",
    "    'dbGaP_AnVIL_phs000235_c2_CGCI_PCR',\n",
    "    'dbGaP_AnVIL_phs000235_c1_CGCI_V4_CRGM',\n",
    "    'dbGaP_AnVIL_phs000235_c2_CGCI_V4_PCR',\n",
    "    'dbGaP_AnVIL_phs000235_c1_TP_CGCI_DS-CA-MDS',\n",
    "    'dbGaP_AnVIL_phs000235_c2_TP_CGCI_PCR',\n",
    "    'dbGaP_AnVIL_phs000235_c3_TP_CGCI_GRU',\n",
    "    'dbGaP_AnVIL_phs000235_c4_TP_CGCI_GRU-IRB',\n",
    "    'dbGaP_AnVIL_phs000971_c1_ClinSeq_GRU',\n",
    "    'dbGaP_AnVIL_phs001011_c1_eMERGE_CCHMC_IIIA_GRU-IRB-NPU',\n",
    "    'dbGaP_AnVIL_phs001395_c1_TOPMed_WGS_HCHS_SOL_HMB-NPU',\n",
    "    'dbGaP_AnVIL_phs001395_c2_TOPMed_WGS_HCHS_SOL_HMB',\n",
    "    'dbGaP_AnVIL_phs001489_c1_CCDG_Epi25_DS-EPSBAID-MD',\n",
    "    'dbGaP_AnVIL_phs001489_c2_CCDG_Epi25_DS-EPSBA-MDS-',\n",
    "    'dbGaP_AnVIL_phs001489_c3_CCDG_Epi25_DS-EPSBACID-M',\n",
    "    'dbGaP_AnVIL_phs001489_c4_CCDG_Epi25_DS-EPCOM-MDS-',\n",
    "    'dbGaP_AnVIL_phs001489_c5_CCDG_Epi25_DS-EPSBACID-N',\n",
    "    'dbGaP_AnVIL_phs001489_c6_CCDG_Epi25_DS-EPI-MULTI-',\n",
    "    'dbGaP_AnVIL_phs001489_c7_CCDG_Epi25_DS-EPASM-MDS',\n",
    "    'dbGaP_AnVIL_phs001489_c8_CCDG_Epi25_HMB-NPU-MDS',\n",
    "    'dbGaP_AnVIL_phs001489_c9_CCDG_Epi25_DS-EPASM-MDS-',\n",
    "    'dbGaP_AnVIL_phs001489_c10_CCDG_Epi25_DS-EP',\n",
    "    'dbGaP_AnVIL_phs001489_c11_CCDG_Epi25_HMB-MDS',\n",
    "    'dbGaP_AnVIL_phs001489_c12_CCDG_Epi25_GRU-IRB',\n",
    "    'dbGaP_AnVIL_phs001489_c13_CCDG_Epi25_GRU',\n",
    "    'dbGaP_AnVIL_phs001489_c14_CCDG_Epi25_DS-CARNEU-MDS',\n",
    "    'dbGaP_AnVIL_phs001489_c15_CCDG_Epi25_DS-SEIZD',\n",
    "    'dbGaP_AnVIL_phs001489_c16_CCDG_Epi25_DS-EP-MDS',\n",
    "    'dbGaP_AnVIL_phs001489_c17_CCDG_Epi25_DS-EP-NPU',\n",
    "    'dbGaP_AnVIL_phs001489_c18_CCDG_Epi25_EPILEPSY_MULT',\n",
    "    'dbGaP_AnVIL_phs001489_c19_CCDG_Epi25_HMB',\n",
    "    'dbGaP_AnVIL_phs001489_c20_CCDG_Epi25_DS-EPI-ADULT-',\n",
    "    'dbGaP_AnVIL_phs001489_c21_CCDG_Epi25_GRU-NPU',\n",
    "    'dbGaP_AnVIL_phs001489_c22_CCDG_Epi25_DS-EAED-MDS',\n",
    "    'dbGaP_AnVIL_phs001489_c23_CCDG_Epi25_DS-NEUROLOGY-',\n",
    "    'dbGaP_AnVIL_phs001489_c24_CCDG_Epi25_DS-EARET-MDS',\n",
    "    'dbGaP_AnVIL_phs001489_c25_CCDG_Epi25_DS-NPD-IRB-NP',\n",
    "    'dbGaP_AnVIL_phs001489_c26_CCDG_Epi25_DS-NEUROLOGY-',\n",
    "    'dbGaP_AnVIL_phs001489_c27_CCDG_Epi25_HMB-IRB-MDS',\n",
    "    'dbGaP_AnVIL_phs001592_c1_CCDG_ATVB_DS-CVD',\n",
    "    'dbGaP_AnVIL_phs001901_c1_CCDG_CoronaryPronePe_DS-CVD-MDS',\n",
    "    'dbGaP_AnVIL_phs002018_c1_CCDG_PartnersBiobank_HMB-MDS',\n",
    "    'dbGaP_AnVIL_phs002032_c1_NeuronalPhenotypes_GRU',\n",
    "    'dbGaP_AnVIL_phs002032_c2_NeuronalPhenotypes_DS-SMA-MDS',\n",
    "    'dbGaP_AnVIL_phs002041_c1_Schizophrenia_Bipola_GRU',\n",
    "    'dbGaP_AnVIL_phs002041_c2_Schizophrenia_Bipola_DS-MLHLTH-MDS',\n",
    "    'dbGaP_AnVIL_phs002041_c3_Schizophrenia_Bipola_DS-SZRD-MDS',\n",
    "    'dbGaP_AnVIL_phs002205_c1_CCDG_IBD_GMbC_Exomes_GRU',\n",
    "    'dbGaP_AnVIL_phs002205_c2_CCDG_IBD_GMbC_Exomes_GRU-NPU',\n",
    "    'dbGaP_AnVIL_phs002206_c1_GenomicAnswersforKid_DS-PEDD-IRB',\n",
    "    'dbGaP_AnVIL_phs002242_c1_CCDG_SWISS_AF_DS-CCSD-NPU-M',\n",
    "    'dbGaP_AnVIL_phs002243_c1_CCDG_PEGASUS_TIMI_54_HMB-MDS',\n",
    "    'dbGaP_AnVIL_phs002324_c1_CSER_DiagnosisPediat_HMB',\n",
    "    'dbGaP_AnVIL_phs002324_c2_CSER_DiagnosisPediat_BD-GC',\n",
    "    'dbGaP_AnVIL_phs001398_c1_CCDG_BRAVE_GRU',\n",
    "    'dbGaP_AnVIL_phs002509_c1_CCDG_GASD_GRU-NPU',\n",
    "    'dbGaP_AnVIL_phs002511_c1_CCDG_SPARK_SFARI_GRU-IRB-PUB',\n",
    "    'dbGaP_AnVIL_phs001746_c1_GTEx_VMRs_GRU',\n",
    "    'dbGaP_AnVIL_phs002245_c1_COVID19_Susceptibili_GRU',\n",
    "    'dbGaP_AnVIL_phs002282_c1_CCDG_GeneDiet_CostaR_DS-CVDRF',\n",
    "    'dbGaP_AnVIL_phs002325_c1_CCDG_CardiovascularB_DS-HBVD',\n",
    "    'dbGaP_AnVIL_phs002512_c1_CCDG_SimonsSearchlig_DS-AUT-IRB-RD',\n",
    "    'dbGaP_AnVIL_phs002307_c1_CSER_SouthSeq_Newbor_GRU',\n",
    "    'dbGaP_AnVIL_phs000920_c2_TOPMed_WGS_GALAII_DS-LD-IRB-COL',\n",
    "    'dbGaP_AnVIL_phs000925_c1_PAGE_IPM_BioMe_GRU',\n",
    "    'dbGaP_AnVIL_phs001033_c1_PAGE_GlobalRP_GRU',\n",
    "    'dbGaP_AnVIL_phs001211_c1_TOPMed_WGS_ARIC_HMB-IRB',\n",
    "    'dbGaP_AnVIL_phs001211_c2_TOPMed_WGS_ARIC_DS-CVD-IRB',\n",
    "    'dbGaP_AnVIL_phs001798_c1_AASP_HGV',\n",
    "    'dbGaP_AnVIL_phs002502_c1_CCDG_ASD_GRU',\n",
    "    'dbGaP_AnVIL_phs002502_c2_CCDG_ASD_DS-ASD',\n",
    "    'dbGaP_AnVIL_phs002502_c10_CCDG_ASD_DS-ASD-RD',\n",
    "    'dbGaP_AnVIL_phs002502_c11_CCDG_ASD_DS-RARED',\n",
    "    'dbGaP_AnVIL_phs002502_c12_CCDG_ASD_DS-NDEVRD',\n",
    "    'dbGaP_AnVIL_phs002502_c3_CCDG_ASD_HMB-MDS',\n",
    "    'dbGaP_AnVIL_phs002502_c4_CCDG_ASD_GRU-NPU',\n",
    "    'dbGaP_AnVIL_phs002502_c5_CCDG_ASD_HMB-NPU',\n",
    "    'dbGaP_AnVIL_phs002502_c6_CCDG_ASD_DS-ASD-NPU',\n",
    "    'dbGaP_AnVIL_phs002502_c7_CCDG_ASD_DS-ASD-NPU-MD',\n",
    "    'dbGaP_AnVIL_phs002502_c8_CCDG_ASD_DS-ASD-MDS',\n",
    "    'dbGaP_AnVIL_phs002502_c9_CCDG_ASD_DS-MHNR-NPU-M',\n",
    "    'dbGaP_AnVIL_phs000220_c1_PAGE_MEC_CRM',\n",
    "    'dbGaP_AnVIL_phs000220_c2_PAGE_MEC_GRU',\n",
    "    'dbGaP_AnVIL_phs002726_c1_CCDG_CardiologyBioba_HMB-MDS',\n",
    "    'dbGaP_AnVIL_phs002774_c1_CCDG_ENGAGE_AF_TIMI4_DS-CCSD-MDS',\n",
    "    'dbGaP_AnVIL_phs002774_c2_CCDG_ENGAGE_AF_TIMI4_DS-BBFOD-MDS',\n",
    "    'dbGaP_AnVIL_phs002235_c1_GMbC_Microbiome_GRU-NPU',\n",
    "    'dbGaP_AnVIL_phs001300_c1_NABEC_LR_WGS_GRU',\n",
    "    'dbGaP_AnVIL_phs002110_c1_CSER_NCGENES2_GRU',\n",
    "    'dbGaP_AnVIL_phs002378_c1_CSER_KidsCanSeq_GRU',\n",
    "    'dbGaP_AnVIL_phs002111_c1_CSER_CHARM_GRU',\n",
    "    'dbGaP_AnVIL_phs000971_c1_CSER_ClinSeq_GRU',\n",
    "    'dbGaP_AnVIL_phs003184_c1_ALS_Compute_Collecti_GRU',\n",
    "    'dbGaP_AnVIL_phs001585_c1_ALS_FTD_GRU',\n",
    "    'dbGaP_AnVIL_phs002337_c1_CSER_NYCKIDSEQ_HMB',\n",
    "    'dbGaP_AnVIL_phs002337_c2_CSER_NYCKIDSEQ_GRU',\n",
    "    'dbGaP_AnVIL_phs003200_c1_MAS_ISO_seq_DS-MSC-MDS',\n",
    "    'dbGaP_AnVIL_phs002502_c13_CCDG_ASD_DS-MLHLTH-IRB',\n",
    "    'dbGaP_AnVIL_phs002502_c14_CCDG_ASD_DS-MBND-MDS',\n",
    "    'dbGaP_AnVIL_phs001642_c5_CCDG_IBD_HMB-IRB-MDS',\n",
    "    'dbGaP_AnVIL_phs001642_c6_CCDG_IBD_DS-IBD-MDS',\n",
    "    'dbGaP_AnVIL_phs001642_c7_CCDG_IBD_HMB-MDS',\n",
    "    'dbGaP_AnVIL_phs001642_c8_CCDG_IBD_DS-DSDI-MDS',\n",
    "    'dbGaP_AnVIL_phs001642_c9_CCDG_IBD_DS-GR-IRB-MDS',\n",
    "    'dbGaP_AnVIL_phs002242_c1_CCDG_SWISS_AF_DS-DCCA-NPU-M',\n",
    "    'dbGaP_AnVIL_phs002324_c3_CSER_P3EGS_GRU',\n",
    "    'dbGaP_AnVIL_phs003047_c1_GREGoR_HMB',\n",
    "    'dbGaP_AnVIL_phs003047_c2_GREGoR_GRU',\n",
    "    'dbGaP_AnVIL_phs000744_c2_MendelianGenomics_Ya_HMB',\n",
    "    'dbGaP_AnVIL_phs000744_c3_MendelianGenomics_Ya_DS-RARED',\n",
    "    'dbGaP_AnVIL_phs000744_c4_MendelianGenomics_Ya_DS-GSD-GSO',\n",
    "    'dbGaP_AnVIL_phs000744_c5_MendelianGenomics_Ya_DS-MC',\n",
    "    'dbGaP_AnVIL_phs000744_c6_MendelianGenomics_Ya_HMB-GSO',\n",
    "    'dbGaP_AnVIL_phs000744_c7_MendelianGenomics_Ya_DS-MCDMS',\n",
    "    'dbGaP_AnVIL_phs000744_c8_MendelianGenomics_Ya_DS-GD',\n",
    "    'dbGaP_AnVIL_phs000744_c9_MendelianGenomics_Ya_HMB-IRB',\n",
    "    'dbGaP_AnVIL_phs000744_c10_MendelianGenomics_Ya_DS-THAL-IRB',\n",
    "    'dbGaP_AnVIL_phs000744_c11_MendelianGenomics_Ya_DS-RD',\n",
    "    'dbGaP_AnVIL_phs000744_c12_MendelianGenomics_Ya_DS-BPEAKD',\n",
    "    'dbGaP_AnVIL_phs002041_c4_Schizophrenia_Bipola_DS-MBND-MDS',\n",
    "    'dbGaP_AnVIL_phs002041_c5_Schizophrenia_Bipola_DS-SZ-MDS',\n",
    "    'dbGaP_AnVIL_phs002041_c6_Schizophrenia_Bipola_HMB-MDS',\n",
    "    'dbGaP_AnVIL_phs001963_c1_ALS_FTD_DementiaSeq_GRU',\n",
    "    'dbGaP_AnVIL_phs001272_c5_BroadCMG_DS-BFD-MDS',\n",
    "    'dbGaP_AnVIL_phs001272_c6_BroadCMG_DS-NEUROLOGY-',\n",
    "    'dbGaP_AnVIL_phs001272_c7_BroadCMG_DS-CVD-MDS',\n",
    "    'dbGaP_AnVIL_phs001272_c8_BroadCMG_GRU-IRB',\n",
    "    'dbGaP_AnVIL_phs002032_c3_NeuronalPhenotypes_DS-MBND-MDS',\n",
    "    'dbGaP_AnVIL_phs001489_c28_CCDG_Epi25_HMB-GSO',\n",
    "    'dbGaP_AnVIL_phs001489_c29_CCDG_Epi25_DS-NSD-NPU-MD',\n",
    "    'dbGaP_AnVIL_phs001489_c30_CCDG_Epi25_DS-NSD-ADULTS',\n",
    "    'dbGaP_AnVIL_phs001489_c31_CCDG_Epi25_DS-NEURO-EP-M',\n",
    "    'dbGaP_AnVIL_phs001489_c32_CCDG_Epi25_DS-MBND-NPU-M',\n",
    "    'dbGaP_AnVIL_phs001489_c33_CCDG_Epi25_DS-EAED-IRB-N',\n",
    "    'dbGaP_AnVIL_phs003444_c1_DepMap_HMB-MDS',\n",
    "    'dbGaP_AnVIL_phs003537_c1_HudsonAlpha_GRU',\n",
    "    'dbGaP_AnVIL_phs002041_c1_Schizophrenia_Bipola_DS-MLHLTH-MDS',\n",
    "    'dbGaP_AnVIL_phs002041_c2_Schizophrenia_Bipola_DS-SZRD-MDS',\n",
    "    'dbGaP_AnVIL_phs002041_c3_Schizophrenia_Bipola_GRU',\n",
    "    'dbGaP_AnVIL_phs003184_c1_ALS_Compute_Collecti_HMB',\n",
    "]\n",
    "\n",
    "# Specify whether the All DUOS Registrations Report should be run:\n",
    "run_duos_registrations_rpt = True\n",
    "\n",
    "# Specify the token that should be used for accessing DUOS production for the All DUOS Registrations Report \n",
    "# (you can use gcloud auth print-access-token on the command line to get this):\n",
    "duos_user_token = \"\"\n",
    "\n",
    "# Specify whether the All Dataset Tickets Report should be run:\n",
    "run_dataset_tickets_rpt = True\n",
    "\n",
    "# Specify the user and api_key to use for accessing Jira for the All Dataset Tickets Report:\n",
    "user = \"ncalvane@broadinstitute.org\"\n",
    "api_key = \"\"\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "tele_ad_map = []\n",
    "if run_ws_access_rpt:\n",
    "    ws_output = gen_workspace_report(workspace_list)\n",
    "    tele_ad_map.extend(ws_output)\n",
    "if run_ss_access_rpt:\n",
    "    ss_output = gen_snapshot_report(snapshot_id_list)\n",
    "    tele_ad_map.extend(ss_output)\n",
    "if run_dbgap_telemetry_group_rpt:\n",
    "    gen_dbgap_report(dbgap_telemetry_group_list, tele_ad_map)  \n",
    "if run_duos_registrations_rpt:\n",
    "    gen_duos_report(duos_user_token)\n",
    "if run_dataset_tickets_rpt:\n",
    "    gen_jira_datasets_report(user, api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## dbGaP Reports\n",
    "Run a report that will fetch and parse the dbGaP XML into a more user-friendly view, for the attributes that are currently of interest to our team (the PHS ID, Study Name, Consent Codes, presense of AnVIL as a trusted partner, and dbGaP Status). This report can be run on either a specified set of PHS IDs, or on all PHS IDs present in dbGaP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     15,
     65
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def format_phs_id(input_str):\n",
    "    try:\n",
    "        num = re.search(\"(phs)?0*([0-9]+)\", input_str, re.IGNORECASE).group(2)\n",
    "    except:\n",
    "        num = \"\"\n",
    "    if num:\n",
    "        output_str = \"phs\" + str(num).zfill(6)\n",
    "    else:\n",
    "        output_str = \"\"\n",
    "    return output_str\n",
    "\n",
    "def parse_dbgap_xml(study_dict, user_input, phs_id, limit_to_latest_version):\n",
    "    parsed_results = []\n",
    "    # Parse XML and pull information of interest\n",
    "    try:\n",
    "        if not isinstance(study_dict[\"dbgapss\"][\"Study\"], list):\n",
    "            study_dict[\"dbgapss\"][\"Study\"] = [study_dict[\"dbgapss\"][\"Study\"]]\n",
    "    except:\n",
    "        parsed_results.append([user_input, phs_id, \"\", \"\", \"\", \"\", \"\", \"Failure\", \"Error retrieving valid dbGaP XML.\"])\n",
    "        return parsed_results\n",
    "    for study in study_dict[\"dbgapss\"][\"Study\"]:\n",
    "        \n",
    "        # Pull base information\n",
    "        accession = study[\"StudyInfo\"][\"@accession\"]\n",
    "        name = study[\"StudyInfo\"][\"StudyNameEntrez\"]\n",
    "        status = study[\"Status\"][\"@title\"]\n",
    "        \n",
    "        # Pull trusted partner information, if available\n",
    "        anvil_trusted_partner = False\n",
    "        try:\n",
    "            if isinstance(study[\"Policy\"][\"TrustedPartners\"][\"TrustedPartner\"], list):\n",
    "                for tp in study[\"Policy\"][\"TrustedPartners\"][\"TrustedPartner\"]:\n",
    "                    if tp[\"@trp_db_name\"] == \"AnVIL\":\n",
    "                        anvil_trusted_partner = True\n",
    "                        break\n",
    "            else:\n",
    "                if study[\"Policy\"][\"TrustedPartners\"][\"TrustedPartner\"][\"@trp_db_name\"] == \"AnVIL\":\n",
    "                    anvil_trusted_partner = True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Pull consent code and DAC information, if available\n",
    "        try:\n",
    "            if isinstance(study[\"Policy\"][\"ConsentGroup\"], list):\n",
    "                for idx, consent in enumerate(study[\"Policy\"][\"ConsentGroup\"]):\n",
    "                    consent_code = consent[\"@name\"]\n",
    "                    dac_id = consent[\"@dac_uid\"]\n",
    "                    dac_name = consent[\"@dac_name\"]\n",
    "                    parsed_results.append([user_input, phs_id, accession, name, idx+1, consent_code, dac_id, dac_name, anvil_trusted_partner, status, \"Success\", \"\"])\n",
    "            else:\n",
    "                consent_code = study[\"Policy\"][\"ConsentGroup\"][\"@name\"]\n",
    "                dac_id = study[\"Policy\"][\"ConsentGroup\"][\"@dac_uid\"]\n",
    "                dac_name = study[\"Policy\"][\"ConsentGroup\"][\"@dac_name\"]\n",
    "                parsed_results.append([user_input, phs_id, accession, name, 1, consent_code, dac_id, dac_name, anvil_trusted_partner, status, \"Success\", \"\"])\n",
    "        except:\n",
    "            parsed_results.append([user_input, phs_id, accession, name, 0, None, None, None, anvil_trusted_partner, status, \"Success\", \"\"])\n",
    "        \n",
    "        # Break loop if only latest version is of interest\n",
    "        if limit_to_latest_version: break\n",
    "    return parsed_results\n",
    "        \n",
    "def gen_dbgap_report(phs_list):\n",
    "    results = []\n",
    "    dbgap_url = \"https://dbgap.ncbi.nlm.nih.gov/ss/dbgapssws.cgi?request=Study&phs=\"\n",
    "    if phs_list:\n",
    "        for phs in phs_list:\n",
    "            # Retrieve dbGaP XML if exists, and parse out information of interest\n",
    "            phs_id = format_phs_id(phs)\n",
    "            if phs_id:\n",
    "                phs_num = phs_id[3:9]\n",
    "            else:\n",
    "                phs_num = \"Invalid\"\n",
    "            print(f\"Processing input '{phs}' ({phs_id})\")\n",
    "            retry_count = 0\n",
    "            while retry_count <= 2:\n",
    "                try:\n",
    "                    response = requests.get(dbgap_url+phs_num)\n",
    "                    break\n",
    "                except:\n",
    "                    retry_count += 1\n",
    "            study_dict = xmltodict.parse(response.content)\n",
    "            output = parse_dbgap_xml(study_dict, phs, phs_id, limit_to_latest_version)\n",
    "            for entry in output:\n",
    "                results.append(entry)\n",
    "    else:\n",
    "        i = 0\n",
    "        error_count = 0\n",
    "        print(\"Processing all existing dbGaP studies. Note that this can take upwards of 45 minutes to run. To collect information on specific studies, list the studies of interest in the phs_list input parameter.\")\n",
    "        print(f\"Start time: {datetime.datetime.now()}\")\n",
    "        # Loop through possible phs IDs, only stopping when 100 sequential IDs yield no results\n",
    "        for phs in [str(item).zfill(6) for item in list(range(1,1000000))]: \n",
    "            # Initialize\n",
    "            i += 1 \n",
    "            if error_count > 99: break\n",
    "\n",
    "            # Retrieve dbGaP XML if exists, and parse out information of interest   \n",
    "            phs_id = format_phs_id(phs)\n",
    "            if phs_id:\n",
    "                phs_num = phs_id[3:9]\n",
    "            else:\n",
    "                phs_num = \"Invalid\" \n",
    "            retry_count = 0\n",
    "            while retry_count <= 2:\n",
    "                try:\n",
    "                    response = requests.get(dbgap_url+phs_num)\n",
    "                    break\n",
    "                except:\n",
    "                    retry_count += 1\n",
    "            study_dict = xmltodict.parse(response.content)\n",
    "            output = parse_dbgap_xml(study_dict, phs, phs_id, limit_to_latest_version)\n",
    "\n",
    "            # If no valid results return, increment error count, otherwise, reset to zero\n",
    "            if len(output) == 1 and output[0][7] == \"Failure\":\n",
    "                error_count += 1\n",
    "            else:\n",
    "                error_count = 0 \n",
    "                for entry in output:\n",
    "                    results.append(entry)\n",
    "        print(f\"End time: {datetime.datetime.now()}\")\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\ndbGaP Report Results: \")\n",
    "    results_df = pd.DataFrame(results, columns = [\"user_input\", \"phs_id\", \"accession\", \"study_name\", \"consent_idx\", \"consent_code\", \"dac_id\", \"dac_name\", \"anvil_trusted_partner\", \"dbgap_status\", \"retrieval_status\", \"errors\"])\n",
    "    display(results_df)\n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify whether only the latest version of the study should be included in the report:\n",
    "limit_to_latest_version = True\n",
    "\n",
    "# Specify the list of PHS numbers (in any format) to generate the report for, or leave empty to generate for all:\n",
    "phs_list = [\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "gen_dbgap_report(phs_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bulk Manage AnVIL Resource Locks\n",
    "List the workspaces and/or snapshots of interest and whether this script should attempt to \"LOCK\" or \"UNLOCK\" them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     49
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def update_workspace_lock_status(action, workspace_list):\n",
    "    results = []\n",
    "    # Validate action\n",
    "    print(f\"Validating provided action: {action}\")\n",
    "    if action not in [\"LOCK\", \"UNLOCK\"]:\n",
    "        results.append([\"ALL\", action, \"Failure\", \"Invalid action specified. Must be LOCK or UNLOCK.\"])\n",
    "    else:\n",
    "        # Loop through and process workspaces\n",
    "        act = action.lower()\n",
    "        for workspace_entry in workspace_list:\n",
    "\n",
    "            # Initialize\n",
    "            acceptable_cloud_types = ['gcp', 'azure']\n",
    "            cloud_provider = workspace_entry[\"cloud_provider\"]\n",
    "            workspace = workspace_entry[\"workspace_name\"]\n",
    "            print(f\"Updating workspace lock status for {workspace}.\")\n",
    "            if cloud_provider not in acceptable_cloud_types:\n",
    "                cloud_types = ', '.join(acceptable_cloud_types)\n",
    "                results.append([workspace, action, \"Failure\", f\"Cloud provider must be one of {cloud_types}. Cloud provider provided was {cloud_provider}. Aborting.\"])\n",
    "                continue\n",
    "            billing_project = 'anvil-datastorage' if cloud_provider == 'gcp' else 'AnVILDataStorage_Azure'\n",
    "            error_str = \"\"\n",
    "\n",
    "            # Establish credentials\n",
    "            creds, project = google.auth.default()\n",
    "            auth_req = google.auth.transport.requests.Request()\n",
    "            creds.refresh(auth_req)\n",
    "\n",
    "            # Change workspace lock status\n",
    "            response = requests.put(\n",
    "                url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{workspace}/{act}\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            )\n",
    "            if response.status_code not in [200, 204]:\n",
    "                error_str = \"Error updating workspace lock status.\"\n",
    "\n",
    "            # Record status\n",
    "            status = \"Success\" if not error_str else \"Failure\"\n",
    "            results.append([workspace, action, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace\", \"action\", \"status\", \"errors\"])\n",
    "    display(results_df)\n",
    "    \n",
    "def update_snapshot_lock_status(action, snapshot_id_list):\n",
    "    results = []\n",
    "    # Validate action\n",
    "    print(f\"Validating provided action: {action}\")\n",
    "    if action not in [\"LOCK\", \"UNLOCK\"]:\n",
    "        results.append([\"ALL\", action, \"Failure\", \"Invalid action specified. Must be LOCK or UNLOCK.\"])\n",
    "    else:\n",
    "        # Loop through and process snapshots\n",
    "        act = action.lower()\n",
    "        for snapshot_id in snapshot_id_list:\n",
    "\n",
    "            # Initialize\n",
    "            print(f\"Updating snapshot lock status for snapshot: {snapshot_id}.\")\n",
    "            error_str = \"\"\n",
    "            \n",
    "            # Establish credentials\n",
    "            creds, project = google.auth.default()\n",
    "            auth_req = google.auth.transport.requests.Request()\n",
    "            creds.refresh(auth_req)\n",
    "\n",
    "            # Update snapshot lock status\n",
    "            if act == \"lock\":\n",
    "                # Lock snapshot\n",
    "                try:\n",
    "                    response = requests.put(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/lock\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    ).json()\n",
    "                    results.append([snapshot_id, action, \"Success\", \"\"])\n",
    "                except Exception as e: \n",
    "                    error_str = f\"Error updating snapshot lock status: {str(e)}\"\n",
    "                    print(error_str)\n",
    "                    results.append([snapshot_id, action, \"Failure\", error_str])\n",
    "            else:\n",
    "                # Fetch exclusive lock from snapshot (if there is one)\n",
    "                try:\n",
    "                    snapshot_detail = requests.get(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    ).json()\n",
    "                    lock_name = snapshot_detail[\"resourceLocks\"].get(\"exclusive\")\n",
    "                    if lock_name:\n",
    "                        # Unlock snapshot (if locked)\n",
    "                        try:\n",
    "                            request_body = {\"lockName\": lock_name, \"forceUnlock\": False}\n",
    "                            response = requests.put(\n",
    "                                url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/unlock\",\n",
    "                                headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                                json=request_body\n",
    "                            ).json()\n",
    "                            results.append([snapshot_id, action, \"Success\", None])\n",
    "                        except Exception as e: \n",
    "                            error_str = f\"Error updating snapshot lock status: {str(e)}\"\n",
    "                            print(error_str)\n",
    "                            results.append([snapshot_id, action, \"Failure\", error_str])\n",
    "                    else:\n",
    "                        results.append([snapshot_id, action, \"Success\", \"No existing lock found on snapshot.\"])\n",
    "                except Exception as e:\n",
    "                    error_str = f\"Error retrieving lock on snapshot: {str(e)}\"\n",
    "                    results.append([snapshot_id, action, \"Failure\", error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"snapshot\", \"action\", \"status\", \"errors\"])\n",
    "    display(results_df)\n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the action to apply to the workspaces and/or snapshots (LOCK/UNLOCK):\n",
    "action = \"LOCK\"\n",
    "\n",
    "# Specify the list of workspaces to apply the action to:\n",
    "workspace_list = [\n",
    "#    {\"workspace_name\": \"workspace name\", \"cloud_provider\": \"Cloud provider - gcp/azure\"    }\n",
    "    {\"workspace_name\": \"AnVIL_TEST_WORKSPACE_AZ\", \"cloud_provider\": \"azure\"},\n",
    "    {\"workspace_name\": \"AnVIL_TEST_WORKSPACE_GCP\", \"cloud_provider\": \"gcp\"}\n",
    "]\n",
    "\n",
    "# Specify the list of snapshots to apply the action to:\n",
    "snapshot_id_list = [\n",
    "    \"snapshot_id\",\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "if workspace_list:\n",
    "    print(\"Processing provided action for specified workspaces...\")\n",
    "    update_workspace_lock_status(action, workspace_list)\n",
    "if snapshot_id_list:\n",
    "    print(\"Processing provided action for specified workspaces...\")\n",
    "    update_snapshot_lock_status(action, snapshot_id_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Update AnVIL Resource Permissions for Release\n",
    "Prior to the formal release of a workspace or snapshot, permissions need to be updated to ensure that users do not have any elevated permissions unnecessarily. This script will do the following:\n",
    "* Confirm that anvil-admins is the only owner/steward on the workspace or snapshot. \n",
    "* Confirm that any auth domain group on the workspace or snapshot is a reader on the resources.\n",
    "* Confirm all other users on the workspace are readers without share or compute permissions. \n",
    "* Confirm that anvil-admins is the only admin in the auth domain group. \n",
    "* Confirm that anvil_devs is a member in the auth domain group. \n",
    "* Confirm that the cloud bucket is set to requester pays (for workspaces only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     199
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def update_workspace_permissions_for_release(workspace_list):\n",
    "    # Set validation values\n",
    "    acceptable_cloud_types = ['gcp', 'azure']\n",
    "    \n",
    "    # Loop through and process workspaces\n",
    "    results = []\n",
    "    for workspace_entry in workspace_list:\n",
    "\n",
    "        # Initialize\n",
    "        workspace = workspace_entry['workspace_name']\n",
    "        print(f\"Processing updates for {workspace}.\")\n",
    "        cloud_provider = workspace_entry['cloud_provider']\n",
    "        if cloud_provider not in acceptable_cloud_types:\n",
    "            cloud_types = ', '.join(acceptable_cloud_types)\n",
    "            results.append(\n",
    "                [workspace, \"Failure\", f\"Cloud provider must be one of {cloud_types}. Cloud provider provided was {cloud_provider}. Aborting workspace deletion.\"]\n",
    "            )\n",
    "            continue\n",
    "        billing_project = 'anvil-datastorage' if cloud_provider == 'gcp' else 'AnVILDataStorage_Azure'\n",
    "        error_list = []\n",
    "        \n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Pull auth domains and workspace resource ID from workspace attributes\n",
    "        ad_list = []\n",
    "        resource_id = \"\"\n",
    "        ws_attributes = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{workspace}?fields=workspace.attributes,workspace.authorizationDomain,workspace.googleProject,workspace.bucketName,workspace.workspaceId\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        try:\n",
    "            resource_id = ws_attributes[\"workspace\"][\"workspaceId\"]\n",
    "            for ad in ws_attributes[\"workspace\"][\"authorizationDomain\"]:\n",
    "                ad_list.append(ad[\"membersGroupName\"])\n",
    "        except:\n",
    "            error_list.append(f\"Error accessing workspace\")\n",
    "\n",
    "        # Ensure anvil-admins is an owner on the workspace      \n",
    "        payload = [{\n",
    "                \"email\": \"anvil-admins@firecloud.org\",\n",
    "                \"accessLevel\": \"OWNER\",\n",
    "                \"canShare\": True,\n",
    "                \"canCompute\": True\n",
    "            }]\n",
    "        response = requests.patch(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{workspace}/acl\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "            json=payload\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(f\"Error updating workspace ACL ({response.text})\")\n",
    "        \n",
    "        # Pull existing workspace policies\n",
    "        response = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{workspace}/acl\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(f\"Error retrieving workspace ACL ({response.text})\")\n",
    "            ws_acl = {\"acl\": {}}\n",
    "        else:\n",
    "            ws_acl = response.json()\n",
    "\n",
    "        # Loop through workspace ACLs and reduce permissions where necessary\n",
    "        user_processed_list = []\n",
    "        for key, val in ws_acl[\"acl\"].items():\n",
    "            user_processed_list.append(key)\n",
    "            # Determine if the user is yourself, and remove yourself if so\n",
    "            response = requests.get(\n",
    "                url=f\"https://sam.dsde-prod.broadinstitute.org/register/user/v2/self/info\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            ).json()\n",
    "            if key == response[\"userEmail\"]:\n",
    "                response = requests.delete(\n",
    "                    url=f\"https://sam.dsde-prod.broadinstitute.org/api/resources/v2/workspace/{resource_id}/leave\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                )\n",
    "                if response.status_code != 204:\n",
    "                    error_list.append(\"Error updating workspace ACL\")\n",
    "            # Otherwise, reduce permissions for non anvil-admins users\n",
    "            elif key not in [\"anvil-admins@firecloud.org\", \"public-workspace-creators@firecloud.org\"]:\n",
    "                if val[\"accessLevel\"] != \"READER\" or val[\"canCompute\"] == True or val[\"canShare\"] == True:\n",
    "                    payload = [{\n",
    "                            \"email\": key,\n",
    "                            \"accessLevel\": \"READER\",\n",
    "                            \"canShare\": False,\n",
    "                            \"canCompute\": False\n",
    "                        }]\n",
    "                    response = requests.patch(\n",
    "                        url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{workspace}/acl\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                        json=payload\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(f\"Error updating workspace ACL ({response.text})\")\n",
    "\n",
    "        # Add permissions for any listed auth domains not already processed\n",
    "        for ad in ad_list:\n",
    "            user = ad + \"@firecloud.org\"\n",
    "            if user not in user_processed_list:\n",
    "                payload = [{\n",
    "                    \"email\": user,\n",
    "                    \"accessLevel\": \"READER\",\n",
    "                    \"canShare\": False,\n",
    "                    \"canCompute\": False\n",
    "                }]\n",
    "                response = requests.patch(\n",
    "                    url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{workspace}/acl\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                    json=payload\n",
    "                )\n",
    "                if response.status_code != 200:\n",
    "                    error_list.append(f\"Error updating workspace ACL ({response.text})\") \n",
    "\n",
    "        # Ensure anvil-admins and anvil_devs have appropriate roles in the auth domains\n",
    "        for auth_domain in ad_list:\n",
    "            anvil_admins_found = False\n",
    "            anvil_devs_found = False\n",
    "            try:\n",
    "                ad_membership = requests.get(\n",
    "                    url=f\"https://api.firecloud.org/api/groups/{auth_domain}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                ).json()  \n",
    "                # Add anvil-admins as an admin if not already one\n",
    "                for user in ad_membership[\"adminsEmails\"]:\n",
    "                    if user == \"anvil-admins@firecloud.org\":\n",
    "                        anvil_admins_found = True\n",
    "                        break   \n",
    "                if not anvil_admins_found:\n",
    "                    response = requests.put(\n",
    "                        url=f\"https://api.firecloud.org/api/groups/{auth_domain}/admin/anvil-admins@firecloud.org\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 204:\n",
    "                        error_list.append(f\"Error adding anvil-admins to auth domain {auth_domain} ({response.text})\")\n",
    "                # Reduce permissions for any other admins (remove from admin and re-add as member)\n",
    "                for user in ad_membership[\"adminsEmails\"]:\n",
    "                    if user != \"anvil-admins@firecloud.org\":\n",
    "                        response = requests.delete(\n",
    "                            url=f\"https://api.firecloud.org/api/groups/{auth_domain}/admin/{user}\",\n",
    "                            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                        )\n",
    "                        if response.status_code != 204:\n",
    "                            error_list.append(f\"Error removing user {user} as admin from auth domain ({auth_domain})\")\n",
    "                        response = requests.put(\n",
    "                            url=f\"https://api.firecloud.org/api/groups/{auth_domain}/member/{user}\",\n",
    "                            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                        )\n",
    "                        if response.status_code != 204:\n",
    "                            error_list.append(f\"Error add user {user} as member on auth domain {auth_domain} ({response.text})\")\n",
    "                # Add anvil_devs as a member if not already one\n",
    "                if \"anvil_devs@firecloud.org\" not in ad_membership[\"adminsEmails\"] and \"anvil_devs@firecloud.org\" not in ad_membership[\"membersEmails\"]:\n",
    "                    response = requests.put(\n",
    "                        url=f\"https://api.firecloud.org/api/groups/{auth_domain}/member/anvil_devs@firecloud.org\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 204:\n",
    "                        error_list.append(f\"Error adding anvil_devs to auth domain {auth_domain} ({response.text})\")\n",
    "                \n",
    "                for user in ad_membership[\"adminsEmails\"]:\n",
    "                    if user == \"anvil-admins@firecloud.org\":\n",
    "                        anvil_admins_found = True\n",
    "                        break \n",
    "            except:\n",
    "                error_list.append(f\"Error retrieving membership for auth domain {auth_domain} ({response.text})\")\n",
    "\n",
    "        # Set GCP workspaces to requester pays\n",
    "        if cloud_provider == \"gcp\":\n",
    "            payload = [{\n",
    "                \"settingType\": \"GcpBucketRequesterPays\",\n",
    "                \"config\": {\n",
    "                  \"enabled\": True\n",
    "                }\n",
    "            }]\n",
    "            response = requests.put(\n",
    "                url=f\"https://api.firecloud.org/api/workspaces/v2/{billing_project}/{workspace}/settings\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                json=payload\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                error_list.append(f\"Error setting workspace to requester pays ({response.text})\")\n",
    "        \n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([workspace, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace\", \"update_status\", \"errors\"])\n",
    "    display(results_df) \n",
    "\n",
    "def update_snapshot_permissions_for_release(snapshot_id_list):\n",
    "    # Loop through and process snapshots\n",
    "    results = []\n",
    "    for snapshot_id in snapshot_id_list:\n",
    "\n",
    "        # Initialize\n",
    "        print(f\"Processing updates for {snapshot_id}.\")\n",
    "        error_list = []\n",
    "        \n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Ensure anvil-admins is a steward on the snapshot\n",
    "        payload = {\n",
    "            \"email\": \"anvil-admins@firecloud.org\"\n",
    "            }\n",
    "        response = requests.post(\n",
    "            url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/policies/steward/members\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "            json=payload\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(\"Error updating snapshot policies\")\n",
    "        \n",
    "        # Pull snapshot auth domains and policies\n",
    "        ad_list = []\n",
    "        ss_steward_list = []\n",
    "        ss_reader_list = []\n",
    "        snapshot_policies = requests.get(\n",
    "            url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/policies\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        try:\n",
    "            ad_list = snapshot_policies[\"authDomain\"]\n",
    "            for policy in snapshot_policies[\"policies\"]:\n",
    "                if policy[\"name\"] == \"steward\":\n",
    "                    for member in policy[\"members\"]:\n",
    "                        ss_steward_list.append(member)\n",
    "                elif policy[\"name\"] == \"reader\":\n",
    "                    for member in policy[\"members\"]:\n",
    "                        ss_reader_list.append(member)\n",
    "        except:\n",
    "            error_list.append(f\"Error accessing snapshot policies.\")\n",
    "        \n",
    "        # Loop through snapshot policies and reduce permissions where necessary\n",
    "        if \"anvil-admins@firecloud.org\" in ss_steward_list:\n",
    "            for ss_steward in ss_steward_list:\n",
    "                if ss_steward != \"anvil-admins@firecloud.org\":\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/policies/steward/members/{ss_steward}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(\"Error updating snapshot policies\")\n",
    "                    if ss_steward not in ss_reader_list:\n",
    "                        payload = {\n",
    "                            \"email\": ss_steward\n",
    "                        }\n",
    "                        response = requests.post(\n",
    "                            url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/policies/reader/members\",\n",
    "                            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                            json=payload\n",
    "                        )\n",
    "                        if response.status_code != 200:\n",
    "                            error_list.append(\"Error updating snapshot policies\")\n",
    "\n",
    "        # Add/update permissions for any listed auth domains\n",
    "        for ad in ad_list:\n",
    "            payload = {\n",
    "                \"email\": ad + \"@firecloud.org\"\n",
    "            }\n",
    "            response = requests.post(\n",
    "                url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/policies/reader/members\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                json=payload\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                error_list.append(\"Error updating snapshot policies\")\n",
    "\n",
    "        # Ensure anvil-admins and anvil_devs have appropriate roles in the auth domains\n",
    "        for auth_domain in ad_list:\n",
    "            anvil_admins_found = False\n",
    "            anvil_devs_found = False\n",
    "            try:\n",
    "                ad_membership = requests.get(\n",
    "                    url=f\"https://api.firecloud.org/api/groups/{auth_domain}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                ).json()  \n",
    "                # Add anvil-admins as an admin if not already one\n",
    "                for user in ad_membership[\"adminsEmails\"]:\n",
    "                    if user == \"anvil-admins@firecloud.org\":\n",
    "                        anvil_admins_found = True\n",
    "                        break   \n",
    "                if not anvil_admins_found:\n",
    "                    response = requests.put(\n",
    "                        url=f\"https://api.firecloud.org/api/groups/{auth_domain}/admin/anvil-admins@firecloud.org\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 204:\n",
    "                        error_list.append(f\"Error adding anvil-admins to auth domain {auth_domain} ({response.text})\")\n",
    "                # Reduce permissions for any other admins (remove from admin and re-add as member)\n",
    "                for user in ad_membership[\"adminsEmails\"]:\n",
    "                    if user != \"anvil-admins@firecloud.org\":\n",
    "                        response = requests.delete(\n",
    "                            url=f\"https://api.firecloud.org/api/groups/{auth_domain}/admin/{user}\",\n",
    "                            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                        )\n",
    "                        if response.status_code != 204:\n",
    "                            error_list.append(f\"Error removing user {user} as admin from auth domain ({auth_domain})\")\n",
    "                        response = requests.put(\n",
    "                            url=f\"https://api.firecloud.org/api/groups/{auth_domain}/member/{user}\",\n",
    "                            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                        )\n",
    "                        if response.status_code != 204:\n",
    "                            error_list.append(f\"Error add user {user} as member on auth domain {auth_domain} ({response.text})\")\n",
    "                # Add anvil_devs as a member if not already one\n",
    "                if \"anvil_devs@firecloud.org\" not in ad_membership[\"adminsEmails\"] and \"anvil_devs@firecloud.org\" not in ad_membership[\"membersEmails\"]:\n",
    "                    response = requests.put(\n",
    "                        url=f\"https://api.firecloud.org/api/groups/{auth_domain}/member/anvil_devs@firecloud.org\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 204:\n",
    "                        error_list.append(f\"Error adding anvil_devs to auth domain {auth_domain} ({response.text})\")\n",
    "                \n",
    "                for user in ad_membership[\"adminsEmails\"]:\n",
    "                    if user == \"anvil-admins@firecloud.org\":\n",
    "                        anvil_admins_found = True\n",
    "                        break \n",
    "            except:\n",
    "                error_list.append(f\"Error retrieving membership for auth domain {auth_domain} ({response.text})\")\n",
    "\n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([snapshot_id, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"snapshot_id\", \"update_status\", \"errors\"])\n",
    "    display(results_df) \n",
    "    \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of workspaces to update permissions for\n",
    "workspace_list = [\n",
    "#    {\"workspace_name\": \"workspace name\", \"cloud_provider\": \"Cloud provider - gcp/azure\"    }\n",
    "    {'workspace_name': 'temp_test', 'cloud_provider': 'gcp'},\n",
    "]\n",
    "\n",
    "# Specify the list of snapshots to update permissions for\n",
    "snapshot_id_list = [\n",
    "    \"snapshot_id\",\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "if workspace_list:\n",
    "    print(\"Updating workspace permissions...\")\n",
    "    update_workspace_permissions_for_release(workspace_list)\n",
    "if snapshot_id_list:\n",
    "    print(\"Updating snapshot permissions...\")\n",
    "    update_snapshot_permissions_for_release(snapshot_id_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Update AnVIL Resource Permissions for Deletion\n",
    "Prior to the deletion of a workspace or snapshot, there may be a desire to perform a \"scream test\" where user access is removed from the workspace or snapshot for a period of time before deletion to see if any issues arise. This script supports this by:\n",
    "* Ensuring anvil-admins is an owner on the workspace or snapshot.  \n",
    "* Removing all users outside of the exemption list from the workspace or snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     110
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def update_workspace_permissions_for_deletion(workspace_list, user_exemption_list):\n",
    "    # Set validation values\n",
    "    acceptable_cloud_types = ['gcp', 'azure']\n",
    "    \n",
    "    # Loop through and process workspaces\n",
    "    results = []\n",
    "    for workspace_entry in workspace_list:\n",
    "\n",
    "        # Set variables\n",
    "        workspace = workspace_entry['workspace_name']\n",
    "        cloud_provider = workspace_entry['cloud_provider']\n",
    "        if cloud_provider not in acceptable_cloud_types:\n",
    "            cloud_types = ', '.join(acceptable_cloud_types)\n",
    "            results.append(\n",
    "                [workspace, \"Failure\", f\"Cloud provider must be one of {cloud_types}. Cloud provider provided was {cloud_provider}. Aborting workspace deletion.\"]\n",
    "            )\n",
    "            continue\n",
    "        billing_project = 'anvil-datastorage' if cloud_provider == 'gcp' else 'AnVILDataStorage_Azure'\n",
    "\n",
    "        # Initialize\n",
    "        print(f\"Processing updates for {workspace}.\")\n",
    "        error_list = []\n",
    "        \n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "        \n",
    "        # Pull workspace details\n",
    "        resource_id = \"\"\n",
    "        ws_attributes = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{workspace}?fields=workspace.attributes,workspace.authorizationDomain,workspace.googleProject,workspace.bucketName,workspace.workspaceId\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        try:\n",
    "            resource_id = ws_attributes[\"workspace\"][\"workspaceId\"]\n",
    "        except:\n",
    "            error_list.append(f\"Error accessing workspace.\")\n",
    "\n",
    "        # Pull existing workspace ACLs\n",
    "        response = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{workspace}/acl\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(\"Error retrieving workspace ACL\")\n",
    "            ws_acl = {\"acl\": {}}\n",
    "        else:\n",
    "            ws_acl = response.json()\n",
    "\n",
    "        # Add anvil-admins as an owner, regardless of current status\n",
    "        payload = [{\n",
    "                \"email\": \"anvil-admins@firecloud.org\",\n",
    "                \"accessLevel\": \"OWNER\",\n",
    "                \"canShare\": True,\n",
    "                \"canCompute\": True\n",
    "            }]\n",
    "        response = requests.patch(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{workspace}/acl\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "            json=payload\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(\"Error updating workspace ACL\")\n",
    "        \n",
    "        # Loop through workspace ACLs and adjust permissions where required\n",
    "        user_exemption_list.append(\"anvil-admins@firecloud.org\")\n",
    "        for key, val in ws_acl[\"acl\"].items():\n",
    "            if key not in user_exemption_list:\n",
    "                # Determine if the user is yourself, and remove yourself if so\n",
    "                response = requests.get(\n",
    "                    url=f\"https://sam.dsde-prod.broadinstitute.org/register/user/v2/self/info\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                ).json()\n",
    "                if key == response[\"userEmail\"]:\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://sam.dsde-prod.broadinstitute.org/api/resources/v2/workspace/{resource_id}/leave\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 204:\n",
    "                        error_list.append(\"Error updating workspace ACL\")\n",
    "                else:\n",
    "                    payload = [{\n",
    "                        \"email\": key,\n",
    "                        \"accessLevel\": \"NO ACCESS\",\n",
    "                        \"canShare\": False,\n",
    "                        \"canCompute\": False\n",
    "                    }]\n",
    "                    response = requests.patch(\n",
    "                        url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{workspace}/acl\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                        json=payload\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(\"Error updating workspace ACL\")\n",
    "\n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([workspace, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace\", \"update_status\", \"errors\"])\n",
    "    display(results_df) \n",
    "    \n",
    "def update_snapshot_permissions_for_deletion(snapshot_id_list, user_exemption_list):\n",
    "    # Loop through and process snapshots\n",
    "    results = []\n",
    "    for snapshot_id in snapshot_id_list:\n",
    "\n",
    "        # Initialize\n",
    "        print(f\"Processing updates for {snapshot_id}.\")\n",
    "        error_list = []\n",
    "\n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Ensure anvil-admins is a steward on the snapshot\n",
    "        payload = {\n",
    "            \"email\": \"anvil-admins@firecloud.org\"\n",
    "            }\n",
    "        response = requests.post(\n",
    "            url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/policies/steward/members\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "            json=payload\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(\"Error updating snapshot policies\")\n",
    "        \n",
    "        # Pull snapshot auth domains and policies\n",
    "        ad_list = []\n",
    "        ss_steward_list = []\n",
    "        ss_reader_list = []\n",
    "        snapshot_policies = requests.get(\n",
    "            url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/policies\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        try:\n",
    "            ad_list = snapshot_policies[\"authDomain\"]\n",
    "            for policy in snapshot_policies[\"policies\"]:\n",
    "                if policy[\"name\"] == \"steward\":\n",
    "                    for member in policy[\"members\"]:\n",
    "                        ss_steward_list.append(member)\n",
    "                elif policy[\"name\"] == \"reader\":\n",
    "                    for member in policy[\"members\"]:\n",
    "                        ss_reader_list.append(member)\n",
    "        except:\n",
    "            error_list.append(f\"Error accessing snapshot policies.\")\n",
    "\n",
    "        # Loop through snapshot policies and reduce permissions where necessary\n",
    "        if \"anvil-admins@firecloud.org\" in ss_steward_list:\n",
    "            for ss_steward in ss_steward_list:\n",
    "                if ss_steward != \"anvil-admins@firecloud.org\" and ss_steward not in user_exemption_list:\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/policies/steward/members/{ss_steward}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(\"Error updating snapshot policies\")\n",
    "            for ss_reader in ss_reader_list:\n",
    "                if ss_reader not in user_exemption_list:\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/policies/reader/members/{ss_reader}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(\"Error updating snapshot policies\") \n",
    "\n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([snapshot_id, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"snapshot_id\", \"update_status\", \"errors\"])\n",
    "    display(results_df) \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of users whose permissions should not be altered (anvil-admins is included by default)\n",
    "user_exemption_list = [\n",
    "]\n",
    "\n",
    "# Specify the list of workspaces to remove permissions for\n",
    "workspace_list = [\n",
    "#    {\"workspace_name\": \"workspace name\", \"cloud_provider\": \"Cloud provider - gcp/azure\"    }\n",
    "     {'workspace_name': 'temp_test', 'cloud_provider': 'gcp'},\n",
    "]\n",
    "\n",
    "# Specify the list of snapshots to remove permissions for\n",
    "snapshot_id_list = [\n",
    "    \"snapshot_id\",\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "if workspace_list:\n",
    "    print(\"Updating workspace permissions...\")\n",
    "    update_workspace_permissions_for_deletion(workspace_list, user_exemption_list)\n",
    "if snapshot_id_list:\n",
    "    print(\"Updating snapshot permissions...\")\n",
    "    update_snapshot_permissions_for_deletion(snapshot_id_list, user_exemption_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Add Authorization Domain Group to Existing Workspace\n",
    "**AZURE ONLY** \n",
    "Adds a data access control group to an existing workspace. This is useful in cases where a data access control group wasn't added at the time of workspace creation and needs to be added, or additional data access control groups need to be added to a workspace (for whatever reason)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def add_data_access_control_group(workspace_list):\n",
    "    # Loop through and process workspaces\n",
    "    results = []\n",
    "    acceptable_cloud_types = ['azure']\n",
    "    \n",
    "    for workspace in workspace_list:\n",
    "        # Initialize \n",
    "        name = workspace['workspace_name']\n",
    "        ad_name = workspace['auth_domain']\n",
    "        cloud_provider = workspace['cloud_provider']\n",
    "        error_list = []\n",
    "        warning_list = []\n",
    "        \n",
    "        # Get billing project based on cloud provider\n",
    "        print(f\"Processing workspace {name}...\")\n",
    "        if cloud_provider not in acceptable_cloud_types:\n",
    "            cloud_types = ', '.join(acceptable_cloud_types)\n",
    "            err_str = f\"Cloud provider must be one of {cloud_types}. Cloud provider provided was {cloud_provider}. Skipping workspace.\"\n",
    "            print(err_str)\n",
    "            results.append([name, \"N/A\", ad_name, \"Failure\", err_str, None])\n",
    "            continue\n",
    "        billing_project = 'anvil-datastorage' if workspace['cloud_provider'] == 'gcp' else 'AnVILDataStorage_Azure'\n",
    "\n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Get workspace ID for the workspace\n",
    "        response = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{name}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        response_json = response.json()\n",
    "        if response.status_code != 200:\n",
    "            results.append([name, billing_project, ad_name, \"Failure\", \"Workspace not found (or user does not have access). Skipping workspace.\", None])\n",
    "            continue\n",
    "        else:\n",
    "            workspace_id = response_json[\"workspace\"][\"workspaceId\"]\n",
    "\n",
    "        # Validate the specified auth domain group\n",
    "        if ad_name:\n",
    "            response = requests.get(\n",
    "                url=f\"https://api.firecloud.org/api/groups/{ad_name}\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            )\n",
    "            if response.status_code == 404:\n",
    "                results.append([name, billing_project, ad_name, \"Failure\", \"Specified auth domain group does not exist. Skipping workspace.\", None])\n",
    "                continue\n",
    "            elif response.status_code not in (200, 403):\n",
    "                results.append([name, billing_project, ad_name, \"Failure\", \"Error validating specified auth domain group. Please try again. Skipping workspace.\", None])\n",
    "                continue\n",
    "        else:\n",
    "            results.append([name, billing_project, ad_name, \"Failure\", \"No auth domain group specified. Skipping workspace.\", None])\n",
    "            continue\n",
    "        \n",
    "        # Build and submit data access control group policy request\n",
    "        add_policy_payload = {\n",
    "            \"addAttributes\": {\n",
    "                \"inputs\": [{\n",
    "                    \"namespace\": \"terra\",\n",
    "                    \"name\": \"group-constraint\",\n",
    "                    \"additionalData\": [{\"key\": \"group\", \"value\": ad_name}]\n",
    "                }]\n",
    "              },\n",
    "              \"updateMode\": \"FAIL_ON_CONFLICT\"\n",
    "        }\n",
    "        response = requests.patch(\n",
    "            url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{workspace_id}/policies\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "            json=add_policy_payload \n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_message = response.text\n",
    "            results.append([name, billing_project, ad_name, \"Failure\", f\"Error updating workspace: {error_message}.\", None])\n",
    "            continue\n",
    "\n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        warning_str = \"; \".join(warning_list)\n",
    "        results.append([name, billing_project, ad_name, status, error_str, warning_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace_name\", \"billing_project\", \"auth_domain_name\", \"status\", \"errors\", \"warnings\"])\n",
    "    display(results_df) \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the name of the workspace, the name of the auth domain group to add to the workspace, \n",
    "# and the cloud the workspace lives on:\n",
    "workspace_list = [\n",
    "#    {\n",
    "#        \"workspace_name\": \"workspace name\",\n",
    "#        \"auth_domain\": \"auth domain (or None). For Azure this is just a Terra group that gets added\",\n",
    "#        \"cloud_provider\": \"Cloud provider - azure\"    \n",
    "#    }\n",
    "    {\n",
    "        \"workspace_name\": \"AnVIL_TEST_WORKSPACE\",\n",
    "        \"auth_domain\": \"AUTH_AnVIL_TEST_WORKSPACE\",\n",
    "        \"cloud_provider\": \"azure\"\n",
    "    },\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "add_data_access_control_group(workspace_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnVIL Resource Creation and Deletion Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Workspace and Auth Domain Group Creation\n",
    "List the desired workspaces to create, the authorization domain group they should have (if any), the role the authorization domain group should have on the workspace once created, and the cloud the workspace should be created on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     42
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def setup_auth_domain(name):\n",
    "    # Initialize\n",
    "    log_items = []\n",
    "    \n",
    "    # Establish credentials\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    \n",
    "    # Attempt to create the group\n",
    "    response = requests.post(\n",
    "        url=f\"https://api.firecloud.org/api/groups/{name}\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "    )\n",
    "    if response.status_code == 409:\n",
    "        log_items.append(\"Error creating auth domain group (group already exists)\")\n",
    "    elif response.status_code != 201:\n",
    "        raise Exception(\"Error creating auth domain group.\")\n",
    "    \n",
    "    # Attempt to add anvil-admins as an admin on the group\n",
    "    response = requests.put(\n",
    "        url=f\"https://api.firecloud.org/api/groups/{name}/admin/anvil-admins@firecloud.org\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "    )\n",
    "    if response.status_code != 204:\n",
    "        log_items.append(\"Error adding anvil-admins as an admin on the auth domain group\") \n",
    "    \n",
    "    # Attempt to add AnVIL_Devs as a member on the group\n",
    "    response = requests.put(\n",
    "        url=f\"https://api.firecloud.org/api/groups/{name}/member/AnVIL_Devs@firecloud.org\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "    )\n",
    "    if response.status_code != 204:\n",
    "        log_items.append(\"Error adding AnVIL_Devs as a member on the auth domain group\")\n",
    "        \n",
    "    # Return log\n",
    "    return log_items\n",
    "\n",
    "def setup_workspaces(workspace_list):\n",
    "    # Loop through and process workspaces\n",
    "    results = []\n",
    "    acceptable_cloud_types = ['gcp', 'azure']\n",
    "    \n",
    "    for workspace in workspace_list:\n",
    "        # Initialize \n",
    "        name = workspace['workspace_name']\n",
    "        ad_name = workspace['auth_domain']\n",
    "        ad_role = workspace['role']\n",
    "        cloud_provider = workspace['cloud_provider']\n",
    "        bucket = \"\"\n",
    "        error_list = []\n",
    "        warning_list = []\n",
    "        \n",
    "        # Get billing project based on cloud provider\n",
    "        print(f\"Processing workspace {name}...\")\n",
    "        if cloud_provider not in acceptable_cloud_types:\n",
    "            cloud_types = ', '.join(acceptable_cloud_types)\n",
    "            err_str = f\"Cloud provider must be one of {cloud_types}. Cloud provider provided was {cloud_provider}. Skipping workspace.\"\n",
    "            logging.error(err_str)\n",
    "            results.append([name, \"N/A\", ad_name, ad_role, \"Failure\", err_str, None])\n",
    "            continue\n",
    "        billing_project = 'anvil-datastorage' if workspace['cloud_provider'] == 'gcp' else 'AnVILDataStorage_Azure'\n",
    "\n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Determine whether the workspace name is taken\n",
    "        response = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{name}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            results.append([name, billing_project, ad_name, ad_role, \"\", \"Failure\", \"Specified workspace name already exists. Aborting workspace creation.\", None])\n",
    "            continue\n",
    "\n",
    "        # Set up auth domain (where specified)\n",
    "        if ad_name:\n",
    "            try:\n",
    "                ad_output = setup_auth_domain(ad_name)\n",
    "                warning_list.extend(ad_output)\n",
    "            except:\n",
    "                results.append([name, billing_project, ad_name, ad_role, \"\", \"Failure\", \"Error creating the auth domain for the workspace. Aborting workspace creation.\", None])\n",
    "                continue\n",
    "\n",
    "        # Build workspace creation payload\n",
    "        create_workspace_payload = {\n",
    "            \"namespace\": billing_project,\n",
    "            \"name\": name,\n",
    "            \"attributes\": {}\n",
    "        }\n",
    "        if ad_name:\n",
    "            create_workspace_payload[\"authorizationDomain\"] = [{\"membersGroupName\": ad_name}]\n",
    "            if cloud_provider == \"azure\":\n",
    "                create_workspace_payload[\"policies\"] = [\n",
    "                    {\n",
    "                        \"namespace\": \"terra\",\n",
    "                        \"name\": \"group-constraint\",\n",
    "                        \"additionalData\": [{\"group\": ad_name}]\n",
    "                    }\n",
    "                ]\n",
    "        else:\n",
    "            create_workspace_payload[\"authorizationDomain\"] = []\n",
    "        \n",
    "        # Submit create workspace request\n",
    "        response = requests.post(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "            json=create_workspace_payload \n",
    "        )\n",
    "        if response.status_code == 409:\n",
    "            results.append([name, billing_project, ad_name, ad_role, \"Failure\", \"Specified workspace name already exists. Aborting workspace creation.\", None])\n",
    "            continue\n",
    "        elif response.status_code != 201:\n",
    "            results.append([name, billing_project, ad_name, ad_role, \"Failure\", \"Error creating workspace. Aborting workspace creation.\", None])\n",
    "            continue\n",
    "        else:\n",
    "            workspace_json = response.json()\n",
    "            bucket = workspace_json[\"bucketName\"]\n",
    "\n",
    "        # Update workspace ACL for anvil-admins\n",
    "        payload = [{\n",
    "            \"email\": \"anvil-admins@firecloud.org\",\n",
    "            \"accessLevel\": \"OWNER\",\n",
    "            \"canShare\": True,\n",
    "            \"canCompute\": True\n",
    "        }]\n",
    "        response = requests.patch(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{name}/acl\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "            json=payload\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            warning_list.append(\"Error adding anvil-admins as owner on the workspace.\")\n",
    "\n",
    "        # Update workspace ACL for auth domain\n",
    "        if ad_name and ad_role in [\"READER\", \"WRITER\", \"OWNER\"]:\n",
    "            ad_email = ad_name + \"@firecloud.org\"\n",
    "            payload = [{\n",
    "                \"email\": ad_email,\n",
    "                \"accessLevel\": ad_role,\n",
    "                \"canShare\": False,\n",
    "                \"canCompute\": False\n",
    "            }]\n",
    "            response = requests.patch(\n",
    "                url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{name}/acl\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                json=payload\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                warning_list.append(\"Error updating auth domain group's role on the workspace.\")\n",
    "\n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        warning_str = \"; \".join(warning_list)\n",
    "        results.append([name, billing_project, ad_name, ad_role, bucket, status, error_str, warning_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace_name\", \"billing_project\", \"auth_domain_name\", \"auth_domain_role\", \"bucket\", \"status\", \"errors\", \"warnings\"])\n",
    "    display(results_df) \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the name of the workspace, the name of the auth domain group to use for the workspace (if any), \n",
    "# the role the auth domain group should have on the workspace, and the cloud the workspace lives on:\n",
    "workspace_list = [\n",
    "#    {\"workspace_name\": \"workspace name\", \"cloud_provider\": \"gcp/azure\", \"auth_domain\": \"auth domain (or None)\", \"role\": \"Auth Domain group role - READER, WRITER, OWNER, NO ACCESS\"},\n",
    "    {\"workspace_name\": \"AnVIL_NIAID_CSP_GRU_WGS_v3\", \"cloud_provider\": \"gcp\", \"auth_domain\": \"AUTH_AnVIL_NIAID_CSP_GRU_WGS_v3\", \"role\": \"READER\"},\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "setup_workspaces(workspace_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Workspace Deletion\n",
    "List the desired workspaces to delete, as well as which cloud they exist on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def delete_workspaces(workspace_list):\n",
    "    # Loop through and process workspaces\n",
    "    results = []\n",
    "    acceptable_cloud_types = ['gcp', 'azure']\n",
    "    \n",
    "    for workspace in workspace_list:\n",
    "        # Initialize\n",
    "        name = workspace['workspace_name']\n",
    "        cloud_provider = workspace['cloud_provider']\n",
    "        \n",
    "        # Get billing project based on cloud provider\n",
    "        print(f\"Processing workspace {name}...\")\n",
    "        if cloud_provider not in acceptable_cloud_types:\n",
    "            cloud_types = ', '.join(acceptable_cloud_types)\n",
    "            results.append(\n",
    "                [name, \"Failure\", f\"Cloud provider must be one of {cloud_types}. Cloud provider provided was {cloud_provider}. Aborting workspace deletion.\"]\n",
    "            )\n",
    "            continue\n",
    "        billing_project = 'anvil-datastorage' if workspace['cloud_provider'] == 'gcp' else 'AnVILDataStorage_Azure'\n",
    "\n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Delete workspace\n",
    "        response = requests.delete(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{billing_project}/{name}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code == 404:\n",
    "            msg = json.loads(response.text)[\"message\"]\n",
    "            results.append([name, \"Failure\", f\"Specified workspace does not exist. Aborting workspace deletion. Message: {msg}\"])\n",
    "            continue\n",
    "        elif response.status_code == 403:\n",
    "            msg = json.loads(response.text)[\"message\"]\n",
    "            results.append([name, \"Failure\", f\"User does not have permission to delete workspace. Aborting workspace deletion. Message: {msg}\"])\n",
    "            continue\n",
    "        elif response.status_code != 202:\n",
    "            results.append([name, \"Failure\", \"Error deleting workspace (unspecified). Aborting workspace deletion.\"])\n",
    "            continue\n",
    "        else:\n",
    "            results.append([name, \"Success\", \"\"])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace_name\", \"status\", \"message\"])\n",
    "    display(results_df) \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the name of the workspace and the cloud the workspace lives on:\n",
    "workspace_list = [\n",
    "#    {\"workspace_name\": \"workspace name\", \"cloud_provider\": \"Cloud provider - gcp/azure\"    }\n",
    "    {\"workspace_name\": \"AnVIL_TEST_WORKSPACE_AZ\", \"cloud_provider\": \"azure\"},\n",
    "    {\"workspace_name\": \"AnVIL_TEST_WORKSPACE_GCP\", \"cloud_provider\": \"gcp\"}\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "delete_workspaces(workspace_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Unpublish Workspace from Firecloud Library\n",
    "In some cases, a workspace may be published in the Firecloud Library, which prevents it from being deleted (which can be identified by a \"You cannot delete this workspace: You must be a curator and either be an owner or have catalog with read+.\" message during deletion. In those cases, the workspace needs to be unpublished from the Firecloud Library before it can be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def unpublish_workspaces_from_library(workspace_list):\n",
    "    # Loop through and process workspaces\n",
    "    results = []\n",
    "    acceptable_cloud_types = ['gcp', 'azure']\n",
    "    \n",
    "    for workspace in workspace_list:\n",
    "        # Initialize\n",
    "        name = workspace['workspace_name']\n",
    "        cloud_provider = workspace['cloud_provider']\n",
    "        \n",
    "        # Get billing project based on cloud provider\n",
    "        print(f\"Processing workspace {name}...\")\n",
    "        if cloud_provider not in acceptable_cloud_types:\n",
    "            cloud_types = ', '.join(acceptable_cloud_types)\n",
    "            results.append(\n",
    "                [name, \"Failure\", f\"Cloud provider must be one of {cloud_types}. Cloud provider provided was {cloud_provider}. Aborting workspace deletion.\"]\n",
    "            )\n",
    "            continue\n",
    "        billing_project = 'anvil-datastorage' if workspace['cloud_provider'] == 'gcp' else 'AnVILDataStorage_Azure'\n",
    "\n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Unpublish workspace\n",
    "        response = requests.delete(\n",
    "                    url=f\"https://api.firecloud.org/api/library/{billing_project}/{name}/published\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                )\n",
    "        if response.status_code == 404:\n",
    "            msg = json.loads(response.text)[\"message\"]\n",
    "            results.append([name, \"Failure\", f\"Specified workspace does not exist. Aborting workspace unpublishing. Message: {msg}\"])\n",
    "            continue\n",
    "        elif response.status_code == 403:\n",
    "            msg = json.loads(response.text)[\"message\"]\n",
    "            results.append([name, \"Failure\", f\"User does not have permission to delete workspace. Aborting workspace unpublishing. Message: {msg}\"])\n",
    "            continue\n",
    "        elif response.status_code not in [200, 204]:\n",
    "            results.append([name, \"Failure\", \"Error unpublishing workspace (unspecified). Aborting workspace unpublishing.\"])\n",
    "            continue\n",
    "        else:\n",
    "            results.append([name, \"Success\", \"\"])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace_name\", \"status\", \"message\"])\n",
    "    display(results_df) \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the name of the workspace and the cloud the workspace lives on:\n",
    "workspace_list = [\n",
    "#    {\"workspace_name\": \"workspace name\", \"cloud_provider\": \"Cloud provider - gcp/azure\"    }\n",
    "    {\"workspace_name\": \"AnVIL_TEST_WORKSPACE_AZ\", \"cloud_provider\": \"azure\"},\n",
    "    {\"workspace_name\": \"AnVIL_TEST_WORKSPACE_GCP\", \"cloud_provider\": \"gcp\"}\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "unpublish_workspaces_from_library(workspace_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnVIL Workspace Tag Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bulk View Workspace Tags\n",
    "Can be used to view the workspace tags currently recorded on the specified workspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def retrieve_ws_tags(workspace_list):\n",
    "    # Loop through and process workspaces\n",
    "    results = []\n",
    "    for workspace in workspace_list:\n",
    "\n",
    "        # Initialize\n",
    "        print(f\"Retrieving workspace tags for {workspace}.\")\n",
    "        error_list = []\n",
    "\n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Retrieve workspace tags\n",
    "        response = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/anvil-datastorage/{workspace}/tags\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(\"Error retrieving workspace tags.\")\n",
    "            tag_str = \"\"\n",
    "        else:\n",
    "            try:\n",
    "                tag_str = \"\"\n",
    "                for item in response.json():\n",
    "                    tag_str += f\"'{item}', \"\n",
    "            except:\n",
    "                error_list.append(\"Error formatting workspace tags.\")\n",
    "\n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([workspace, tag_str.strip(), status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace\", \"tags\", \"retrieval_status\", \"errors\"])\n",
    "    display(results_df)\n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of workspaces to view the workspace tags for:\n",
    "workspace_list = [\n",
    "    \"WORKSPACE_1\",\n",
    "    \"WORKSPACE_2\"\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "retrieve_ws_tags(workspace_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bulk Update Workspace Tags\n",
    "Can be used to update the workspace tags recorded on the specified workspaces. This includes the ability to both add and remove workspace tags in bulk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def update_ws_tags(workspace_list, tags_to_remove_list, tags_to_remove_regex_list, tags_to_add_list):\n",
    "    # Loop through and process workspaces\n",
    "    results = []\n",
    "    for workspace in workspace_list:\n",
    "\n",
    "        # Initialize\n",
    "        print(f\"Processing workspace tag updates for {workspace}.\")\n",
    "        error_list = []\n",
    "\n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Remove workspace tags explicitly listed\n",
    "        if tags_to_remove_list:\n",
    "            response = requests.delete(\n",
    "                url=f\"https://api.firecloud.org/api/workspaces/anvil-datastorage/{workspace}/tags\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                json=tags_to_remove_list\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                error_list.append(\"Error removing workspace tags by list.\")\n",
    "\n",
    "        # Remove workspace tags by regex\n",
    "        if tags_to_remove_regex_list:\n",
    "            # Retrieve existing workspace tags\n",
    "            response = requests.get(\n",
    "                url=f\"https://api.firecloud.org/api/workspaces/anvil-datastorage/{workspace}/tags\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                error_list.append(\"Error retrieving workspace tags.\")\n",
    "                tag_list = []\n",
    "            else:\n",
    "                tag_list = response.json()\n",
    "\n",
    "            # Compare tags to specified regex to identify tags to remove\n",
    "            regex_removal_list = []\n",
    "            for tag in tag_list:\n",
    "                for regex in tags_to_remove_regex_list:\n",
    "                    if re.search(regex, tag):\n",
    "                        regex_removal_list.append(tag)\n",
    "                        break\n",
    "\n",
    "            # Remove identified workspace tags\n",
    "            if regex_removal_list:\n",
    "                response = requests.delete(\n",
    "                    url=f\"https://api.firecloud.org/api/workspaces/anvil-datastorage/{workspace}/tags\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                    json=regex_removal_list\n",
    "                )\n",
    "                if response.status_code != 200:\n",
    "                    error_list.append(\"Error removing workspace tags by regex.\")    \n",
    "\n",
    "        # Add new workspace tags explicitly listed\n",
    "        if tags_to_add_list:\n",
    "            response = requests.patch(\n",
    "                url=f\"https://api.firecloud.org/api/workspaces/anvil-datastorage/{workspace}/tags\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                json=tags_to_add_list\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                error_list.append(\"Error adding workspace tags by list.\")\n",
    "\n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([workspace, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace\", \"update_status\", \"errors\"])\n",
    "    display(results_df)    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of workspaces to apply the below changes to:\n",
    "workspace_list = [\n",
    "    \"WORKSPACE_1\",\n",
    "    \"WORKSPACE_2\"\n",
    "]\n",
    "\n",
    "# Specify the exact tags that should be removed from the workspace (if any):\n",
    "tags_to_remove_list = []\n",
    "\n",
    "# Specify the list of regex expressions that should be used to identify tags to remove (if any):\n",
    "tags_to_remove_regex_list = []\n",
    "\n",
    "# Specify the tags that should be added to the workspace (if any):\n",
    "tags_to_add_list = []\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "update_ws_tags(workspace_list, tags_to_remove_list, tags_to_remove_regex_list, tags_to_add_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnVIL Workspace Attribute Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Look Up Workspace Attribute Names\n",
    "This is intended to be a quick way to translate from the common/display names for properties that appear in workspaces to the underlying attribute property names needed to update the attributes in any way. For example, \"Study Design\" in a workspace is the display name for the \"library:studyDesign\" property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def look_up_ws_attr(search_term_list):\n",
    "    attr_schema_url = \"https://raw.githubusercontent.com/broadinstitute/firecloud-orchestration/develop/src/main/resources/library/attribute-definitions.json\"\n",
    "    response = requests.get(attr_schema_url)\n",
    "    attr_schema = json.loads(response.text)\n",
    "\n",
    "    results = []\n",
    "    for term in search_term_list:\n",
    "        lookup_str = term.replace(\" \", \"\").lower()\n",
    "        for key, val in attr_schema[\"properties\"].items():\n",
    "            try:\n",
    "                title_str = val[\"title\"].replace(\" \", \"\").lower()\n",
    "                if lookup_str in title_str:\n",
    "                    results.append([term, key, val[\"title\"]])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    print(\"Results:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"search_term\", \"property\", \"property_title\"])\n",
    "    display(results_df) \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify a list of attribute names to look up:\n",
    "search_term_list = [\"Cohort Description\"]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "look_up_ws_attr(search_term_list)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Update Workspace Attributes\n",
    "List the workspace attributes that should be updated, their updated values, and the update behavior. The behavior specified will have the following effects:\n",
    "* __VIEW__ will NOT alter the workspace attributes in any way, but rather can be used to display the existing attribute values \n",
    "* __UPDATE__ will only update the workspace attributes specified by the user, and leave all other existing values as is\n",
    "* __REPLACE__ will remove all existing workspace attributes and replace them with the workspace attributes specified by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     14
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def get_attribute(ws_attribute):\n",
    "    attr_schema_url = \"https://raw.githubusercontent.com/broadinstitute/firecloud-orchestration/develop/src/main/resources/library/attribute-definitions.json\"\n",
    "    response = requests.get(attr_schema_url)\n",
    "    attr_schema = json.loads(response.text)\n",
    "    # Search for attribute and return the type\n",
    "    if attr_schema[\"properties\"].get(ws_attribute):\n",
    "        return True, attr_schema[\"properties\"][ws_attribute][\"type\"]\n",
    "    else:\n",
    "        return False, \"\"\n",
    "\n",
    "def update_ws_attr(attr_updates, update_behavior):\n",
    "    # Loop through and process workspaces\n",
    "    results = []\n",
    "    for workspace in attr_updates.keys():\n",
    "        \n",
    "        # Initialize\n",
    "        print(f\"Processing workspace attribute updates for {workspace}.\")\n",
    "        error_list = []\n",
    "\n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "        \n",
    "        # Set base attribute json according to specified update behavior\n",
    "        if update_behavior in [\"VIEW\", \"UPDATE\"]:\n",
    "            # Fetch existing attributes for workspace\n",
    "            response = requests.get(\n",
    "                url=f\"https://api.firecloud.org/api/library/anvil-datastorage/{workspace}/metadata\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                error_list.append(\"Error retrieving workspace attributes\")\n",
    "            else:\n",
    "                attr_json = response.json() \n",
    "                print(\"Original attribute JSON:\")\n",
    "                print(attr_json)\n",
    "        elif update_behavior == \"REPLACE\":\n",
    "            attr_json = {}\n",
    "        else:\n",
    "            error_list.append(f\"Unknown update behavior '{update_behavior}'' specified (must be VIEW, UPDATE, or REPLACE\")\n",
    "            \n",
    "        # Update base attribute json with user input\n",
    "        if not error_list and update_behavior != \"VIEW\":\n",
    "            for attr_entry, attr_value in attr_updates[workspace].items():\n",
    "                valid, attr_type = get_attribute(attr_entry)\n",
    "                if valid:\n",
    "                    if attr_type == \"string\":\n",
    "                        attr_json[attr_entry] = str(attr_value)\n",
    "                    elif attr_type == \"array\":\n",
    "                        attr_json[attr_entry] = [str(attr_value)]\n",
    "                    elif attr_type == \"integer\":\n",
    "                        try:\n",
    "                            attr_json[attr_entry] = int(attr_value)\n",
    "                        except:\n",
    "                            error_list.append(f\"Error converting value for attribute '{attr_entry}' to integer\")\n",
    "                    elif attr_type == \"boolean\":\n",
    "                        try:\n",
    "                            if str(attr_value).lower() == \"true\":\n",
    "                                attr_json[attr_entry] = True\n",
    "                            elif str(attr_value).lower() == \"false\":\n",
    "                                attr_json[attr_entry] = False\n",
    "                            else:\n",
    "                                raise(exception)\n",
    "                        except:\n",
    "                            error_list.append(f\"Error converting value for attribute '{attr_entry}' to boolean\")\n",
    "                else:\n",
    "                    error_list.append(f\"Attribute '{attr_entry}' is not recognized\")\n",
    "            if update_behavior == \"UPDATE\":\n",
    "                print(\"\\nUpdated attribute JSON:\")\n",
    "            else:\n",
    "                print(\"Replacement attribute JSON:\")\n",
    "            print(attr_json)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        # Submit updated workspace attributes to workspace\n",
    "        if not error_list and update_behavior != \"VIEW\":\n",
    "            response = requests.put(\n",
    "                url=f\"https://api.firecloud.org/api/library/anvil-datastorage/{workspace}/metadata?validate=false\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                json=attr_json\n",
    "            )\n",
    "            if response.status_code != 200:\n",
    "                error_list.append(\"Error updating workspace attributes\")\n",
    "        \n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Errors\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([workspace, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"Results:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace\", \"update_status\", \"errors\"])\n",
    "    display(results_df)   \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the behavior of the update (VIEW, UPDATE, or REPLACE), based on the definitions above:\n",
    "update_behavior = \"VIEW\"\n",
    "\n",
    "# Specify the workspace attribute updates, using the form {\"workspace\": {\"attribute\": \"value\"}}:\n",
    "attr_updates = {\n",
    "    \"workspace\": {\"attribute_1\": \"value\", \"attribute_2\": \"value\"}\n",
    "}\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "update_ws_attr(attr_updates, update_behavior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snapshot Property Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Bulk Update Snapshot Properties\n",
    "List the snapshots that should be updated and the updated property values that should be applied to them. The properties that can be updated are as follows:\n",
    "* **phs_id** updates the \"phsId\" property on the dataset the snapshot is sourced from, if you have permissions to do so\n",
    "* **consent_code** updates the \"consentCode\" property on the snapshot\n",
    "* **consent_name** updates the \"properties\" property on the dataset the snapshot is sourced from to include the consent name, if you have permissions to do so\n",
    "* **duos_id** attaches a DUOS ID to the snapshot, which populates the \"duosFirecloudGroup\" property on the snapshot\n",
    "* **dataset_ticket** updates the \"properties\" property on the dataset the snapshot is sourced from to include the dataset ticket, if you have permissions to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def update_snapshot_properties(snapshot_update_list, ignore_empty):\n",
    "    # Loop through and process update requests\n",
    "    results = []\n",
    "    for snapshot_update_entry in snapshot_update_list:\n",
    "        snapshot_id = snapshot_update_entry[\"snapshot_id\"]\n",
    "        dataset_id = \"\"\n",
    "        phs_id = snapshot_update_entry[\"phs_id\"] if snapshot_update_entry[\"phs_id\"] else \"\"\n",
    "        consent_code = snapshot_update_entry[\"consent_code\"] if snapshot_update_entry[\"consent_code\"] else \"\"\n",
    "        consent_name = snapshot_update_entry[\"consent_name\"] if snapshot_update_entry[\"consent_name\"] else \"\"\n",
    "        duos_id = snapshot_update_entry[\"duos_id\"] if snapshot_update_entry[\"duos_id\"] else \"\"\n",
    "        dataset_ticket = snapshot_update_entry[\"dataset_ticket\"] if snapshot_update_entry[\"dataset_ticket\"] else \"\"\n",
    "        current_phs_id = \"\"\n",
    "        current_consent_code = \"\"\n",
    "        current_duos_id = \"\"\n",
    "        current_dataset_ticket = \"\"\n",
    "        print(f\"Processing update request for snapshot {snapshot_id}: {str(snapshot_update_entry)}\")\n",
    "        \n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "        \n",
    "        # Retrieve snapshot\n",
    "        status = \"Success\"\n",
    "        error_list = []\n",
    "        if phs_id or consent_name or dataset_ticket or consent_code or duos_id or ignore_empty == False:\n",
    "            snapshot_response = requests.get(\n",
    "                url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "            ).json()\n",
    "            try:\n",
    "                dataset_id = snapshot_response[\"source\"][0][\"dataset\"][\"id\"]\n",
    "                current_phs_id = snapshot_response[\"source\"][0][\"dataset\"][\"phsId\"]\n",
    "                current_consent_code = snapshot_response[\"consentCode\"]\n",
    "                dataset_props = snapshot_response[\"source\"][0][\"datasetProperties\"]\n",
    "                if dataset_props:\n",
    "                    current_consent_name = dataset_props.get(\"consent_name\")\n",
    "                    current_dataset_ticket = dataset_props.get(\"dataset_ticket\")\n",
    "                else:\n",
    "                    current_consent_name = \"\"\n",
    "                    current_dataset_ticket = \"\"\n",
    "                    dataset_props = {}\n",
    "                current_duos_id = snapshot_response[\"duosFirecloudGroup\"].get(\"duosId\") if snapshot_response[\"duosFirecloudGroup\"] != None else \"\"\n",
    "            except:\n",
    "                error_list.append(f\"Error retrieving snapshot ({str(e)})\")\n",
    "        if error_list:\n",
    "            error_str = \"; \".join(error_list)\n",
    "            results.append([snapshot_id, \"phs_id\", phs_id, status, error_str])\n",
    "            results.append([snapshot_id, \"consent_code\", consent_code, status, error_str])\n",
    "            results.append([snapshot_id, \"consent_name\", consent_name, status, error_str])\n",
    "            results.append([snapshot_id, \"duos_id\", duos_id, status, error_str])\n",
    "        else:\n",
    "            \n",
    "            # Process phs_id, consent_name, and dataset_ticket update\n",
    "            status = \"Success\"\n",
    "            error_list = []\n",
    "            if phs_id or consent_name or dataset_ticket or ignore_empty == False:\n",
    "\n",
    "                # Update dataset\n",
    "                if dataset_id:\n",
    "                    payload = {}\n",
    "                    if ignore_empty == False or (phs_id and phs_id != current_phs_id):\n",
    "                        payload[\"phsId\"] = phs_id\n",
    "                    if ignore_empty == False or (consent_name and consent_name != current_consent_name):\n",
    "                        dataset_props[\"consent_name\"] = consent_name\n",
    "                        payload[\"properties\"] = dataset_props\n",
    "                    if ignore_empty == False or (dataset_ticket and dataset_ticket != current_dataset_ticket):\n",
    "                        dataset_props[\"dataset_ticket\"] = dataset_ticket\n",
    "                        payload[\"properties\"] = dataset_props\n",
    "                    if payload:\n",
    "                        response = requests.patch(\n",
    "                            url=f\"https://data.terra.bio/api/repository/v1/datasets/{dataset_id}\",\n",
    "                            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                            json=payload\n",
    "                        )\n",
    "                        if response.status_code != 200:\n",
    "                            error_list.append(f\"Error updating dataset ({response.text})\")\n",
    "\n",
    "                # Record status\n",
    "                status = \"Success\" if not error_list else \"Failure\"\n",
    "                error_str = \"; \".join(error_list)\n",
    "                if phs_id or ignore_empty == False:\n",
    "                    results.append([snapshot_id, \"phs_id\", phs_id, status, error_str])\n",
    "                else:\n",
    "                    results.append([snapshot_id, \"phs_id\", phs_id, \"Ignored\", \"\"]) \n",
    "                if consent_name or ignore_empty == False:\n",
    "                    results.append([snapshot_id, \"consent_name\", consent_name, status, error_str])\n",
    "                else:\n",
    "                    results.append([snapshot_id, \"consent_name\", consent_name, \"Ignored\", \"\"]) \n",
    "                if dataset_ticket or ignore_empty == False:\n",
    "                    results.append([snapshot_id, \"dataset_ticket\", dataset_ticket, status, error_str])\n",
    "                else:\n",
    "                    results.append([snapshot_id, \"dataset_ticket\", dataset_ticket, \"Ignored\", \"\"]) \n",
    "\n",
    "            # Process consent_code update\n",
    "            status = \"Success\"\n",
    "            error_list = []\n",
    "            if consent_code or ignore_empty == False:\n",
    "\n",
    "                # Update snapshot\n",
    "                if consent_code != current_consent_code:\n",
    "                    payload = {\n",
    "                        \"consentCode\": consent_code\n",
    "                        }\n",
    "                    response = requests.patch(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                        json=payload\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(f\"Error updating snapshot ({response.text})\")\n",
    "\n",
    "                # Record status\n",
    "                status = \"Success\" if not error_list else \"Failure\"\n",
    "                error_str = \"; \".join(error_list)\n",
    "                results.append([snapshot_id, \"consent_code\", consent_code, status, error_str])\n",
    "            else:\n",
    "                results.append([snapshot_id, \"consent_code\", consent_code, \"Ignored\", \"\"])\n",
    "\n",
    "            # Process duos_id update\n",
    "            status = \"Success\"\n",
    "            error_list = []\n",
    "            if duos_id or ignore_empty == False:\n",
    "\n",
    "                # Update snapshot\n",
    "                if not duos_id:\n",
    "                    response = requests.delete(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/unlinkDuosDataset\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(f\"Error updating snapshot ({response.text})\")\n",
    "                elif duos_id != current_duos_id:\n",
    "                    response = requests.put(\n",
    "                        url=f\"https://data.terra.bio/api/repository/v1/snapshots/{snapshot_id}/linkDuosDataset/{duos_id}\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                    )\n",
    "                    if response.status_code != 200:\n",
    "                        error_list.append(f\"Error updating snapshot ({response.text})\")\n",
    "\n",
    "                # Record status\n",
    "                status = \"Success\" if not error_list else \"Failure\"\n",
    "                error_str = \"; \".join(error_list)\n",
    "                results.append([snapshot_id, \"duos_id\", duos_id, status, error_str])\n",
    "            else:\n",
    "                results.append([snapshot_id, \"duos_id\", duos_id, \"Ignored\", \"\"])\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"snapshot_id\", \"property\", \"value\", \"update_status\", \"errors\"])\n",
    "    display(results_df) \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of snapshots to update and the updated property values to apply\n",
    "snapshot_update_list = [\n",
    "#     {\"snapshot_id\": \"snapshot_id\", \"phs_id\": \"phs000123\", \"consent_code\": \"c1\", \"consent_name\": \"DUOS-123456\", \"duos_id\": \"\", \"dataset_ticket\": \"ANVIL-123\"}\n",
    "#     {'snapshot_id': '1c1804a9-990d-4238-b7e4-b6d27691794a', 'phs_id': 'phs001211', 'consent_code': 'c1', 'consent_name': 'HMB-IRB-NPU-MDS', 'duos_id': '', 'dataset_ticket': 'ANVIL-69'},\n",
    "    {'snapshot_id': 'a4512cc0-f2b7-453a-82e2-1a6bf6a2c2eb', 'phs_id': '', 'consent_code': '', 'consent_name': '', 'duos_id': '', 'dataset_ticket': 'ANVIL-767'},\n",
    "]\n",
    "\n",
    "# Specify whether empty properties should be ignored when performing the update. Setting this to \"false\" will instead\n",
    "# overwrite the existing property values with a blank value.\n",
    "ignore_empty = True\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "update_snapshot_properties(snapshot_update_list, ignore_empty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnVIL File Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Sync between two google buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Syncs files/folders between two GCS directories. \n",
    "- Input: List of tuples, where the first value is the gsURI of the source directory and the second value is the gsURI of the destination directory: ('source', 'destination')\n",
    "- This will not remove any files from either directory.\n",
    "- This will only sync files which are not the same between the two directories.\n",
    "\n",
    "Note: This may take some time, depending on the number of files and the number of directories to sync between. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import argparse\n",
    "\n",
    "########## INPUTS ##########\n",
    "directories = [\n",
    "    ('gs://fc-secure-3cfbf1b0-bde8-454b-804f-b648b222ea2b/gru/20230517/', 'gs://fc-secure-5a44bb4a-54c7-4a0a-856d-aeb1d5fd3056/')\n",
    "]\n",
    "############################\n",
    "for source_directory, destination_directory in directories:\n",
    "    print(f\"Running rsync for source directory: {source_directory} to destination directory: {destination_directory}\")\n",
    "    subprocess.call(\n",
    "        [\"gsutil\", \"-m\", \"rsync\", \"-r\",\n",
    "            source_directory, destination_directory]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
