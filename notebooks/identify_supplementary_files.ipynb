{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version\n",
    "#print('Version 1.0.0: 2/22/2023 8:30pm - Nate Calvanese - Initial version')\n",
    "print('Version 1.0.1: 3/13/2023 2:14pm - Nate Calvanese - Updated query for identifying records to flag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ingest_pipeline_utilities.ipynb\n",
      "Version 1.0.25: 3/10/2023 8:46am - Nate Calvanese - Turned on the predictable file IDs dataset creation parameter\n",
      "importing Jupyter notebook from source_files_creation.ipynb\n",
      "Version 1.0.9: 2/25/2023 3:15pm - Nate Calvanese - Replaced FAPI with utils functions\n",
      "importing Jupyter notebook from build_file_inventory.ipynb\n",
      "Version 2.0.0: 3/7/2022 9:32pm - Nate Calvanese - Massive performance improvement with use of gsutil parsing\n",
      "importing Jupyter notebook from process_table_data.ipynb\n",
      "Version: 1.0.9: 3/8/2023 12:09pm - Nate Calvanese - Performance improvements for file ref lookups\n",
      "importing Jupyter notebook from build_mapping_query.ipynb\n",
      "Version 1.0.12: 2/21/2023 2:50pm - Nate Calvanese - Added support for $BQ_DATASET substitution variable\n",
      "importing Jupyter notebook from output_data_validation.ipynb\n",
      "Version 2.0.6: 2/28/2023 11:33am -- Updated notebook to be usable in dev (removed TDR host hardcoding)\n",
      "importing Jupyter notebook from resolve_dangling_foreign_keys.ipynb\n",
      "Version 1.0.1: 10/24/2022 3:18pm - Nate Calvanese - Added pass through of params dict\n",
      "importing Jupyter notebook from infer_file_relationships.ipynb\n",
      "Version 1.0.1: 11/8/2022 4:03pm - Nate Calvanese - Expanded query to cover additional cases\n",
      "importing Jupyter notebook from identify_supplementary_files.ipynb\n",
      "Version 1.0.1: 3/13/2023 2:14pm - Nate Calvanese - Updated query for identifying records to flag\n"
     ]
    }
   ],
   "source": [
    "## imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import data_repo_client\n",
    "from google.cloud import bigquery\n",
    "import ingest_pipeline_utilities as utils\n",
    "import output_data_validation as odv\n",
    "import logging\n",
    "from time import sleep\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify supplementary files and update anvil_file records to properly mark them\n",
    "def identify_supplementary_files(params, dataset_id):\n",
    "    \n",
    "    # Establish TDR API client and retrieve the schema for the specified dataset\n",
    "    logging.info(\"Attempting to identify the TDR object, and the necessary attributes...\")\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    full_tdr_schema, bq_project, bq_schema, skip_bq_queries = odv.retrieve_tdr_schema(dataset_id, \"dataset\", api_client)\n",
    "    if skip_bq_queries:\n",
    "        return \"Error retrieving BQ project and schema\", \"Error\"\n",
    "    \n",
    "    # Determine if field needs to be added, and add if so\n",
    "    logging.info(\"Adding anvil_file.is_supplementary to dataset schema, if necessary.\")\n",
    "    field_found = False\n",
    "    for table in full_tdr_schema[\"tables\"]:\n",
    "        if table[\"name\"] == \"anvil_file\":\n",
    "            for col in table[\"columns\"]:\n",
    "                if col[\"name\"] == \"is_supplementary\":\n",
    "                    field_found = True\n",
    "                    logging.info(\"Field already found! Skipping schema update.\")\n",
    "                    break\n",
    "            break\n",
    "    if field_found == False:\n",
    "        logging.info(\"Field not found. Running dataset schema update.\")\n",
    "        schema_update_request = {\n",
    "            \"description\": \"Adding is_supplementary column to anvil_file\",\n",
    "            \"changes\": {\n",
    "                \"addColumns\": [\n",
    "                  {\n",
    "                    \"tableName\": \"anvil_file\",\n",
    "                    \"columns\": [\n",
    "                      {\n",
    "                        \"name\": \"is_supplementary\",\n",
    "                        \"datatype\": \"boolean\",\n",
    "                        \"array_of\": False,\n",
    "                        \"required\": False\n",
    "                      }\n",
    "                    ]\n",
    "                  }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                schema_update_result, job_id = utils.wait_for_tdr_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "                logging.info(\"Dataset schema update succeeded!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error on dataset schema update: {}\".format(str(e)))\n",
    "                attempt_counter += 1\n",
    "                if attempt_counter < 2:\n",
    "                    logging.info(\"Retrying dataset schema update (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(15)\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Unable to update dataset schema.\")\n",
    "                    return \"Error updating TDR dataset schema\", \"Error\"\n",
    "        \n",
    "    # Re-process anvil_file data to include is_supplementary (where appropriate) and ingest into TDR dataset\n",
    "    logging.info(\"Re-processing existing anvil_file data to include is_supplementary value.\")\n",
    "    client = bigquery.Client()\n",
    "    target_file = \"anvil_file.json\"\n",
    "    destination_dir = params[\"t_output_dir\"]\n",
    "    query = \"\"\"BEGIN\n",
    "        \n",
    "        CREATE TEMPORARY TABLE activity_exp AS WITH activity_agg\n",
    "        AS\n",
    "        (\n",
    "          SELECT used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_activity`\n",
    "          UNION ALL \n",
    "          SELECT [] AS used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_alignmentactivity`\n",
    "          UNION ALL \n",
    "          SELECT used_biosample_id, generated_file_id, [] AS used_file_id FROM `{project}.{dataset}.anvil_assayactivity`\n",
    "          UNION ALL \n",
    "          SELECT used_biosample_id, generated_file_id, [] AS used_file_id FROM `{project}.{dataset}.anvil_sequencingactivity`\n",
    "          UNION ALL \n",
    "          SELECT [] AS used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_variantcallingactivity`\n",
    "        )\n",
    "        SELECT file_id, int_file_id, biosample_id\n",
    "        FROM activity_agg\n",
    "            LEFT JOIN UNNEST(used_biosample_id) AS biosample_id\n",
    "            LEFT JOIN UNNEST(generated_file_id) as file_id\n",
    "            LEFT JOIN UNNEST(used_file_id) as int_file_id\n",
    "        ;\n",
    "        \n",
    "        CREATE TEMPORARY TABLE act_exp_lookup\n",
    "        AS\n",
    "        (\n",
    "            SELECT file_id, MAX(biosample_id) AS biosample_id\n",
    "          FROM\n",
    "          (\n",
    "            --Level 1:\n",
    "            SELECT file_id, biosample_id\n",
    "            FROM activity_exp\n",
    "            WHERE int_file_id IS NULL AND file_id IS NOT NULl AND biosample_id IS NOT NULL\n",
    "            --Level 2:\n",
    "            UNION ALL\n",
    "            SELECT a2.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "            WHERE a2.int_file_id IS NOT NULL AND a2.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "            --Level 3:\n",
    "            UNION ALL\n",
    "            SELECT a3.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "              LEFT JOIN activity_exp a3\n",
    "              ON a2.file_id = a3.int_file_id\n",
    "            WHERE a3.int_file_id IS NOT NULL AND a3.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "            --Level 4:\n",
    "            UNION ALL\n",
    "            SELECT a4.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "              LEFT JOIN activity_exp a3\n",
    "              ON a2.file_id = a3.int_file_id\n",
    "              LEFT JOIN activity_exp a4\n",
    "              ON a3.file_id = a4.int_file_id\n",
    "            WHERE a4.int_file_id IS NOT NULL AND a4.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "            --Level 5:\n",
    "            UNION ALL\n",
    "            SELECT a5.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "              LEFT JOIN activity_exp a3\n",
    "              ON a2.file_id = a3.int_file_id\n",
    "              LEFT JOIN activity_exp a4\n",
    "              ON a3.file_id = a4.int_file_id\n",
    "              LEFT JOIN activity_exp a5\n",
    "              ON a4.file_id = a5.int_file_id\n",
    "            WHERE a5.int_file_id IS NOT NULL AND a5.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "          )\n",
    "          GROUP BY file_id\n",
    "        );\n",
    "        \n",
    "        SELECT t1.file_id, data_modality, file_format, file_size, file_md5sum, reference_assembly, file_name, file_ref, source_datarepo_row_ids,\n",
    "        CASE WHEN t2.biosample_id IS NULL THEN TRUE ELSE FALSE END AS is_supplementary\n",
    "        FROM `{project}.{dataset}.anvil_file` t1\n",
    "          LEFT JOIN act_exp_lookup t2\n",
    "          ON t1.file_id = t2.file_id\n",
    "        ;\n",
    "        \n",
    "        END\n",
    "        \"\"\".format(project=bq_project, dataset=bq_schema)\n",
    "    try:\n",
    "        df = client.query(query).result().to_dataframe()\n",
    "        records_json = df.to_json(orient='records') \n",
    "        records_list = json.loads(records_json)\n",
    "        records_cnt = len(records_list)\n",
    "        with open(target_file, 'w') as outfile:\n",
    "            for idx, val in enumerate(records_list):\n",
    "                json.dump(val, outfile)\n",
    "                if idx < (records_cnt - 1):\n",
    "                    outfile.write('\\n')\n",
    "        !gsutil cp $target_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "        !rm $target_file\n",
    "        logging.info(\"Successfully created new anvil_file.json file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error creating new anvil_file.json file: {}\".format(str(e)))\n",
    "        return \"Error creating new anvil_file.json file\", \"Error\"\n",
    "\n",
    "    # Ingest updated anvil_file data\n",
    "    logging.info(\"Ingesting updated anvil_file data into TDR dataset.\")\n",
    "    source_full_file_path = \"{}/{}/{}\".format(ws_bucket, destination_dir, \"anvil_file.json\")\n",
    "    ingest_request = {\n",
    "        \"table\": \"anvil_file\",\n",
    "        \"profile_id\": params[\"profile_id\"],\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"json\",\n",
    "        \"load_tag\": \"Ingest for {}\".format(dataset_id),\n",
    "        \"path\": source_full_file_path\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"Ingest from file anvil_file.json succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            ingest_str = \"Ingest Succeeded: {}\".format(str(ingest_request_result)[0:1000])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Dataset Ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Dataset Ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded.\")\n",
    "                return \"Error ingesting new anvil_file.json file into TDR dataset\", \"Error\"\n",
    "    \n",
    "    # Return success message if no failures recorded\n",
    "    logging.info(\"Supplementary file identification ran successfully.\")\n",
    "    return ingest_str, \"Success\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/13/2023 06:22:24 PM - INFO: Attempting to identify the TDR object, and the necessary attributes...\n",
      "03/13/2023 06:22:25 PM - INFO: Adding anvil_file.is_supplementary to dataset schema, if necessary.\n",
      "03/13/2023 06:22:25 PM - INFO: Field already found! Skipping schema update.\n",
      "03/13/2023 06:22:25 PM - INFO: Re-processing existing anvil_file data to include is_supplementary value.\n",
      "03/13/2023 06:22:41 PM - INFO: Successfully created new anvil_file.json file.\n",
      "03/13/2023 06:22:41 PM - INFO: Ingesting updated anvil_file data into TDR dataset.\n",
      "TDR Job ID: -h3Os7FnTrG0RX2dIOYCLg\n",
      "03/13/2023 06:23:52 PM - INFO: Ingest from file anvil_file.json succeeded: {'dataset_id': 'f85ea65e-1943-4bd6-a541-71c5d8465ca9', 'dataset': 'ANVIL_ccdg_broad_ai_ibd_daly_vermeire_gsa_20221121', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/f85ea65e-1943-4bd6-a541-71c5d8465ca9/table_data/anvil_file.json', 'load_tag': 'Ingest for f85ea65e-1943-4bd6-a541-71c5d8465ca9', 'row_count': 24261, 'bad_row_count': 0, 'load_result': None}\n",
      "03/13/2023 06:23:52 PM - INFO: Supplementary file identification ran successfully.\n",
      "Success\n",
      "Ingest Succeeded: {'dataset_id': 'f85ea65e-1943-4bd6-a541-71c5d8465ca9', 'dataset': 'ANVIL_ccdg_broad_ai_ibd_daly_vermeire_gsa_20221121', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/f85ea65e-1943-4bd6-a541-71c5d8465ca9/table_data/anvil_file.json', 'load_tag': 'Ingest for f85ea65e-1943-4bd6-a541-71c5d8465ca9', 'row_count': 24261, 'bad_row_count': 0, 'load_result': None}\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# dataset_id = \"f85ea65e-1943-4bd6-a541-71c5d8465ca9\"\n",
    "# params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "# params[\"t_output_dir\"] = \"ingest_pipeline/output/transformed/anvil/{}/table_data\".format(dataset_id)\n",
    "# output, status = identify_supplementary_files(params, dataset_id)\n",
    "# print(status)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
