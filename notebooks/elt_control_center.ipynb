{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print(\"Version 1.0.0: 09/23/2022 5:06pm - Nate Calvanese - First version created\")\n",
    "#print(\"Version 1.0.1: 09/26/2022 11:18m - Nate Calvanese - Fixed bug in default dataset naming\")\n",
    "#print(\"Version 1.0.2: 09/27/2022 2:43pm - Nate Calvanese - Added ability to aggregate multiple workspaces into one dataset\")\n",
    "#print(\"Version 1.0.3: 10/5/2022 1:32pm - Nate Calvanese - Added support for chunking up ingest requests\")\n",
    "#print(\"Version 1.0.4: 10/6/2022 10:35am - Nate Calvanese - Updated use of TDR utility functions\")\n",
    "#print(\"Version 1.0.5: 10/13/2022 10:54am - Nate Calvanese - Parameter tweaks for latest changes\")\n",
    "#print(\"Version 1.0.6: 10/21/2022 10:53am - Nate Calvanese - Version stamp for latest changes to supporting notebooks\")\n",
    "#print(\"Version 1.0.7: 10/24/2022 4:58pm - Nate Calvanese - Added support for project entity name derivation\")\n",
    "#print(\"Version 1.0.8: 10/26/2022 4:24pm - Nate Calvanese - Added support for batching mapping activities in section 3\")\n",
    "#print('Version 1.0.9: 2/21/2023 2:50pm - Nate Calvanese - Added support for $BQ_DATASET substitution variable in mapping section')\n",
    "#print('Version 1.0.10: 3/8/2023 8:17am - Nate Calvanese - Performance improvements')\n",
    "#print('Version 1.0.11: 7/11/2023 8:17am - Nate Calvanese - Added auth domain back as reader on snapshots')\n",
    "#print('Version 1.0.12: 9/1/2023 10:16am - Nate Calvanese - Added functionality to enable/disable secure monitoring for public datasets.')\n",
    "#print('Version 1.0.13: 12/15/2023 9:00am - Nate Calvanese - Added functionality to optionally truncate tables before ingest')\n",
    "print('Version 1.0.14: 1/12/2024 11:28am - Nate Calvanese - Added max_combined_rec_ref_size as a global parameter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Common Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional modules (one time effort per cloud environment)\n",
    "#!pip install --upgrade pip import_ipynb data_repo_client urllib3 xmltodict azure-storage-blob\n",
    "#!pip install data_repo_client==1.409.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace environment variables\n",
    "import os\n",
    "import re\n",
    "print(\"Recording workspace environment variables:\")\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "print(f\"Workspace name = {ws_name}\")\n",
    "print(f\"Workspace project = {ws_project}\")\n",
    "print(f\"Workspace bucket = {ws_bucket}\")\n",
    "print(f\"Workspace bucket name = {ws_bucket_name}\")\n",
    "\n",
    "# Copy latest version of the pipeline notebooks to the cloud environment (uncomment if any notebooks have changed since last run)\n",
    "# print(\"\\nCopying latest pipeline notebooks to the cloud environment:\")\n",
    "# !gsutil -m cp $ws_bucket/notebooks/*.ipynb .\n",
    "\n",
    "# Additional imports\n",
    "print(\"\\nRunning imports:\")\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "from firecloud import api as fapi\n",
    "import data_repo_client\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_mapping_query as bmq\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "from time import sleep\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pyarrow.parquet as pq\n",
    "from azure.storage.blob import BlobClient, ContainerClient\n",
    "\n",
    "# Common pipeline variables (AnVIL)\n",
    "ws_attributes = utils.get_workspace_attributes(ws_project, ws_name)\n",
    "params = {}\n",
    "params[\"ws_name\"] = ws_name\n",
    "params[\"ws_project\"] = ws_project\n",
    "params[\"ws_bucket\"] = ws_bucket\n",
    "params[\"ws_bucket_name\"] = ws_bucket_name\n",
    "params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\" \n",
    "params[\"google_project\"] = ws_attributes[\"googleProject\"]\n",
    "params[\"create_file_table\"] = True\n",
    "params[\"file_table_name\"] = \"file_inventory\"\n",
    "params[\"ingest_user_to_add\"] = \"tdr_sa\"  # tdr_sa or anvil_tdr_ingest\n",
    "params[\"global_file_exclusions\"] = [\"SubsetHailJointCall\", \".vds/\"]\n",
    "params[\"max_combined_rec_ref_size\"] = 40000\n",
    "\n",
    "# Configure logging format\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"EL\" Pipeline: Load Dataset to TDR in Source Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## >>> Run Variables <<<\n",
    "# For datasets split across multiple workspaces, set the staging area and target TDR dataset to the \n",
    "# same value to collect all of the source data and process it together.\n",
    "workspace_run_list = [\n",
    "    #[\"Workspace_Name\", \"Workspace_Project\", Public (True/False), \"Staging Area (Leave empty for default)\", \"Target_TDR_Dataset_Name (Leave empty for default)\", Run (True/False)]\n",
    "#     [\"ANVIL_Workspace_1\", \"anvil-datastorage\", False, \"\", \"\", False],\n",
    "#     [\"ANVIL_Workspace_2\", \"anvil-datastorage\", False, \"\", \"\", False],\n",
    "    ['AnVIL_HPRC', 'anvil-datastorage', True, '', 'ANVIL_HPRC_20240401', True],\n",
    "]\n",
    "params[\"skip_source_files_creation\"] = False\n",
    "params[\"skip_file_inventory_creation\"] = False\n",
    "params[\"skip_table_data_processing\"] = False\n",
    "params[\"skip_ingests\"] = False\n",
    "params[\"trunc_before_ingest\"] = True\n",
    "params[\"skip_snapshot_creation\"] = True\n",
    "params[\"snapshot_readers_list\"] = [\"auth-domain\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "\n",
    "\n",
    "## >>> File Inventory Variables <<<\n",
    "# The GCS bucket associated with the source workspace will be automatically included in the file inventory build. To specify \n",
    "# additional GCS buckets to include in the file inventory build, add entries to the below dictionary.\n",
    "params[\"additional_file_inventory_sources\"] = {}\n",
    "# EXAMPLE:\n",
    "# params[\"additional_file_inventory_sources\"] = {\n",
    "#     \"staging_area\": {\n",
    "#         \"bucket_name\": {\n",
    "#             \"include_dirs\": [], # Leave empty to include all directories in bucket\n",
    "#             \"exclude_dirs\": [] # Exclusions will take precedence over inclusions\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "## >>> Ingest Variables <<<\n",
    "# For cases where you only want to ingest a subset of files, use the below dictionary to specify exactly what should be ingested.\n",
    "params[\"ingest_list_override\"] = {\n",
    "}\n",
    "# EXAMPLE:\n",
    "# params[\"ingest_list_override\"] = {\n",
    "#     \"ws_table\": [\"ws_table_0.json\"], # Leave empty to run ingest for every file for target table\n",
    "# }\n",
    "\n",
    "\n",
    "## >>> File Reference Variables <<<\n",
    "# Fields containing GCS links will be identified automatically by the pipeline. The below dict should contain any fields\n",
    "# that contain file references that aren't proper GCS links in the workspace tables.\n",
    "data_file_refs_dict = {   \n",
    "}\n",
    "# Definitions:\n",
    "#    Required Fields: column, method, mode, create_new field\n",
    "#    Optional Fields: match_multiple_files (default to True), match_regex (default to None), match_type (default to 'partial'), new_field_name (default to None)\n",
    "#    Methods: \n",
    "#       file_path_match -- Field contains a full or partial file path, which can be matched to the file inventory to grab the file(s) referenced \n",
    "#       tdr_file_id -- Field contains file UUIDs of files already ingested into the target TDR dataset\n",
    "#    Modes:\n",
    "#       fileref_in_line -- Populates the field with a file reference object\n",
    "#       fileref_table_ref -- Populates the field with an ID that joins to a file table. If no file table built, falls back on fileref_in_line logic.\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "# Print variables\n",
    "print(\"Pipeline run variables set:\")\n",
    "print(\"Profile ID: \" + params[\"profile_id\"])\n",
    "print(\"Ingests to run: \")\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "for workspace in workspace_run_list:\n",
    "    if workspace[5] == True:\n",
    "        ws_attributes = utils.get_workspace_attributes(workspace[1], workspace[0])\n",
    "        params[\"phs_id\"] = utils.format_phs_id(ws_attributes[\"attributes\"][\"phs_id\"]) if ws_attributes[\"attributes\"].get(\"phs_id\") else \"\"\n",
    "        auth_list = ws_attributes[\"authorizationDomain\"] if ws_attributes.get(\"authorizationDomain\") else []\n",
    "        params[\"auth_domains\"] = [x[\"membersGroupName\"] for x in auth_list]\n",
    "        params[\"consent_name\"] = ws_attributes[\"attributes\"][\"library:dataUseRestriction\"] if ws_attributes[\"attributes\"].get(\"library:dataUseRestriction\") else \"\"\n",
    "        params[\"data_files_src_bucket\"] = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "        params[\"public_dataset\"] = workspace[2]\n",
    "        workspace[4] = workspace[4] if workspace[4] else utils.format_dataset_name(workspace[0])\n",
    "        workspace[3] = workspace[3] if workspace[3] else workspace[0]\n",
    "        print(\"- Workspace [\" + workspace[1] + \"/\" + workspace[0] + \"] to TDR dataset [\" + workspace[4] + \"] via Staging Area [\" + workspace[3] + \"]\")\n",
    "        print(\"\\t- PHS ID = \" + params[\"phs_id\"])\n",
    "        print(\"\\t- Consent Short Name = \" + params[\"consent_name\"])\n",
    "        print(\"\\t- Auth Domains = \" + str(params[\"auth_domains\"]))\n",
    "        print(\"\\t- Public Dataset = \" + str(params[\"public_dataset\"]))\n",
    "        print(\"\\t- Data Files Source Bucket = \" + params[\"data_files_src_bucket\"])\n",
    "print(\"Skip source files creation? \" + str(params[\"skip_source_files_creation\"]))\n",
    "print(\"Skip file inventory creation? \" + str(params[\"skip_file_inventory_creation\"]))\n",
    "print(\"Skip table data processing? \" + str(params[\"skip_table_data_processing\"]))\n",
    "print(\"Skip ingests? \" + str(params[\"skip_ingests\"]))\n",
    "print(\"Truncate tables before ingest? \" + str(params[\"trunc_before_ingest\"]))\n",
    "print(\"Ingest override list: \" + str(params[\"ingest_list_override\"]))\n",
    "print(\"Skip snapshot creation? \" + str(params[\"skip_snapshot_creation\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through and execute workspace connector pipeline (\"E\") for listed workspaces\n",
    "if params[\"skip_source_files_creation\"] == True:\n",
    "    logging.info(\"Skipping source file creation, per user request.\")\n",
    "else:\n",
    "    for workspace in workspace_run_list:\n",
    "        if workspace[5] == True:\n",
    "            params[\"data_file_refs\"] = data_file_refs_dict  \n",
    "            utils.run_ws_connector_pipeline(workspace, params)\n",
    "\n",
    "# Aggregate staging area to target dataset combinations, loop through them, and execute ingest pipeline (\"L\")\n",
    "pipeline_run_list = []\n",
    "for workspace in workspace_run_list:\n",
    "    if workspace[5] == True:\n",
    "        temp_list = [workspace[3], workspace[4], workspace[2]]\n",
    "        if temp_list not in pipeline_run_list:\n",
    "            pipeline_run_list.append(temp_list)\n",
    "for pipeline in pipeline_run_list:\n",
    "    utils.run_el_pipeline(pipeline, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Development\n",
    "Work through the following steps for each dataset that needs to be processed through the transformation pipeline in Step 4, specifying the target schema (\"mapping target\") and mapping specification (\"mapping_target_spec\") you would like to use for transformation. Note that you can use the logs or results_dict from the previous step to retrieve the dataset_id values of interest, or retrieve them directly from TDR via the UI or Swagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Mapping Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## >>> Mapping Variables <<<\n",
    "# For each dataset specified, include an appropriate mapping target and mapping target specification\n",
    "datasets_to_map_list = [\n",
    "    #[\"dataset_id\", \"mapping_target\", \"mapping_target_spec\", Run (True/False)]\n",
    "    ['ec6f49a2-176c-4564-82c5-e751baab46aa', 'anvil', 'gtex_ext_3', True],\n",
    "]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "# Print variables\n",
    "print(\"Datasets to map: \")\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "final_datasets_to_map_dict = {}\n",
    "skip_dataset_list_access = []\n",
    "skip_dataset_list_mapping = []\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "for dataset in datasets_to_map_list:\n",
    "    if dataset[3]:\n",
    "        dataset_id = dataset[0]\n",
    "        mapping_target = dataset[1]\n",
    "        mapping_target_spec = dataset[2]\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "            skip_dataset_list_access.append(dataset_id)\n",
    "        try:\n",
    "            blob = bucket.blob(\"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target))\n",
    "            content = json.loads(blob.download_as_string(client=None))\n",
    "            blob = bucket.blob(\"ingest_pipeline/mapping/{}/{}/mapping_specification.json\".format(mapping_target, mapping_target_spec))\n",
    "            content = json.loads(blob.download_as_string(client=None))\n",
    "        except:\n",
    "            skip_dataset_list_mapping.append(dataset_id)\n",
    "        if dataset_id not in skip_dataset_list_access and dataset_id not in skip_dataset_list_mapping:\n",
    "            final_datasets_to_map_dict[dataset_id] = {}\n",
    "            final_datasets_to_map_dict[dataset_id][\"mapping_target\"] = mapping_target \n",
    "            final_datasets_to_map_dict[dataset_id][\"mapping_target_spec\"] = mapping_target_spec\n",
    "            print(\"\\t- \" + dataset_name + \" ({})\".format(dataset_id) + \" with {}/{}\".format(mapping_target, mapping_target_spec))\n",
    "if skip_dataset_list_access:\n",
    "    print(\"Datasets to skip due to non-existence or inaccessibility to the current user:\")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(skip_dataset_list_access))\n",
    "if skip_dataset_list_mapping:\n",
    "    print(\"Datasets to skip due to invalid mapping target or mapping target specification:\")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(skip_dataset_list_mapping))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Missing Relationships to TDR Dataset Schema\n",
    "Relationships are needed by the mapping query constructor to build appropriate joins between tables. If no joins are required between tables, this step is unnecessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record relationships to potentially add to the source datasets. Note that there may be more relationships to add\n",
    "# than those listed below, so add to this list as necessary.\n",
    "potential_relationships = [\n",
    "    [\"subject.family_id\", \"family.family_id\"],\n",
    "    [\"sample.subject_id\", \"subject.subject_id\"],\n",
    "    [\"sample.t_01_subject_id\", \"subject.subject_id\"],\n",
    "    [\"sequencing.sample_id\", \"sample.sample_id\"],\n",
    "    [\"sequencing.sample\", \"sample.sample_id\"],\n",
    "    [\"sequencing.sample_alias\", \"sample.sample_id\"],\n",
    "    [\"sample.participant\", \"participant.participant_id\"],\n",
    "    [\"sample.participant_id\", \"participant.participant_id\"],\n",
    "    [\"discovery.sample_id\", \"sample.sample_id\"],\n",
    "    [\"discovery.subject_id\", \"subject.subject_id\"],\n",
    "    [\"qc_result_sample.qc_result_sample_id\", \"sample.sample_id\"],\n",
    "    [\"interval.chromosome\", \"chromosome.chromosome_id\"]\n",
    "]\n",
    "\n",
    "# Loop through datasets and process potential relationship additions\n",
    "results = []\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "for dataset_id in final_datasets_to_map_dict:\n",
    "    print(\"Processing potential relationships for dataset_id = {}\".format(dataset_id))\n",
    "    \n",
    "    # Retrieve source schema\n",
    "    src_schema_dict = {}\n",
    "    try:\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "        results.append([dataset_id, \"Error\"])\n",
    "\n",
    "    # Loop through potential relationships and add those present for the source dataset\n",
    "    additional_relationships = []\n",
    "    for rel in potential_relationships:\n",
    "        from_table = rel[0].split(\".\")[0] \n",
    "        from_column = rel[0].split(\".\")[1]\n",
    "        to_table = rel[1].split(\".\")[0]\n",
    "        to_column = rel[1].split(\".\")[1]\n",
    "        if bmq.confirm_column_exists(src_schema_dict, from_table, from_column) and bmq.confirm_column_exists(src_schema_dict, to_table, to_column):\n",
    "            relationship_found = False\n",
    "            for rel_entry in src_schema_dict[\"relationships\"]:\n",
    "                if rel_entry[\"_from\"][\"table\"] == from_table and rel_entry[\"_from\"][\"column\"] == from_column and rel_entry[\"to\"][\"table\"] == to_table and rel_entry[\"to\"][\"column\"] == to_column:\n",
    "                    relationship_found = True\n",
    "                elif rel_entry[\"_from\"][\"table\"] == to_table and rel_entry[\"_from\"][\"column\"] == to_column and rel_entry[\"to\"][\"table\"] == from_table and rel_entry[\"to\"][\"column\"] == from_column:\n",
    "                    relationship_found = True\n",
    "            if not relationship_found:\n",
    "                rel_dict = {\n",
    "                    \"name\": from_table + \"_\" + from_column + \"__to__\" + to_table + \"_\" + to_column,\n",
    "                    \"from\": {\"table\": from_table, \"column\": from_column},\n",
    "                    \"to\": {\"table\": to_table, \"column\": to_column}\n",
    "                }\n",
    "                additional_relationships.append(rel_dict)\n",
    "\n",
    "    # Submit the schema update request for the TDR dataset\n",
    "    if additional_relationships:\n",
    "        schema_update_request = {\n",
    "            \"description\": \"Adding relationships to support query construction.\",\n",
    "            \"changes\": {\n",
    "                \"addRelationships\": additional_relationships\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            resp = utils.wait_for_tdr_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "            print(\"Schema update successful: \" + str(resp)[0:1000])\n",
    "            results.append([dataset_id, \"Success\"])\n",
    "        except Exception as e:\n",
    "            print(\"Error running schema update: \" + str(e))\n",
    "            results.append([dataset_id, \"Error\"])\n",
    "    else:\n",
    "        print(\"No additional relationships to add to schema.\")\n",
    "        results.append([dataset_id, \"Success\"])\n",
    "\n",
    "print(\"Processing of potential relationships for specified datasets complete.\")\n",
    "print(\"\\nResults:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"dataset\", \"status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Mapping Artifacts and Run Query Construction\n",
    "Retrieve the artifacts you would like to use to construct transformation queries for your datasets, based on the previously specified target schema and mapping specification. These transformation queries will then be dynamically constructed based on the appropriate target schema, mapping specification, and source schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through datasets and process transformation query construction\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "results = []\n",
    "for dataset_id in final_datasets_to_map_dict:\n",
    "    print(\"Building transformation queries for dataset_id = {}\".format(dataset_id))\n",
    "\n",
    "    # Collect mapping variables\n",
    "    mapping_target = final_datasets_to_map_dict[dataset_id][\"mapping_target\"]\n",
    "    mapping_target_spec = final_datasets_to_map_dict[dataset_id][\"mapping_target_spec\"]\n",
    "    \n",
    "    # Retrieve source schema\n",
    "    src_schema_dict = {}\n",
    "    try:\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"name\"] = response[\"name\"]\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        phs_id = response[\"phs_id\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "    # Set dataset name and project name parameters to substitute into transform queries\n",
    "    dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "    project_name_value = re.sub(\"'\", \"\", utils.derive_project_name(dataset_id, phs_id, dataset_name_value))\n",
    "\n",
    "    # Retrieve target schema and mapping specification\n",
    "    target_schema_dict = {}\n",
    "    mapping_spec = {}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "        target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "        blob_string = blob.download_as_text(client=None)\n",
    "        blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "        blob_string = blob_string.replace(\"$PROJECT_NAME\", project_name_value)\n",
    "        blob_string = blob_string.replace(\"$BQ_DATASET\", bq_project + \".\" + bq_schema)\n",
    "        mapping_spec = json.loads(blob_string)\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "\n",
    "    # Build queries from mapping specification\n",
    "    query_dict = {}\n",
    "    if target_schema_dict:\n",
    "        for target_table in target_schema_dict[\"tables\"]:\n",
    "            table_name = target_table[\"name\"]\n",
    "            missing_artifacts = False\n",
    "            if src_schema_dict and mapping_spec:\n",
    "                query_dict[table_name] = bmq.build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema)\n",
    "            else:\n",
    "                missing_artifacts = True\n",
    "                query_dict[table_name] = {\"query\": \"\", \"syntax_check\": \"\"} \n",
    "        if missing_artifacts == True:\n",
    "            print(\"Source schema dictionary and/or mapping specification missing. Unable to generate queries.\")\n",
    "            results.append([dataset_id, \"Error\"])\n",
    "    else:\n",
    "        print(\"Target schema dictionary missing. Unable to generate queries.\")\n",
    "        results.append([dataset_id, \"Error\"])\n",
    "    \n",
    "    # Evaluate queries -- Publish if no issues found, otherwise convert to dataframe and display\n",
    "    failure_count = 0\n",
    "    for key, val in query_dict.items():\n",
    "        if val[\"syntax_check\"] != \"Passed\" and val[\"syntax_check\"] != None:\n",
    "            failure_count += 1\n",
    "    if failure_count == 0:\n",
    "        print(\"No failures found in query construction, publishing to the cloud.\")\n",
    "        results.append([dataset_id, \"Success\"])\n",
    "        # Copy target schema file to output folder for mapping target\n",
    "        source_path = \"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target)\n",
    "        destination_path = \"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(mapping_target, dataset_id)\n",
    "        !gsutil cp $ws_bucket/$source_path $ws_bucket/$destination_path 2> stdout\n",
    "\n",
    "        # Limit query dict to valid queries, write out, and copy to output folder for mapping target\n",
    "        valid_query_dict = {}\n",
    "        for target, val in query_dict.items():\n",
    "            if val[\"syntax_check\"] == \"Passed\":\n",
    "                valid_query_dict[target] = val\n",
    "        final_query_dict = {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"transforms\": valid_query_dict\n",
    "        }\n",
    "        query_dict_json = json.dumps(final_query_dict)\n",
    "        query_output_file = \"transform_query_set.json\"\n",
    "        with open(query_output_file, 'w') as outfile:\n",
    "            outfile.write(query_dict_json)\n",
    "        destination_path = \"ingest_pipeline/output/transformed/{}/{}/queries\".format(mapping_target, dataset_id)\n",
    "        !gsutil cp $query_output_file $ws_bucket/$destination_path/ 2> stdout\n",
    "    else:\n",
    "        print(\"Failures found in query construction, must be resolved before publishing.\")\n",
    "        print(\"Query building results:\")\n",
    "        results.append([dataset_id, \"Error\"])\n",
    "        query_df = pd.DataFrame.from_dict(query_dict, orient=\"index\")\n",
    "        query_df.index.name = \"target_table\"\n",
    "        query_df.reset_index(inplace=True)\n",
    "        display(query_df)\n",
    "\n",
    "print(\"Transformation query construction and processing complete.\")\n",
    "print(\"\\nResults:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"dataset\", \"status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Vocabulary Mapping\n",
    "For target attributes leveraging the \"VOCAB_MAP\" transformation, evaluate whether the source values have a record in the dsp-data-ingest.transform_resources.vocab_map table. If additional mappings are needed, these should be put into place before the transformation queries are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display parameter\n",
    "show_only_missing_maps = True\n",
    "\n",
    "# Loop through datasets and process vocabulary mapping evaluation\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "for dataset_id in final_datasets_to_map_dict:\n",
    "    print(\"Evaluating vocabulary mapping for dataset_id = {}\".format(dataset_id))\n",
    "\n",
    "    # Collect mapping variables\n",
    "    mapping_target = final_datasets_to_map_dict[dataset_id][\"mapping_target\"]\n",
    "    mapping_target_spec = final_datasets_to_map_dict[dataset_id][\"mapping_target_spec\"]\n",
    "    \n",
    "    # Retrieve source schema\n",
    "    src_schema_dict = {}\n",
    "    try:\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"name\"] = response[\"name\"]\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        phs_id = response[\"phs_id\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "    # Set dataset name and project name parameters to substitute into transform queries\n",
    "    dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "    project_name_value = utils.derive_project_name(dataset_id, phs_id, dataset_name_value)\n",
    "\n",
    "    # Retrieve target schema and mapping specification\n",
    "    target_schema_dict = {}\n",
    "    mapping_spec = {}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "        target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "        blob_string = blob.download_as_text(client=None)\n",
    "        blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "        blob_string = blob_string.replace(\"$PROJECT_NAME\", project_name_value)\n",
    "        mapping_spec = json.loads(blob_string)\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "\n",
    "    # Evaluate vocab mapping and display results\n",
    "    df = bmq.evaluate_vocab_mapping(mapping_spec, src_schema_dict, target_schema_dict, bq_project, bq_schema)\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Missing mapped_value view:\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    display(df[df[\"mapped_value\"].isnull() & df[\"source_value\"].notnull()])\n",
    "    if not show_only_missing_maps:\n",
    "        print(\"\\n-------------------------------------------\")\n",
    "        print(\"Full view:\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        display(df)\n",
    "    \n",
    "print(\"Vocabulary mapping evaluation and processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## [Optional] Update/Override Generated Queries as Necessary\n",
    "Review any queries that have not passed the syntax check, as these need to be remedied before they can be published and executed. Any other queries that do not align with expectations can be overridden by either A) Updating the mapping target specification and re-running the previous step, or B) Manually overriding the query below. Option B should only be used in one-off cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Build Base Query Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Input the appropriate dataset and mapping target specification\n",
    "dataset_id = \"f1e1ef01-d52d-423e-a65b-3a1d26c7ee9d\"\n",
    "mapping_target = \"anvil\"\n",
    "mapping_target_spec = \"cmg_ext_2\"\n",
    "\n",
    "# Retrieve source schema\n",
    "src_schema_dict = {}\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "try:\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "    src_schema_dict[\"name\"] = response[\"name\"]\n",
    "    src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "    src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "    bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "    bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    phs_id = response[\"phs_id\"]\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "# Set dataset name and project name parameters to substitute into transform queries\n",
    "dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "project_name_value = utils.derive_project_name(dataset_id, phs_id, dataset_name_value)\n",
    "\n",
    "# Retrieve target schema and mapping specification\n",
    "target_schema_dict = {}\n",
    "mapping_spec = {}\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "    target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "    blob_string = blob.download_as_text(client=None)\n",
    "    blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "    blob_string = blob_string.replace(\"$PROJECT_NAME\", project_name_value)\n",
    "    mapping_spec = json.loads(blob_string)\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "\n",
    "# Build queries from mapping specification\n",
    "query_dict = {}\n",
    "if target_schema_dict:\n",
    "    for target_table in target_schema_dict[\"tables\"]:\n",
    "        table_name = target_table[\"name\"]\n",
    "        missing_artifacts = False\n",
    "        if src_schema_dict and mapping_spec:\n",
    "            query_dict[table_name] = bmq.build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema)\n",
    "        else:\n",
    "            missing_artifacts = True\n",
    "            query_dict[table_name] = {\"query\": \"\", \"syntax_check\": \"\"} \n",
    "    if missing_artifacts == True:\n",
    "        print(\"Source schema dictionary and/or mapping specification missing. Unable to generate queries.\")\n",
    "else:\n",
    "    print(\"Target schema dictionary missing. Unable to generate queries.\")\n",
    "    \n",
    "# Display query dictionary\n",
    "query_df = pd.DataFrame.from_dict(query_dict, orient=\"index\")\n",
    "query_df.index.name = \"target_table\"\n",
    "query_df.reset_index(inplace=True)\n",
    "display(query_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Update Query Dict as Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To update the query definition for particular target table, input the target table and query below\n",
    "target_table = \"anvil_donor\"\n",
    "query = \"SELECT 1\"\n",
    "\n",
    "# Run syntax check\n",
    "query_dict[target_table][\"query\"] = query\n",
    "query_dict[target_table][\"syntax_check\"] = bmq.run_syntax_check(query)\n",
    "print(query_dict[target_table])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Publish Updated Query Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Copy target schema file to output folder for mapping target\n",
    "source_path = \"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target)\n",
    "destination_path = \"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(mapping_target, dataset_id)\n",
    "!gsutil cp $ws_bucket/$source_path $ws_bucket/$destination_path 2> stdout\n",
    "\n",
    "# Limit query dict to valid queries, write out, and copy to output folder for mapping target\n",
    "valid_query_dict = {}\n",
    "for target, val in query_dict.items():\n",
    "    if val[\"syntax_check\"] == \"Passed\":\n",
    "        valid_query_dict[target] = val\n",
    "final_query_dict = {\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"transforms\": valid_query_dict\n",
    "}\n",
    "query_dict_json = json.dumps(final_query_dict)\n",
    "query_output_file = \"transform_query_set.json\"\n",
    "with open(query_output_file, 'w') as outfile:\n",
    "    outfile.write(query_dict_json)\n",
    "destination_path = \"ingest_pipeline/output/transformed/{}/{}/queries\".format(mapping_target, dataset_id)\n",
    "!gsutil cp $query_output_file $ws_bucket/$destination_path/ 2> stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"T\" Pipeline: Load Additional Transformed Tables to TDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run Variables\n",
    "dataset_id_run_list = [\n",
    "    #[\"dataset_id\", Run (True/False)],   \n",
    "    ['ec6f49a2-176c-4564-82c5-e751baab46aa', True],\n",
    "]\n",
    "params[\"mapping_target\"] = \"anvil\"\n",
    "params[\"skip_transforms\"] = False\n",
    "params[\"transform_list_override\"] = [] # Leave empty to run transforms for all files, otherwise populate with target table names \n",
    "params[\"skip_schema_extension\"] = False\n",
    "params[\"skip_ingests\"] = False\n",
    "params[\"trunc_before_ingest\"] = True\n",
    "params[\"ingest_list_override\"] = [] # Leave empty to run ingests for all files, otherwise populate with target table names\n",
    "params[\"skip_file_relation_inference\"] = False\n",
    "params[\"skip_dangling_fk_resolution\"] = False\n",
    "params[\"skip_supplementary_file_identification\"] = False\n",
    "params[\"skip_snapshot_creation\"] = False\n",
    "params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\", \"auth-domain\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "params[\"skip_data_validation\"] = False\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Print variables\n",
    "print(\"Pipeline run variables set:\")\n",
    "print(\"Profile ID: \" + params[\"profile_id\"])\n",
    "print(\"Mapping Target: \" + params[\"mapping_target\"])\n",
    "print(\"Datasets to run: \")\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "dataset_skip_list = []\n",
    "for dataset in dataset_id_run_list:\n",
    "    if dataset[1]:\n",
    "        dataset_id = dataset[0]\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "            phs_id = dataset_info[\"phs_id\"]\n",
    "            consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "            auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "            src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "            dataset_skip_list.append(dataset_id)\n",
    "        if dataset_name:\n",
    "            dataset_id = dataset[0]\n",
    "            print(\"- \" + dataset_name + \" ({})\".format(dataset_id))\n",
    "            print(\"\\t- PHS ID = \" + phs_id)\n",
    "            print(\"\\t- Consent Short Name = \" + consent_name)\n",
    "            print(\"\\t- Auth Domains = \" + str(auth_domains))\n",
    "            print(\"\\t- Source Workspaces = \" + str(src_workspaces))\n",
    "if dataset_skip_list:\n",
    "    print(\"Datasets to skip (they either don't exist or aren't accessible to the current user): \")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(dataset_skip_list)) \n",
    "print(\"Skip transforms? \" + str(params[\"skip_transforms\"]))\n",
    "print(\"Transforms override list: \" + str(params[\"transform_list_override\"]))\n",
    "print(\"Skip schema extension? \" + str(params[\"skip_schema_extension\"]))\n",
    "print(\"Skip ingests? \" + str(params[\"skip_ingests\"]))\n",
    "print(\"Truncate tables before ingest? \" + str(params[\"trunc_before_ingest\"]))\n",
    "print(\"Ingest override list: \" + str(params[\"ingest_list_override\"]))\n",
    "print(\"Skip file relationship inference? \" + str(params[\"skip_file_relation_inference\"]))\n",
    "print(\"Skip dangling foreign key resolution? \" + str(params[\"skip_dangling_fk_resolution\"]))\n",
    "print(\"Skip supplementary file identification? \" + str(params[\"skip_supplementary_file_identification\"]))\n",
    "print(\"Skip snapshot creation? \" + str(params[\"skip_snapshot_creation\"]))\n",
    "print(\"Skip data validation? \" + str(params[\"skip_data_validation\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and execute pipeline for listed workspaces\n",
    "for dataset in dataset_id_run_list:\n",
    "    if dataset[1]:\n",
    "        dataset_id = dataset[0]\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "            phs_id = dataset_info[\"phs_id\"]\n",
    "            consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "            auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "            src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "        if dataset_name:\n",
    "            params[\"dataset_id\"] = dataset_id\n",
    "            params[\"dataset_name\"] = dataset_name\n",
    "            params[\"phs_id\"] = phs_id\n",
    "            params[\"consent_name\"] = consent_name\n",
    "            params[\"auth_domains\"] = auth_domains\n",
    "            utils.run_t_pipeline(params)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utility Scripts\n",
    "Uncomment sections as necessary to accomplish various miscellaneous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Collect AnVIL Snapshots and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Dataset_ID Filter (leave empty for all)\n",
    "dataset_id_list = [\n",
    "    '27acea14-41c9-4bf9-ad43-3ebb3ce90456',\n",
    "    'ec6f49a2-176c-4564-82c5-e751baab46aa',\n",
    "    'd596ee91-481c-4eb5-9a8a-88c1e10ba9b6',\n",
    "    '032d39fb-d278-427d-b7d2-de648a25a20c',\n",
    "    'f9224ea2-dd31-421d-80d4-f35082ef8d68',\n",
    "    'd7bcfc5d-e258-4bd6-a413-bb7a118e6bff',\n",
    "    '6d18aafc-0240-499c-902e-a72a5b98ff0a',\n",
    "    '6fd0f009-3c34-4529-9a38-c59745545490',\n",
    "]\n",
    "\n",
    "# Collect Anvil datasets and snapshots\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d %H:%M:%S\")\n",
    "logging.info(f\"Start time: {current_datetime_string}\")\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "datasets_list = datasets_api.enumerate_datasets(filter=\"anvil\", limit=2000)\n",
    "dataset_list_len = len(datasets_list.items)\n",
    "records_list = []\n",
    "dataset_count = 0\n",
    "for dataset_entry in datasets_list.items:\n",
    "    dataset_count += 1\n",
    "    logging.info(f\"Processing dataset {dataset_count} of {dataset_list_len}\")\n",
    "    if len(dataset_id_list) == 0 or dataset_entry.id in dataset_id_list:\n",
    "        if re.match(\"^ANVIL_[a-zA-Z0-9-_]+_[0-9]{8}\", dataset_entry.name.upper()):\n",
    "            dataset_detail = datasets_api.retrieve_dataset(id=dataset_entry.id, include=[\"PROPERTIES\", \"DATA_PROJECT\"])\n",
    "            snapshots_list = snapshots_api.enumerate_snapshots(dataset_ids=[dataset_entry.id], limit=1000)\n",
    "            try:\n",
    "                source_workspace = \", \".join(dataset_detail.properties[\"source_workspaces\"])\n",
    "            except:\n",
    "                source_workspace = \"\"\n",
    "            if len(snapshots_list.items) == 0:\n",
    "                record = [None, None, None, None, None, None, None, None, None, dataset_entry.id, dataset_entry.name, dataset_detail.ingest_service_account, dataset_entry.created_date[0:10], dataset_entry.cloud_platform, dataset_entry.secure_monitoring_enabled, source_workspace]\n",
    "                records_list.append(record)\n",
    "            else:\n",
    "                snapshot_list_len = len(snapshots_list.items)\n",
    "                snapshot_count = 0\n",
    "                for snapshot_entry in snapshots_list.items:\n",
    "                    snapshot_count += 1\n",
    "                    logging.info(f\"Processing snapshot {snapshot_count} of {snapshot_list_len} for dataset {dataset_count}\")\n",
    "                    # Get public policy information\n",
    "                    creds, project = google.auth.default()\n",
    "                    auth_req = google.auth.transport.requests.Request()\n",
    "                    creds.refresh(auth_req)\n",
    "                    public_flag = \"N\"\n",
    "                    public_response = requests.get(\n",
    "                        url=f\"https://sam.dsde-prod.broadinstitute.org/api/resources/v2/datasnapshot/{snapshot_entry.id}/policies/reader/public\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                    )\n",
    "                    if public_response.text == \"true\":\n",
    "                        public_flag = \"Y\"\n",
    "                    # Get snapshot DUOS ID and Lock status\n",
    "                    api_client = utils.refresh_tdr_api_client()\n",
    "                    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "                    snapshot_detail = snapshots_api.retrieve_snapshot(id=snapshot_entry.id, include=[\"DUOS\"])\n",
    "                    duos_id = \"\"\n",
    "                    if snapshot_detail.duos_firecloud_group:\n",
    "                        duos_id = snapshot_detail.duos_firecloud_group.duos_id\n",
    "                    lock_name = snapshot_detail.resource_locks.exclusive\n",
    "                    if lock_name:\n",
    "                        lock_status = True\n",
    "                    else:\n",
    "                        lock_status = False\n",
    "                    # Get snapshot readers and auth domain\n",
    "                    snapshot_policy_response = snapshots_api.retrieve_snapshot_policies(id=snapshot_entry.id)\n",
    "                    for role in snapshot_policy_response.policies:\n",
    "                        if role.name == \"reader\":\n",
    "                            readers = \", \".join(role.members)\n",
    "                    ad_groups = \"\"\n",
    "                    if snapshot_policy_response.auth_domain:\n",
    "                        ad_groups = \", \".join(snapshot_policy_response.auth_domain)\n",
    "                    record = [snapshot_entry.id, snapshot_entry.name, snapshot_entry.created_date[0:10], public_flag, readers, ad_groups, duos_id, snapshot_entry.data_project, lock_status, dataset_entry.id, dataset_entry.name, dataset_detail.ingest_service_account, dataset_entry.created_date[0:10], dataset_entry.cloud_platform, dataset_entry.secure_monitoring_enabled, source_workspace]\n",
    "                    records_list.append(record)\n",
    "df = pd.DataFrame(records_list, columns =[\"Snapshot ID\", \"Snapshot Name\", \"Snapshot Created Date\", \"Snapshot Public\", \"Snapshot Readers\", \"Snapshot Auth Domain\", \"Snapshot DUOS ID\", \"Snapshot Data Project\", \"Snapshot Locked\", \"Source Dataset ID\", \"Source Dataset Name\", \"Source Dataset SA\", \"Source Dataset Created Date\", \"Cloud Platform\", \"Secure Monitoring\", \"Source Workspace\"])\n",
    "df_sorted = df.sort_values([\"Source Workspace\", \"Source Dataset Name\", \"Snapshot Name\"], ascending=[True, True, True], ignore_index=True)\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d %H:%M:%S\")\n",
    "logging.info(f\"End time: {current_datetime_string}\")\n",
    "display(df_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Soft Deletion of TDR Dataset Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     15,
     39,
     106
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "dataset_id_list = [\n",
    "    \"ec6f49a2-176c-4564-82c5-e751baab46aa\"\n",
    "]\n",
    "#table_list = [\"anvil_activity\", \"anvil_alignmentactivity\", \"anvil_antibody\", \"anvil_assayactivity\", \"anvil_biosample\", \"anvil_dataset\", \"anvil_diagnosis\", \"anvil_donor\", \"anvil_file\", \"anvil_project\", \"anvil_sequencingactivity\", \"anvil_variantcallingactivity\"]\n",
    "#table_list = [\"file_inventory\", \"sample\", \"subject\", \"workspace_attributes\", \"sequencing\", \"qc_result_sample\", \"family\", \"chromosome\", \"interval\", \"participant\", \"discovery\", \"sample_set\", \"vcf\"]\n",
    "table_list = ['file_inventory']\n",
    "delete_all_records = True\n",
    "delete_record_list = [] # Will be ignored if delete_all_records is set to True\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to delete rows from a dataset\n",
    "def delete_datarepo_rows(dataset_id, table_name, datarepo_row_ids):\n",
    "    print(\"Attempting to delete specified rows from {} for dataset {}\".format(table_name, dataset_id))\n",
    "    if datarepo_row_ids:\n",
    "        data_deletion_payload = {\n",
    "            \"deleteType\": \"soft\",\n",
    "            \"specType\": \"jsonArray\",\n",
    "            \"tables\": [{\n",
    "              \"tableName\": table_name,\n",
    "              \"jsonArraySpec\": {\n",
    "                \"rowIds\": datarepo_row_ids\n",
    "              }\n",
    "            }]\n",
    "        }\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            data_deletion_result, job_id = utils.wait_for_tdr_job(datasets_api.apply_dataset_data_deletion(id=dataset_id, data_deletion_request=data_deletion_payload))\n",
    "            print(\"Result: {}\".format(data_deletion_result))\n",
    "        except Exception as e:\n",
    "            print(\"Error: {}\".format(str(e)))\n",
    "    else:\n",
    "        print(\"No datarepo_row_ids specified for deletion.\")\n",
    "\n",
    "# Function to collect all datarepo rows for a particular table within a dataset\n",
    "def collect_all_datarepo_rows(dataset_id, table_name):\n",
    "    try:\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        if response[\"access_information\"][\"big_query\"]: \n",
    "            cloud = \"gcp\"\n",
    "            bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "            bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        else:\n",
    "            cloud = \"azure\"\n",
    "            for parquet_table in response[\"access_information\"][\"parquet\"][\"tables\"]:\n",
    "                if parquet_table[\"name\"] == table_name:\n",
    "                    sas_url = parquet_table[\"url\"] + \"?\" + parquet_table[\"sas_token\"]\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving dataset information: {}\".format(str(e)))\n",
    "    if cloud == \"gcp\":\n",
    "        client = bigquery.Client()\n",
    "        query = \"SELECT datarepo_row_id FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = table_name)\n",
    "        try:\n",
    "            query_job = client.query(query)\n",
    "            results = [row[\"datarepo_row_id\"] for row in query_job]\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(\"Error retrieving datarepo_row_id list: {}\".format(str(e)))\n",
    "    else:\n",
    "#         blob_client = BlobClient.from_blob_url(sas_url)\n",
    "#         downloaded_blob = blob_client.download_blob()\n",
    "#         bytes_io = BytesIO(downloaded_blob.readall())\n",
    "#         df_blob = pd.read_parquet(bytes_io)\n",
    "        retrieval_error = False\n",
    "        max_page_size = 1000\n",
    "        records_fetched = 0 \n",
    "        total_record_count = 1\n",
    "        results = []\n",
    "        while records_fetched < total_record_count and not retrieval_error:\n",
    "            row_start = records_fetched\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                payload = {\n",
    "                  \"offset\": row_start,\n",
    "                  \"limit\": max_page_size,\n",
    "                  \"sort\": \"datarepo_row_id\",\n",
    "                  \"direction\": \"asc\",\n",
    "                  \"filter\": \"\"\n",
    "                }\n",
    "                try:\n",
    "                    dataset_results = datasets_api.query_dataset_data_by_id(id=dataset_id, table=table_name, query_data_request_model=payload).to_dict() \n",
    "                    total_record_count = dataset_results[\"total_row_count\"]\n",
    "                    for record in dataset_results[\"result\"]:\n",
    "                        results.append(record[\"datarepo_row_id\"])\n",
    "                        records_fetched += 1\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 5:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        warn_str = \"Error retrieving data_repo_row_ids for table.\"\n",
    "                        logging.warning(warn_str)\n",
    "                        retrieval_error = True\n",
    "                        break\n",
    "        return results\n",
    "    \n",
    "# Function to loop through datasets and delete\n",
    "def execute_deletions(dataset_id_list, table_list, delete_all_records, delete_record_list):\n",
    "    for dataset_id in dataset_id_list:\n",
    "        print(f\"Processing record deletions for dataset {dataset_id}\")\n",
    "        for table in table_list:\n",
    "            print(f\"Processing record deletion for {table}\")\n",
    "            if delete_all_records:\n",
    "                datarepo_row_ids = collect_all_datarepo_rows(dataset_id, table)\n",
    "            else:\n",
    "                datarepo_row_ids = delete_record_list\n",
    "            if datarepo_row_ids:\n",
    "                delete_datarepo_rows(dataset_id, table, datarepo_row_ids)\n",
    "            else:\n",
    "                print(\"No records specified for deletion.\")\n",
    "                \n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "execute_deletions(dataset_id_list, table_list, delete_all_records, delete_record_list)              \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Lock/Unlock Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def update_snapshot_lock_status(snapshot_action, snapshot_id_list):\n",
    "    results = []\n",
    "    # Validate snapshot action\n",
    "    print(f\"Validating provided snapshot action: {snapshot_action}\")\n",
    "    if snapshot_action not in [\"LOCK\", \"UNLOCK\"]:\n",
    "        results.append([\"ALL\", snapshot_action, \"Failure\", \"Invalid snapshot action specified. Must be LOCK or UNLOCK.\"])\n",
    "    else:\n",
    "        # Loop through and process snapshots\n",
    "        act = snapshot_action.lower()\n",
    "        for snapshot_id in snapshot_id_list:\n",
    "\n",
    "            # Initialize\n",
    "            print(f\"Updating snapshot lock status for snapshot: {snapshot_id}.\")\n",
    "            error_str = \"\"\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "\n",
    "            # Update snapshot lock status\n",
    "            if act == \"lock\":\n",
    "                # Lock snapshot\n",
    "                try:\n",
    "                    response = snapshots_api.lock_snapshot(id=snapshot_id)\n",
    "                    results.append([snapshot_id, snapshot_action, \"Success\", None])\n",
    "                except Exception as e: \n",
    "                    error_str = f\"Error updating snapshot lock status: {str(e)}\"\n",
    "                    print(error_str)\n",
    "                    results.append([snapshot_id, snapshot_action, \"Failure\", error_str])\n",
    "            else:\n",
    "                # Fetch exclusive lock from snapshot (if there is one)\n",
    "                try:\n",
    "                    snapshot_detail = snapshots_api.retrieve_snapshot(id=snapshot_id).to_dict()\n",
    "                    lock_name = snapshot_detail[\"resource_locks\"].get(\"exclusive\")\n",
    "                    if lock_name:\n",
    "                        # Unlock snapshot (if locked)\n",
    "                        try:\n",
    "                            request_body = {\"lockName\": lock_name, \"forceUnlock\": False}\n",
    "                            response = snapshots_api.unlock_snapshot(id=snapshot_id, unlock_resource_request=request_body)\n",
    "                            results.append([snapshot_id, snapshot_action, \"Success\", None])\n",
    "                        except Exception as e: \n",
    "                            error_str = f\"Error updating snapshot lock status: {str(e)}\"\n",
    "                            print(error_str)\n",
    "                            results.append([snapshot_id, snapshot_action, \"Failure\", error_str])\n",
    "                    else:\n",
    "                        results.append([snapshot_id, snapshot_action, \"Success\", \"No existing lock found on snapshot.\"])\n",
    "                except Exception as e:\n",
    "                    error_str = f\"Error retrieving lock on snapshot: {str(e)}\"\n",
    "                    results.append([snapshot_id, snapshot_action, \"Failure\", error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"snapshot\", \"action\", \"status\", \"errors\"])\n",
    "    display(results_df)\n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the action to apply to the snapshots (LOCK/UNLOCK):\n",
    "snapshot_action = \"UNLOCK\"\n",
    "\n",
    "# Specify the list of snapshots to apply the action to:\n",
    "snapshot_id_list = [\n",
    "    \"c3e5c093-3156-4b4c-be3a-2c307c3d8b23\"\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "update_snapshot_lock_status(snapshot_action, snapshot_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## TDR Dataset and/or Snapshot Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     12,
     23
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to delete a specific TDR Snapshot\n",
    "def delete_snapshot(snapshot_id):\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete snapshot = {}\".format(snapshot_id))\n",
    "    try:\n",
    "        delete_snapshot_result, job_id = utils.wait_for_tdr_job(snapshots_api.delete_snapshot(id=snapshot_id))\n",
    "        print(\"Result: {}\".format(delete_snapshot_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset\n",
    "def delete_dataset(dataset_id):\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {}\".format(dataset_id))\n",
    "    try:\n",
    "        delete_dataset_result, job_id = utils.wait_for_tdr_job(datasets_api.delete_dataset(id=dataset_id))\n",
    "        print(\"Result: {}\".format(delete_dataset_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset and all of its Snapshots\n",
    "def delete_dataset_and_all_snapshots(dataset_id):\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {} and all associated snapshots\".format(dataset_id))\n",
    "    dataset_id_list = [dataset_id]\n",
    "    # Delete snapshots\n",
    "    snapshot_list = snapshots_api.enumerate_snapshots(dataset_ids=dataset_id_list)\n",
    "    if snapshot_list.items:\n",
    "        for snapshot in snapshot_list.items:\n",
    "            snapshot_id = str(snapshot.id)\n",
    "            delete_snapshot(snapshot_id)\n",
    "            sleep(10)\n",
    "    # Delete dataset\n",
    "    delete_dataset(dataset_id)\n",
    "\n",
    "# # Delete snapshots\n",
    "# snapshot_id_list = [\n",
    "#     '011b65c5-fd63-478c-9396-a16c96f61a11',\n",
    "# ]\n",
    "# for snapshot_id in snapshot_id_list:\n",
    "#     delete_snapshot(snapshot_id)\n",
    "\n",
    "# Delete datasets and all their associated snapshots\n",
    "dataset_id_list = [\n",
    "'a43075e3-9abd-4518-bc17-ff162d60cbde',\n",
    "'7d546f72-1688-417b-8af2-2f9c98034cd4',\n",
    "'ed82e510-37aa-47f6-88f0-b2ba33e0fdb0',\n",
    "'77d3754a-6e43-432f-afa8-c8a24c77faab',\n",
    "'dea7d0d6-e27a-4447-b06f-1136c6bab6e3',\n",
    "]\n",
    "for dataset_id in dataset_id_list:\n",
    "    delete_dataset_and_all_snapshots(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Clean Up Outdated AnVIL TDR Service Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_sa_list = [\n",
    "]\n",
    "\n",
    "# Establish credentials\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "\n",
    "# Get current anvil_tdr_ingest membership\n",
    "group = \"anvil_tdr_ingest\"\n",
    "group_members = requests.get(\n",
    "    url=f\"https://api.firecloud.org/api/groups/{group}\",\n",
    "    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    ").json()\n",
    "\n",
    "# Loop through anvil_tdr_ingest membership and remove outdated users\n",
    "user_cnt = 0\n",
    "success_cnt = 0\n",
    "for member in group_members[\"membersEmails\"]:\n",
    "    if \"tdr-ingest-sa\" in member and member not in valid_sa_list:\n",
    "        user_cnt += 1\n",
    "        response = requests.delete(\n",
    "            url=f\"https://api.firecloud.org/api/groups/{group}/member/{member}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code == 204:\n",
    "            success_cnt += 1\n",
    "print(f\"Group ({group}) clean-up: \")\n",
    "print(f\"\\t- Users to remove: {user_cnt}\")\n",
    "print(f\"\\t- Users removed successfully: {success_cnt}\")\n",
    "\n",
    "# Get current workspace membership\n",
    "ws_members = requests.get(\n",
    "    url=f\"https://api.firecloud.org/api/workspaces/{ws_project}/{ws_name}/acl\",\n",
    "    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    ").json()\n",
    "\n",
    "# Loop through workspace membership and remove outdated users\n",
    "user_cnt = 0\n",
    "success_cnt = 0\n",
    "for member in ws_members[\"acl\"].keys():\n",
    "    if \"tdr-ingest-sa\" in member and member not in valid_sa_list:\n",
    "        user_cnt += 1\n",
    "        payload = [{\n",
    "            \"email\": member,\n",
    "            \"accessLevel\": \"NO ACCESS\",\n",
    "            \"canShare\": False,\n",
    "            \"canCompute\": False\n",
    "        }]\n",
    "        response = requests.patch(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{ws_project}/{ws_name}/acl\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}, \n",
    "            json=payload\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            success_cnt += 1\n",
    "print(f\"Workspace ({ws_project}/{ws_name}) clean-up: \")\n",
    "print(f\"\\t- Users to remove: {user_cnt}\")\n",
    "print(f\"\\t- Users removed successfully: {success_cnt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Other Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gsutil -u anvil-datastorage ls gs://fc-secure-33cad843-3453-42ea-bf50-0eda2b52171d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
