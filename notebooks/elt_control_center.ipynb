{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print(\"Version 1.0.0: 09/23/2022 5:06pm - Nate Calvanese - First version created\")\n",
    "#print(\"Version 1.0.1: 09/26/2022 11:18m - Nate Calvanese - Fixed bug in default dataset naming\")\n",
    "#print(\"Version 1.0.2: 09/27/2022 2:43pm - Nate Calvanese - Added ability to aggregate multiple workspaces into one dataset\")\n",
    "#print(\"Version 1.0.3: 10/5/2022 1:32pm - Nate Calvanese - Added support for chunking up ingest requests\")\n",
    "#print(\"Version 1.0.4: 10/6/2022 10:35am - Nate Calvanese - Updated use of TDR utility functions\")\n",
    "#print(\"Version 1.0.5: 10/13/2022 10:54am - Nate Calvanese - Parameter tweaks for latest changes\")\n",
    "#print(\"Version 1.0.6: 10/21/2022 10:53am - Nate Calvanese - Version stamp for latest changes to supporting notebooks\")\n",
    "#print(\"Version 1.0.7: 10/24/2022 4:58pm - Nate Calvanese - Added support for project entity name derivation\")\n",
    "#print(\"Version 1.0.8: 10/26/2022 4:24pm - Nate Calvanese - Added support for batching mapping activities in section 3\")\n",
    "#print('Version 1.0.9: 2/21/2023 2:50pm - Nate Calvanese - Added support for $BQ_DATASET substitution variable in mapping section')\n",
    "#print('Version 1.0.10: 3/8/2023 8:17am - Nate Calvanese - Performance improvements')\n",
    "#print('Version 1.0.11: 7/11/2023 8:17am - Nate Calvanese - Added auth domain back as reader on snapshots')\n",
    "#print('Version 1.0.12: 9/1/2023 10:16am - Nate Calvanese - Added functionality to enable/disable secure monitoring for public datasets.')\n",
    "#print('Version 1.0.13: 12/15/2023 9:00am - Nate Calvanese - Added functionality to optionally truncate tables before ingest')\n",
    "print('Version 1.0.14: 1/12/2024 11:28am - Nate Calvanese - Added max_combined_rec_ref_size as a global parameter')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Common Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional modules (one time effort per cloud environment)\n",
    "!pip install --upgrade pip import_ipynb data_repo_client urllib3 xmltodict azure-storage-blob\n",
    "# !pip install data_repo_client==1.409.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording workspace environment variables:\n",
      "Workspace name = anvil_workspace_ingest_resources_dev\n",
      "Workspace project = dsp-data-ingest\n",
      "Workspace bucket = gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03\n",
      "Workspace bucket name = fc-2a9eefc3-0302-427f-9ac3-82f078741c03\n",
      "\n",
      "Running imports:\n",
      "importing Jupyter notebook from ingest_pipeline_utilities.ipynb\n",
      "Version 1.0.45: 10/18/2024 2:09pm - Nate Calvanese - Fixed performance bug with find_and_add_fileref_fields function.\n",
      "importing Jupyter notebook from source_files_creation.ipynb\n",
      "Version 1.0.9: 2/25/2023 3:15pm - Nate Calvanese - Replaced FAPI with utils functions\n",
      "importing Jupyter notebook from build_file_inventory.ipynb\n",
      "Version 2.0.4: 10/18/2024 2:19pm - Nate Calvanese - Updated get_objects_list function to not use fuzzy matching for full file paths\n",
      "importing Jupyter notebook from process_table_data.ipynb\n",
      "Version: 1.0.10: 1/12/2024 11:25am - Nate Calvanese - Made max_combined_rec_ref_size configurable\n",
      "importing Jupyter notebook from build_mapping_query.ipynb\n",
      "Version 1.0.17: 9/19/2024 3:51pm - Nate Calvanese - Tweaked VOCAB_MAP function to trim whitespace when joining non-array values to the vocab table\n",
      "importing Jupyter notebook from output_data_validation.ipynb\n",
      "Version 2.0.8: 9/20/2024 9:06pm -- Added high-priority flags in the object returned by the function\n",
      "importing Jupyter notebook from resolve_dangling_foreign_keys.ipynb\n",
      "Version 1.0.4: 09/26/2024 9:28am - Nate Calvanese - Improved logic for handling part_of_dataset field\n",
      "importing Jupyter notebook from infer_file_relationships.ipynb\n",
      "Version 1.0.3: 12/11/2023 1:25pm - Nate Calvanese - Fixed bug in query logic to correct source_datarepo_row_ids\n",
      "importing Jupyter notebook from identify_supplementary_files.ipynb\n",
      "Version 1.0.2: 10/4/2023 10:40am - Nate Calvanese - Updated query logic and added validation\n"
     ]
    }
   ],
   "source": [
    "# Workspace environment variables\n",
    "import os\n",
    "import re\n",
    "print(\"Recording workspace environment variables:\")\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "print(f\"Workspace name = {ws_name}\")\n",
    "print(f\"Workspace project = {ws_project}\")\n",
    "print(f\"Workspace bucket = {ws_bucket}\")\n",
    "print(f\"Workspace bucket name = {ws_bucket_name}\")\n",
    "\n",
    "# Copy latest version of the pipeline notebooks to the cloud environment (uncomment if any notebooks have changed since last run)\n",
    "# print(\"\\nCopying latest pipeline notebooks to the cloud environment:\")\n",
    "# !gsutil -m cp $ws_bucket/notebooks/*.ipynb .\n",
    "\n",
    "# Additional imports\n",
    "print(\"\\nRunning imports:\")\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "from firecloud import api as fapi\n",
    "import data_repo_client\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_mapping_query as bmq\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "from time import sleep\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pyarrow.parquet as pq\n",
    "from azure.storage.blob import BlobClient, ContainerClient\n",
    "\n",
    "# Common pipeline variables (AnVIL)\n",
    "ws_attributes = utils.get_workspace_attributes(ws_project, ws_name)\n",
    "params = {}\n",
    "params[\"ws_name\"] = ws_name\n",
    "params[\"ws_project\"] = ws_project\n",
    "params[\"ws_bucket\"] = ws_bucket\n",
    "params[\"ws_bucket_name\"] = ws_bucket_name\n",
    "params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\" \n",
    "params[\"google_project\"] = ws_attributes[\"googleProject\"]\n",
    "params[\"create_file_table\"] = True\n",
    "params[\"file_table_name\"] = \"file_inventory\"\n",
    "params[\"ingest_user_to_add\"] = \"tdr_sa\"  # tdr_sa or anvil_tdr_ingest\n",
    "params[\"global_file_exclusions\"] = [\"SubsetHailJointCall\", \".vds/\", \"ingest_ignore\"]\n",
    "params[\"max_combined_rec_ref_size\"] = 40000\n",
    "\n",
    "# Configure logging format\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"EL\" Pipeline: Load Dataset to TDR in Source Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline run variables set:\n",
      "Profile ID: e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\n",
      "Ingests to run: \n",
      "- Workspace [anvil-datastorage/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS] to TDR dataset [ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023] via Staging Area [AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS]\n",
      "\t- PHS ID = phs001569\n",
      "\t- Consent Short Name = GRU\n",
      "\t- Auth Domains = ['AUTH_ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS']\n",
      "\t- Public Dataset = False\n",
      "\t- Data Files Source Bucket = fc-secure-cbf1f8fb-8185-46c9-9034-63073cbe7be7\n",
      "- Workspace [anvil-datastorage/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES] to TDR dataset [ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023] via Staging Area [AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES]\n",
      "\t- PHS ID = phs001569\n",
      "\t- Consent Short Name = GRU\n",
      "\t- Auth Domains = ['AUTH_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES']\n",
      "\t- Public Dataset = False\n",
      "\t- Data Files Source Bucket = fc-secure-d8de1fe3-972d-480f-a8a8-2bbc251add30\n",
      "Skip source files creation? False\n",
      "Skip file inventory creation? False\n",
      "Skip table data processing? False\n",
      "Skip ingests? False\n",
      "Truncate tables before ingest? True\n",
      "Ingest override list: {}\n",
      "Skip snapshot creation? True\n"
     ]
    }
   ],
   "source": [
    "## >>> Run Variables <<<\n",
    "# For datasets split across multiple workspaces, set the staging area and target TDR dataset to the \n",
    "# same value to collect all of the source data and process it together.\n",
    "workspace_run_list = [\n",
    "    #[\"Workspace_Name\", \"Workspace_Project\", Public (True/False), \"Staging Area (Leave empty for default)\", \"Target_TDR_Dataset_Name (Leave empty for default)\", Run (True/False)]\n",
    "#     [\"ANVIL_Workspace_1\", \"anvil-datastorage\", False, \"\", \"\", False],\n",
    "#     [\"ANVIL_Workspace_2\", \"anvil-datastorage\", False, \"\", \"\", False],\n",
    "    ['AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'anvil-datastorage', False, '', '', True],\n",
    "    ['AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'anvil-datastorage', False, '', '', True],\n",
    "]\n",
    "params[\"skip_source_files_creation\"] = False\n",
    "params[\"skip_file_inventory_creation\"] = False\n",
    "params[\"skip_table_data_processing\"] = False\n",
    "params[\"skip_ingests\"] = False\n",
    "params[\"trunc_before_ingest\"] = True\n",
    "params[\"skip_snapshot_creation\"] = True\n",
    "params[\"snapshot_readers_list\"] = [\"auth-domain\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "\n",
    "\n",
    "## >>> File Inventory Variables <<<\n",
    "# The GCS bucket associated with the source workspace will be automatically included in the file inventory build. To specify \n",
    "# additional GCS buckets to include in the file inventory build, add entries to the below dictionary.\n",
    "params[\"additional_file_inventory_sources\"] = {}\n",
    "# EXAMPLE:\n",
    "# params[\"additional_file_inventory_sources\"] = {\n",
    "#     \"staging_area\": {\n",
    "#         \"bucket_name\": {\n",
    "#             \"include_dirs\": [], # Leave empty to include all directories in bucket\n",
    "#             \"exclude_dirs\": [] # Exclusions will take precedence over inclusions\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "## >>> Ingest Variables <<<\n",
    "# For cases where you only want to ingest a subset of files, use the below dictionary to specify exactly what should be ingested.\n",
    "params[\"ingest_list_override\"] = {\n",
    "}\n",
    "# EXAMPLE:\n",
    "# params[\"ingest_list_override\"] = {\n",
    "#     \"ws_table\": [\"ws_table_0.json\"], # Leave empty to run ingest for every file for target table\n",
    "# }\n",
    "\n",
    "\n",
    "## >>> File Reference Variables <<<\n",
    "# Fields containing GCS links will be identified automatically by the pipeline. The below dict should contain any fields\n",
    "# that contain file references that aren't proper GCS links in the workspace tables.\n",
    "data_file_refs_dict = {   \n",
    "}\n",
    "# Definitions:\n",
    "#    Required Fields: column, method, mode, create_new field\n",
    "#    Optional Fields: match_multiple_files (default to True), match_regex (default to None), match_type (default to 'partial'), new_field_name (default to None)\n",
    "#    Methods: \n",
    "#       file_path_match -- Field contains a full or partial file path, which can be matched to the file inventory to grab the file(s) referenced \n",
    "#       tdr_file_id -- Field contains file UUIDs of files already ingested into the target TDR dataset\n",
    "#    Modes:\n",
    "#       fileref_in_line -- Populates the field with a file reference object\n",
    "#       fileref_table_ref -- Populates the field with an ID that joins to a file table. If no file table built, falls back on fileref_in_line logic.\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "# Print variables\n",
    "print(\"Pipeline run variables set:\")\n",
    "print(\"Profile ID: \" + params[\"profile_id\"])\n",
    "print(\"Ingests to run: \")\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "for workspace in workspace_run_list:\n",
    "    if workspace[5] == True:\n",
    "        ws_attributes = utils.get_workspace_attributes(workspace[1], workspace[0])\n",
    "        params[\"phs_id\"] = utils.format_phs_id(ws_attributes[\"attributes\"][\"phs_id\"]) if ws_attributes[\"attributes\"].get(\"phs_id\") else \"\"\n",
    "        auth_list = ws_attributes[\"authorizationDomain\"] if ws_attributes.get(\"authorizationDomain\") else []\n",
    "        params[\"auth_domains\"] = [x[\"membersGroupName\"] for x in auth_list]\n",
    "        params[\"consent_name\"] = ws_attributes[\"attributes\"][\"library:dataUseRestriction\"] if ws_attributes[\"attributes\"].get(\"library:dataUseRestriction\") else \"\"\n",
    "        if not params[\"consent_name\"]:\n",
    "            ws_tags = ws_attributes[\"attributes\"].get(\"tag:tags\")\n",
    "            if ws_tags:\n",
    "                for ws_tag in ws_tags:\n",
    "                    if \"consent_code:\" in ws_tag:\n",
    "                        params[\"consent_name\"] = ws_tag.replace(\"consent_code:\", \"\").strip()\n",
    "                        break\n",
    "        params[\"data_files_src_bucket\"] = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "        params[\"public_dataset\"] = workspace[2]\n",
    "        workspace[4] = workspace[4] if workspace[4] else utils.format_dataset_name(workspace[0])\n",
    "        workspace[3] = workspace[3] if workspace[3] else workspace[0]\n",
    "        print(\"- Workspace [\" + workspace[1] + \"/\" + workspace[0] + \"] to TDR dataset [\" + workspace[4] + \"] via Staging Area [\" + workspace[3] + \"]\")\n",
    "        print(\"\\t- PHS ID = \" + params[\"phs_id\"])\n",
    "        print(\"\\t- Consent Short Name = \" + params[\"consent_name\"])\n",
    "        print(\"\\t- Auth Domains = \" + str(params[\"auth_domains\"]))\n",
    "        print(\"\\t- Public Dataset = \" + str(params[\"public_dataset\"]))\n",
    "        print(\"\\t- Data Files Source Bucket = \" + params[\"data_files_src_bucket\"])\n",
    "print(\"Skip source files creation? \" + str(params[\"skip_source_files_creation\"]))\n",
    "print(\"Skip file inventory creation? \" + str(params[\"skip_file_inventory_creation\"]))\n",
    "print(\"Skip table data processing? \" + str(params[\"skip_table_data_processing\"]))\n",
    "print(\"Skip ingests? \" + str(params[\"skip_ingests\"]))\n",
    "print(\"Truncate tables before ingest? \" + str(params[\"trunc_before_ingest\"]))\n",
    "print(\"Ingest override list: \" + str(params[\"ingest_list_override\"]))\n",
    "print(\"Skip snapshot creation? \" + str(params[\"skip_snapshot_creation\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/23/2024 01:24:40 PM - INFO: Starting Workspace Connector Pipeline for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.\n",
      "10/23/2024 01:24:40 PM - INFO: Creating or updating provenance.json file for Staging Area: AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS\n",
      "10/23/2024 01:24:41 PM - INFO: Unable to retrieve provenance.json file. Creating new provenance.json file.\n",
      "10/23/2024 01:24:44 PM - INFO: Additional file reference fields found and marked for processing: qc_result_sample.cram, sample.crai_path, sample.md5_path, sample.cram_path\n",
      "10/23/2024 01:24:57 PM - INFO: Running source files creation.\n",
      "10/23/2024 01:24:57 PM - INFO: List of entity tables in current workspace: participant, subject, qc_result_sample, sample_set, sample\n",
      "10/23/2024 01:24:57 PM - INFO: Starting download of tsv file for participant table.\n",
      "10/23/2024 01:24:58 PM - INFO: Copying participant_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/participant\n",
      "10/23/2024 01:25:01 PM - INFO: Starting download of tsv file for subject table.\n",
      "10/23/2024 01:25:02 PM - INFO: Copying subject_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/subject\n",
      "10/23/2024 01:25:05 PM - INFO: Starting download of tsv file for qc_result_sample table.\n",
      "10/23/2024 01:25:06 PM - INFO: Copying qc_result_sample_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/qc_result_sample\n",
      "10/23/2024 01:25:09 PM - INFO: Starting download of tsv file for sample_set table.\n",
      "10/23/2024 01:25:09 PM - INFO: Copying sample_set_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/sample_set\n",
      "10/23/2024 01:25:13 PM - INFO: Starting download of tsv file for sample table.\n",
      "10/23/2024 01:25:14 PM - INFO: Copying sample_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/sample\n",
      "10/23/2024 01:25:18 PM - INFO: Starting download of workspace attribute information.\n",
      "10/23/2024 01:25:18 PM - INFO: Copying workspace_attributes_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/workspace_attributes\n",
      "10/23/2024 01:25:21 PM - INFO: Download and copy of tsv files complete. Validation results: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>entity_type</th>\n",
       "      <th>tsv_file_count</th>\n",
       "      <th>data_model_count</th>\n",
       "      <th>record_count_validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>participant</td>\n",
       "      <td>1136</td>\n",
       "      <td>1136</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject</td>\n",
       "      <td>1136</td>\n",
       "      <td>1136</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qc_result_sample</td>\n",
       "      <td>1135</td>\n",
       "      <td>1135</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_set</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample</td>\n",
       "      <td>1136</td>\n",
       "      <td>1136</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     entity_type    tsv_file_count  data_model_count record_count_validation\n",
       "0       participant      1136             1136                Passed        \n",
       "1           subject      1136             1136                Passed        \n",
       "2  qc_result_sample      1135             1135                Passed        \n",
       "3        sample_set         1                1                Passed        \n",
       "4            sample      1136             1136                Passed        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/23/2024 01:25:23 PM - INFO: The Workspace Connector Pipeline has completed for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.\n",
      "10/23/2024 01:25:31 PM - INFO: Pipeline Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Workspace</th>\n",
       "      <th>Staging Area</th>\n",
       "      <th>Time</th>\n",
       "      <th>Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 13:24:57</td>\n",
       "      <td>Create or Update Staging Area Provenance</td>\n",
       "      <td>Success</td>\n",
       "      <td>{\"phs_id\": \"phs001569\", \"consent_name\": \"GRU\", \"source_workspaces\": [\"AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS\"], \"auth_domains\": [\"AUTH_ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS\"], \"data_files_src_buckets\": {\"fc-secure-cbf1f8fb-8185-46c9-9034-63073cbe7be7\": {\"include_dirs\": [], \"exclude_dirs\": []}}, \"data_file_refs\": {\"qc_result_sample\": [{\"column\": \"cram\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}], \"sample\": [{\"column\": \"crai_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"md5_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"cram_path\", \"method\": \"fil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 13:25:23</td>\n",
       "      <td>Create Source Files</td>\n",
       "      <td>Success</td>\n",
       "      <td>[{\"entity_type\": \"participant\", \"tsv_file_count\": \"1136\", \"data_model_count\": 1136, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"subject\", \"tsv_file_count\": \"1136\", \"data_model_count\": 1136, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"qc_result_sample\", \"tsv_file_count\": \"1135\", \"data_model_count\": 1135, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"sample_set\", \"tsv_file_count\": \"1\", \"data_model_count\": 1, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"sample\", \"tsv_file_count\": \"1136\", \"data_model_count\": 1136, \"record_count_validation\": \"Passed\"}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Workspace                             Staging Area                      Time                            Step                    Status   \\\n",
       "0  AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 13:24:57  Create or Update Staging Area Provenance  Success   \n",
       "1  AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 13:25:23                       Create Source Files  Success   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Message                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "0  {\"phs_id\": \"phs001569\", \"consent_name\": \"GRU\", \"source_workspaces\": [\"AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS\"], \"auth_domains\": [\"AUTH_ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS\"], \"data_files_src_buckets\": {\"fc-secure-cbf1f8fb-8185-46c9-9034-63073cbe7be7\": {\"include_dirs\": [], \"exclude_dirs\": []}}, \"data_file_refs\": {\"qc_result_sample\": [{\"column\": \"cram\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}], \"sample\": [{\"column\": \"crai_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"md5_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"cram_path\", \"method\": \"fil  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                        [{\"entity_type\": \"participant\", \"tsv_file_count\": \"1136\", \"data_model_count\": 1136, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"subject\", \"tsv_file_count\": \"1136\", \"data_model_count\": 1136, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"qc_result_sample\", \"tsv_file_count\": \"1135\", \"data_model_count\": 1135, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"sample_set\", \"tsv_file_count\": \"1\", \"data_model_count\": 1, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"sample\", \"tsv_file_count\": \"1136\", \"data_model_count\": 1136, \"record_count_validation\": \"Passed\"}]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/23/2024 01:25:32 PM - INFO: Starting Workspace Connector Pipeline for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.\n",
      "10/23/2024 01:25:32 PM - INFO: Creating or updating provenance.json file for Staging Area: AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES\n",
      "10/23/2024 01:25:32 PM - INFO: Existing provenance.json file found. Updating with new information.\n",
      "10/23/2024 01:26:07 PM - INFO: Running source files creation.\n",
      "10/23/2024 01:26:07 PM - INFO: List of entity tables in current workspace: participant, sample, sample_set, subject\n",
      "10/23/2024 01:26:07 PM - INFO: Starting download of tsv file for participant table.\n",
      "10/23/2024 01:26:08 PM - INFO: Copying participant_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/participant\n",
      "10/23/2024 01:26:12 PM - INFO: Starting download of tsv file for sample table.\n",
      "10/23/2024 01:26:36 PM - INFO: Copying sample_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/sample\n",
      "10/23/2024 01:26:39 PM - INFO: Starting download of tsv file for sample_set table.\n",
      "10/23/2024 01:26:41 PM - INFO: Copying sample_set_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/sample_set\n",
      "10/23/2024 01:26:45 PM - INFO: Starting download of tsv file for subject table.\n",
      "10/23/2024 01:26:52 PM - INFO: Copying subject_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/subject\n",
      "10/23/2024 01:26:56 PM - INFO: Starting download of workspace attribute information.\n",
      "10/23/2024 01:26:56 PM - INFO: Copying workspace_attributes_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.tsv to gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/workspace_attributes\n",
      "10/23/2024 01:26:59 PM - INFO: Download and copy of tsv files complete. Validation results: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>entity_type</th>\n",
       "      <th>tsv_file_count</th>\n",
       "      <th>data_model_count</th>\n",
       "      <th>record_count_validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>participant</td>\n",
       "      <td>16808</td>\n",
       "      <td>16808</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample</td>\n",
       "      <td>16808</td>\n",
       "      <td>16808</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_set</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject</td>\n",
       "      <td>16808</td>\n",
       "      <td>16808</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_type  tsv_file_count  data_model_count record_count_validation\n",
       "0  participant      16808            16808               Passed        \n",
       "1       sample      16808            16808               Passed        \n",
       "2   sample_set          1                1               Passed        \n",
       "3      subject      16808            16808               Passed        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/23/2024 01:27:01 PM - INFO: The Workspace Connector Pipeline has completed for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.\n",
      "10/23/2024 01:27:09 PM - INFO: Pipeline Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Workspace</th>\n",
       "      <th>Staging Area</th>\n",
       "      <th>Time</th>\n",
       "      <th>Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 13:26:07</td>\n",
       "      <td>Create or Update Staging Area Provenance</td>\n",
       "      <td>Success</td>\n",
       "      <td>{\"phs_id\": \"phs001569\", \"consent_name\": \"GRU\", \"source_workspaces\": [\"AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES\"], \"auth_domains\": [\"AUTH_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES\"], \"data_files_src_buckets\": {\"fc-secure-d8de1fe3-972d-480f-a8a8-2bbc251add30\": {\"include_dirs\": [], \"exclude_dirs\": []}}, \"data_file_refs\": {\"qc_result_sample\": [{\"column\": \"cram\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}], \"sample\": [{\"column\": \"crai_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"md5_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"cram_path\", \"method\": \"file_pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 13:27:01</td>\n",
       "      <td>Create Source Files</td>\n",
       "      <td>Success</td>\n",
       "      <td>[{\"entity_type\": \"participant\", \"tsv_file_count\": \"16808\", \"data_model_count\": 16808, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"sample\", \"tsv_file_count\": \"16808\", \"data_model_count\": 16808, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"sample_set\", \"tsv_file_count\": \"1\", \"data_model_count\": 1, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"subject\", \"tsv_file_count\": \"16808\", \"data_model_count\": 16808, \"record_count_validation\": \"Passed\"}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Workspace                           Staging Area                     Time                            Step                    Status   \\\n",
       "0  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 13:26:07  Create or Update Staging Area Provenance  Success   \n",
       "1  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 13:27:01                       Create Source Files  Success   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Message                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "0  {\"phs_id\": \"phs001569\", \"consent_name\": \"GRU\", \"source_workspaces\": [\"AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES\"], \"auth_domains\": [\"AUTH_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES\"], \"data_files_src_buckets\": {\"fc-secure-d8de1fe3-972d-480f-a8a8-2bbc251add30\": {\"include_dirs\": [], \"exclude_dirs\": []}}, \"data_file_refs\": {\"qc_result_sample\": [{\"column\": \"cram\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}], \"sample\": [{\"column\": \"crai_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"md5_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"cram_path\", \"method\": \"file_pa  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [{\"entity_type\": \"participant\", \"tsv_file_count\": \"16808\", \"data_model_count\": 16808, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"sample\", \"tsv_file_count\": \"16808\", \"data_model_count\": 16808, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"sample_set\", \"tsv_file_count\": \"1\", \"data_model_count\": 1, \"record_count_validation\": \"Passed\"}, {\"entity_type\": \"subject\", \"tsv_file_count\": \"16808\", \"data_model_count\": 16808, \"record_count_validation\": \"Passed\"}]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/23/2024 01:27:09 PM - INFO: Starting Extract and Load (EL) Pipeline for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.\n",
      "10/23/2024 01:27:10 PM - INFO: Building file inventory.\n",
      "10/23/2024 01:29:15 PM - INFO: Recording inventory entries from fc-secure-cbf1f8fb-8185-46c9-9034-63073cbe7be7 (975003 objects total)\n",
      "10/23/2024 01:54:15 PM - INFO: 97500 files recorded (~10%)\n",
      "10/23/2024 01:55:19 PM - INFO: 195000 files recorded (~20%)\n",
      "10/23/2024 01:56:22 PM - INFO: 292500 files recorded (~30%)\n",
      "10/23/2024 01:57:30 PM - INFO: 390000 files recorded (~40%)\n",
      "10/23/2024 01:58:34 PM - INFO: 487500 files recorded (~50%)\n",
      "10/23/2024 01:59:39 PM - INFO: 585000 files recorded (~60%)\n",
      "10/23/2024 02:00:43 PM - INFO: 682500 files recorded (~70%)\n",
      "10/23/2024 02:01:48 PM - INFO: 780000 files recorded (~80%)\n",
      "10/23/2024 02:02:51 PM - INFO: 877500 files recorded (~90%)\n",
      "10/23/2024 02:03:57 PM - INFO: 975000 files recorded (~100%)\n",
      "10/23/2024 02:03:57 PM - INFO: All inventory entries recorded (975003 objects total).\n",
      "10/23/2024 02:03:57 PM - INFO: Removing file objects in global file exclusion list: SubsetHailJointCall; .vds/\n",
      "10/23/2024 02:03:57 PM - INFO: 3408 objects remain after file removal.\n",
      "10/23/2024 02:04:03 PM - INFO: File Inventory build succeeded. 3408 distinct files found after deduplication. 0 retries required.\n",
      "10/23/2024 02:04:03 PM - INFO: Processing table data for ingest.\n",
      "10/23/2024 02:04:08 PM - INFO: Target tables and files to be processed: {\"participant\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/participant/participant_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv\"], \"qc_result_sample\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/qc_result_sample/qc_result_sample_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv\"], \"sample\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/sample/sample_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv\"], \"sample_set\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/sample_set/sample_set_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv\"], \"subject\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/subject/subject_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv\"], \"workspace_attributes\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/workspace_attributes/workspace_attributes_AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.tsv\"], \"file_inventory\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/data_files/file_inventory/file_inventory.tsv\"]}\n",
      "10/23/2024 02:04:08 PM - INFO: Processing files for target table: participant.\n",
      "10/23/2024 02:04:13 PM - INFO: Processing files for target table: qc_result_sample.\n",
      "10/23/2024 02:04:17 PM - INFO: Processing files for target table: sample.\n",
      "10/23/2024 02:04:21 PM - INFO: Processing files for target table: sample_set.\n",
      "10/23/2024 02:04:24 PM - INFO: Processing files for target table: subject.\n",
      "10/23/2024 02:04:28 PM - INFO: Processing files for target table: workspace_attributes.\n",
      "10/23/2024 02:04:32 PM - INFO: Processing files for target table: file_inventory.\n",
      "10/23/2024 02:04:36 PM - INFO: Creating schema object and copying to cloud storage.\n",
      "10/23/2024 02:04:40 PM - INFO: File processing complete. Status: Success. Details: {\"file_inventory_population\": \"File inventory populated\", \"participant\": \"No errors raised\", \"qc_result_sample\": \"No errors raised\", \"sample\": \"No errors raised\", \"sample_set\": \"No errors raised\", \"subject\": \"No errors raised\", \"workspace_attributes\": \"No errors raised\", \"file_inventory\": \"No errors raised\"}. Tables to ingest: participant, qc_result_sample, sample, sample_set, subject, workspace_attributes, file_inventory\n",
      "10/23/2024 02:04:40 PM - INFO: Attempting to create or retrieve the specified TDR dataset.\n",
      "10/23/2024 02:04:41 PM - INFO: Creating new dataset: ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023\n",
      "TDR Job ID: B75adRljT92kw_bc4osCQQ\n",
      "10/23/2024 02:05:42 PM - INFO: Dataset Creation succeeded: {'id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'name': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'description': 'TDR Dataset for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'defaultProfileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'createdDate': '2024-10-23T14:05:21.370245Z', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': True, 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-561d6a93', 'storageAccount': None, 'phsId': 'phs001569', 'selfHosted': True, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': []}}\n",
      "10/23/2024 02:05:42 PM - INFO: Sleeping for a few minutes to let policy/permission changes propogate for new dataset...\n",
      "10/23/2024 02:06:12 PM - INFO: Setting up dataset ingest service account (SA).\n",
      "10/23/2024 02:06:13 PM - INFO: Adding dataset SA to anvil_tdr_ingest group.\n",
      "10/23/2024 02:06:35 PM - INFO: Dataset service account tdr-ingest-sa@datarepo-561d6a93.iam.gserviceaccount.com added to anvil_tdr_ingest sucessfully.\n",
      "10/23/2024 02:06:35 PM - INFO: Adding dataset SA to workspace anvil_workspace_ingest_resources_dev.\n",
      "10/23/2024 02:06:44 PM - INFO: User tdr-ingest-sa@datarepo-561d6a93.iam.gserviceaccount.com added to workspace as READER (without SHARE) permissions successfully.\n",
      "10/23/2024 02:06:44 PM - INFO: Adding dataset SA to workspace AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.\n",
      "10/23/2024 02:06:50 PM - INFO: User tdr-ingest-sa@datarepo-561d6a93.iam.gserviceaccount.com added to workspace as READER (without SHARE) permissions successfully.\n",
      "10/23/2024 02:06:50 PM - INFO: Sleeping for a few minutes to let policy/permission changes propogate...\n",
      "10/23/2024 02:13:50 PM - INFO: Running dataset ingests.\n",
      "10/23/2024 02:14:03 PM - INFO: Running ingests for target table: participant\n",
      "10/23/2024 02:14:03 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/participant/participant_0.json...\n",
      "10/23/2024 02:14:03 PM - INFO: Running ingest from participant_0.json to table participant.\n",
      "TDR Job ID: R3Snd0DVRyyhsdpCUg3Weg\n",
      "10/23/2024 02:14:34 PM - INFO: Ingest from file participant_0.json succeeded: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'participant', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/participant/participant_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 1136, 'bad_row_count': 0, 'load_result': None}\n",
      "10/23/2024 02:14:34 PM - INFO: Running ingests for target table: qc_result_sample\n",
      "10/23/2024 02:14:34 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/qc_result_sample/qc_result_sample_0.json...\n",
      "10/23/2024 02:14:34 PM - INFO: Running ingest from qc_result_sample_0.json to table qc_result_sample.\n",
      "TDR Job ID: thlRcNzFSXm-nYZvXsJk6Q\n",
      "10/23/2024 02:16:26 PM - INFO: Ingest from file qc_result_sample_0.json succeeded: Job succeeded, but error retrieving job result: (500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:16:16 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'eZAx8wxG', 'Content-Type': 'application/json', 'Content-Length': '368191', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"ecd0e3b1-a177-4487-8e33-0084688cf148\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/23/2024 02:16:26 PM - INFO: Running ingests for target table: sample\n",
      "10/23/2024 02:16:26 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/sample/sample_0.json...\n",
      "10/23/2024 02:16:26 PM - INFO: Running ingest from sample_0.json to table sample.\n",
      "TDR Job ID: SIqy24JyTQKASlhTXixIKw\n",
      "10/23/2024 02:19:09 PM - INFO: Ingest from file sample_0.json succeeded: Job succeeded, but error retrieving job result: (500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:18:58 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'ebWMGVXM', 'Content-Type': 'application/json', 'Content-Length': '374117', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"ecd0e3b1-a177-4487-8e33-0084688cf148\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_E\n",
      "10/23/2024 02:19:09 PM - INFO: Running ingests for target table: sample_set\n",
      "10/23/2024 02:19:09 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/sample_set/sample_set_0.json...\n",
      "10/23/2024 02:19:09 PM - INFO: Running ingest from sample_set_0.json to table sample_set.\n",
      "TDR Job ID: yd8Qw_ZUST2-pOr793XSlg\n",
      "10/23/2024 02:19:30 PM - INFO: Ingest from file sample_set_0.json succeeded: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'sample_set', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/sample_set/sample_set_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "10/23/2024 02:19:30 PM - INFO: Running ingests for target table: subject\n",
      "10/23/2024 02:19:30 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/subject/subject_0.json...\n",
      "10/23/2024 02:19:30 PM - INFO: Running ingest from subject_0.json to table subject.\n",
      "TDR Job ID: iUs5OLkzTiiFfj78q_b2lw\n",
      "10/23/2024 02:19:50 PM - INFO: Ingest from file subject_0.json succeeded: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'subject', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/subject/subject_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 1136, 'bad_row_count': 0, 'load_result': None}\n",
      "10/23/2024 02:19:50 PM - INFO: Running ingests for target table: workspace_attributes\n",
      "10/23/2024 02:19:50 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/workspace_attributes/workspace_attributes_0.json...\n",
      "10/23/2024 02:19:50 PM - INFO: Running ingest from workspace_attributes_0.json to table workspace_attributes.\n",
      "TDR Job ID: QnZiF1GEQm2T3wCPa1rNlA\n",
      "10/23/2024 02:20:11 PM - INFO: Ingest from file workspace_attributes_0.json succeeded: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'workspace_attributes', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/workspace_attributes/workspace_attributes_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 40, 'bad_row_count': 0, 'load_result': None}\n",
      "10/23/2024 02:20:11 PM - INFO: Running ingests for target table: file_inventory\n",
      "10/23/2024 02:20:11 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/file_inventory/file_inventory_0.json...\n",
      "10/23/2024 02:20:11 PM - INFO: Running ingest from file_inventory_0.json to table file_inventory.\n",
      "TDR Job ID: ocWw4sFZSeqBWYrD5ivwYg\n",
      "10/23/2024 02:22:53 PM - INFO: Ingest from file file_inventory_0.json succeeded: Job succeeded, but error retrieving job result: (500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:22:43 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'a85NLnO0', 'Content-Type': 'application/json', 'Content-Length': '374131', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"ecd0e3b1-a177-4487-8e33-0084688cf148\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_E\n",
      "10/23/2024 02:22:53 PM - INFO: Skipping snapshot creation on user request.\n",
      "10/23/2024 02:22:53 PM - INFO: The ingest pipeline has completed for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS.\n",
      "10/23/2024 02:23:02 PM - INFO: Pipeline Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Staging Area</th>\n",
       "      <th>Time</th>\n",
       "      <th>Step</th>\n",
       "      <th>Task</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 13:27:10</td>\n",
       "      <td>Initialization</td>\n",
       "      <td>Provenance File Retrieval</td>\n",
       "      <td>Success</td>\n",
       "      <td>{\"phs_id\": \"phs001569\", \"consent_name\": \"GRU\", \"source_workspaces\": [\"AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS\"], \"auth_domains\": [\"AUTH_ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS\"], \"data_files_src_buckets\": {\"fc-secure-cbf1f8fb-8185-46c9-9034-63073cbe7be7\": {\"include_dirs\": [], \"exclude_dirs\": []}}, \"data_file_refs\": {\"qc_result_sample\": [{\"column\": \"cram\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}], \"sample\": [{\"column\": \"crai_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"md5_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"cram_path\", \"method\": \"fil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:04:03</td>\n",
       "      <td>File Inventory Creation</td>\n",
       "      <td>Build File Inventory</td>\n",
       "      <td>Success</td>\n",
       "      <td>3408 files found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:04:40</td>\n",
       "      <td>Table Data Processing</td>\n",
       "      <td>Ingest Pre-Processing</td>\n",
       "      <td>Success</td>\n",
       "      <td>{\"file_inventory_population\": \"File inventory populated\", \"participant\": \"No errors raised\", \"qc_result_sample\": \"No errors raised\", \"sample\": \"No errors raised\", \"sample_set\": \"No errors raised\", \"subject\": \"No errors raised\", \"workspace_attributes\": \"No errors raised\", \"file_inventory\": \"No errors raised\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:04:41</td>\n",
       "      <td>Dataset Creation or Retrieval</td>\n",
       "      <td>Enumerate Datasets</td>\n",
       "      <td>Success</td>\n",
       "      <td>0 datasets found. Matching dataset_id =</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:05:42</td>\n",
       "      <td>Dataset Creation or Retrieval</td>\n",
       "      <td>Create New Dataset</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: B75adRljT92kw_bc4osCQQ - Truncated Response: {'id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'name': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'description': 'TDR Dataset for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'defaultProfileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'createdDate': '2024-10-23T14:05:21.370245Z', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': True, 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-561d6a93', 'storageAccount': None, 'phsId': 'phs001569', 'selfHosted': True, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': []}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:06:35</td>\n",
       "      <td>Dataset Service Account Setup</td>\n",
       "      <td>Add SA to Anvil Ingest Group</td>\n",
       "      <td>Success</td>\n",
       "      <td>tdr-ingest-sa@datarepo-561d6a93.iam.gserviceaccount.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:06:44</td>\n",
       "      <td>Dataset Service Account Setup</td>\n",
       "      <td>Add Ingest User to Workspace anvil_workspace_ingest_resources_dev</td>\n",
       "      <td>Success</td>\n",
       "      <td>tdr-ingest-sa@datarepo-561d6a93.iam.gserviceaccount.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:06:50</td>\n",
       "      <td>Dataset Service Account Setup</td>\n",
       "      <td>Add Ingest User to Workspace AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>Success</td>\n",
       "      <td>tdr-ingest-sa@datarepo-561d6a93.iam.gserviceaccount.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:14:34</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: participant - File: participant_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: R3Snd0DVRyyhsdpCUg3Weg - Truncated Response: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'participant', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/participant/participant_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 1136, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:16:26</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: qc_result_sample - File: qc_result_sample_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: thlRcNzFSXm-nYZvXsJk6Q - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:16:16 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'eZAx8wxG', 'Content-Type': 'application/json', 'Content-Length': '368191', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"ecd0e3b1-a177-4487-8e33-0084688cf148\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:19:09</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: sample - File: sample_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: SIqy24JyTQKASlhTXixIKw - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:18:58 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'ebWMGVXM', 'Content-Type': 'application/json', 'Content-Length': '374117', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"ecd0e3b1-a177-4487-8e33-0084688cf148\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:19:30</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: sample_set - File: sample_set_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: yd8Qw_ZUST2-pOr793XSlg - Truncated Response: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'sample_set', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/sample_set/sample_set_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:19:50</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: subject - File: subject_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: iUs5OLkzTiiFfj78q_b2lw - Truncated Response: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'subject', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/subject/subject_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 1136, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:20:11</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: workspace_attributes - File: workspace_attributes_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: QnZiF1GEQm2T3wCPa1rNlA - Truncated Response: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'workspace_attributes', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/workspace_attributes/workspace_attributes_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 40, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:22:53</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: file_inventory - File: file_inventory_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: ocWw4sFZSeqBWYrD5ivwYg - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:22:43 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'a85NLnO0', 'Content-Type': 'application/json', 'Content-Length': '374131', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"ecd0e3b1-a177-4487-8e33-0084688cf148\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS</td>\n",
       "      <td>2024-10-23 14:22:53</td>\n",
       "      <td>Snapshot Creation</td>\n",
       "      <td>Create and Share Snapshot</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>User request</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Staging Area                      Time                      Step                                              Task                                 Status   \\\n",
       "0   AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 13:27:10                 Initialization                                           Provenance File Retrieval  Success   \n",
       "1   AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:04:03        File Inventory Creation                                                Build File Inventory  Success   \n",
       "2   AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:04:40          Table Data Processing                                               Ingest Pre-Processing  Success   \n",
       "3   AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:04:41  Dataset Creation or Retrieval                                                  Enumerate Datasets  Success   \n",
       "4   AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:05:42  Dataset Creation or Retrieval                                                  Create New Dataset  Success   \n",
       "5   AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:06:35  Dataset Service Account Setup                                        Add SA to Anvil Ingest Group  Success   \n",
       "6   AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:06:44  Dataset Service Account Setup   Add Ingest User to Workspace anvil_workspace_ingest_resources_dev  Success   \n",
       "7   AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:06:50  Dataset Service Account Setup  Add Ingest User to Workspace AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  Success   \n",
       "8   AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:14:34                Dataset Ingests                       Table: participant - File: participant_0.json  Success   \n",
       "9   AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:16:26                Dataset Ingests             Table: qc_result_sample - File: qc_result_sample_0.json  Success   \n",
       "10  AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:19:09                Dataset Ingests                                 Table: sample - File: sample_0.json  Success   \n",
       "11  AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:19:30                Dataset Ingests                         Table: sample_set - File: sample_set_0.json  Success   \n",
       "12  AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:19:50                Dataset Ingests                               Table: subject - File: subject_0.json  Success   \n",
       "13  AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:20:11                Dataset Ingests     Table: workspace_attributes - File: workspace_attributes_0.json  Success   \n",
       "14  AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:22:53                Dataset Ingests                 Table: file_inventory - File: file_inventory_0.json  Success   \n",
       "15  AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS  2024-10-23 14:22:53              Snapshot Creation                                           Create and Share Snapshot  Skipped   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Message                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "0                                                           {\"phs_id\": \"phs001569\", \"consent_name\": \"GRU\", \"source_workspaces\": [\"AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS\"], \"auth_domains\": [\"AUTH_ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS\"], \"data_files_src_buckets\": {\"fc-secure-cbf1f8fb-8185-46c9-9034-63073cbe7be7\": {\"include_dirs\": [], \"exclude_dirs\": []}}, \"data_file_refs\": {\"qc_result_sample\": [{\"column\": \"cram\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}], \"sample\": [{\"column\": \"crai_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"md5_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"cram_path\", \"method\": \"fil  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   3408 files found  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {\"file_inventory_population\": \"File inventory populated\", \"participant\": \"No errors raised\", \"qc_result_sample\": \"No errors raised\", \"sample\": \"No errors raised\", \"sample_set\": \"No errors raised\", \"subject\": \"No errors raised\", \"workspace_attributes\": \"No errors raised\", \"file_inventory\": \"No errors raised\"}  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0 datasets found. Matching dataset_id =   \n",
       "4                                                                                                                                                                                                                                Job_ID: B75adRljT92kw_bc4osCQQ - Truncated Response: {'id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'name': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'description': 'TDR Dataset for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'defaultProfileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'createdDate': '2024-10-23T14:05:21.370245Z', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': True, 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-561d6a93', 'storageAccount': None, 'phsId': 'phs001569', 'selfHosted': True, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': []}}  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tdr-ingest-sa@datarepo-561d6a93.iam.gserviceaccount.com  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tdr-ingest-sa@datarepo-561d6a93.iam.gserviceaccount.com  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tdr-ingest-sa@datarepo-561d6a93.iam.gserviceaccount.com  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Job_ID: R3Snd0DVRyyhsdpCUg3Weg - Truncated Response: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'participant', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/participant/participant_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 1136, 'bad_row_count': 0, 'load_result': None}  \n",
       "9   Job_ID: thlRcNzFSXm-nYZvXsJk6Q - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:16:16 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'eZAx8wxG', 'Content-Type': 'application/json', 'Content-Length': '368191', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"ecd0e3b1-a177-4487-8e33-0084688cf148\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_E  \n",
       "10  Job_ID: SIqy24JyTQKASlhTXixIKw - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:18:58 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'ebWMGVXM', 'Content-Type': 'application/json', 'Content-Length': '374117', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"ecd0e3b1-a177-4487-8e33-0084688cf148\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_E  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Job_ID: yd8Qw_ZUST2-pOr793XSlg - Truncated Response: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'sample_set', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/sample_set/sample_set_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}  \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Job_ID: iUs5OLkzTiiFfj78q_b2lw - Truncated Response: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'subject', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/subject/subject_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 1136, 'bad_row_count': 0, 'load_result': None}  \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Job_ID: QnZiF1GEQm2T3wCPa1rNlA - Truncated Response: {'dataset_id': 'ecd0e3b1-a177-4487-8e33-0084688cf148', 'dataset': 'ANVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS_20241023', 'table': 'workspace_attributes', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS/table_data/workspace_attributes/workspace_attributes_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_EOCAD_PROMIS_WGS', 'row_count': 40, 'bad_row_count': 0, 'load_result': None}  \n",
       "14  Job_ID: ocWw4sFZSeqBWYrD5ivwYg - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:22:43 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'a85NLnO0', 'Content-Type': 'application/json', 'Content-Length': '374131', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"ecd0e3b1-a177-4487-8e33-0084688cf148\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_E  \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      User request  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/23/2024 02:23:02 PM - INFO: Starting Extract and Load (EL) Pipeline for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.\n",
      "10/23/2024 02:23:02 PM - INFO: Building file inventory.\n",
      "10/23/2024 02:23:15 PM - INFO: Recording inventory entries from fc-secure-d8de1fe3-972d-480f-a8a8-2bbc251add30 (97744 objects total)\n",
      "10/23/2024 02:25:49 PM - INFO: 9774 files recorded (~10%)\n",
      "10/23/2024 02:25:55 PM - INFO: 19548 files recorded (~20%)\n",
      "10/23/2024 02:26:01 PM - INFO: 29322 files recorded (~30%)\n",
      "10/23/2024 02:26:08 PM - INFO: 39096 files recorded (~40%)\n",
      "10/23/2024 02:26:14 PM - INFO: 48870 files recorded (~50%)\n",
      "10/23/2024 02:26:20 PM - INFO: 58644 files recorded (~60%)\n",
      "10/23/2024 02:26:27 PM - INFO: 68418 files recorded (~70%)\n",
      "10/23/2024 02:26:33 PM - INFO: 78192 files recorded (~80%)\n",
      "10/23/2024 02:26:39 PM - INFO: 87966 files recorded (~90%)\n",
      "10/23/2024 02:26:46 PM - INFO: 97740 files recorded (~100%)\n",
      "10/23/2024 02:26:46 PM - INFO: All inventory entries recorded (97744 objects total).\n",
      "10/23/2024 02:26:46 PM - INFO: Removing file objects in global file exclusion list: SubsetHailJointCall; .vds/\n",
      "10/23/2024 02:26:46 PM - INFO: 54173 objects remain after file removal.\n",
      "10/23/2024 02:26:54 PM - INFO: File Inventory build succeeded. 54173 distinct files found after deduplication. 0 retries required.\n",
      "10/23/2024 02:26:54 PM - INFO: Processing table data for ingest.\n",
      "10/23/2024 02:27:00 PM - INFO: Target tables and files to be processed: {\"participant\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/participant/participant_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.tsv\"], \"sample\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/sample/sample_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.tsv\"], \"sample_set\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/sample_set/sample_set_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.tsv\"], \"subject\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/subject/subject_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.tsv\"], \"workspace_attributes\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/workspace_attributes/workspace_attributes_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.tsv\"], \"file_inventory\": [\"ingest_pipeline/input/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/data_files/file_inventory/file_inventory.tsv\"]}\n",
      "10/23/2024 02:27:00 PM - INFO: Processing files for target table: participant.\n",
      "10/23/2024 02:27:04 PM - INFO: Processing files for target table: sample.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:101: DtypeWarning: Columns (68,69) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/23/2024 02:27:19 PM - INFO: Processing files for target table: sample_set.\n",
      "10/23/2024 02:27:23 PM - INFO: Processing files for target table: subject.\n",
      "10/23/2024 02:27:28 PM - INFO: Processing files for target table: workspace_attributes.\n",
      "10/23/2024 02:27:31 PM - INFO: Processing files for target table: file_inventory.\n",
      "10/23/2024 02:27:46 PM - INFO: Creating schema object and copying to cloud storage.\n",
      "10/23/2024 02:27:50 PM - INFO: File processing complete. Status: Success. Details: {\"file_inventory_population\": \"File inventory populated\", \"participant\": \"No errors raised\", \"sample\": \"No errors raised\", \"sample_set\": \"No errors raised\", \"subject\": \"No errors raised\", \"workspace_attributes\": \"No errors raised\", \"file_inventory\": \"No errors raised\"}. Tables to ingest: participant, sample, sample_set, subject, workspace_attributes, file_inventory\n",
      "10/23/2024 02:27:51 PM - INFO: Attempting to create or retrieve the specified TDR dataset.\n",
      "10/23/2024 02:27:51 PM - INFO: Creating new dataset: ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023\n",
      "TDR Job ID: Qs-xnebgS0-4fqnT2_Sivw\n",
      "10/23/2024 02:28:52 PM - INFO: Dataset Creation succeeded: {'id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'name': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'description': 'TDR Dataset for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'defaultProfileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'createdDate': '2024-10-23T14:28:32.823376Z', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': True, 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-87d2134c', 'storageAccount': None, 'phsId': 'phs001569', 'selfHosted': True, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': []}}\n",
      "10/23/2024 02:28:53 PM - INFO: Sleeping for a few minutes to let policy/permission changes propogate for new dataset...\n",
      "10/23/2024 02:29:23 PM - INFO: Setting up dataset ingest service account (SA).\n",
      "10/23/2024 02:29:23 PM - INFO: Adding dataset SA to anvil_tdr_ingest group.\n",
      "10/23/2024 02:29:47 PM - INFO: Dataset service account tdr-ingest-sa@datarepo-87d2134c.iam.gserviceaccount.com added to anvil_tdr_ingest sucessfully.\n",
      "10/23/2024 02:29:47 PM - INFO: Adding dataset SA to workspace anvil_workspace_ingest_resources_dev.\n",
      "10/23/2024 02:29:57 PM - INFO: User tdr-ingest-sa@datarepo-87d2134c.iam.gserviceaccount.com added to workspace as READER (without SHARE) permissions successfully.\n",
      "10/23/2024 02:29:57 PM - INFO: Adding dataset SA to workspace AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.\n",
      "10/23/2024 02:30:04 PM - INFO: User tdr-ingest-sa@datarepo-87d2134c.iam.gserviceaccount.com added to workspace as READER (without SHARE) permissions successfully.\n",
      "10/23/2024 02:30:04 PM - INFO: Sleeping for a few minutes to let policy/permission changes propogate...\n",
      "10/23/2024 02:37:04 PM - INFO: Running dataset ingests.\n",
      "10/23/2024 02:37:15 PM - INFO: Running ingests for target table: participant\n",
      "10/23/2024 02:37:15 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/participant/participant_0.json...\n",
      "10/23/2024 02:37:15 PM - INFO: Running ingest from participant_0.json to table participant.\n",
      "TDR Job ID: FSFzp5JBQuiLnn5XodqAxg\n",
      "10/23/2024 02:37:46 PM - INFO: Ingest from file participant_0.json succeeded: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'participant', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/participant/participant_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 16808, 'bad_row_count': 0, 'load_result': None}\n",
      "10/23/2024 02:37:46 PM - INFO: Running ingests for target table: sample\n",
      "10/23/2024 02:37:46 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/sample/sample_0.json...\n",
      "10/23/2024 02:37:46 PM - INFO: Running ingest from sample_0.json to table sample.\n",
      "TDR Job ID: P85e7vzNSpKvc9G76AFaBw\n",
      "10/23/2024 02:51:06 PM - INFO: Ingest from file sample_0.json succeeded: Job succeeded, but error retrieving job result: (500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:50:56 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'ekMJGJd6', 'Content-Type': 'application/json', 'Content-Length': '378107', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P\n",
      "10/23/2024 02:51:06 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/sample/sample_1.json...\n",
      "10/23/2024 02:51:06 PM - INFO: Running ingest from sample_1.json to table sample.\n",
      "TDR Job ID: kp6wZb1NTHuPFi2Jky4gvg\n",
      "10/23/2024 03:04:26 PM - INFO: Ingest from file sample_1.json succeeded: Job succeeded, but error retrieving job result: (500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 15:04:16 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'e2WWLNwE', 'Content-Type': 'application/json', 'Content-Length': '378995', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P\n",
      "10/23/2024 03:04:26 PM - INFO: Running ingests for target table: sample_set\n",
      "10/23/2024 03:04:26 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/sample_set/sample_set_0.json...\n",
      "10/23/2024 03:04:26 PM - INFO: Running ingest from sample_set_0.json to table sample_set.\n",
      "TDR Job ID: ovzkAbeFSsG86AE_S1l4QQ\n",
      "10/23/2024 03:04:46 PM - INFO: Ingest from file sample_set_0.json succeeded: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'sample_set', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/sample_set/sample_set_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "10/23/2024 03:04:46 PM - INFO: Running ingests for target table: subject\n",
      "10/23/2024 03:04:46 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/subject/subject_0.json...\n",
      "10/23/2024 03:04:47 PM - INFO: Running ingest from subject_0.json to table subject.\n",
      "TDR Job ID: ty3ffpy4RXmhmoXkSAfuvA\n",
      "10/23/2024 03:05:07 PM - INFO: Ingest from file subject_0.json succeeded: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'subject', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/subject/subject_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 16808, 'bad_row_count': 0, 'load_result': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/23/2024 03:05:07 PM - INFO: Running ingests for target table: workspace_attributes\n",
      "10/23/2024 03:05:07 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/workspace_attributes/workspace_attributes_0.json...\n",
      "10/23/2024 03:05:07 PM - INFO: Running ingest from workspace_attributes_0.json to table workspace_attributes.\n",
      "TDR Job ID: U45B1CqxQSabV4-LURDPzA\n",
      "10/23/2024 03:05:28 PM - INFO: Ingest from file workspace_attributes_0.json succeeded: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'workspace_attributes', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/workspace_attributes/workspace_attributes_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 42, 'bad_row_count': 0, 'load_result': None}\n",
      "10/23/2024 03:05:28 PM - INFO: Running ingests for target table: file_inventory\n",
      "10/23/2024 03:05:28 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/file_inventory/file_inventory_0.json...\n",
      "10/23/2024 03:05:28 PM - INFO: Running ingest from file_inventory_0.json to table file_inventory.\n",
      "TDR Job ID: QyFOrsDTSDqzXdJ5ftYAKw\n",
      "10/23/2024 03:18:47 PM - INFO: Ingest from file file_inventory_0.json succeeded: Job succeeded, but error retrieving job result: (500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 15:18:37 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': '6Bxv4g3G', 'Content-Type': 'application/json', 'Content-Length': '372418', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P\n",
      "10/23/2024 03:18:47 PM - INFO: Checking for file ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/file_inventory/file_inventory_1.json...\n",
      "10/23/2024 03:18:47 PM - INFO: Running ingest from file_inventory_1.json to table file_inventory.\n",
      "TDR Job ID: uaTTsFZhRIqkqfdZNkYy7Q\n",
      "10/23/2024 03:32:37 PM - INFO: Ingest from file file_inventory_1.json succeeded: Job succeeded, but error retrieving job result: (500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 15:32:27 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': '6zr5ZR8g', 'Content-Type': 'application/json', 'Content-Length': '371408', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P\n",
      "10/23/2024 03:32:37 PM - INFO: Skipping snapshot creation on user request.\n",
      "10/23/2024 03:32:37 PM - INFO: The ingest pipeline has completed for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES.\n",
      "10/23/2024 03:32:47 PM - INFO: Pipeline Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Staging Area</th>\n",
       "      <th>Time</th>\n",
       "      <th>Step</th>\n",
       "      <th>Task</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 14:23:02</td>\n",
       "      <td>Initialization</td>\n",
       "      <td>Provenance File Retrieval</td>\n",
       "      <td>Success</td>\n",
       "      <td>{\"phs_id\": \"phs001569\", \"consent_name\": \"GRU\", \"source_workspaces\": [\"AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES\"], \"auth_domains\": [\"AUTH_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES\"], \"data_files_src_buckets\": {\"fc-secure-d8de1fe3-972d-480f-a8a8-2bbc251add30\": {\"include_dirs\": [], \"exclude_dirs\": []}}, \"data_file_refs\": {\"qc_result_sample\": [{\"column\": \"cram\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}], \"sample\": [{\"column\": \"crai_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"md5_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"cram_path\", \"method\": \"file_pa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 14:26:54</td>\n",
       "      <td>File Inventory Creation</td>\n",
       "      <td>Build File Inventory</td>\n",
       "      <td>Success</td>\n",
       "      <td>54173 files found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 14:27:51</td>\n",
       "      <td>Table Data Processing</td>\n",
       "      <td>Ingest Pre-Processing</td>\n",
       "      <td>Success</td>\n",
       "      <td>{\"file_inventory_population\": \"File inventory populated\", \"participant\": \"No errors raised\", \"sample\": \"No errors raised\", \"sample_set\": \"No errors raised\", \"subject\": \"No errors raised\", \"workspace_attributes\": \"No errors raised\", \"file_inventory\": \"No errors raised\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 14:27:51</td>\n",
       "      <td>Dataset Creation or Retrieval</td>\n",
       "      <td>Enumerate Datasets</td>\n",
       "      <td>Success</td>\n",
       "      <td>0 datasets found. Matching dataset_id =</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 14:28:52</td>\n",
       "      <td>Dataset Creation or Retrieval</td>\n",
       "      <td>Create New Dataset</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: Qs-xnebgS0-4fqnT2_Sivw - Truncated Response: {'id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'name': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'description': 'TDR Dataset for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'defaultProfileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'createdDate': '2024-10-23T14:28:32.823376Z', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': True, 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-87d2134c', 'storageAccount': None, 'phsId': 'phs001569', 'selfHosted': True, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': []}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 14:29:47</td>\n",
       "      <td>Dataset Service Account Setup</td>\n",
       "      <td>Add SA to Anvil Ingest Group</td>\n",
       "      <td>Success</td>\n",
       "      <td>tdr-ingest-sa@datarepo-87d2134c.iam.gserviceaccount.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 14:29:57</td>\n",
       "      <td>Dataset Service Account Setup</td>\n",
       "      <td>Add Ingest User to Workspace anvil_workspace_ingest_resources_dev</td>\n",
       "      <td>Success</td>\n",
       "      <td>tdr-ingest-sa@datarepo-87d2134c.iam.gserviceaccount.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 14:30:04</td>\n",
       "      <td>Dataset Service Account Setup</td>\n",
       "      <td>Add Ingest User to Workspace AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>Success</td>\n",
       "      <td>tdr-ingest-sa@datarepo-87d2134c.iam.gserviceaccount.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 14:37:46</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: participant - File: participant_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: FSFzp5JBQuiLnn5XodqAxg - Truncated Response: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'participant', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/participant/participant_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 16808, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 14:51:06</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: sample - File: sample_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: P85e7vzNSpKvc9G76AFaBw - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:50:56 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'ekMJGJd6', 'Content-Type': 'application/json', 'Content-Length': '378107', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 15:04:26</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: sample - File: sample_1.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: kp6wZb1NTHuPFi2Jky4gvg - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 15:04:16 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'e2WWLNwE', 'Content-Type': 'application/json', 'Content-Length': '378995', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 15:04:46</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: sample_set - File: sample_set_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: ovzkAbeFSsG86AE_S1l4QQ - Truncated Response: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'sample_set', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/sample_set/sample_set_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 15:05:07</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: subject - File: subject_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: ty3ffpy4RXmhmoXkSAfuvA - Truncated Response: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'subject', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/subject/subject_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 16808, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 15:05:28</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: workspace_attributes - File: workspace_attributes_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: U45B1CqxQSabV4-LURDPzA - Truncated Response: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'workspace_attributes', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/workspace_attributes/workspace_attributes_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 42, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 15:18:47</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: file_inventory - File: file_inventory_0.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: QyFOrsDTSDqzXdJ5ftYAKw - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 15:18:37 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': '6Bxv4g3G', 'Content-Type': 'application/json', 'Content-Length': '372418', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 15:32:37</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: file_inventory - File: file_inventory_1.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: uaTTsFZhRIqkqfdZNkYy7Q - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 15:32:27 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': '6zr5ZR8g', 'Content-Type': 'application/json', 'Content-Length': '371408', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES</td>\n",
       "      <td>2024-10-23 15:32:37</td>\n",
       "      <td>Snapshot Creation</td>\n",
       "      <td>Create and Share Snapshot</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>User request</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Staging Area                     Time                      Step                                             Task                                 Status   \\\n",
       "0   AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 14:23:02                 Initialization                                          Provenance File Retrieval  Success   \n",
       "1   AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 14:26:54        File Inventory Creation                                               Build File Inventory  Success   \n",
       "2   AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 14:27:51          Table Data Processing                                              Ingest Pre-Processing  Success   \n",
       "3   AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 14:27:51  Dataset Creation or Retrieval                                                 Enumerate Datasets  Success   \n",
       "4   AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 14:28:52  Dataset Creation or Retrieval                                                 Create New Dataset  Success   \n",
       "5   AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 14:29:47  Dataset Service Account Setup                                       Add SA to Anvil Ingest Group  Success   \n",
       "6   AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 14:29:57  Dataset Service Account Setup  Add Ingest User to Workspace anvil_workspace_ingest_resources_dev  Success   \n",
       "7   AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 14:30:04  Dataset Service Account Setup   Add Ingest User to Workspace AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  Success   \n",
       "8   AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 14:37:46                Dataset Ingests                      Table: participant - File: participant_0.json  Success   \n",
       "9   AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 14:51:06                Dataset Ingests                                Table: sample - File: sample_0.json  Success   \n",
       "10  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 15:04:26                Dataset Ingests                                Table: sample - File: sample_1.json  Success   \n",
       "11  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 15:04:46                Dataset Ingests                        Table: sample_set - File: sample_set_0.json  Success   \n",
       "12  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 15:05:07                Dataset Ingests                              Table: subject - File: subject_0.json  Success   \n",
       "13  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 15:05:28                Dataset Ingests    Table: workspace_attributes - File: workspace_attributes_0.json  Success   \n",
       "14  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 15:18:47                Dataset Ingests                Table: file_inventory - File: file_inventory_0.json  Success   \n",
       "15  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 15:32:37                Dataset Ingests                Table: file_inventory - File: file_inventory_1.json  Success   \n",
       "16  AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES  2024-10-23 15:32:37              Snapshot Creation                                          Create and Share Snapshot  Skipped   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Message                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "0                                                           {\"phs_id\": \"phs001569\", \"consent_name\": \"GRU\", \"source_workspaces\": [\"AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES\"], \"auth_domains\": [\"AUTH_AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES\"], \"data_files_src_buckets\": {\"fc-secure-d8de1fe3-972d-480f-a8a8-2bbc251add30\": {\"include_dirs\": [], \"exclude_dirs\": []}}, \"data_file_refs\": {\"qc_result_sample\": [{\"column\": \"cram\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}], \"sample\": [{\"column\": \"crai_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"md5_path\", \"method\": \"file_path_match\", \"match_multiple_files\": false, \"match_regex\": null, \"match_type\": \"exact\", \"mode\": \"fileref_in_line\", \"create_new_field\": false, \"new_field_name\": null}, {\"column\": \"cram_path\", \"method\": \"file_pa  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  54173 files found  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {\"file_inventory_population\": \"File inventory populated\", \"participant\": \"No errors raised\", \"sample\": \"No errors raised\", \"sample_set\": \"No errors raised\", \"subject\": \"No errors raised\", \"workspace_attributes\": \"No errors raised\", \"file_inventory\": \"No errors raised\"}  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0 datasets found. Matching dataset_id =   \n",
       "4                                                                                                                                                                                                                                    Job_ID: Qs-xnebgS0-4fqnT2_Sivw - Truncated Response: {'id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'name': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'description': 'TDR Dataset for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'defaultProfileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'createdDate': '2024-10-23T14:28:32.823376Z', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': True, 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-87d2134c', 'storageAccount': None, 'phsId': 'phs001569', 'selfHosted': True, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': []}}  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tdr-ingest-sa@datarepo-87d2134c.iam.gserviceaccount.com  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tdr-ingest-sa@datarepo-87d2134c.iam.gserviceaccount.com  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            tdr-ingest-sa@datarepo-87d2134c.iam.gserviceaccount.com  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Job_ID: FSFzp5JBQuiLnn5XodqAxg - Truncated Response: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'participant', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/participant/participant_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 16808, 'bad_row_count': 0, 'load_result': None}  \n",
       "9   Job_ID: P85e7vzNSpKvc9G76AFaBw - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 14:50:56 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'ekMJGJd6', 'Content-Type': 'application/json', 'Content-Length': '378107', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P  \n",
       "10  Job_ID: kp6wZb1NTHuPFi2Jky4gvg - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 15:04:16 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'e2WWLNwE', 'Content-Type': 'application/json', 'Content-Length': '378995', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Job_ID: ovzkAbeFSsG86AE_S1l4QQ - Truncated Response: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'sample_set', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/sample_set/sample_set_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}  \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Job_ID: ty3ffpy4RXmhmoXkSAfuvA - Truncated Response: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'subject', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/subject/subject_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 16808, 'bad_row_count': 0, 'load_result': None}  \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Job_ID: U45B1CqxQSabV4-LURDPzA - Truncated Response: {'dataset_id': 'b2b217c2-4b68-4820-bf9d-e2927bfe8706', 'dataset': 'ANVIL_CCDG_Broad_CVD_PROMIS_GRU_WES_20241023', 'table': 'workspace_attributes', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source/AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES/table_data/workspace_attributes/workspace_attributes_0.json', 'load_tag': 'Ingest for AnVIL_CCDG_Broad_CVD_PROMIS_GRU_WES', 'row_count': 42, 'bad_row_count': 0, 'load_result': None}  \n",
       "14  Job_ID: QyFOrsDTSDqzXdJ5ftYAKw - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 15:18:37 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': '6Bxv4g3G', 'Content-Type': 'application/json', 'Content-Length': '372418', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P  \n",
       "15  Job_ID: uaTTsFZhRIqkqfdZNkYy7Q - Truncated Response: Job succeeded, but error retrieving job result: (500)\\nReason: Internal Server Error\\nHTTP response headers: HTTPHeaderDict({'Date': 'Wed, 23 Oct 2024 15:32:27 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': '6zr5ZR8g', 'Content-Type': 'application/json', 'Content-Length': '371408', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\\nHTTP response body: {\"message\":\"Failed to deserialize value '[\\\"bio.terra.model.IngestResponseModel\\\",{\\\"dataset_id\\\":\\\"b2b217c2-4b68-4820-bf9d-e2927bfe8706\\\",\\\"dataset\\\":\\\"ANVIL_CCDG_Broad_CVD_P  \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      User request  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through and execute workspace connector pipeline (\"E\") for listed workspaces\n",
    "if params[\"skip_source_files_creation\"] == True:\n",
    "    logging.info(\"Skipping source file creation, per user request.\")\n",
    "else:\n",
    "    for workspace in workspace_run_list:\n",
    "        if workspace[5] == True:\n",
    "            params[\"data_file_refs\"] = data_file_refs_dict  \n",
    "            utils.run_ws_connector_pipeline(workspace, params)\n",
    "\n",
    "# Aggregate staging area to target dataset combinations, loop through them, and execute ingest pipeline (\"L\")\n",
    "pipeline_run_list = []\n",
    "for workspace in workspace_run_list:\n",
    "    if workspace[5] == True:\n",
    "        temp_list = [workspace[3], workspace[4], workspace[2]]\n",
    "        if temp_list not in pipeline_run_list:\n",
    "            pipeline_run_list.append(temp_list)\n",
    "for pipeline in pipeline_run_list:\n",
    "    utils.run_el_pipeline(pipeline, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Development\n",
    "Work through the following steps for each dataset that needs to be processed through the transformation pipeline in Step 4, specifying the target schema (\"mapping target\") and mapping specification (\"mapping_target_spec\") you would like to use for transformation. Note that you can use the logs or results_dict from the previous step to retrieve the dataset_id values of interest, or retrieve them directly from TDR via the UI or Swagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Mapping Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets to map: \n",
      "\t- ANVIL_DepMap_HMB_20240827 (cb7dccc5-171c-48bf-9e5e-07bd6f52b34a) with anvil/depmap_1\n",
      "\t- ANVIL_HudsonAlpha_LR_v1_GRU_20241018 (ac48514d-0b01-4a92-b164-821fa3e05d7a) with anvil/hudsonalpha_1\n"
     ]
    }
   ],
   "source": [
    "## >>> Mapping Variables <<<\n",
    "# For each dataset specified, include an appropriate mapping target and mapping target specification\n",
    "datasets_to_map_list = [\n",
    "    #[\"dataset_id\", \"mapping_target\", \"mapping_target_spec\", Run (True/False)]\n",
    "    ['cb7dccc5-171c-48bf-9e5e-07bd6f52b34a', 'anvil', 'depmap_1', True],\n",
    "    ['ac48514d-0b01-4a92-b164-821fa3e05d7a', 'anvil', 'hudsonalpha_1', True],\n",
    "]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "# Print variables\n",
    "print(\"Datasets to map: \")\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "final_datasets_to_map_dict = {}\n",
    "skip_dataset_list_access = []\n",
    "skip_dataset_list_mapping = []\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "for dataset in datasets_to_map_list:\n",
    "    if dataset[3]:\n",
    "        dataset_id = dataset[0]\n",
    "        mapping_target = dataset[1]\n",
    "        mapping_target_spec = dataset[2]\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "            skip_dataset_list_access.append(dataset_id)\n",
    "        try:\n",
    "            blob = bucket.blob(\"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target))\n",
    "            content = json.loads(blob.download_as_string(client=None))\n",
    "            blob = bucket.blob(\"ingest_pipeline/mapping/{}/{}/mapping_specification.json\".format(mapping_target, mapping_target_spec))\n",
    "            content = json.loads(blob.download_as_string(client=None))\n",
    "        except:\n",
    "            skip_dataset_list_mapping.append(dataset_id)\n",
    "        if dataset_id not in skip_dataset_list_access and dataset_id not in skip_dataset_list_mapping:\n",
    "            final_datasets_to_map_dict[dataset_id] = {}\n",
    "            final_datasets_to_map_dict[dataset_id][\"mapping_target\"] = mapping_target \n",
    "            final_datasets_to_map_dict[dataset_id][\"mapping_target_spec\"] = mapping_target_spec\n",
    "            print(\"\\t- \" + dataset_name + \" ({})\".format(dataset_id) + \" with {}/{}\".format(mapping_target, mapping_target_spec))\n",
    "if skip_dataset_list_access:\n",
    "    print(\"Datasets to skip due to non-existence or inaccessibility to the current user:\")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(skip_dataset_list_access))\n",
    "if skip_dataset_list_mapping:\n",
    "    print(\"Datasets to skip due to invalid mapping target or mapping target specification:\")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(skip_dataset_list_mapping))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Missing Relationships to TDR Dataset Schema\n",
    "Relationships are needed by the mapping query constructor to build appropriate joins between tables. If no joins are required between tables, this step is unnecessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing potential relationships for dataset_id = cb7dccc5-171c-48bf-9e5e-07bd6f52b34a\n",
      "TDR Job ID: m2xayXLfQg2MQ3nGsft20g\n",
      "Schema update successful: ({'id': 'cb7dccc5-171c-48bf-9e5e-07bd6f52b34a', 'name': 'ANVIL_DepMap_HMB_20240827', 'description': 'TDR Dataset for AnVIL_DepMap_HMB', 'defaultProfileId': None, 'dataProject': None, 'defaultSnapshotId': None, 'schema': {'tables': [{'name': 'workspace_attributes', 'columns': [{'name': 'attribute', 'datatype': 'string', 'array_of': False, 'required': False}, {'name': 'value', 'datatype': 'string', 'array_of': False, 'required': False}, {'name': 'ingest_provenance', 'datatype': 'string', 'array_of': False, 'required': False}], 'primaryKey': [], 'partitionMode': 'none', 'datePartitionOptions': None, 'intPartitionOptions': None, 'rowCount': None}, {'name': 'biosample', 'columns': [{'name': 'biosample_id', 'datatype': 'string', 'array_of': False, 'required': False}, {'name': 'cell_format', 'datatype': 'string', 'array_of': False, 'required': False}, {'name': 'donor_id', 'datatype': 'string', 'array_of': False, 'required': False}, {'name': 'growth_media', 'datatype': 'string', 'array_of': Fa\n",
      "Processing potential relationships for dataset_id = ac48514d-0b01-4a92-b164-821fa3e05d7a\n",
      "TDR Job ID: iDZwumhZQdKUpybPdJWzvA\n",
      "Schema update successful: ({'id': 'ac48514d-0b01-4a92-b164-821fa3e05d7a', 'name': 'ANVIL_HudsonAlpha_LR_v1_GRU_20241018', 'description': 'TDR Dataset for AnVIL_HudsonAlpha_GRU_Deposit', 'defaultProfileId': None, 'dataProject': None, 'defaultSnapshotId': None, 'schema': {'tables': [{'name': 'project', 'columns': [{'name': 'project_id', 'datatype': 'string', 'array_of': False, 'required': False}, {'name': 'funded_by', 'datatype': 'string', 'array_of': False, 'required': False}, {'name': 'principal_investigator', 'datatype': 'string', 'array_of': False, 'required': False}, {'name': 'registered_identifier', 'datatype': 'string', 'array_of': False, 'required': False}, {'name': 'title', 'datatype': 'string', 'array_of': False, 'required': False}, {'name': 'ingest_provenance', 'datatype': 'string', 'array_of': False, 'required': False}], 'primaryKey': [], 'partitionMode': 'none', 'datePartitionOptions': None, 'intPartitionOptions': None, 'rowCount': None}, {'name': 'workspace_attributes', 'columns': [{'name': 'attribu\n",
      "Processing of potential relationships for specified datasets complete.\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb7dccc5-171c-48bf-9e5e-07bd6f52b34a</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac48514d-0b01-4a92-b164-821fa3e05d7a</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dataset                 status \n",
       "0  cb7dccc5-171c-48bf-9e5e-07bd6f52b34a  Success\n",
       "1  ac48514d-0b01-4a92-b164-821fa3e05d7a  Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record relationships to potentially add to the source datasets. Note that there may be more relationships to add\n",
    "# than those listed below, so add to this list as necessary.\n",
    "potential_relationships = [\n",
    "    [\"subject.family_id\", \"family.family_id\"],\n",
    "    [\"sample.subject_id\", \"subject.subject_id\"],\n",
    "    [\"sample.t_01_subject_id\", \"subject.subject_id\"],\n",
    "    [\"sequencing.sample_id\", \"sample.sample_id\"],\n",
    "    [\"sequencing.sample\", \"sample.sample_id\"],\n",
    "    [\"sequencing.sample_alias\", \"sample.sample_id\"],\n",
    "    [\"sample.participant\", \"participant.participant_id\"],\n",
    "    [\"sample.participant_id\", \"participant.participant_id\"],\n",
    "    [\"discovery.sample_id\", \"sample.sample_id\"],\n",
    "    [\"discovery.subject_id\", \"subject.subject_id\"],\n",
    "    [\"qc_result_sample.qc_result_sample_id\", \"sample.sample_id\"],\n",
    "    [\"interval.chromosome\", \"chromosome.chromosome_id\"],\n",
    "    [\"analyte.participant_id\", \"participant.participant_id\"],\n",
    "    [\"participant.family_id\", \"family.family_id\"],\n",
    "    [\"phenotype.participant_id\", \"participant.participant_id\"],\n",
    "    [\"experiment_rna_short_read.analyte_id\", \"analyte.analyte_id\"],\n",
    "    [\"experiment_dna_short_read.analyte_id\", \"analyte.analyte_id\"],\n",
    "    [\"aligned_rna_short_read.experiment_rna_short_read_id\", \"experiment_rna_short_read.experiment_rna_short_read_id\"],\n",
    "    [\"aligned_dna_short_read.experiment_dna_short_read_id\", \"experiment_dna_short_read.experiment_dna_short_read_id\"],\n",
    "    [\"aligned_dna_short_read_set.aligned_dna_short_reads\", \"aligned_dna_short_read.aligned_dna_short_read_id\"],\n",
    "    [\"called_variants_dna_short_read.aligned_dna_short_read_set_id\", \"aligned_dna_short_read_set.aligned_dna_short_read_set_id\"],\n",
    "    [\"biosample.donor_id\", \"donor.donor_id\"],\n",
    "]\n",
    "\n",
    "# Loop through datasets and process potential relationship additions\n",
    "results = []\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "for dataset_id in final_datasets_to_map_dict:\n",
    "    print(\"Processing potential relationships for dataset_id = {}\".format(dataset_id))\n",
    "    \n",
    "    # Retrieve source schema\n",
    "    src_schema_dict = {}\n",
    "    try:\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "        results.append([dataset_id, \"Error\"])\n",
    "\n",
    "    # Loop through potential relationships and add those present for the source dataset\n",
    "    additional_relationships = []\n",
    "    for rel in potential_relationships:\n",
    "        from_table = rel[0].split(\".\")[0] \n",
    "        from_column = rel[0].split(\".\")[1]\n",
    "        to_table = rel[1].split(\".\")[0]\n",
    "        to_column = rel[1].split(\".\")[1]\n",
    "        if bmq.confirm_column_exists(src_schema_dict, from_table, from_column) and bmq.confirm_column_exists(src_schema_dict, to_table, to_column):\n",
    "            relationship_found = False\n",
    "            for rel_entry in src_schema_dict[\"relationships\"]:\n",
    "                if rel_entry[\"_from\"][\"table\"] == from_table and rel_entry[\"_from\"][\"column\"] == from_column and rel_entry[\"to\"][\"table\"] == to_table and rel_entry[\"to\"][\"column\"] == to_column:\n",
    "                    relationship_found = True\n",
    "                elif rel_entry[\"_from\"][\"table\"] == to_table and rel_entry[\"_from\"][\"column\"] == to_column and rel_entry[\"to\"][\"table\"] == from_table and rel_entry[\"to\"][\"column\"] == from_column:\n",
    "                    relationship_found = True\n",
    "            if not relationship_found:\n",
    "                rel_dict = {\n",
    "                    \"name\": from_table + \"_\" + from_column + \"__to__\" + to_table + \"_\" + to_column,\n",
    "                    \"from\": {\"table\": from_table, \"column\": from_column},\n",
    "                    \"to\": {\"table\": to_table, \"column\": to_column}\n",
    "                }\n",
    "                additional_relationships.append(rel_dict)\n",
    "\n",
    "    # Submit the schema update request for the TDR dataset\n",
    "    if additional_relationships:\n",
    "        schema_update_request = {\n",
    "            \"description\": \"Adding relationships to support query construction.\",\n",
    "            \"changes\": {\n",
    "                \"addRelationships\": additional_relationships\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            resp = utils.wait_for_tdr_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "            print(\"Schema update successful: \" + str(resp)[0:1000])\n",
    "            results.append([dataset_id, \"Success\"])\n",
    "        except Exception as e:\n",
    "            print(\"Error running schema update: \" + str(e))\n",
    "            results.append([dataset_id, \"Error\"])\n",
    "    else:\n",
    "        print(\"No additional relationships to add to schema.\")\n",
    "        results.append([dataset_id, \"Success\"])\n",
    "\n",
    "print(\"Processing of potential relationships for specified datasets complete.\")\n",
    "print(\"\\nResults:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"dataset\", \"status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Mapping Artifacts and Run Query Construction\n",
    "Retrieve the artifacts you would like to use to construct transformation queries for your datasets, based on the previously specified target schema and mapping specification. These transformation queries will then be dynamically constructed based on the appropriate target schema, mapping specification, and source schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through datasets and process transformation query construction\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "results = []\n",
    "for dataset_id in final_datasets_to_map_dict:\n",
    "    print(\"Building transformation queries for dataset_id = {}\".format(dataset_id))\n",
    "\n",
    "    # Collect mapping variables\n",
    "    mapping_target = final_datasets_to_map_dict[dataset_id][\"mapping_target\"]\n",
    "    mapping_target_spec = final_datasets_to_map_dict[dataset_id][\"mapping_target_spec\"]\n",
    "    \n",
    "    # Retrieve source schema\n",
    "    src_schema_dict = {}\n",
    "    try:\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"name\"] = response[\"name\"]\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        phs_id = response[\"phs_id\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "    # Set dataset name and project name parameters to substitute into transform queries\n",
    "    dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "    project_name_value = re.sub(\"'\", \"\", utils.derive_project_name(dataset_id, phs_id, dataset_name_value))\n",
    "\n",
    "    # Retrieve target schema and mapping specification\n",
    "    target_schema_dict = {}\n",
    "    mapping_spec = {}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "        target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "        blob_string = blob.download_as_text(client=None)\n",
    "        blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "        blob_string = blob_string.replace(\"$PROJECT_NAME\", project_name_value)\n",
    "        blob_string = blob_string.replace(\"$BQ_DATASET\", bq_project + \".\" + bq_schema)\n",
    "        mapping_spec = json.loads(blob_string)\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "\n",
    "    # Update aliases in mapping specification\n",
    "    mapping_spec = bmq.update_mapping_spec_aliases(mapping_spec, src_schema_dict)\n",
    "    \n",
    "    # Build queries from mapping specification\n",
    "    query_dict = {}\n",
    "    if target_schema_dict:\n",
    "        for target_table in target_schema_dict[\"tables\"]:\n",
    "            table_name = target_table[\"name\"]\n",
    "            missing_artifacts = False\n",
    "            if src_schema_dict and mapping_spec:\n",
    "                query_dict[table_name] = bmq.build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema)\n",
    "            else:\n",
    "                missing_artifacts = True\n",
    "                query_dict[table_name] = {\"query\": \"\", \"syntax_check\": \"\"} \n",
    "        if missing_artifacts == True:\n",
    "            print(\"Source schema dictionary and/or mapping specification missing. Unable to generate queries.\")\n",
    "            results.append([dataset_id, \"Error\"])\n",
    "    else:\n",
    "        print(\"Target schema dictionary missing. Unable to generate queries.\")\n",
    "        results.append([dataset_id, \"Error\"])\n",
    "    \n",
    "    # Evaluate queries -- Publish if no issues found, otherwise convert to dataframe and display\n",
    "    failure_count = 0\n",
    "    for key, val in query_dict.items():\n",
    "        if val[\"syntax_check\"] != \"Passed\" and val[\"syntax_check\"] != None:\n",
    "            failure_count += 1\n",
    "    if failure_count == 0:\n",
    "        print(\"No failures found in query construction, publishing to the cloud.\")\n",
    "        results.append([dataset_id, \"Success\"])\n",
    "        # Copy target schema file to output folder for mapping target\n",
    "        source_path = \"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target)\n",
    "        destination_path = \"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(mapping_target, dataset_id)\n",
    "        !gsutil cp $ws_bucket/$source_path $ws_bucket/$destination_path 2> stdout\n",
    "\n",
    "        # Limit query dict to valid queries, write out, and copy to output folder for mapping target\n",
    "        valid_query_dict = {}\n",
    "        for target, val in query_dict.items():\n",
    "            if val[\"syntax_check\"] == \"Passed\":\n",
    "                valid_query_dict[target] = val\n",
    "        final_query_dict = {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"transforms\": valid_query_dict\n",
    "        }\n",
    "        query_dict_json = json.dumps(final_query_dict)\n",
    "        query_output_file = \"transform_query_set.json\"\n",
    "        with open(query_output_file, 'w') as outfile:\n",
    "            outfile.write(query_dict_json)\n",
    "        destination_path = \"ingest_pipeline/output/transformed/{}/{}/queries\".format(mapping_target, dataset_id)\n",
    "        !gsutil cp $query_output_file $ws_bucket/$destination_path/ 2> stdout\n",
    "    else:\n",
    "        print(\"Failures found in query construction, must be resolved before publishing.\")\n",
    "        print(\"Query building results:\")\n",
    "        results.append([dataset_id, \"Error\"])\n",
    "        query_df = pd.DataFrame.from_dict(query_dict, orient=\"index\")\n",
    "        query_df.index.name = \"target_table\"\n",
    "        query_df.reset_index(inplace=True)\n",
    "        display(query_df)\n",
    "\n",
    "print(\"Transformation query construction and processing complete.\")\n",
    "print(\"\\nResults:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"dataset\", \"status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Vocabulary Mapping\n",
    "For target attributes leveraging the \"VOCAB_MAP\" transformation, evaluate whether the source values have a record in the dsp-data-ingest.transform_resources.vocab_map table. If additional mappings are needed, these should be put into place before the transformation queries are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display parameter\n",
    "show_only_missing_maps = True\n",
    "\n",
    "# Loop through datasets and process vocabulary mapping evaluation\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "for dataset_id in final_datasets_to_map_dict:\n",
    "    print(\"Evaluating vocabulary mapping for dataset_id = {}\".format(dataset_id))\n",
    "\n",
    "    # Collect mapping variables\n",
    "    mapping_target = final_datasets_to_map_dict[dataset_id][\"mapping_target\"]\n",
    "    mapping_target_spec = final_datasets_to_map_dict[dataset_id][\"mapping_target_spec\"]\n",
    "    \n",
    "    # Retrieve source schema\n",
    "    src_schema_dict = {}\n",
    "    try:\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"name\"] = response[\"name\"]\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        phs_id = response[\"phs_id\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "    # Set dataset name and project name parameters to substitute into transform queries\n",
    "    dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "    project_name_value = utils.derive_project_name(dataset_id, phs_id, dataset_name_value)\n",
    "\n",
    "    # Retrieve target schema and mapping specification\n",
    "    target_schema_dict = {}\n",
    "    mapping_spec = {}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "        target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "        blob_string = blob.download_as_text(client=None)\n",
    "        blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "        blob_string = blob_string.replace(\"$PROJECT_NAME\", project_name_value)\n",
    "        mapping_spec = json.loads(blob_string)\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "\n",
    "    # Evaluate vocab mapping and display results\n",
    "    df = bmq.evaluate_vocab_mapping(mapping_spec, src_schema_dict, target_schema_dict, bq_project, bq_schema)\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Missing mapped_value view:\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    display(df[df[\"mapped_value\"].isnull() & df[\"source_value\"].notnull()])\n",
    "    if not show_only_missing_maps:\n",
    "        print(\"\\n-------------------------------------------\")\n",
    "        print(\"Full view:\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        display(df)\n",
    "    \n",
    "print(\"Vocabulary mapping evaluation and processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## [Optional] Update/Override Generated Queries as Necessary\n",
    "Review any queries that have not passed the syntax check, as these need to be remedied before they can be published and executed. Any other queries that do not align with expectations can be overridden by either A) Updating the mapping target specification and re-running the previous step, or B) Manually overriding the query below. Option B should only be used in one-off cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Build Base Query Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Input the appropriate dataset and mapping target specification\n",
    "dataset_id = \"f1e1ef01-d52d-423e-a65b-3a1d26c7ee9d\"\n",
    "mapping_target = \"anvil\"\n",
    "mapping_target_spec = \"cmg_ext_2\"\n",
    "\n",
    "# Retrieve source schema\n",
    "src_schema_dict = {}\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "try:\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "    src_schema_dict[\"name\"] = response[\"name\"]\n",
    "    src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "    src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "    bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "    bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    phs_id = response[\"phs_id\"]\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "# Set dataset name and project name parameters to substitute into transform queries\n",
    "dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "project_name_value = utils.derive_project_name(dataset_id, phs_id, dataset_name_value)\n",
    "\n",
    "# Retrieve target schema and mapping specification\n",
    "target_schema_dict = {}\n",
    "mapping_spec = {}\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "    target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "    blob_string = blob.download_as_text(client=None)\n",
    "    blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "    blob_string = blob_string.replace(\"$PROJECT_NAME\", project_name_value)\n",
    "    mapping_spec = json.loads(blob_string)\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "\n",
    "# Build queries from mapping specification\n",
    "query_dict = {}\n",
    "if target_schema_dict:\n",
    "    for target_table in target_schema_dict[\"tables\"]:\n",
    "        table_name = target_table[\"name\"]\n",
    "        missing_artifacts = False\n",
    "        if src_schema_dict and mapping_spec:\n",
    "            query_dict[table_name] = bmq.build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema)\n",
    "        else:\n",
    "            missing_artifacts = True\n",
    "            query_dict[table_name] = {\"query\": \"\", \"syntax_check\": \"\"} \n",
    "    if missing_artifacts == True:\n",
    "        print(\"Source schema dictionary and/or mapping specification missing. Unable to generate queries.\")\n",
    "else:\n",
    "    print(\"Target schema dictionary missing. Unable to generate queries.\")\n",
    "    \n",
    "# Display query dictionary\n",
    "query_df = pd.DataFrame.from_dict(query_dict, orient=\"index\")\n",
    "query_df.index.name = \"target_table\"\n",
    "query_df.reset_index(inplace=True)\n",
    "display(query_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Update Query Dict as Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To update the query definition for particular target table, input the target table and query below\n",
    "target_table = \"anvil_donor\"\n",
    "query = \"SELECT 1\"\n",
    "\n",
    "# Run syntax check\n",
    "query_dict[target_table][\"query\"] = query\n",
    "query_dict[target_table][\"syntax_check\"] = bmq.run_syntax_check(query)\n",
    "print(query_dict[target_table])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Publish Updated Query Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Copy target schema file to output folder for mapping target\n",
    "source_path = \"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target)\n",
    "destination_path = \"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(mapping_target, dataset_id)\n",
    "!gsutil cp $ws_bucket/$source_path $ws_bucket/$destination_path 2> stdout\n",
    "\n",
    "# Limit query dict to valid queries, write out, and copy to output folder for mapping target\n",
    "valid_query_dict = {}\n",
    "for target, val in query_dict.items():\n",
    "    if val[\"syntax_check\"] == \"Passed\":\n",
    "        valid_query_dict[target] = val\n",
    "final_query_dict = {\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"transforms\": valid_query_dict\n",
    "}\n",
    "query_dict_json = json.dumps(final_query_dict)\n",
    "query_output_file = \"transform_query_set.json\"\n",
    "with open(query_output_file, 'w') as outfile:\n",
    "    outfile.write(query_dict_json)\n",
    "destination_path = \"ingest_pipeline/output/transformed/{}/{}/queries\".format(mapping_target, dataset_id)\n",
    "!gsutil cp $query_output_file $ws_bucket/$destination_path/ 2> stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# \"T\" Pipeline: Load Additional Transformed Tables to TDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run Variables\n",
    "dataset_id_run_list = [\n",
    "    #[\"dataset_id\", Run (True/False)],   \n",
    "    ['8da05494-fe7a-4af5-b257-bada143ee426', True],\n",
    "    ['8b098ab4-df02-4619-8ded-657e496695c1', True],\n",
    "    ['373ff2e8-0f63-4179-a55c-3fe0b85556aa', True],\n",
    "    ['31e61d00-61cc-46f2-a793-8ea8dfbb0832', True],\n",
    "]\n",
    "params[\"mapping_target\"] = \"anvil\"\n",
    "params[\"skip_transforms\"] = False\n",
    "params[\"transform_list_override\"] = [] # Leave empty to run transforms for all files, otherwise populate with target table names \n",
    "params[\"skip_schema_extension\"] = False\n",
    "params[\"skip_ingests\"] = False\n",
    "params[\"trunc_before_ingest\"] = True\n",
    "params[\"ingest_list_override\"] = [] # Leave empty to run ingests for all files, otherwise populate with target table names\n",
    "params[\"skip_file_relation_inference\"] = False\n",
    "params[\"skip_dangling_fk_resolution\"] = False\n",
    "params[\"skip_supplementary_file_identification\"] = False\n",
    "params[\"skip_snapshot_creation\"] = False\n",
    "params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\", \"auth-domain\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "params[\"skip_data_validation\"] = False\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Print variables\n",
    "print(\"Pipeline run variables set:\")\n",
    "print(\"Profile ID: \" + params[\"profile_id\"])\n",
    "print(\"Mapping Target: \" + params[\"mapping_target\"])\n",
    "print(\"Datasets to run: \")\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "dataset_skip_list = []\n",
    "for dataset in dataset_id_run_list:\n",
    "    if dataset[1]:\n",
    "        dataset_id = dataset[0]\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "            phs_id = dataset_info[\"phs_id\"]\n",
    "            consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "            auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "            src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "            dataset_skip_list.append(dataset_id)\n",
    "        if dataset_name:\n",
    "            dataset_id = dataset[0]\n",
    "            print(\"- \" + dataset_name + \" ({})\".format(dataset_id))\n",
    "            print(\"\\t- PHS ID = \" + phs_id)\n",
    "            print(\"\\t- Consent Short Name = \" + consent_name)\n",
    "            print(\"\\t- Auth Domains = \" + str(auth_domains))\n",
    "            print(\"\\t- Source Workspaces = \" + str(src_workspaces))\n",
    "if dataset_skip_list:\n",
    "    print(\"Datasets to skip (they either don't exist or aren't accessible to the current user): \")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(dataset_skip_list)) \n",
    "print(\"Skip transforms? \" + str(params[\"skip_transforms\"]))\n",
    "print(\"Transforms override list: \" + str(params[\"transform_list_override\"]))\n",
    "print(\"Skip schema extension? \" + str(params[\"skip_schema_extension\"]))\n",
    "print(\"Skip ingests? \" + str(params[\"skip_ingests\"]))\n",
    "print(\"Truncate tables before ingest? \" + str(params[\"trunc_before_ingest\"]))\n",
    "print(\"Ingest override list: \" + str(params[\"ingest_list_override\"]))\n",
    "print(\"Skip file relationship inference? \" + str(params[\"skip_file_relation_inference\"]))\n",
    "print(\"Skip dangling foreign key resolution? \" + str(params[\"skip_dangling_fk_resolution\"]))\n",
    "print(\"Skip supplementary file identification? \" + str(params[\"skip_supplementary_file_identification\"]))\n",
    "print(\"Skip snapshot creation? \" + str(params[\"skip_snapshot_creation\"]))\n",
    "print(\"Skip data validation? \" + str(params[\"skip_data_validation\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Loop through and execute pipeline for listed workspaces\n",
    "for dataset in dataset_id_run_list:\n",
    "    if dataset[1]:\n",
    "        dataset_id = dataset[0]\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "            phs_id = dataset_info[\"phs_id\"]\n",
    "            consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "            auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "            src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "        if dataset_name:\n",
    "            params[\"dataset_id\"] = dataset_id\n",
    "            params[\"dataset_name\"] = dataset_name\n",
    "            params[\"phs_id\"] = phs_id\n",
    "            params[\"consent_name\"] = consent_name\n",
    "            params[\"auth_domains\"] = auth_domains\n",
    "            utils.run_t_pipeline(params)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Scripts\n",
    "Uncomment sections as necessary to accomplish various miscellaneous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Collect AnVIL Snapshots and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Dataset_ID Filter\n",
    "dataset_id_list = [\n",
    "    'b12fb9be-2ce0-4bfd-8503-732fabba06ab',\n",
    "    'd48adc59-8934-41bb-9720-63e71f1933be'\n",
    "]\n",
    "\n",
    "# Collect Anvil datasets and snapshots\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d %H:%M:%S\")\n",
    "logging.info(f\"Start time: {current_datetime_string}\")\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "datasets_list = datasets_api.enumerate_datasets(filter=\"anvil\", limit=2000)\n",
    "if dataset_id_list:\n",
    "    dataset_list_len = min(len(datasets_list.items), len(dataset_id_list))\n",
    "else:\n",
    "    dataset_list_len = len(datasets_list.items)\n",
    "records_list = []\n",
    "dataset_count = 0\n",
    "for dataset_entry in datasets_list.items:\n",
    "    if len(dataset_id_list) == 0 or dataset_entry.id in dataset_id_list:\n",
    "        dataset_count += 1\n",
    "        logging.info(f\"Processing dataset {dataset_count} of {dataset_list_len}\")\n",
    "        if re.match(\"^ANVIL_[a-zA-Z0-9-_]+_[0-9]{8}\", dataset_entry.name.upper()):\n",
    "            dataset_detail = datasets_api.retrieve_dataset(id=dataset_entry.id, include=[\"PROPERTIES\", \"DATA_PROJECT\"])\n",
    "            snapshots_list = snapshots_api.enumerate_snapshots(dataset_ids=[dataset_entry.id], limit=1000)\n",
    "            try:\n",
    "                source_workspace = \", \".join(dataset_detail.properties[\"source_workspaces\"])\n",
    "            except:\n",
    "                source_workspace = \"\"\n",
    "            if len(snapshots_list.items) == 0:\n",
    "                record = [None, None, None, None, None, None, None, None, None, dataset_entry.id, dataset_entry.name, dataset_detail.ingest_service_account, dataset_entry.created_date[0:10], dataset_entry.cloud_platform, dataset_entry.secure_monitoring_enabled, source_workspace]\n",
    "                records_list.append(record)\n",
    "            else:\n",
    "                snapshot_list_len = len(snapshots_list.items)\n",
    "                snapshot_count = 0\n",
    "                for snapshot_entry in snapshots_list.items:\n",
    "                    snapshot_count += 1\n",
    "                    logging.info(f\"Processing snapshot {snapshot_count} of {snapshot_list_len} for dataset {dataset_count}\")\n",
    "                    # Get public policy information\n",
    "                    creds, project = google.auth.default()\n",
    "                    auth_req = google.auth.transport.requests.Request()\n",
    "                    creds.refresh(auth_req)\n",
    "                    public_flag = \"N\"\n",
    "                    public_response = requests.get(\n",
    "                        url=f\"https://sam.dsde-prod.broadinstitute.org/api/resources/v2/datasnapshot/{snapshot_entry.id}/policies/reader/public\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                    )\n",
    "                    if public_response.text == \"true\":\n",
    "                        public_flag = \"Y\"\n",
    "                    # Get snapshot DUOS ID and Lock status\n",
    "                    api_client = utils.refresh_tdr_api_client()\n",
    "                    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "                    snapshot_detail = snapshots_api.retrieve_snapshot(id=snapshot_entry.id, include=[\"DUOS\"])\n",
    "                    duos_id = \"\"\n",
    "                    if snapshot_detail.duos_firecloud_group:\n",
    "                        duos_id = snapshot_detail.duos_firecloud_group.duos_id\n",
    "                    lock_name = snapshot_detail.resource_locks.exclusive\n",
    "                    if lock_name:\n",
    "                        lock_status = True\n",
    "                    else:\n",
    "                        lock_status = False\n",
    "                    # Get snapshot readers and auth domain\n",
    "                    snapshot_policy_response = snapshots_api.retrieve_snapshot_policies(id=snapshot_entry.id)\n",
    "                    for role in snapshot_policy_response.policies:\n",
    "                        if role.name == \"reader\":\n",
    "                            readers = \", \".join(role.members)\n",
    "                    ad_groups = \"\"\n",
    "                    if snapshot_policy_response.auth_domain:\n",
    "                        ad_groups = \", \".join(snapshot_policy_response.auth_domain)\n",
    "                    record = [snapshot_entry.id, snapshot_entry.name, snapshot_entry.created_date[0:10], public_flag, readers, ad_groups, duos_id, snapshot_entry.data_project, lock_status, dataset_entry.id, dataset_entry.name, dataset_detail.ingest_service_account, dataset_entry.created_date[0:10], dataset_entry.cloud_platform, dataset_entry.secure_monitoring_enabled, source_workspace]\n",
    "                    records_list.append(record)\n",
    "df = pd.DataFrame(records_list, columns =[\"Snapshot ID\", \"Snapshot Name\", \"Snapshot Created Date\", \"Snapshot Public\", \"Snapshot Readers\", \"Snapshot Auth Domain\", \"Snapshot DUOS ID\", \"Snapshot Data Project\", \"Snapshot Locked\", \"Source Dataset ID\", \"Source Dataset Name\", \"Source Dataset SA\", \"Source Dataset Created Date\", \"Cloud Platform\", \"Secure Monitoring\", \"Source Workspace\"])\n",
    "df_sorted = df.sort_values([\"Source Workspace\", \"Source Dataset Name\", \"Snapshot Name\"], ascending=[True, True, True], ignore_index=True)\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d %H:%M:%S\")\n",
    "logging.info(f\"End time: {current_datetime_string}\")\n",
    "display(df_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Deletion of TDR Dataset Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     8,
     129,
     149,
     153
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing record deletions for dataset 04a874df-c57b-40fc-9139-bc3a05129115\n",
      "Processing record deletion for file_inventory\n",
      "Attempting to delete specified rows from file_inventory for dataset 04a874df-c57b-40fc-9139-bc3a05129115\n",
      "TDR Job ID: oBqszfLPShyzkOv-tFt3MA\n",
      "Result: {'objectState': 'deleted'}\n"
     ]
    }
   ],
   "source": [
    "# Input parameters\n",
    "dataset_id_list = [\n",
    "    '04a874df-c57b-40fc-9139-bc3a05129115',\n",
    "]\n",
    "#table_list = [\"anvil_activity\", \"anvil_alignmentactivity\", \"anvil_antibody\", \"anvil_assayactivity\", \"anvil_biosample\", \"anvil_dataset\", \"anvil_diagnosis\", \"anvil_donor\", \"anvil_file\", \"anvil_project\", \"anvil_sequencingactivity\", \"anvil_variantcallingactivity\"]\n",
    "#table_list = [\"file_inventory\", \"sample\", \"subject\", \"workspace_attributes\", \"sequencing\", \"qc_result_sample\", \"family\", \"chromosome\", \"interval\", \"participant\", \"discovery\", \"sample_set\", \"vcf\"]\n",
    "table_list = [\"file_inventory\"]\n",
    "delete_all_records = False\n",
    "delete_record_list = [\n",
    "    '8acf1aab-5b13-43be-8e8f-6def3d694291',\n",
    "    'a34cfccb-bd7d-4486-8eb7-7e207615f90c',\n",
    "    '524dfe4b-fa62-4505-afa6-3bdb89766602',\n",
    "    'f04a39d2-ed93-40cd-96bb-7e80dd10e388',\n",
    "    '349c520e-693c-4553-9a64-fb6d1a3661e4',\n",
    "    '21ac6979-0128-4c5b-a638-293dfaf41012',\n",
    "    'b4e151db-b145-4e33-b547-449755c21849',\n",
    "    '415a3143-875a-4897-afec-5806072f6123',\n",
    "    '257965d1-c7bb-4126-b81f-3638da6cafea',\n",
    "    '8ccfbb24-98c7-48ad-b05b-eabfb2c291b7',\n",
    "    'be955e44-f676-4d37-ac5f-d54daaf047cc',\n",
    "    'f9a89223-9f35-4f2e-8d57-44962092871e',\n",
    "    'd802fd35-a3ac-4eb7-99cd-9ca2f1603460',\n",
    "    '1e8a67f6-3b2b-443d-908a-495c750fcaa6',\n",
    "    '80cb6afa-e7e3-4e63-937b-f92ad06eb23c',\n",
    "    '3f185a9e-ea58-44c9-b5b0-1b1d1f9d82ab',\n",
    "    '40a8d9ef-c314-4ce3-a442-abc47e12ae36',\n",
    "    'ce8e0b34-7398-4ff0-a10b-f33e25d3c5ab',\n",
    "    'd5f3468e-d796-40ad-b92c-c22ca37e0144',\n",
    "    '67672f40-d0be-4f3a-af19-bcb9041559c0',\n",
    "    '41d7b873-b627-49d1-80a6-a556470a44eb',\n",
    "    '3d8ba316-b069-4576-ad0b-1d9eb185a704',\n",
    "    '272d8bff-4daa-4753-b6f3-820cf21cc8ab',\n",
    "    '2758ffad-a34e-4039-afd1-2dd93f54a87f',\n",
    "    '366ada53-eded-4f96-a4eb-b6c7c1e8566c',\n",
    "    '12604f35-16b1-4ad6-bdf4-e0f748c938ae',\n",
    "    'a7931170-3b4a-4ce3-9c18-68c163da2798',\n",
    "    'b64a59e2-3d66-4b2b-bd07-cb1235940602',\n",
    "    '055c51c4-4125-4ecf-a955-25ed8e658b6a',\n",
    "    'f6869819-7e60-436d-bd38-46b8b82b1faa',\n",
    "    '6143267e-6a43-45e8-8c4d-d9764f4fb588',\n",
    "    '4ca652e3-37aa-4a02-a0b9-9463db8dcaaf',\n",
    "    '08f940a3-567c-4087-a2dd-183a68f34025',\n",
    "    'e84c3162-5be9-47cc-92ee-99556e0566d1',\n",
    "    'a427caf2-1549-4b09-bd08-1c319e82a361',\n",
    "    'a4eef201-efea-4cac-aeee-d9930f585b64',\n",
    "    'abaed295-730b-4eca-95bd-2afa326ac009',\n",
    "    '9eb74281-aa5f-433f-b328-9edbe45dee57',\n",
    "    '9ea685bc-bfeb-4d55-b0d0-96e1442311e1',\n",
    "    '00f86ebf-2c09-46fc-b4e7-ccc100fc4428',\n",
    "    '316599df-717a-493a-a582-75dfac72700d',\n",
    "    'b439ead0-e8df-4342-a165-df78d40b3bf2',\n",
    "    'fac47df5-3e08-497d-88a1-10cb134c862c',\n",
    "    '3b5d82d5-4fa2-45b6-9a34-6ab936bc9cb3',\n",
    "    '173a3141-f876-412b-a11b-373fdd11c2aa',\n",
    "    '947a23a1-3087-469b-a623-d95ea55bf4ed',\n",
    "    '8a8d2067-b92a-46b7-81fe-f0c5548b9b31',\n",
    "    'e685c321-c18e-4a1e-adb8-26eabb648a47',\n",
    "    'da7fec1e-a3bb-45f5-aee0-e3f773e627df',\n",
    "    'ddd72800-a109-419c-bd13-bea1c4886a75',\n",
    "    '945338cd-5500-4764-ad4a-d5a0cc7bac90',\n",
    "    '9d50888d-9e99-4b9e-87db-af6063655c7e',\n",
    "    '8c01cba3-8a72-43d3-9533-d207dc5070d6',\n",
    "    '3aad620b-dce2-429b-b520-f69d3bf62b95',\n",
    "    '8e9a2cc6-567d-443b-ba0d-3557f202a0d0',\n",
    "    '9b52fe8f-ee7a-4907-961f-813454fef8bc',\n",
    "    '76834b24-b370-4ae4-8e1a-db9526237f7f',\n",
    "    '1aa37616-83ab-434d-938d-d36f6446d65b',\n",
    "    '923e1bc9-997d-46b9-a34a-ea83b8742d03',\n",
    "    'ab15eac9-5cf6-4d28-a4b4-6ce8c415df88',\n",
    "    '70c3b527-8bda-4504-9727-1c002021e9cd',\n",
    "    '9d3793bd-ed16-48bf-b8e8-e875e0059e08',\n",
    "    'ebe5b83b-f6f4-466c-a69a-63a55bdec00c',\n",
    "    '5b0019e0-54b8-481d-a569-31351e406c4e',\n",
    "    '991f34c5-676b-4e55-8a52-31a6eecb41d9',\n",
    "    '888d1f83-7b25-405b-ae14-ca74f1d0034c',\n",
    "    '260eb1e4-6ec4-4ef9-be8a-f4547914957e',\n",
    "    '5c9ee153-bb52-491a-9935-d484d07d8003',\n",
    "    '45ec79a3-3d88-47d7-8c92-fee0f0be33a2',\n",
    "    '68ce92a9-671d-45a1-a982-6062b711d602',\n",
    "    'ece23424-47f1-4a9b-9bfb-13194892566e',\n",
    "    '873d9dba-afa1-498d-b0d6-fe583010cd22',\n",
    "    '1b3bf439-0a21-47f3-9a5b-9608249c0c24',\n",
    "    '520ad874-da93-4291-8be3-345c92ce6b1d',\n",
    "    'e748be16-9050-45c9-a05a-4c83e96ea109',\n",
    "    '646b4c50-21bd-42f2-b796-5e468a1e2482',\n",
    "    '09fca946-881a-4289-b1d4-39c2035ccb73',\n",
    "    '0b76531d-e69b-4fdd-a1d7-7821a1ec1920',\n",
    "    'f3462a40-e210-4ad9-9c57-458c71669fea',\n",
    "    'be9e2ddc-9f44-41fa-8c4a-afdfb0427868',\n",
    "    '66e910aa-0b64-4367-8f47-5daf59ab3bd9',\n",
    "    '83933015-a449-4dab-a00c-60cdc027850c',\n",
    "    'e1e5e341-8f23-4316-86cb-2f6699b33c8f',\n",
    "    'e83d41e7-b258-41e9-a072-3b1869edcd44',\n",
    "    '42d07a25-1320-47a8-9f82-9a54aa8ccdc8',\n",
    "    '901e4cc0-397a-4ce8-aa90-dc3a7ccf8012',\n",
    "    'dc0affc6-7162-4778-a18e-1b7f5e9fd6ce',\n",
    "    '8a61f4f7-18f6-4410-8c57-2ead5e23bd25',\n",
    "    'c61a2190-d18c-4b15-b3c8-c9d8b443e8d1',\n",
    "    '271234a1-832b-46b5-a648-f9b07fe0f600',\n",
    "    '83b54e82-5c5a-4006-b524-e63e7bb15f01',\n",
    "    '4ee2f1f8-6bea-47e4-8229-36e78ac23e6f',\n",
    "    'ed541762-a443-4fb3-88fe-89f936b58e49',\n",
    "    '7e4ce661-9bcf-4cf1-9d44-c95cec0ce0fa',\n",
    "    '9c772f97-d17d-44e1-af96-315d9bfd4d45',\n",
    "    '119d215e-5e85-4ac9-81d8-efc507dbb262',\n",
    "    'e438d337-5657-4e65-a89e-2d85c30b7931',\n",
    "    '649e50f2-2ec7-4abc-a15d-19b0c938bfaf',\n",
    "    'd1acf55b-6133-48b7-b669-80e758b218e1',\n",
    "    '43d4349e-c114-422a-ae0a-bd6be74d8457',\n",
    "    'd41c3efe-659f-4568-87fc-d366546f5386',\n",
    "    'ff3e1801-8b78-4e3e-8722-64d791af4009',\n",
    "    'a0326832-af55-4591-b7cf-71767e094232',\n",
    "    'b436c85c-f1d8-4fce-96b5-0f765cd882bc',\n",
    "    '14d8a1b4-c2e1-43c3-9941-6d3a6ef81a92',\n",
    "    'd30b3c83-5a13-4e34-aea5-f7f2b8579c82',\n",
    "    'caa2e92d-c366-4a88-b1ea-ac57b391828e',\n",
    "    'afe8260c-697a-487d-9b71-360ed04c744e',\n",
    "    '2dfeaea5-3caf-48ca-b610-3f8f5422a5f6',\n",
    "    '57185c6e-8d25-49cb-9871-56ff564a4917',\n",
    "    'b57c59f9-8dc7-4092-9e3f-aa7c6e033a51',\n",
    "    '5ab6cc40-620d-4a96-9cf2-9eb2047e16f4',\n",
    "    '02f27d6f-0cb8-4307-b06b-ad545a4acca4',\n",
    "    '157b85f3-7bcb-47b0-8e34-1453c2d3ae48',\n",
    "    'ae2a67f8-e082-49a9-92ff-6317605e04a1',\n",
    "] # Will be ignored if delete_all_records is set to True\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to delete rows from a dataset\n",
    "def delete_datarepo_rows(dataset_id, table_name, datarepo_row_ids):\n",
    "    print(\"Attempting to delete specified rows from {} for dataset {}\".format(table_name, dataset_id))\n",
    "    if datarepo_row_ids:\n",
    "        data_deletion_payload = {\n",
    "            \"deleteType\": \"soft\",\n",
    "            \"specType\": \"jsonArray\",\n",
    "            \"tables\": [{\n",
    "              \"tableName\": table_name,\n",
    "              \"jsonArraySpec\": {\n",
    "                \"rowIds\": datarepo_row_ids\n",
    "              }\n",
    "            }]\n",
    "        }\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            data_deletion_result, job_id = utils.wait_for_tdr_job(datasets_api.apply_dataset_data_deletion(id=dataset_id, data_deletion_request=data_deletion_payload))\n",
    "            print(\"Result: {}\".format(data_deletion_result))\n",
    "        except Exception as e:\n",
    "            print(\"Error: {}\".format(str(e)))\n",
    "    else:\n",
    "        print(\"No datarepo_row_ids specified for deletion.\")\n",
    "\n",
    "# Function to collect all datarepo rows for a particular table within a dataset\n",
    "def collect_all_datarepo_rows(dataset_id, table_name):\n",
    "    try:\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        if response[\"access_information\"][\"big_query\"]: \n",
    "            cloud = \"gcp\"\n",
    "            bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "            bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        else:\n",
    "            cloud = \"azure\"\n",
    "            for parquet_table in response[\"access_information\"][\"parquet\"][\"tables\"]:\n",
    "                if parquet_table[\"name\"] == table_name:\n",
    "                    sas_url = parquet_table[\"url\"] + \"?\" + parquet_table[\"sas_token\"]\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving dataset information: {}\".format(str(e)))\n",
    "    if cloud == \"gcp\":\n",
    "        client = bigquery.Client()\n",
    "        query = \"SELECT datarepo_row_id FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = table_name)\n",
    "        try:\n",
    "            query_job = client.query(query)\n",
    "            results = [row[\"datarepo_row_id\"] for row in query_job]\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(\"Error retrieving datarepo_row_id list: {}\".format(str(e)))\n",
    "    else:\n",
    "#         blob_client = BlobClient.from_blob_url(sas_url)\n",
    "#         downloaded_blob = blob_client.download_blob()\n",
    "#         bytes_io = BytesIO(downloaded_blob.readall())\n",
    "#         df_blob = pd.read_parquet(bytes_io)\n",
    "        retrieval_error = False\n",
    "        max_page_size = 1000\n",
    "        records_fetched = 0 \n",
    "        total_record_count = 1\n",
    "        results = []\n",
    "        while records_fetched < total_record_count and not retrieval_error:\n",
    "            row_start = records_fetched\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                payload = {\n",
    "                  \"offset\": row_start,\n",
    "                  \"limit\": max_page_size,\n",
    "                  \"sort\": \"datarepo_row_id\",\n",
    "                  \"direction\": \"asc\",\n",
    "                  \"filter\": \"\"\n",
    "                }\n",
    "                try:\n",
    "                    dataset_results = datasets_api.query_dataset_data_by_id(id=dataset_id, table=table_name, query_data_request_model=payload).to_dict() \n",
    "                    total_record_count = dataset_results[\"total_row_count\"]\n",
    "                    for record in dataset_results[\"result\"]:\n",
    "                        results.append(record[\"datarepo_row_id\"])\n",
    "                        records_fetched += 1\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 5:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        warn_str = \"Error retrieving data_repo_row_ids for table.\"\n",
    "                        logging.warning(warn_str)\n",
    "                        retrieval_error = True\n",
    "                        break\n",
    "        return results\n",
    "    \n",
    "# Function to loop through datasets and delete\n",
    "def execute_deletions(dataset_id_list, table_list, delete_all_records, delete_record_list):\n",
    "    for dataset_id in dataset_id_list:\n",
    "        print(f\"Processing record deletions for dataset {dataset_id}\")\n",
    "        for table in table_list:\n",
    "            print(f\"Processing record deletion for {table}\")\n",
    "            if delete_all_records:\n",
    "                datarepo_row_ids = collect_all_datarepo_rows(dataset_id, table)\n",
    "            else:\n",
    "                datarepo_row_ids = delete_record_list\n",
    "            if datarepo_row_ids:\n",
    "                delete_datarepo_rows(dataset_id, table, datarepo_row_ids)\n",
    "            else:\n",
    "                print(\"No records specified for deletion.\")\n",
    "                \n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "execute_deletions(dataset_id_list, table_list, delete_all_records, delete_record_list)              \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lock/Unlock Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def update_snapshot_lock_status(snapshot_action, snapshot_id_list):\n",
    "    results = []\n",
    "    # Validate snapshot action\n",
    "    print(f\"Validating provided snapshot action: {snapshot_action}\")\n",
    "    if snapshot_action not in [\"LOCK\", \"UNLOCK\"]:\n",
    "        results.append([\"ALL\", snapshot_action, \"Failure\", \"Invalid snapshot action specified. Must be LOCK or UNLOCK.\"])\n",
    "    else:\n",
    "        # Loop through and process snapshots\n",
    "        act = snapshot_action.lower()\n",
    "        for snapshot_id in snapshot_id_list:\n",
    "\n",
    "            # Initialize\n",
    "            print(f\"Updating snapshot lock status for snapshot: {snapshot_id}.\")\n",
    "            error_str = \"\"\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "\n",
    "            # Update snapshot lock status\n",
    "            if act == \"lock\":\n",
    "                # Lock snapshot\n",
    "                try:\n",
    "                    response = snapshots_api.lock_snapshot(id=snapshot_id)\n",
    "                    results.append([snapshot_id, snapshot_action, \"Success\", None])\n",
    "                except Exception as e: \n",
    "                    error_str = f\"Error updating snapshot lock status: {str(e)}\"\n",
    "                    print(error_str)\n",
    "                    results.append([snapshot_id, snapshot_action, \"Failure\", error_str])\n",
    "            else:\n",
    "                # Fetch exclusive lock from snapshot (if there is one)\n",
    "                try:\n",
    "                    snapshot_detail = snapshots_api.retrieve_snapshot(id=snapshot_id).to_dict()\n",
    "                    lock_name = snapshot_detail[\"resource_locks\"].get(\"exclusive\")\n",
    "                    if lock_name:\n",
    "                        # Unlock snapshot (if locked)\n",
    "                        try:\n",
    "                            request_body = {\"lockName\": lock_name, \"forceUnlock\": False}\n",
    "                            response = snapshots_api.unlock_snapshot(id=snapshot_id, unlock_resource_request=request_body)\n",
    "                            results.append([snapshot_id, snapshot_action, \"Success\", None])\n",
    "                        except Exception as e: \n",
    "                            error_str = f\"Error updating snapshot lock status: {str(e)}\"\n",
    "                            print(error_str)\n",
    "                            results.append([snapshot_id, snapshot_action, \"Failure\", error_str])\n",
    "                    else:\n",
    "                        results.append([snapshot_id, snapshot_action, \"Success\", \"No existing lock found on snapshot.\"])\n",
    "                except Exception as e:\n",
    "                    error_str = f\"Error retrieving lock on snapshot: {str(e)}\"\n",
    "                    results.append([snapshot_id, snapshot_action, \"Failure\", error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"snapshot\", \"action\", \"status\", \"errors\"])\n",
    "    display(results_df)\n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the action to apply to the snapshots (LOCK/UNLOCK):\n",
    "snapshot_action = \"UNLOCK\"\n",
    "\n",
    "# Specify the list of snapshots to apply the action to:\n",
    "snapshot_id_list = [\n",
    "    \"c3e5c093-3156-4b4c-be3a-2c307c3d8b23\"\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "update_snapshot_lock_status(snapshot_action, snapshot_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lock/Unlock Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def update_dataset_lock_status(dataset_action, dataset_id_list):\n",
    "    results = []\n",
    "    # Validate dataset action\n",
    "    print(f\"Validating provided dataset action: {dataset_action}\")\n",
    "    if dataset_action not in [\"LOCK\", \"UNLOCK\"]:\n",
    "        results.append([\"ALL\", dataset_action, \"Failure\", \"Invalid dataset action specified. Must be LOCK or UNLOCK.\"])\n",
    "    else:\n",
    "        # Loop through and process datasets\n",
    "        act = dataset_action.lower()\n",
    "        for dataset_id in dataset_id_list:\n",
    "\n",
    "            # Initialize\n",
    "            print(f\"Updating dataset lock status for dataset: {dataset_id}.\")\n",
    "            error_str = \"\"\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "            # Update dataset lock status\n",
    "            if act == \"lock\":\n",
    "                # Lock dataset\n",
    "                try:\n",
    "                    response = datasets_api.lock_dataset(id=dataset_id)\n",
    "                    results.append([dataset_id, dataset_action, \"Success\", None])\n",
    "                except Exception as e: \n",
    "                    error_str = f\"Error updating dataset lock status: {str(e)}\"\n",
    "                    print(error_str)\n",
    "                    results.append([dataset_id, dataset_action, \"Failure\", error_str])\n",
    "            else:\n",
    "                # Fetch exclusive lock from dataset (if there is one)\n",
    "                try:\n",
    "                    dataset_detail = datasets_api.retrieve_dataset(id=dataset_id).to_dict()\n",
    "                    exclusive_lock_name = dataset_detail[\"resource_locks\"].get(\"exclusive\")\n",
    "                    shared_locks = dataset_detail[\"resource_locks\"].get(\"shared\") \n",
    "                    if exclusive_lock_name:\n",
    "                        # Unlock dataset (if locked with exclusive lock)\n",
    "                        try:\n",
    "                            request_body = {\"lockName\": exclusive_lock_name, \"forceUnlock\": False}\n",
    "                            response = datasets_api.unlock_dataset(id=dataset_id, unlock_resource_request=request_body)\n",
    "                            results.append([dataset_id, dataset_action, \"Success\", None])\n",
    "                        except Exception as e: \n",
    "                            error_str = f\"Error updating dataset lock status: {str(e)}\"\n",
    "                            print(error_str)\n",
    "                            results.append([dataset_id, dataset_action, \"Failure\", error_str])\n",
    "                    elif shared_locks:\n",
    "                        # Unlock dataset (if locked with shared locks)\n",
    "                        for lock_name in shared_locks:\n",
    "                            try:\n",
    "                                request_body = {\"lockName\": lock_name, \"forceUnlock\": False}\n",
    "                                response = datasets_api.unlock_dataset(id=dataset_id, unlock_resource_request=request_body)\n",
    "                                results.append([dataset_id, dataset_action, \"Success\", None])\n",
    "                            except Exception as e: \n",
    "                                error_str = f\"Error updating dataset lock status: {str(e)}\"\n",
    "                                print(error_str)\n",
    "                                results.append([dataset_id, dataset_action, \"Failure\", error_str])\n",
    "                    else:\n",
    "                        results.append([dataset_id, dataset_action, \"Success\", \"No existing lock found on dataset.\"])\n",
    "                except Exception as e:\n",
    "                    error_str = f\"Error retrieving lock on dataset: {str(e)}\"\n",
    "                    results.append([dataset_id, dataset_action, \"Failure\", error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset\", \"action\", \"status\", \"errors\"])\n",
    "    display(results_df)\n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the action to apply to the datasets (LOCK/UNLOCK):\n",
    "dataset_action = \"UNLOCK\"\n",
    "\n",
    "# Specify the list of datasets to apply the action to:\n",
    "dataset_id_list = [\n",
    "    '32c09444-3d4a-44d5-af6b-07eef92189db',\n",
    "    '280c5d6f-39a3-4d1d-aad2-a174451cd9b2',\n",
    "    'e4ccd185-2b0c-445d-9c57-0dc45c8f9d7e',\n",
    "    '20ddfcd5-d456-431b-9f05-781e05d873d6',\n",
    "    '15d41c35-943c-474b-afa6-e1c6d6e4be2b',\n",
    "    '3ef7966a-ec1e-4dba-9d31-cdb33692e78f',\n",
    "    'dd6c6688-b73a-464c-86d9-3369fdf98268',\n",
    "    '15b153f5-ed02-4216-8f96-99743b8b4fc3',\n",
    "    '747858c0-d139-4f52-9f0e-a618b880d6d6',\n",
    "    '3376a8b6-7ef6-4191-97ab-a547da0d330d',\n",
    "    'f85e467a-958f-4da5-a01b-8df883e69122',\n",
    "    '0b25d09e-b2d9-4452-9810-1d0ef777f9d6',\n",
    "    '6ac178b7-a923-407f-8cd8-1733e1b2ebd5',\n",
    "    'c9dd3578-01db-4687-9807-4f71368941d1',\n",
    "    '5edcc3db-c676-412a-9506-600959bb81f2',\n",
    "    'ccc524ab-d9ad-467c-a25b-9a14fb05e976',\n",
    "    '4b341ba9-49a5-43a2-9b7e-cc96beb59946',\n",
    "    '0c18589c-6432-4a6c-90ce-985a47a66f39',\n",
    "    'e6b15b39-daba-431f-a918-e4e43e702c30',\n",
    "    'aa314675-af62-41df-b5cb-3b22558e903b',\n",
    "    '20741062-7d1d-44b7-bc33-39c9ad26e414',\n",
    "    '69ce1be3-1815-43a4-bdd2-4696d9c8d09a',\n",
    "    '76dd508c-aa80-4e54-9ac4-23b5e0545316',\n",
    "    '1c6bef41-3cfa-46b2-b183-0a523e417457',\n",
    "    '18716daf-4223-44a9-bba9-fc9baeef7d07',\n",
    "    '373ff2e8-0f63-4179-a55c-3fe0b85556aa',\n",
    "    '31e61d00-61cc-46f2-a793-8ea8dfbb0832',\n",
    "    '462d992a-7c13-45ac-a6da-1254fc3a9031',\n",
    "    '74d1e549-5ae8-4410-9428-f8f2cc85fa80',\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "update_dataset_lock_status(dataset_action, dataset_id_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TDR Dataset and/or Snapshot Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     12,
     23
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to delete dataset = cc9b89e8-30ea-4149-9c37-3000128ad42c and all associated snapshots\n",
      "Attempting to delete dataset = cc9b89e8-30ea-4149-9c37-3000128ad42c\n",
      "TDR Job ID: UawR-vXuT_u7lvFTFvaBag\n",
      "Result: {'objectState': 'deleted'}\n"
     ]
    }
   ],
   "source": [
    "# Function to delete a specific TDR Snapshot\n",
    "def delete_snapshot(snapshot_id):\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete snapshot = {}\".format(snapshot_id))\n",
    "    try:\n",
    "        delete_snapshot_result, job_id = utils.wait_for_tdr_job(snapshots_api.delete_snapshot(id=snapshot_id))\n",
    "        print(\"Result: {}\".format(delete_snapshot_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset\n",
    "def delete_dataset(dataset_id):\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {}\".format(dataset_id))\n",
    "    try:\n",
    "        delete_dataset_result, job_id = utils.wait_for_tdr_job(datasets_api.delete_dataset(id=dataset_id))\n",
    "        print(\"Result: {}\".format(delete_dataset_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset and all of its Snapshots\n",
    "def delete_dataset_and_all_snapshots(dataset_id):\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {} and all associated snapshots\".format(dataset_id))\n",
    "    dataset_id_list = [dataset_id]\n",
    "    # Delete snapshots\n",
    "    snapshot_list = snapshots_api.enumerate_snapshots(dataset_ids=dataset_id_list)\n",
    "    if snapshot_list.items:\n",
    "        for snapshot in snapshot_list.items:\n",
    "            snapshot_id = str(snapshot.id)\n",
    "            delete_snapshot(snapshot_id)\n",
    "            sleep(10)\n",
    "    # Delete dataset\n",
    "    delete_dataset(dataset_id)\n",
    "\n",
    "# # Delete snapshots\n",
    "# snapshot_id_list = [\n",
    "# ]\n",
    "# for snapshot_id in snapshot_id_list:\n",
    "#     delete_snapshot(snapshot_id)\n",
    "\n",
    "# Delete datasets and all their associated snapshots\n",
    "dataset_id_list = [\n",
    "    'cc9b89e8-30ea-4149-9c37-3000128ad42c',\n",
    "]\n",
    "for dataset_id in dataset_id_list:\n",
    "    delete_dataset_and_all_snapshots(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Clean Up Outdated AnVIL TDR Service Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_sa_list = [\n",
    "]\n",
    "\n",
    "# Establish credentials\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "\n",
    "# Get current anvil_tdr_ingest membership\n",
    "group = \"anvil_tdr_ingest\"\n",
    "group_members = requests.get(\n",
    "    url=f\"https://api.firecloud.org/api/groups/{group}\",\n",
    "    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    ").json()\n",
    "\n",
    "# Loop through anvil_tdr_ingest membership and remove outdated users\n",
    "user_cnt = 0\n",
    "success_cnt = 0\n",
    "for member in group_members[\"membersEmails\"]:\n",
    "    if \"tdr-ingest-sa\" in member and member not in valid_sa_list:\n",
    "        user_cnt += 1\n",
    "        response = requests.delete(\n",
    "            url=f\"https://api.firecloud.org/api/groups/{group}/member/{member}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code == 204:\n",
    "            success_cnt += 1\n",
    "print(f\"Group ({group}) clean-up: \")\n",
    "print(f\"\\t- Users to remove: {user_cnt}\")\n",
    "print(f\"\\t- Users removed successfully: {success_cnt}\")\n",
    "\n",
    "# Get current workspace membership\n",
    "ws_members = requests.get(\n",
    "    url=f\"https://api.firecloud.org/api/workspaces/{ws_project}/{ws_name}/acl\",\n",
    "    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    ").json()\n",
    "\n",
    "# Loop through workspace membership and remove outdated users\n",
    "user_cnt = 0\n",
    "success_cnt = 0\n",
    "for member in ws_members[\"acl\"].keys():\n",
    "    if \"tdr-ingest-sa\" in member and member not in valid_sa_list:\n",
    "        user_cnt += 1\n",
    "        payload = [{\n",
    "            \"email\": member,\n",
    "            \"accessLevel\": \"NO ACCESS\",\n",
    "            \"canShare\": False,\n",
    "            \"canCompute\": False\n",
    "        }]\n",
    "        response = requests.patch(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{ws_project}/{ws_name}/acl\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}, \n",
    "            json=payload\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            success_cnt += 1\n",
    "print(f\"Workspace ({ws_project}/{ws_name}) clean-up: \")\n",
    "print(f\"\\t- Users to remove: {user_cnt}\")\n",
    "print(f\"\\t- Users removed successfully: {success_cnt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Other Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gsutil -u anvil-datastorage ls gs://fc-secure-33cad843-3453-42ea-bf50-0eda2b52171d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
