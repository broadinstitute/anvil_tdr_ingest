{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print(\"Version 1.0.0: 09/23/2022 5:06pm - Nate Calvanese - First version created\")\n",
    "#print(\"Version 1.0.1: 09/26/2022 11:18m - Nate Calvanese - Fixed bug in default dataset naming\")\n",
    "#print(\"Version 1.0.2: 09/27/2022 2:43pm - Nate Calvanese - Added ability to aggregate multiple workspaces into one dataset\")\n",
    "#print(\"Version 1.0.3: 10/5/2022 1:32pm - Nate Calvanese - Added support for chunking up ingest requests\")\n",
    "#print(\"Version 1.0.4: 10/6/2022 10:35am - Nate Calvanese - Updated use of TDR utility functions\")\n",
    "#print(\"Version 1.0.5: 10/13/2022 10:54am - Nate Calvanese - Parameter tweaks for latest changes\")\n",
    "#print(\"Version 1.0.6: 10/21/2022 10:53am - Nate Calvanese - Version stamp for latest changes to supporting notebooks\")\n",
    "#print(\"Version 1.0.7: 10/24/2022 4:58pm - Nate Calvanese - Added support for project entity name derivation\")\n",
    "#print(\"Version 1.0.8: 10/26/2022 4:24pm - Nate Calvanese - Added support for batching mapping activities in section 3\")\n",
    "#print('Version 1.0.9: 2/21/2023 2:50pm - Nate Calvanese - Added support for $BQ_DATASET substitution variable in mapping section')\n",
    "#print('Version 1.0.10: 3/8/2023 8:17am - Nate Calvanese - Performance improvements')\n",
    "#print('Version 1.0.11: 7/11/2023 8:17am - Nate Calvanese - Added auth domain back as reader on snapshots')\n",
    "#print('Version 1.0.12: 9/1/2023 10:16am - Nate Calvanese - Added functionality to enable/disable secure monitoring for public datasets.')\n",
    "#print('Version 1.0.13: 12/15/2023 9:00am - Nate Calvanese - Added functionality to optionally truncate tables before ingest')\n",
    "#print('Version 1.0.14: 1/12/2024 11:28am - Nate Calvanese - Added max_combined_rec_ref_size as a global parameter')\n",
    "#print('Version 1.0.15: 2/5/2025 3:10pm - Nate Calvanese - Updated input parameters for T pipeline')\n",
    "#print('Version 1.0.16: 2/20/2025 11:46am - Nate Calvanese - Updated input parameters for mapping section')\n",
    "#print('Version 1.0.17: 3/4/2025 10:10am - Nate Calvanese - Updated input parameters for transformation section')\n",
    "#print('Version 1.1.0 5/7/2025 3:14pm - Nate Calvanese - Refactored notebook to support running in dev as well as prod.')\n",
    "#print('Version 1.2.0 7/30/2025 1:49pm - Nate Calvanese - Updated section 3.2 to limit certain relationships to a target mapping')\n",
    "print('Version 1.2.0 7/30/2025 9:16pm - Nate Calvanese - Added functionality for comparing row counts between objects')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Common Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional modules (one time effort per cloud environment)\n",
    "!pip install --upgrade pip import_ipynb data_repo_client urllib3 xmltodict azure-storage-blob\n",
    "# !pip install data_repo_client==1.409.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copy latest version of the pipeline notebooks to the cloud environment (uncomment if any notebooks have changed since last run)\n",
    "# print(\"\\nCopying latest pipeline notebooks to the cloud environment:\")\n",
    "# !gsutil -m cp $ws_bucket/notebooks/*.ipynb .\n",
    "\n",
    "# # Manually copy in single notebook\n",
    "# import os\n",
    "# ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "# !gsutil cp $ws_bucket/notebooks/ingest_pipeline_utilities.ipynb .\n",
    "# # !gsutil cp $ws_bucket/notebooks/build_mapping_query.ipynb ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace environment variables\n",
    "import os\n",
    "import re\n",
    "print(\"Recording workspace environment variables:\")\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "print(f\"Workspace name = {ws_name}\")\n",
    "print(f\"Workspace project = {ws_project}\")\n",
    "print(f\"Workspace bucket = {ws_bucket}\")\n",
    "print(f\"Workspace bucket name = {ws_bucket_name}\")\n",
    "\n",
    "# Copy latest version of the pipeline notebooks to the cloud environment (uncomment if any notebooks have changed since last run)\n",
    "# print(\"\\nCopying latest pipeline notebooks to the cloud environment:\")\n",
    "# !gsutil -m cp $ws_bucket/notebooks/*.ipynb .\n",
    "\n",
    "# Additional imports\n",
    "print(\"\\nRunning imports:\")\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from firecloud import api as fapi\n",
    "import data_repo_client\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_mapping_query as bmq\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "from time import sleep\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pyarrow.parquet as pq\n",
    "from azure.storage.blob import BlobClient, ContainerClient\n",
    "\n",
    "# Common pipeline variables (AnVIL)\n",
    "params = {}\n",
    "params[\"run_env\"] = \"prod\" # 'prod' or 'dev'\n",
    "if params[\"run_env\"] == \"prod\":\n",
    "    params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\" \n",
    "    params[\"terra_url\"] = \"https://api.firecloud.org\"\n",
    "    params[\"tdr_url\"] = \"https://data.terra.bio\"\n",
    "else:\n",
    "    params[\"profile_id\"] = \"ab050a35-e597-4c81-9d24-331a49e86016\" \n",
    "    params[\"terra_url\"] = \"https://firecloud-orchestration.dsde-dev.broadinstitute.org\"\n",
    "    params[\"tdr_url\"] = \"https://jade.datarepo-dev.broadinstitute.org\"\n",
    "ws_attributes = utils.get_workspace_attributes(ws_project, ws_name, params[\"terra_url\"])\n",
    "params[\"ws_name\"] = ws_name\n",
    "params[\"ws_project\"] = ws_project\n",
    "params[\"ws_bucket\"] = ws_bucket\n",
    "params[\"ws_bucket_name\"] = ws_bucket_name\n",
    "params[\"google_project\"] = ws_attributes[\"googleProject\"]\n",
    "params[\"create_file_table\"] = True\n",
    "params[\"file_table_name\"] = \"file_inventory\"\n",
    "#params[\"ingest_user_to_add\"] = \"tdr_sa\"  # tdr_sa or anvil_tdr_ingest\n",
    "params[\"global_file_exclusions\"] = [\"SubsetHailJointCall\", \".vds/\", \"ingest_ignore\"]\n",
    "params[\"max_combined_rec_ref_size\"] = 40000\n",
    "\n",
    "# Configure logging format\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# \"EL\" Pipeline: Load Dataset to TDR in Source Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## >>> Run Variables <<<\n",
    "# For datasets split across multiple workspaces, set the staging area and target TDR dataset to the \n",
    "# same value to collect all of the source data and process it together.\n",
    "workspace_run_list = [\n",
    "    #[\"Workspace_Name\", \"Workspace_Project\", Public (True/False), \"Staging Area (Leave empty for default)\", \"Target_TDR_Dataset_Name (Leave empty for default)\", Run (True/False)]\n",
    "    ['AnVIL_CCDG_Broad_NP_Epilepsy_USAMON_GRU_GSA-MD', 'anvil-datastorage', False, '', 'ANVIL_CCDG_Broad_NP_Epilepsy_USAMON_GRU_GSA_MD_20250730', True],\n",
    "#     ['AnVIL_CCDG_WashU_CVD_EOCAD_METSIM_WGS', 'anvil-datastorage', False, '', 'ANVIL_CCDG_WashU_CVD_EOCAD_METSIM_WGS_20250730', True],\n",
    "]\n",
    "params[\"skip_source_files_creation\"] = True\n",
    "params[\"skip_file_inventory_creation\"] = True\n",
    "params[\"skip_table_data_processing\"] = True\n",
    "params[\"skip_ingests\"] = False\n",
    "params[\"trunc_before_ingest\"] = True\n",
    "params[\"skip_snapshot_creation\"] = True\n",
    "params[\"snapshot_readers_list\"] = [\"auth-domain\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "\n",
    "\n",
    "## >>> File Inventory Variables <<<\n",
    "# The GCS bucket associated with the source workspace will be automatically included in the file inventory build. To specify \n",
    "# additional GCS buckets to include in the file inventory build, add entries to the below dictionary.\n",
    "params[\"additional_file_inventory_sources\"] = {}\n",
    "# EXAMPLE:\n",
    "# params[\"additional_file_inventory_sources\"] = {\n",
    "#     \"staging_area\": {\n",
    "#         \"bucket_name\": {\n",
    "#             \"include_dirs\": [], # Leave empty to include all directories in bucket\n",
    "#             \"exclude_dirs\": [] # Exclusions will take precedence over inclusions\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "## >>> Ingest Variables <<<\n",
    "# For cases where you only want to ingest a subset of files, use the below dictionary to specify exactly what should be ingested.\n",
    "params[\"ingest_list_override\"] = {\n",
    "    \"sample\": []\n",
    "}\n",
    "# EXAMPLE:\n",
    "# params[\"ingest_list_override\"] = {\n",
    "#     \"ws_table\": [\"ws_table_0.json\"], # Leave empty to run ingest for every file for target table\n",
    "# }\n",
    "\n",
    "\n",
    "## >>> File Reference Variables <<<\n",
    "# Fields containing GCS links will be identified automatically by the pipeline. The below dict should contain any fields\n",
    "# that contain file references that aren't proper GCS links in the workspace tables.\n",
    "data_file_refs_dict = {}\n",
    "# Definitions:\n",
    "#    Required Fields: column, method, mode, create_new field\n",
    "#    Optional Fields: match_multiple_files (default to True), match_regex (default to None), match_type (default to 'partial'), new_field_name (default to None)\n",
    "#    Methods: \n",
    "#       file_path_match -- Field contains a full or partial file path, which can be matched to the file inventory to grab the file(s) referenced \n",
    "#       tdr_file_id -- Field contains file UUIDs of files already ingested into the target TDR dataset\n",
    "#    Modes:\n",
    "#       fileref_in_line -- Populates the field with a file reference object\n",
    "#       fileref_table_ref -- Populates the field with an ID that joins to a file table. If no file table built, falls back on fileref_in_line logic.\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "# Print variables\n",
    "print(\"Pipeline run variables set:\")\n",
    "print(\"Profile ID: \" + params[\"profile_id\"])\n",
    "print(\"Ingests to run: \")\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "for workspace in workspace_run_list:\n",
    "    if workspace[5] == True:\n",
    "        ws_attributes = utils.get_workspace_attributes(workspace[1], workspace[0], params[\"terra_url\"])\n",
    "        params[\"phs_id\"] = utils.format_phs_id(ws_attributes[\"attributes\"][\"phs_id\"]) if ws_attributes[\"attributes\"].get(\"phs_id\") else \"\"\n",
    "        auth_list = ws_attributes[\"authorizationDomain\"] if ws_attributes.get(\"authorizationDomain\") else []\n",
    "        params[\"auth_domains\"] = [x[\"membersGroupName\"] for x in auth_list]\n",
    "        params[\"consent_name\"] = ws_attributes[\"attributes\"][\"library:dataUseRestriction\"] if ws_attributes[\"attributes\"].get(\"library:dataUseRestriction\") else \"\"\n",
    "        if not params[\"consent_name\"]:\n",
    "            ws_tags = ws_attributes[\"attributes\"].get(\"tag:tags\")\n",
    "            if ws_tags:\n",
    "                for ws_tag in ws_tags:\n",
    "                    if \"consent_code:\" in ws_tag:\n",
    "                        params[\"consent_name\"] = ws_tag.replace(\"consent_code:\", \"\").strip()\n",
    "                        break\n",
    "        params[\"data_files_src_bucket\"] = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "        params[\"public_dataset\"] = workspace[2]\n",
    "        workspace[4] = workspace[4] if workspace[4] else utils.format_dataset_name(workspace[0])\n",
    "        workspace[3] = workspace[3] if workspace[3] else workspace[0]\n",
    "        print(\"- Workspace [\" + workspace[1] + \"/\" + workspace[0] + \"] to TDR dataset [\" + workspace[4] + \"] via Staging Area [\" + workspace[3] + \"]\")\n",
    "        print(\"\\t- PHS ID = \" + params[\"phs_id\"])\n",
    "        print(\"\\t- Consent Short Name = \" + params[\"consent_name\"])\n",
    "        print(\"\\t- Auth Domains = \" + str(params[\"auth_domains\"]))\n",
    "        print(\"\\t- Public Dataset = \" + str(params[\"public_dataset\"]))\n",
    "        print(\"\\t- Data Files Source Bucket = \" + params[\"data_files_src_bucket\"])\n",
    "print(\"Skip source files creation? \" + str(params[\"skip_source_files_creation\"]))\n",
    "print(\"Skip file inventory creation? \" + str(params[\"skip_file_inventory_creation\"]))\n",
    "print(\"Skip table data processing? \" + str(params[\"skip_table_data_processing\"]))\n",
    "print(\"Skip ingests? \" + str(params[\"skip_ingests\"]))\n",
    "print(\"Truncate tables before ingest? \" + str(params[\"trunc_before_ingest\"]))\n",
    "print(\"Ingest override list: \" + str(params[\"ingest_list_override\"]))\n",
    "print(\"Skip snapshot creation? \" + str(params[\"skip_snapshot_creation\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through and execute workspace connector pipeline (\"E\") for listed workspaces\n",
    "!mkdir -p pipeline_results\n",
    "if params[\"skip_source_files_creation\"] == True:\n",
    "    logging.info(\"Skipping source file creation, per user request.\")\n",
    "else:\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    e_output_file = \"pipeline_results/e_pipeline_results_\" + current_datetime_string + \".json\"\n",
    "    logging.info(f\"E Pipeline Results File: {e_output_file}\")\n",
    "    params[\"e_pipeline_results\"] = []\n",
    "    for workspace in workspace_run_list:\n",
    "        if workspace[5] == True:\n",
    "            params[\"data_file_refs\"] = data_file_refs_dict  \n",
    "            utils.run_ws_connector_pipeline(workspace, params)\n",
    "            params[\"e_pipeline_results\"].extend(params[\"pipeline_results\"])\n",
    "            if params[\"e_pipeline_results\"]:\n",
    "                with open(e_output_file, \"w\") as e_out:\n",
    "                    json.dump(params[\"e_pipeline_results\"], e_out)\n",
    "\n",
    "# Aggregate staging area to target dataset combinations, loop through them, and execute ingest pipeline (\"L\")\n",
    "!mkdir -p pipeline_results\n",
    "pipeline_run_list = []\n",
    "for workspace in workspace_run_list:\n",
    "    if workspace[5] == True:\n",
    "        temp_list = [workspace[3], workspace[4], workspace[2]]\n",
    "        if temp_list not in pipeline_run_list:\n",
    "            pipeline_run_list.append(temp_list)\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "l_output_file = \"pipeline_results/l_pipeline_results_\" + current_datetime_string + \".json\"\n",
    "logging.info(f\"L Pipeline Results File: {l_output_file}\")\n",
    "params[\"l_pipeline_results\"] = []\n",
    "for pipeline in pipeline_run_list:\n",
    "    utils.run_el_pipeline(pipeline, params)\n",
    "    params[\"l_pipeline_results\"].extend(params[\"pipeline_results\"])\n",
    "    if params[\"l_pipeline_results\"]:\n",
    "        with open(l_output_file, \"w\") as l_out:\n",
    "            json.dump(params[\"l_pipeline_results\"], l_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mapping Development\n",
    "Work through the following steps for each dataset that needs to be processed through the transformation pipeline in Step 4, specifying the target schema (\"mapping target\") and mapping specification (\"mapping_target_spec\") you would like to use for transformation. You can also specify the PHS ID (\"phs\") and consent group (\"consent\") for the dataset in order to pass that information into the transformations, to cover cases where it is not provided by the data submitters in their data tables. Note that you can use the logs or results_dict from the previous step to retrieve the dataset_id values of interest, or retrieve them directly from TDR via the UI or Swagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Dataset Mapping Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## >>> Mapping Variables <<<\n",
    "# For each dataset specified, include an appropriate mapping target and mapping target specification\n",
    "datasets_to_map_list = [\n",
    "    #[\"dataset_id\", \"mapping_target\", \"mapping_target_spec\", \"phs\", \"consent\", Run (True/False)]\n",
    "#     ['1e84fc06-90e2-4b76-bae1-81b92822e761', 'anvil', 'gafk_1', 'phs002206', 'DS-PEDD-IRB', True],\n",
    "#     ['110c5620-4e55-4738-a490-b45098e52bb0', 'anvil', 'igvf_1', 'phs003472', 'HMB-MDS', True],\n",
    "    ['7f8e26ba-b3ff-4751-b17d-44560b22922f', 'anvil', 'gregor_1', 'phs003047', 'GRU', True],\n",
    "    ['6b2574fe-523c-4d71-ab66-d336a79cc9e0', 'anvil', 'anvil_1', 'phs003018', 'NRES', True],\n",
    "]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "# Print variables\n",
    "print(\"Datasets to map: \")\n",
    "api_client = utils.refresh_tdr_api_client(params[\"tdr_url\"])\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "final_datasets_to_map_dict = {}\n",
    "skip_dataset_list_access = []\n",
    "skip_dataset_list_mapping = []\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "for dataset in datasets_to_map_list:\n",
    "    if dataset[5]:\n",
    "        dataset_id = dataset[0]\n",
    "        mapping_target = dataset[1]\n",
    "        mapping_target_spec = dataset[2]\n",
    "        mapping_phs = dataset[3]\n",
    "        mapping_consent = dataset[4]\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "            skip_dataset_list_access.append(dataset_id)\n",
    "        try:\n",
    "            blob = bucket.blob(\"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target))\n",
    "            content = json.loads(blob.download_as_string(client=None))\n",
    "            blob = bucket.blob(\"ingest_pipeline/mapping/{}/{}/mapping_specification.json\".format(mapping_target, mapping_target_spec))\n",
    "            content = json.loads(blob.download_as_string(client=None))\n",
    "        except:\n",
    "            skip_dataset_list_mapping.append(dataset_id)\n",
    "        if dataset_id not in skip_dataset_list_access and dataset_id not in skip_dataset_list_mapping:\n",
    "            final_datasets_to_map_dict[dataset_id] = {}\n",
    "            final_datasets_to_map_dict[dataset_id][\"mapping_target\"] = mapping_target \n",
    "            final_datasets_to_map_dict[dataset_id][\"mapping_target_spec\"] = mapping_target_spec\n",
    "            final_datasets_to_map_dict[dataset_id][\"mapping_phs\"] = mapping_phs \n",
    "            final_datasets_to_map_dict[dataset_id][\"mapping_consent\"] = mapping_consent\n",
    "            print(f\"\\t - {dataset_name} ({dataset_id}) with {mapping_target}/{mapping_target_spec}\")\n",
    "if skip_dataset_list_access:\n",
    "    print(\"Datasets to skip due to non-existence or inaccessibility to the current user:\")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(skip_dataset_list_access))\n",
    "if skip_dataset_list_mapping:\n",
    "    print(\"Datasets to skip due to invalid mapping target or mapping target specification:\")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(skip_dataset_list_mapping))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Add Missing Relationships to TDR Dataset Schema\n",
    "Relationships are needed by the mapping query constructor to build appropriate joins between tables. If no joins are required between tables, this step is unnecessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_datasets_to_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Record relationships to potentially add to the source datasets. Note that there may be more relationships to add\n",
    "# than those listed below, so add to this list as necessary.\n",
    "potential_relationships = {\n",
    "    \"ALL_DATASETS\": [\n",
    "        [\"subject.family_id\", \"family.family_id\"],\n",
    "        [\"sample.subject_id\", \"subject.subject_id\"],\n",
    "        [\"sample.t_01_subject_id\", \"subject.subject_id\"],\n",
    "        [\"sequencing.sample_id\", \"sample.sample_id\"],\n",
    "        [\"sequencing.sample\", \"sample.sample_id\"],\n",
    "        [\"sequencing.sample_alias\", \"sample.sample_id\"],\n",
    "        [\"sample.participant\", \"participant.participant_id\"],\n",
    "        [\"sample.participant_id\", \"participant.participant_id\"],\n",
    "        [\"discovery.sample_id\", \"sample.sample_id\"],\n",
    "        [\"discovery.subject_id\", \"subject.subject_id\"],\n",
    "        [\"qc_result_sample.qc_result_sample_id\", \"sample.sample_id\"],\n",
    "        [\"interval.chromosome\", \"chromosome.chromosome_id\"],\n",
    "        [\"analyte.participant_id\", \"participant.participant_id\"],\n",
    "        [\"participant.family_id\", \"family.family_id\"],\n",
    "        [\"phenotype.participant_id\", \"participant.participant_id\"],\n",
    "        [\"biosample.donor_id\", \"donor.donor_id\"]\n",
    "    ],\n",
    "    \"gregor\": [\n",
    "        # GREGoR RNA Short Read\n",
    "        [\"experiment_rna_short_read.analyte_id\", \"analyte.analyte_id\"],\n",
    "        [\"aligned_rna_short_read.experiment_rna_short_read_id\", \"experiment_rna_short_read.experiment_rna_short_read_id\"],\n",
    "        [\"aligned_rna_short_read_set.aligned_rna_short_reads\", \"aligned_rna_short_read.aligned_rna_short_read_id\"],\n",
    "        [\"called_variants_rna_short_read.aligned_rna_short_read_set_id\", \"aligned_rna_short_read_set.aligned_rna_short_read_set_id\"],\n",
    "        # GREGoR DNA Short Read\n",
    "        [\"experiment_dna_short_read.analyte_id\", \"analyte.analyte_id\"],\n",
    "        [\"aligned_dna_short_read.experiment_dna_short_read_id\", \"experiment_dna_short_read.experiment_dna_short_read_id\"],\n",
    "        [\"aligned_dna_short_read_set.aligned_dna_short_reads\", \"aligned_dna_short_read.aligned_dna_short_read_id\"],\n",
    "        [\"called_variants_dna_short_read.aligned_dna_short_read_set_id\", \"aligned_dna_short_read_set.aligned_dna_short_read_set_id\"],\n",
    "        # GREGoR Nanopore\n",
    "        [\"experiment_nanopore.analyte_id\", \"analyte.analyte_id\"],\n",
    "        [\"aligned_nanopore.experiment_nanopore_id\", \"experiment_nanopore.experiment_nanopore_id\"],\n",
    "        [\"aligned_nanopore_set.aligned_nanopores\", \"aligned_nanopore.aligned_nanopore_id\"],\n",
    "        [\"called_variants_nanopore.aligned_nanopore_set_id\", \"aligned_nanopore_set.aligned_nanopore_set_id\"],\n",
    "        # GREGoR PacBio\n",
    "        [\"experiment_pac_bio.analyte_id\", \"analyte.analyte_id\"],\n",
    "        [\"aligned_pac_bio.experiment_pac_bio_id\", \"experiment_pac_bio.experiment_pac_bio_id\"],\n",
    "        [\"aligned_pac_bio_set.aligned_pac_bios\", \"aligned_pac_bio.aligned_pac_bio_id\"],\n",
    "        [\"called_variants_pac_bio.aligned_pac_bio_set_id\", \"aligned_pac_bio_set.aligned_pac_bio_set_id\"],\n",
    "        # GREGoR ATAC Short Read\n",
    "        [\"experiment_atac_short_read.analyte_id\", \"analyte.analyte_id\"],\n",
    "        [\"aligned_atac_short_read.experiment_atac_short_read_id\", \"experiment_atac_short_read.experiment_atac_short_read_id\"],\n",
    "        [\"aligned_atac_short_read_set.aligned_atac_short_reads\", \"aligned_atac_short_read.aligned_atac_short_read_id\"],\n",
    "        [\"called_variants_atac_short_read.aligned_atac_short_read_set_id\", \"aligned_atac_short_read_set.aligned_atac_short_read_set_id\"],\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Loop through datasets and process potential relationship additions\n",
    "results = []\n",
    "api_client = utils.refresh_tdr_api_client(params[\"tdr_url\"])\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "for dataset_id, dataset_details in final_datasets_to_map_dict.items():\n",
    "    print(\"Processing potential relationships for dataset_id = {}\".format(dataset_id))\n",
    "    mapping_target_spec = dataset_details[\"mapping_target_spec\"]\n",
    "    \n",
    "    # Retrieve source schema\n",
    "    src_schema_dict = {}\n",
    "    try:\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "        results.append([dataset_id, \"Error\"])\n",
    "\n",
    "    # Loop through potential relationships and add those present for the source dataset\n",
    "    additional_relationships = []\n",
    "    for rel_set, relationships in potential_relationships.items():\n",
    "        if rel_set == \"ALL_DATASETS\" or rel_set in mapping_target_spec:\n",
    "            for rel in relationships:\n",
    "                from_table = rel[0].split(\".\")[0] \n",
    "                from_column = rel[0].split(\".\")[1]\n",
    "                to_table = rel[1].split(\".\")[0]\n",
    "                to_column = rel[1].split(\".\")[1]\n",
    "                if bmq.confirm_column_exists(src_schema_dict, from_table, from_column) and bmq.confirm_column_exists(src_schema_dict, to_table, to_column):\n",
    "                    relationship_found = False\n",
    "                    for rel_entry in src_schema_dict[\"relationships\"]:\n",
    "                        if rel_entry[\"_from\"][\"table\"] == from_table and rel_entry[\"_from\"][\"column\"] == from_column and rel_entry[\"to\"][\"table\"] == to_table and rel_entry[\"to\"][\"column\"] == to_column:\n",
    "                            relationship_found = True\n",
    "                        elif rel_entry[\"_from\"][\"table\"] == to_table and rel_entry[\"_from\"][\"column\"] == to_column and rel_entry[\"to\"][\"table\"] == from_table and rel_entry[\"to\"][\"column\"] == from_column:\n",
    "                            relationship_found = True\n",
    "                    if not relationship_found:\n",
    "                        rel_dict = {\n",
    "                            \"name\": from_table + \"_\" + from_column + \"__to__\" + to_table + \"_\" + to_column,\n",
    "                            \"from\": {\"table\": from_table, \"column\": from_column},\n",
    "                            \"to\": {\"table\": to_table, \"column\": to_column}\n",
    "                        }\n",
    "                        additional_relationships.append(rel_dict)\n",
    "\n",
    "    # Submit the schema update request for the TDR dataset\n",
    "    if additional_relationships:\n",
    "        schema_update_request = {\n",
    "            \"description\": \"Adding relationships to support query construction.\",\n",
    "            \"changes\": {\n",
    "                \"addRelationships\": additional_relationships\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            resp = utils.wait_for_tdr_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request), params[\"run_env\"])\n",
    "            print(\"Schema update successful: \" + str(resp)[0:1000])\n",
    "            results.append([dataset_id, \"Success\"])\n",
    "        except Exception as e:\n",
    "            print(\"Error running schema update: \" + str(e))\n",
    "            results.append([dataset_id, \"Error\"])\n",
    "    else:\n",
    "        print(\"No additional relationships to add to schema.\")\n",
    "        results.append([dataset_id, \"Success\"])\n",
    "\n",
    "print(\"Processing of potential relationships for specified datasets complete.\")\n",
    "print(\"\\nResults:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"dataset\", \"status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Retrieve Mapping Artifacts and Run Query Construction\n",
    "Retrieve the artifacts you would like to use to construct transformation queries for your datasets, based on the previously specified target schema and mapping specification. These transformation queries will then be dynamically constructed based on the appropriate target schema, mapping specification, and source schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Loop through datasets and process transformation query construction\n",
    "api_client = utils.refresh_tdr_api_client(params[\"tdr_url\"])\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "results = []\n",
    "for dataset_id in final_datasets_to_map_dict:\n",
    "    print(\"Building transformation queries for dataset_id = {}\".format(dataset_id))\n",
    "\n",
    "    # Collect mapping variables\n",
    "    mapping_target = final_datasets_to_map_dict[dataset_id][\"mapping_target\"]\n",
    "    mapping_target_spec = final_datasets_to_map_dict[dataset_id][\"mapping_target_spec\"]\n",
    "    mapping_phs = final_datasets_to_map_dict[dataset_id][\"mapping_phs\"]\n",
    "    mapping_consent = final_datasets_to_map_dict[dataset_id][\"mapping_consent\"]\n",
    "    \n",
    "    # Retrieve source schema\n",
    "    src_schema_dict = {}\n",
    "    try:\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"name\"] = response[\"name\"]\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        phs_id = response[\"phs_id\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "    # Set dataset name and project name parameters to substitute into transform queries\n",
    "    dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "    project_name_value = re.sub(\"'\", \"\", utils.derive_project_name(dataset_id, mapping_phs, dataset_name_value, params))\n",
    "    phs_id_value = mapping_phs\n",
    "    consent_group_value = mapping_consent\n",
    "\n",
    "    # Retrieve target schema and mapping specification\n",
    "    target_schema_dict = {}\n",
    "    mapping_spec = {}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "        target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "        blob_string = blob.download_as_text(client=None)\n",
    "        blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "        blob_string = blob_string.replace(\"$PROJECT_NAME\", project_name_value)\n",
    "        blob_string = blob_string.replace(\"$PROJECT_PHS\", phs_id_value)\n",
    "        blob_string = blob_string.replace(\"$DATASET_CONSENT\", consent_group_value)\n",
    "        blob_string = blob_string.replace(\"$BQ_DATASET\", bq_project + \".\" + bq_schema)\n",
    "        mapping_spec = json.loads(blob_string)\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "\n",
    "    # Update aliases in mapping specification\n",
    "    mapping_spec = bmq.update_mapping_spec_aliases(mapping_spec, src_schema_dict)\n",
    "    \n",
    "    # Build queries from mapping specification\n",
    "    query_dict = {}\n",
    "    if target_schema_dict:\n",
    "        for target_table in target_schema_dict[\"tables\"]:\n",
    "            table_name = target_table[\"name\"]\n",
    "            missing_artifacts = False\n",
    "            if src_schema_dict and mapping_spec:\n",
    "                query_dict[table_name] = bmq.build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema)\n",
    "            else:\n",
    "                missing_artifacts = True\n",
    "                query_dict[table_name] = {\"query\": \"\", \"syntax_check\": \"\"} \n",
    "        if missing_artifacts == True:\n",
    "            print(\"Source schema dictionary and/or mapping specification missing. Unable to generate queries.\")\n",
    "            results.append([dataset_id, \"Error\"])\n",
    "    else:\n",
    "        print(\"Target schema dictionary missing. Unable to generate queries.\")\n",
    "        results.append([dataset_id, \"Error\"])\n",
    "    \n",
    "    # Evaluate queries -- Publish if no issues found, otherwise convert to dataframe and display\n",
    "    failure_count = 0\n",
    "    for key, val in query_dict.items():\n",
    "        if val[\"syntax_check\"] != \"Passed\" and val[\"syntax_check\"] != None:\n",
    "            failure_count += 1\n",
    "    if failure_count == 0:\n",
    "        print(\"No failures found in query construction, publishing to the cloud.\")\n",
    "        results.append([dataset_id, \"Success\"])\n",
    "        # Copy target schema file to output folder for mapping target\n",
    "        source_path = \"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target)\n",
    "        destination_path = \"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(mapping_target, dataset_id)\n",
    "        !gsutil cp $ws_bucket/$source_path $ws_bucket/$destination_path 2> stdout\n",
    "\n",
    "        # Limit query dict to valid queries, write out, and copy to output folder for mapping target\n",
    "        valid_query_dict = {}\n",
    "        for target, val in query_dict.items():\n",
    "            if val[\"syntax_check\"] == \"Passed\":\n",
    "                valid_query_dict[target] = val\n",
    "        final_query_dict = {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"transforms\": valid_query_dict\n",
    "        }\n",
    "        query_dict_json = json.dumps(final_query_dict)\n",
    "        query_output_file = \"transform_query_set.json\"\n",
    "        with open(query_output_file, 'w') as outfile:\n",
    "            outfile.write(query_dict_json)\n",
    "        destination_path = \"ingest_pipeline/output/transformed/{}/{}/queries\".format(mapping_target, dataset_id)\n",
    "        !gsutil cp $query_output_file $ws_bucket/$destination_path/ 2> stdout\n",
    "    else:\n",
    "        print(\"Failures found in query construction, must be resolved before publishing.\")\n",
    "        print(\"Query building results:\")\n",
    "        results.append([dataset_id, \"Error\"])\n",
    "        query_df = pd.DataFrame.from_dict(query_dict, orient=\"index\")\n",
    "        query_df.index.name = \"target_table\"\n",
    "        query_df.reset_index(inplace=True)\n",
    "        display(query_df)\n",
    "\n",
    "print(\"Transformation query construction and processing complete.\")\n",
    "print(\"\\nResults:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"dataset\", \"status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Evaluate Vocabulary Mapping\n",
    "For target attributes leveraging the \"VOCAB_MAP\" transformation, evaluate whether the source values have a record in the dsp-data-ingest.transform_resources.vocab_map table. If additional mappings are needed, these should be put into place before the transformation queries are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set display parameter\n",
    "show_only_missing_maps = True\n",
    "\n",
    "# Loop through datasets and process vocabulary mapping evaluation\n",
    "api_client = utils.refresh_tdr_api_client(params[\"tdr_url\"])\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "for dataset_id in final_datasets_to_map_dict:\n",
    "    print(\"Evaluating vocabulary mapping for dataset_id = {}\".format(dataset_id))\n",
    "\n",
    "    # Collect mapping variables\n",
    "    mapping_target = final_datasets_to_map_dict[dataset_id][\"mapping_target\"]\n",
    "    mapping_target_spec = final_datasets_to_map_dict[dataset_id][\"mapping_target_spec\"]\n",
    "    mapping_phs = final_datasets_to_map_dict[dataset_id][\"mapping_phs\"]\n",
    "    mapping_consent = final_datasets_to_map_dict[dataset_id][\"mapping_consent\"]\n",
    "    \n",
    "    # Retrieve source schema\n",
    "    src_schema_dict = {}\n",
    "    try:\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"name\"] = response[\"name\"]\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        phs_id = response[\"phs_id\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "    # Set dataset name and project name parameters to substitute into transform queries\n",
    "    dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "    project_name_value = utils.derive_project_name(dataset_id, phs_id, dataset_name_value, params)\n",
    "    phs_id_value = mapping_phs\n",
    "    consent_group_value = mapping_consent\n",
    "\n",
    "    # Retrieve target schema and mapping specification\n",
    "    target_schema_dict = {}\n",
    "    mapping_spec = {}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "        target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "        blob_string = blob.download_as_text(client=None)\n",
    "        blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "        blob_string = blob_string.replace(\"$PROJECT_NAME\", project_name_value)\n",
    "        blob_string = blob_string.replace(\"$PROJECT_PHS\", phs_id_value)\n",
    "        blob_string = blob_string.replace(\"$DATASET_CONSENT\", consent_group_value)\n",
    "        blob_string = blob_string.replace(\"$BQ_DATASET\", bq_project + \".\" + bq_schema)\n",
    "        mapping_spec = json.loads(blob_string)\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "\n",
    "    # Evaluate vocab mapping and display results\n",
    "    df = bmq.evaluate_vocab_mapping(mapping_spec, src_schema_dict, target_schema_dict, bq_project, bq_schema)\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Missing mapped_value view:\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    display(df[df[\"mapped_value\"].isnull() & df[\"source_value\"].notnull()])\n",
    "    if not show_only_missing_maps:\n",
    "        print(\"\\n-------------------------------------------\")\n",
    "        print(\"Full view:\")\n",
    "        print(\"-------------------------------------------\")\n",
    "        display(df)\n",
    "    \n",
    "print(\"Vocabulary mapping evaluation and processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## [Optional] Update/Override Generated Queries as Necessary\n",
    "Review any queries that have not passed the syntax check, as these need to be remedied before they can be published and executed. Any other queries that do not align with expectations can be overridden by either A) Updating the mapping target specification and re-running the previous step, or B) Manually overriding the query below. Option B should only be used in one-off cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Build Base Query Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Input the appropriate dataset and mapping target specification\n",
    "dataset_id = \"f1e1ef01-d52d-423e-a65b-3a1d26c7ee9d\"\n",
    "mapping_target = \"anvil\"\n",
    "mapping_target_spec = \"cmg_ext_2\"\n",
    "\n",
    "# Retrieve source schema\n",
    "src_schema_dict = {}\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "try:\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "    src_schema_dict[\"name\"] = response[\"name\"]\n",
    "    src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "    src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "    bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "    bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    phs_id = response[\"phs_id\"]\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "# Set dataset name and project name parameters to substitute into transform queries\n",
    "dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "project_name_value = utils.derive_project_name(dataset_id, phs_id, dataset_name_value)\n",
    "\n",
    "# Retrieve target schema and mapping specification\n",
    "target_schema_dict = {}\n",
    "mapping_spec = {}\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "    target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "    blob_string = blob.download_as_text(client=None)\n",
    "    blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "    blob_string = blob_string.replace(\"$PROJECT_NAME\", project_name_value)\n",
    "    mapping_spec = json.loads(blob_string)\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "\n",
    "# Build queries from mapping specification\n",
    "query_dict = {}\n",
    "if target_schema_dict:\n",
    "    for target_table in target_schema_dict[\"tables\"]:\n",
    "        table_name = target_table[\"name\"]\n",
    "        missing_artifacts = False\n",
    "        if src_schema_dict and mapping_spec:\n",
    "            query_dict[table_name] = bmq.build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema)\n",
    "        else:\n",
    "            missing_artifacts = True\n",
    "            query_dict[table_name] = {\"query\": \"\", \"syntax_check\": \"\"} \n",
    "    if missing_artifacts == True:\n",
    "        print(\"Source schema dictionary and/or mapping specification missing. Unable to generate queries.\")\n",
    "else:\n",
    "    print(\"Target schema dictionary missing. Unable to generate queries.\")\n",
    "    \n",
    "# Display query dictionary\n",
    "query_df = pd.DataFrame.from_dict(query_dict, orient=\"index\")\n",
    "query_df.index.name = \"target_table\"\n",
    "query_df.reset_index(inplace=True)\n",
    "display(query_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Update Query Dict as Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To update the query definition for particular target table, input the target table and query below\n",
    "target_table = \"anvil_donor\"\n",
    "query = \"SELECT 1\"\n",
    "\n",
    "# Run syntax check\n",
    "query_dict[target_table][\"query\"] = query\n",
    "query_dict[target_table][\"syntax_check\"] = bmq.run_syntax_check(query)\n",
    "print(query_dict[target_table])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Publish Updated Query Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Copy target schema file to output folder for mapping target\n",
    "source_path = \"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target)\n",
    "destination_path = \"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(mapping_target, dataset_id)\n",
    "!gsutil cp $ws_bucket/$source_path $ws_bucket/$destination_path 2> stdout\n",
    "\n",
    "# Limit query dict to valid queries, write out, and copy to output folder for mapping target\n",
    "valid_query_dict = {}\n",
    "for target, val in query_dict.items():\n",
    "    if val[\"syntax_check\"] == \"Passed\":\n",
    "        valid_query_dict[target] = val\n",
    "final_query_dict = {\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"transforms\": valid_query_dict\n",
    "}\n",
    "query_dict_json = json.dumps(final_query_dict)\n",
    "query_output_file = \"transform_query_set.json\"\n",
    "with open(query_output_file, 'w') as outfile:\n",
    "    outfile.write(query_dict_json)\n",
    "destination_path = \"ingest_pipeline/output/transformed/{}/{}/queries\".format(mapping_target, dataset_id)\n",
    "!gsutil cp $query_output_file $ws_bucket/$destination_path/ 2> stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# \"T\" Pipeline: Load Additional Transformed Tables to TDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run Variables\n",
    "dataset_id_run_list = [\n",
    "    #[\"dataset_id\", \"auth_domain\", \"phs\", \"consent_code\", \"consent\", \"dataset_ticket\", Run (True/False)],   \n",
    "    ['b8da0ffa-8ff8-439a-be42-80353a43c5c5', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_AUSALF_HMB_IRB_WES', 'phs001489', 'c27', 'HMB-IRB-MDS', 'ANVIL-218', True],\n",
    "    ['13f2c58b-70c2-4dfe-9a24-b3d05572ad8f', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_AUSAUS_EP_BA_CN_ID_MDS_WES', 'phs001489', 'c3', 'DS-EPSBACID-MDS-RD', 'ANVIL-220', True],\n",
    "    ['ad3758aa-1acb-42a8-b956-9c6355ddb1ac', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_AUSAUS_EPI_BA_ID_MDS_WES', 'phs001489', 'c1', 'DS-EPSBAID-MDS-RD', 'ANVIL-222', True],\n",
    "    ['a1782b2d-57f6-4e61-839d-2a659e576343', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_AUSAUS_EPIL_BA_MDS_WES', 'phs001489', 'c2', 'DS-EPSBA-MDS-RD', 'ANVIL-224', True],\n",
    "    ['6f543971-3165-4ab4-944b-412cdb2dcb95', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_AUSRMB_DS_EAED_IRB_WES', 'phs001489', 'c33', 'DS-EAED-IRB-NPU-MDS', 'ANVIL-225', True],\n",
    "    ['68c04e98-2db0-4a1d-a4c8-fd087d1f8836', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_AUSRMB_DS_EAED_MDS_WES', 'phs001489', 'c22', 'DS-EAED-MDS', 'ANVIL-228', True],\n",
    "    ['65d29659-803e-44e8-9392-b89d5d7b13f6', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_AUTMUV_DS_NS_ADLT_WES', 'phs001489', 'c30', 'DS-NSD-ADULTS-NPU-MDS', 'ANVIL-229', True],\n",
    "    ['bb403e99-d70b-4296-83dd-f63148a3a3f5', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_AUTMUV_DS_NS_WES', 'phs001489', 'c29', 'DS-NSD-NPU-MDS', 'ANVIL-232', True],\n",
    "    ['0a90128d-a601-4adb-b7dd-e28e6c2e3983', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_BELATW_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-234', True],\n",
    "    ['3125a5df-f5d6-4355-8a64-ef67d9ef6768', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_BELULB_DS_EP_NPU_WES', 'phs001489', 'c17', 'DS-EP-NPU', 'ANVIL-236', True],\n",
    "    ['74758e76-8bfb-4775-8b34-5362df49b55b', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_BRAUSP_DS_WES', 'phs001489', 'c32', 'DS-MBND-NPU-MDS', 'ANVIL-238', True],\n",
    "    ['04de9328-da6a-42fd-b4a1-1c9dbc99d256', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_CANCAL_GRU_v2_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-239', True],\n",
    "    ['cc39ad3d-9884-4ecb-8633-9444f613741c', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_CANUTN_DS_EP_WES', 'phs001489', 'c10', 'DS-EP', 'ANVIL-242', True],\n",
    "    ['8b27e095-d690-4c40-b1de-5197ba15234c', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_CHEUBB_HMB_IRB_MDS_WES', 'phs001489', 'c27', 'HMB-IRB-MDS', 'ANVIL-244', True],\n",
    "    ['92942434-e960-4b2e-95f8-b04b816b44d4', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_CYPCYP_HMB_NPU_MDS_WES', 'phs001489', 'c8', 'HMB-NPU-MDS', 'ANVIL-246', True],\n",
    "    ['de213a72-4128-46ef-a321-55f85bc6c7ed', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_CZEMTH_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-248', True],\n",
    "    ['58e05ce6-4e1d-4c5f-a6a5-cca94d457739', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_DEUPUM_HMB_MDS_WES_Year3', 'phs001489', 'c11', 'HMB-MDS', 'ANVIL-250', True],\n",
    "    ['f63fdec8-154e-43ee-965c-4ccea37ef852', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_DEUUGS_DS_EP_MDS_WES', 'phs001489', 'c16', 'DS-EP-MDS', 'ANVIL-252', True],\n",
    "    ['11fb953c-1580-4f96-ab48-249e6e96f9a7', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_DEUUKB_HMB_NPU_MDS_WES', 'phs001489', 'c8', 'HMB-NPU-MDS', 'ANVIL-254', True],\n",
    "    ['3db6d9cb-923f-44fe-a8d3-1c861541b39d', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_DEUUKL_HMB_WES', 'phs001489', 'c19', 'HMB', 'ANVIL-256', True],\n",
    "    ['8b371868-d4ed-4fed-ad10-bde377107592', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_DEUULG_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-258', True],\n",
    "    ['4f6027b3-9f3a-4a17-ba2d-58204cc6ea3f', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_DEUUTB_HMB_NPU_MDS_WES', 'phs001489', 'c8', 'HMB-NPU-MDS', 'ANVIL-261', True],\n",
    "    ['c6217265-252c-46a6-9b92-286442ddc83c', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_FINKPH_EPIL_MDS_WES', 'phs001489', 'c4', 'DS-EPCOM-MDS-RD', 'ANVIL-262', True],\n",
    "    ['92ed1a10-4b14-46c3-b3f2-8f3f0d44f62f', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_FINUVH_HMB_NPU_MDS_WES', 'phs001489', 'c8', 'HMB-NPU-MDS', 'ANVIL-265', True],\n",
    "    ['e05d8c1e-10d6-4ab4-8033-f25471cee13e', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_FRALYU_HMB_WES', 'phs001489', 'c11', 'HMB-MDS', 'ANVIL-267', True],\n",
    "    ['0a41abbf-7b9b-4671-b2fd-a61392a179ec', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_GBRSWU_CARDI_NEURO_WES', 'phs001489', 'c14', 'DS-CARNEU-MDS', 'ANVIL-269', True],\n",
    "    ['8e966c04-a747-47a1-9160-7da1b07031d9', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_GBRUCL_DS_EARET_MDS_WES', 'phs001489', 'c24', 'DS-EARET-MDS', 'ANVIL-271', True],\n",
    "    ['5fd356c2-cbae-453f-a585-82e802417041', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_GBRUNL_EP_ETIOLOGY_MDS_WES', 'phs001489', 'c9', 'DS-EPASM-MDS-RD', 'ANVIL-273', True],\n",
    "    ['7df4b9aa-c2d5-4df7-8381-126328e159df', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_GBRUNL_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-275', True],\n",
    "    ['44deae8b-ce4b-4246-b3e6-69a70c867cc7', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_GHAKNT_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-277', True],\n",
    "    ['4ff73d9e-8286-4fe0-b813-7f2821a49884', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_HKGHKK_HMB_MDS_WES', 'phs001489', 'c11', 'HMB-MDS', 'ANVIL-279', True],\n",
    "    ['3e25160d-37e2-4ede-b405-5c30a0c2567a', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_HKOSB_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-281', True],\n",
    "    ['4200aab1-089c-4d1a-a010-cef4354a1d2f', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_HRVUZG_HMB_MDS_WES', 'phs001489', 'c11', 'HMB-MDS', 'ANVIL-283', True],\n",
    "    ['c7989cca-b3c9-4361-be78-f71162dfee7e', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_IRLRCI_GRU_IRB_WES', 'phs001489', 'c12', 'GRU-IRB', 'ANVIL-285', True],\n",
    "    ['d3d5c207-5ec4-40de-8a26-416d5faaba5d', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_ITAICB_HMB_NPU_MDS_WES', 'phs001489', 'c8', 'HMB-NPU-MDS', 'ANVIL-287', True],\n",
    "    ['186c2956-36d8-4318-9b7a-f0303ad97694', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_ITAIGI_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-289', True],\n",
    "    ['e0f56709-eb6c-4e7f-bbff-5fe18fb89f61', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_ITAUBG_DS_EPI_NPU_MDS_WES', 'phs001489', 'c20', 'DS-EPI-ADULT-NPU-MDS', 'ANVIL-291', True],\n",
    "    ['f2206d9b-2582-4204-b81a-bb833f9d8e2f', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_ITAUMC_DS_NEURO_MDS_WES', 'phs001489', 'c23', 'DS-NEUROLOGY-MDS', 'ANVIL-293', True],\n",
    "    ['44d0ff4d-f5ad-491b-a617-f8872e3a1431', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_ITAUMR_GRU_NPU_WES', 'phs001489', 'c21', 'GRU-NPU', 'ANVIL-295', True],\n",
    "    ['4846c93f-46a0-4bf2-a34d-ba2b55513036', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_JPNFKA_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-297', True],\n",
    "    ['44203821-dbad-41f2-bac1-983da35842a4', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_JPNRKI_DS_NPD_IRB_NPU_WES', 'phs001489', 'c25', 'DS-NPD-IRB-NPU', 'ANVIL-299', True],\n",
    "    ['9fad8665-e36b-4efe-a8da-19d897f89b1c', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_KENKIL_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-301', True],\n",
    "    ['cbba277b-d6d5-4ab6-89e4-ffc58f6656e6', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_LEBABM_DS_Epilepsy_WES', 'phs001489', 'c16', 'DS-EP-MDS', 'ANVIL-303', True],\n",
    "    ['2d344396-765b-4864-9def-52e1632288e9', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_LEBABM_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-305', True],\n",
    "    ['0e9c0430-9d99-4e91-84ef-a9b34e068de9', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_LTUUHK_HMB_NPU_MDS_WES', 'phs001489', 'c8', 'HMB-NPU-MDS', 'ANVIL-307', True],\n",
    "    ['a35d5089-fcc4-40b1-991e-30b633c4f7d4', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_NZLUTO_EPIL_BC_ID_MDS_WES', 'phs001489', 'c5', 'DS-EPSBACID-NPU-MDS-RD', 'ANVIL-309', True],\n",
    "    ['7f67466f-90f7-4dbb-8a2f-6f0201f05e7f', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_TURBZU_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-311', True],\n",
    "    ['f8f89ba2-da06-4b1c-a945-ea7c634966cf', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_TURIBU_DS_NEURO_AD_NPU_WES', 'phs001489', 'c26', 'DS-NEUROLOGY-ADULTS-NPU', 'ANVIL-313', True],\n",
    "    ['f59f4876-bb40-44f5-a24f-d06e7ce0c955', 'AnVIL_CCDG_Broad_NP_Epilepsy_TWNCGM_HMB_NPU_AdultsONLY_WES', 'phs001489', 'c8', 'HMB-NPU-MDS', 'ANVIL-318', True],\n",
    "    ['09c76765-5f11-49c2-b2ec-dbe8bb6e8a10', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USABCH_EPI_MUL_CON_MDS_WES', 'phs001489', 'c6', 'DS-EPI-MULTI-MDS', 'ANVIL-320', True],\n",
    "    ['899134c5-81c2-48e9-b411-c0b85770ab12', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USABLC_GRU_NPU_WES', 'phs001489', 'c21', 'GRU-NPU', 'ANVIL-322', True],\n",
    "    ['d6badb13-5719-42e0-b44a-42aeb518f124', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USACCF_HMB_MDS_WES', 'phs001489', 'c11', 'HMB-MDS', 'ANVIL-324', True],\n",
    "    ['e0e24a53-3b37-4665-9cba-cfa80e31a3c3', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USACCH_DS_NEURO_MDS_WES', 'phs001489', 'c23', 'DS-NEUROLOGY-MDS', 'ANVIL-326', True],\n",
    "    ['eee9db44-de4f-4c3d-803e-f2e751b24d23', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USACHP_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-328', True],\n",
    "    ['f2ac8c6f-3b5d-43a6-9843-176df23bce8c', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USACRW_DS_EP_MDS_WES', 'phs001489', 'c16', 'DS-EP-MDS', 'ANVIL-330', True],\n",
    "    ['a1468614-947d-4789-a516-8b64b036e02b', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USACRW_DS_SEIZD_WES', 'phs001489', 'c15', 'DS-SEIZD', 'ANVIL-332', True],\n",
    "    ['9c679f21-5457-4161-a6f9-f711e408276f', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USACRW_EPI_ASZ_MED_MDS_WES', 'phs001489', 'c7', 'DS-EPASM-MDS', 'ANVIL-334', True],\n",
    "    ['278d687b-6dd0-4e9e-ba1b-a1467f2b00df', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAEGP_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-336', True],\n",
    "    ['49ed4ad0-eeb9-4377-90c0-42f9235efc1c', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAFEB_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-338', True],\n",
    "    ['3663ca97-38ef-49ee-b559-f934cd4c4cb6', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAHEP_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-340', True],\n",
    "    ['e6820957-741d-4c9c-9010-b51a192b321d', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USALCH_HMB_WES', 'phs001489', 'c11', 'HMB-MDS', 'ANVIL-342', True],\n",
    "    ['b823a87b-cc65-4547-9a16-9f8c18932806', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAMGH_HMB_MDS_WES', 'phs001489', 'c11', 'HMB-MDS', 'ANVIL-344', True],\n",
    "    ['442e4540-312c-42e3-a8f0-985966340fe5', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAMGH_MGBB_HMB_MDS_WES', 'phs001489', 'c11', 'HMB-MDS', 'ANVIL-346', True],\n",
    "    ['9fa5196b-29ba-4e52-a49e-5b073b79cc72', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAMON_GRU_NPU_WES', 'phs001489', 'c21', 'GRU-NPU', 'ANVIL-351', True],\n",
    "    ['2deee87e-9aef-49dc-8d8f-42294717f2cc', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAMON_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-352', True],\n",
    "    ['f06949e0-e526-4b33-b00e-2691cc9e694a', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAMON_HMB_WES', 'phs001489', 'c8', 'HMB-NPU-MDS', 'ANVIL-354', True],\n",
    "    ['fcccfacc-05fb-4814-ba7a-bc6d2757e213', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAMSS_DS_EP_NEURO_MDS_WES', 'phs001489', 'c16', 'DS-EP-MDS', 'ANVIL-356', True],\n",
    "    ['b307a1c6-8e72-456b-8669-cabd4851cd72', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USANCH_DS_NEURO_MDS_WES', 'phs001489', 'c31', 'DS-NEURO-EP-MDS', 'ANVIL-358', True],\n",
    "    ['41579c21-3a39-429c-b91e-7c5d0eb08f2a', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAUPN_Marsh_GRU_NPU_WES', 'phs001489', 'c21', 'GRU-NPU', 'ANVIL-363', True],\n",
    "    ['b6dccf69-84ed-4b58-ac6e-d681f3163d92', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAUPN_Marsh_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-364', True],\n",
    "    ['348b5d5b-f819-459b-a419-a33312e0e908', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAUPN_Rader_GRU_WES', 'phs001489', 'c13', 'GRU', 'ANVIL-366', True],\n",
    "    ['cf8faf19-45c5-479d-93d8-1e13b954642a', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAVAN_HMB_GSO_WES', 'phs001489', 'c28', 'HMB-GSO', 'ANVIL-368', True],\n",
    "    ['e69f28e9-73bf-44b6-90eb-40ae566ccb3f', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_USAVANcontrols_HMB_GSO_WES', 'phs001489', 'c28', 'HMB-GSO', 'ANVIL-370', True],\n",
    "    ['b2f559a5-c853-4bfa-9546-cde5aa380c51', 'AUTH_AnVIL_CCDG_Broad_NP_Epilepsy_ZAFAGN_DS_EPI_COMO_MDS_WES', 'phs001489', 'c16', 'DS-EP-MDS', 'ANVIL-372', True],\n",
    "    ['52b75821-1d51-49d2-bff2-d79d45f3ed7d', 'AUTH_AnVIL_CCDG_Broad_Spalletta_HMB_NPU_MDS_WES', 'phs001489', 'c8', 'HMB-NPU-MDS', 'ANVIL-373', True],  \n",
    "]\n",
    "params[\"mapping_target\"] = \"anvil\"\n",
    "params[\"skip_transforms\"] = False\n",
    "params[\"transform_list_override\"] = [] # Leave empty to run transforms for all files, otherwise populate with target table names \n",
    "params[\"skip_dataset_updates\"] = False\n",
    "params[\"skip_ingests\"] = False\n",
    "params[\"trunc_before_ingest\"] = True\n",
    "params[\"ingest_list_override\"] = [] # Leave empty to run ingests for all files, otherwise populate with target table names\n",
    "params[\"skip_file_relation_inference\"] = False\n",
    "params[\"skip_dangling_fk_resolution\"] = False\n",
    "params[\"skip_supplementary_file_identification\"] = False\n",
    "params[\"skip_snapshot_creation\"] = False\n",
    "params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\", \"auth-domain\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "params[\"skip_data_validation\"] = True\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Print variables\n",
    "print(\"Pipeline run variables set:\")\n",
    "print(\"Profile ID: \" + params[\"profile_id\"])\n",
    "print(\"Mapping Target: \" + params[\"mapping_target\"])\n",
    "print(\"Datasets to run: \")\n",
    "api_client = utils.refresh_tdr_api_client(params[\"tdr_url\"])\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "dataset_skip_list = []\n",
    "for dataset in dataset_id_run_list:\n",
    "    if dataset[6]:\n",
    "        dataset_id = dataset[0]\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "            phs_id = dataset[2]\n",
    "            consent_code = dataset[3]\n",
    "            consent_name = dataset[4]\n",
    "            auth_domains = [dataset[1]] if dataset[1] else []\n",
    "            dataset_ticket = dataset[5]\n",
    "            if dataset_info[\"properties\"]:\n",
    "                src_workspaces = dataset_info[\"properties\"].get(\"source_workspaces\")\n",
    "            else:\n",
    "                src_workspaces = []\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "            dataset_skip_list.append(dataset_id)\n",
    "        if dataset_name:\n",
    "            dataset_id = dataset[0]\n",
    "            print(\"- \" + dataset_name + \" ({})\".format(dataset_id))\n",
    "            print(\"\\t- PHS ID = \" + str(phs_id))\n",
    "            print(\"\\t- Consent Short Name = \" + str(consent_name))\n",
    "            print(\"\\t- Consent Code = \" + str(consent_code))\n",
    "            print(\"\\t- Auth Domains = \" + str(auth_domains))\n",
    "            print(\"\\t- Source Workspaces = \" + str(src_workspaces))\n",
    "            print(\"\\t- Dataset Ticket = \" + str(dataset_ticket))\n",
    "if dataset_skip_list:\n",
    "    print(\"Datasets to skip (they either don't exist or aren't accessible to the current user): \")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(dataset_skip_list)) \n",
    "print(\"Skip transforms? \" + str(params[\"skip_transforms\"]))\n",
    "print(\"Transforms override list: \" + str(params[\"transform_list_override\"]))\n",
    "print(\"Skip dataset schema and property updates? \" + str(params[\"skip_dataset_updates\"]))\n",
    "print(\"Skip ingests? \" + str(params[\"skip_ingests\"]))\n",
    "print(\"Truncate tables before ingest? \" + str(params[\"trunc_before_ingest\"]))\n",
    "print(\"Ingest override list: \" + str(params[\"ingest_list_override\"]))\n",
    "print(\"Skip file relationship inference? \" + str(params[\"skip_file_relation_inference\"]))\n",
    "print(\"Skip dangling foreign key resolution? \" + str(params[\"skip_dangling_fk_resolution\"]))\n",
    "print(\"Skip supplementary file identification? \" + str(params[\"skip_supplementary_file_identification\"]))\n",
    "print(\"Skip snapshot creation? \" + str(params[\"skip_snapshot_creation\"]))\n",
    "print(\"Skip data validation? \" + str(params[\"skip_data_validation\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Loop through and execute pipeline for listed workspaces\n",
    "!mkdir -p pipeline_results\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "t_output_file = \"pipeline_results/t_pipeline_results_\" + current_datetime_string + \".json\"\n",
    "logging.info(f\"T Pipeline Results File: {t_output_file}\")\n",
    "params[\"t_pipeline_results\"] = []\n",
    "for dataset in dataset_id_run_list:\n",
    "    if dataset[6]:\n",
    "        dataset_id = dataset[0]\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client(params[\"tdr_url\"])\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "            phs_id = dataset[2]\n",
    "            if dataset_info[\"properties\"]:\n",
    "                consent_code = dataset[3]\n",
    "                consent_name = dataset[4]\n",
    "                auth_domains = [dataset[1]] if dataset[1] else []\n",
    "                src_workspaces = dataset_info[\"properties\"].get(\"source_workspaces\")\n",
    "                dataset_ticket = dataset[5]\n",
    "            else:\n",
    "                consent_code = dataset[3]\n",
    "                consent_name = dataset[4] \n",
    "                auth_domains = [dataset[1]] if dataset[1] else []\n",
    "                src_workspaces = []\n",
    "                dataset_ticket = dataset[5]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "        if dataset_name:\n",
    "            params[\"dataset_id\"] = dataset_id\n",
    "            params[\"dataset_name\"] = dataset_name\n",
    "            params[\"phs_id\"] = phs_id\n",
    "            params[\"consent_name\"] = consent_name\n",
    "            params[\"consent_code\"] = consent_code\n",
    "            params[\"auth_domains\"] = auth_domains\n",
    "            params[\"dataset_ticket\"] = dataset_ticket\n",
    "            utils.run_t_pipeline(params)\n",
    "            params[\"t_pipeline_results\"].extend(params[\"pipeline_results\"])\n",
    "            if params[\"t_pipeline_results\"]:\n",
    "                with open(t_output_file, \"w\") as t_out:\n",
    "                    json.dump(params[\"t_pipeline_results\"], t_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Scripts\n",
    "Uncomment sections as necessary to accomplish various miscellaneous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Monitor or Review Pipeline Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!ls pipeline_results\n",
    "# !rm pipeline_results/*_pipeline_results_* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Results file to load\n",
    "results_file = \"pipeline_results/t_pipeline_results_202507300051.json\"\n",
    "\n",
    "# Load results\n",
    "with open(results_file, \"r\") as f:\n",
    "    loaded_results = json.load(f)\n",
    "if \"e_pipeline\" in results_file:\n",
    "    df_results = pd.DataFrame(loaded_results, columns = [\"Workspace\", \"Staging Area\", \"Time\", \"Step\", \"Status\", \"Message\"])\n",
    "elif \"l_pipeline\" in results_file:\n",
    "    df_results = pd.DataFrame(loaded_results, columns = [\"Staging Area\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "else:\n",
    "    df_results = pd.DataFrame(loaded_results, columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Display all results\n",
    "#display(df_results)\n",
    "\n",
    "## Display all failures\n",
    "#display(df_results[df_results[\"Status\"] != \"Success\"])\n",
    "\n",
    "## Display specific L pipeline results\n",
    "#display(df_results[(df_results[\"Staging Area\"] == \"AnVIL_CCDG_Broad_Spalletta_HMB_NPU_MDS_WES\") & ((df_results[\"Task\"] == \"Create New Dataset\") | (df_results[\"Status\"] == \"Error\"))])\n",
    "\n",
    "## Display specific T pipeline results\n",
    "#display(df_results[(df_results[\"Dataset\"].str.contains(\"b8da0ffa-8ff8-439a-be42-80353a43c5c5\", case=False, na=False))])\n",
    "#display(df_results[(df_results[\"Dataset\"].str.contains(\"13f2c58b-70c2-4dfe-9a24-b3d05572ad8f\", case=False, na=False)) & ((df_results[\"Task\"] == \"Create and Share Snapshot\") | (df_results[\"Status\"] == \"Error\"))])\n",
    "display(df_results[((df_results[\"Task\"] == \"Create and Share Snapshot\") | (df_results[\"Status\"] == \"Error\"))])\n",
    "\n",
    "# Excel functions\n",
    "# =REGEXEXTRACT(G2, \"'id': '(.*)', 'name'\",2,0)\n",
    "# =REGEXEXTRACT(G2, \"'name': '(.*)', 'description'\",2,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Collect AnVIL Snapshots and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "tdr_url = \"https://data.terra.bio\"\n",
    "sam_url = \"https://sam.dsde-prod.broadinstitute.org\"\n",
    "#billing_profile_list = [\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", \"9ee23bed-b46c-4561-9103-d2a723113f7f\"]\n",
    "billing_profile_list = [\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"]\n",
    "dataset_id_list = [\n",
    "]\n",
    "\n",
    "# Collect Anvil datasets and snapshots\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d %H:%M:%S\")\n",
    "logging.info(f\"Start time: {current_datetime_string}\")\n",
    "api_client = utils.refresh_tdr_api_client(tdr_url)\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "datasets_list = datasets_api.enumerate_datasets(limit=5000)\n",
    "if dataset_id_list:\n",
    "    dataset_list_len = min(len(datasets_list.items), len(dataset_id_list))\n",
    "else:\n",
    "    dataset_list_len = len(datasets_list.items)\n",
    "records_list = []\n",
    "dataset_count = 0\n",
    "for dataset_entry in datasets_list.items:\n",
    "    if len(dataset_id_list) == 0 or dataset_entry.id in dataset_id_list:\n",
    "        dataset_count += 1\n",
    "        logging.info(f\"Processing dataset {dataset_count} of {dataset_list_len}\")\n",
    "        api_client = utils.refresh_tdr_api_client(tdr_url)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "        if dataset_entry.default_profile_id in billing_profile_list:\n",
    "            dataset_detail = datasets_api.retrieve_dataset(id=dataset_entry.id, include=[\"PROPERTIES\", \"DATA_PROJECT\"])\n",
    "            snapshots_list = snapshots_api.enumerate_snapshots(dataset_ids=[dataset_entry.id], limit=1000)\n",
    "            ds_props = dataset_detail.properties\n",
    "            if ds_props:\n",
    "                props_src_ws = dataset_detail.properties.get(\"source_workspaces\")\n",
    "                props_ds_ticket = dataset_detail.properties.get(\"dataset_ticket\")\n",
    "            else: \n",
    "                props_src_ws = \"\"\n",
    "                props_ds_ticket = \"\"\n",
    "            dataset_identifier = \"\"\n",
    "            if props_ds_ticket:\n",
    "                dataset_identifier = props_ds_ticket\n",
    "            elif props_src_ws:\n",
    "                dataset_identifier = \", \".join(props_src_ws)\n",
    "            if len(snapshots_list.items) == 0:\n",
    "                record = [None, None, None, None, None, None, None, None, None, dataset_entry.id, dataset_entry.name, dataset_detail.ingest_service_account, dataset_entry.created_date[0:10], dataset_entry.cloud_platform, dataset_entry.secure_monitoring_enabled, dataset_identifier]\n",
    "                records_list.append(record)\n",
    "            else:\n",
    "                snapshot_list_len = len(snapshots_list.items)\n",
    "                snapshot_count = 0\n",
    "                for snapshot_entry in snapshots_list.items:\n",
    "                    snapshot_count += 1\n",
    "                    logging.info(f\"Processing snapshot {snapshot_count} of {snapshot_list_len} for dataset {dataset_count}\")\n",
    "                    # Get public policy information\n",
    "                    creds, project = google.auth.default()\n",
    "                    auth_req = google.auth.transport.requests.Request()\n",
    "                    creds.refresh(auth_req)\n",
    "                    public_flag = \"N\"\n",
    "                    public_response = requests.get(\n",
    "                        url=f\"{sam_url}/api/resources/v2/datasnapshot/{snapshot_entry.id}/policies/reader/public\",\n",
    "                        headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "                    )\n",
    "                    if public_response.text == \"true\":\n",
    "                        public_flag = \"Y\"\n",
    "                    # Get snapshot DUOS ID and Lock status\n",
    "                    api_client = utils.refresh_tdr_api_client(tdr_url)\n",
    "                    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "                    duos_id = \"\"\n",
    "                    lock_status = False\n",
    "                    try:\n",
    "                        snapshot_detail = snapshots_api.retrieve_snapshot(id=snapshot_entry.id, include=[\"DUOS\"])\n",
    "                        if snapshot_detail.duos_firecloud_group:\n",
    "                            duos_id = snapshot_detail.duos_firecloud_group.duos_id\n",
    "                        lock_name = snapshot_detail.resource_locks.exclusive\n",
    "                        if lock_name:\n",
    "                            lock_status = True\n",
    "                    except:\n",
    "                        pass\n",
    "                    # Get snapshot readers and auth domain\n",
    "                    try:\n",
    "                        snapshot_policy_response = snapshots_api.retrieve_snapshot_policies(id=snapshot_entry.id)\n",
    "                        for role in snapshot_policy_response.policies:\n",
    "                            if role.name == \"reader\":\n",
    "                                readers = \", \".join(role.members)\n",
    "                        ad_groups = \"\"\n",
    "                        if snapshot_policy_response.auth_domain:\n",
    "                            ad_groups = \", \".join(snapshot_policy_response.auth_domain)\n",
    "                    except:\n",
    "                        ad_groups = \"ACCESS_MISSING\"\n",
    "                    record = [snapshot_entry.id, snapshot_entry.name, snapshot_entry.created_date[0:10], public_flag, readers, ad_groups, duos_id, snapshot_entry.data_project, lock_status, dataset_entry.id, dataset_entry.name, dataset_detail.ingest_service_account, dataset_entry.created_date[0:10], dataset_entry.cloud_platform, dataset_entry.secure_monitoring_enabled, dataset_identifier]\n",
    "                    records_list.append(record)    \n",
    "df = pd.DataFrame(records_list, columns =[\"Snapshot ID\", \"Snapshot Name\", \"Snapshot Created Date\", \"Snapshot Public\", \"Snapshot Readers\", \"Snapshot Auth Domain\", \"Snapshot DUOS ID\", \"Snapshot Data Project\", \"Snapshot Locked\", \"Source Dataset ID\", \"Source Dataset Name\", \"Source Dataset SA\", \"Source Dataset Created Date\", \"Cloud Platform\", \"Secure Monitoring\", \"Dataset Identifier\"])\n",
    "df_sorted = df.sort_values([\"Dataset Identifier\", \"Source Dataset Name\", \"Snapshot Name\"], ascending=[True, True, True], ignore_index=True)\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d %H:%M:%S\")\n",
    "logging.info(f\"End time: {current_datetime_string}\")\n",
    "display(df_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Soft Deletion of TDR Dataset Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     17,
     41,
     108
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Input parameters\n",
    "run_env = \"prod\"\n",
    "tdr_url = \"https://data.terra.bio\"\n",
    "dataset_id_list = [\n",
    "    '1e84fc06-90e2-4b76-bae1-81b92822e761'\n",
    "]\n",
    "table_list = [\"file\"]\n",
    "#table_list = [\"anvil_activity\", \"anvil_alignmentactivity\", \"anvil_antibody\", \"anvil_assayactivity\", \"anvil_biosample\", \"anvil_dataset\", \"anvil_diagnosis\", \"anvil_donor\", \"anvil_file\", \"anvil_project\", \"anvil_sequencingactivity\", \"anvil_variantcallingactivity\"]\n",
    "#table_list = [\"file_inventory\", \"sample\", \"subject\", \"workspace_attributes\", \"sequencing\", \"qc_result_sample\", \"family\", \"chromosome\", \"interval\", \"participant\", \"discovery\", \"sample_set\", \"vcf\"]\n",
    "#table_list = [\"anvil_dataset\", \"anvil_project\"]\n",
    "#table_list = [\"sequence_file\", \"specimen_from_organism\", \"imaging_protocol\", \"ipsc_induction_protocol\", \"image_file\", \"analysis_process\", \"cell_line\", \"supplementary_file\", \"protocol\", \"cell_suspension\", \"analysis_protocol\", \"dissociation_protocol\", \"project\", \"sequencing_protocol\", \"donor_organism\", \"enrichment_protocol\", \"organoid\", \"collection_protocol\", \"library_preparation_protocol\", \"analysis_file\", \"differentiation_protocol\", \"aggregate_generation_protocol\", \"process\", \"links\", \"reference_file\", \"imaging_preparation_protocol\", \"imaged_specimen\"] #HCA/Lungmap\n",
    "delete_all_records = True\n",
    "delete_record_list = [\n",
    "] # Will be ignored if delete_all_records is set to True\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to delete rows from a dataset\n",
    "def delete_datarepo_rows(dataset_id, table_name, datarepo_row_ids, tdr_url):\n",
    "    print(\"Attempting to delete specified rows from {} for dataset {}\".format(table_name, dataset_id))\n",
    "    if datarepo_row_ids:\n",
    "        data_deletion_payload = {\n",
    "            \"deleteType\": \"soft\",\n",
    "            \"specType\": \"jsonArray\",\n",
    "            \"tables\": [{\n",
    "              \"tableName\": table_name,\n",
    "              \"jsonArraySpec\": {\n",
    "                \"rowIds\": datarepo_row_ids\n",
    "              }\n",
    "            }]\n",
    "        }\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client(tdr_url)\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            data_deletion_result, job_id = utils.wait_for_tdr_job(datasets_api.apply_dataset_data_deletion(id=dataset_id, data_deletion_request=data_deletion_payload), run_env)\n",
    "            print(\"Result: {}\".format(data_deletion_result))\n",
    "        except Exception as e:\n",
    "            print(\"Error: {}\".format(str(e)))\n",
    "    else:\n",
    "        print(\"No datarepo_row_ids specified for deletion.\")\n",
    "\n",
    "# Function to collect all datarepo rows for a particular table within a dataset\n",
    "def collect_all_datarepo_rows(dataset_id, table_name, tdr_url):\n",
    "    try:\n",
    "        api_client = utils.refresh_tdr_api_client(tdr_url)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        if response[\"access_information\"][\"big_query\"]: \n",
    "            cloud = \"gcp\"\n",
    "            bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "            bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        else:\n",
    "            cloud = \"azure\"\n",
    "            for parquet_table in response[\"access_information\"][\"parquet\"][\"tables\"]:\n",
    "                if parquet_table[\"name\"] == table_name:\n",
    "                    sas_url = parquet_table[\"url\"] + \"?\" + parquet_table[\"sas_token\"]\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving dataset information: {}\".format(str(e)))\n",
    "    if cloud == \"gcp\":\n",
    "        client = bigquery.Client()\n",
    "        query = \"SELECT datarepo_row_id FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = table_name)\n",
    "        try:\n",
    "            query_job = client.query(query)\n",
    "            results = [row[\"datarepo_row_id\"] for row in query_job]\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(\"Error retrieving datarepo_row_id list: {}\".format(str(e)))\n",
    "    else:\n",
    "#         blob_client = BlobClient.from_blob_url(sas_url)\n",
    "#         downloaded_blob = blob_client.download_blob()\n",
    "#         bytes_io = BytesIO(downloaded_blob.readall())\n",
    "#         df_blob = pd.read_parquet(bytes_io)\n",
    "        retrieval_error = False\n",
    "        max_page_size = 1000\n",
    "        records_fetched = 0 \n",
    "        total_record_count = 1\n",
    "        results = []\n",
    "        while records_fetched < total_record_count and not retrieval_error:\n",
    "            row_start = records_fetched\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                payload = {\n",
    "                  \"offset\": row_start,\n",
    "                  \"limit\": max_page_size,\n",
    "                  \"sort\": \"datarepo_row_id\",\n",
    "                  \"direction\": \"asc\",\n",
    "                  \"filter\": \"\"\n",
    "                }\n",
    "                try:\n",
    "                    dataset_results = datasets_api.query_dataset_data_by_id(id=dataset_id, table=table_name, query_data_request_model=payload).to_dict() \n",
    "                    total_record_count = dataset_results[\"total_row_count\"]\n",
    "                    for record in dataset_results[\"result\"]:\n",
    "                        results.append(record[\"datarepo_row_id\"])\n",
    "                        records_fetched += 1\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 5:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        warn_str = \"Error retrieving data_repo_row_ids for table.\"\n",
    "                        logging.warning(warn_str)\n",
    "                        retrieval_error = True\n",
    "                        break\n",
    "        return results\n",
    "    \n",
    "# Function to loop through datasets and delete\n",
    "def execute_deletions(dataset_id_list, table_list, delete_all_records, delete_record_list, tdr_url):\n",
    "    for dataset_id in dataset_id_list:\n",
    "        print(f\"Processing record deletions for dataset {dataset_id}\")\n",
    "        for table in table_list:\n",
    "            print(f\"Processing record deletion for {table}\")\n",
    "            if delete_all_records:\n",
    "                datarepo_row_ids = collect_all_datarepo_rows(dataset_id, table, tdr_url)\n",
    "            else:\n",
    "                datarepo_row_ids = delete_record_list\n",
    "            if datarepo_row_ids:\n",
    "                delete_datarepo_rows(dataset_id, table, datarepo_row_ids, tdr_url)\n",
    "            else:\n",
    "                print(\"No records specified for deletion.\")\n",
    "                \n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "execute_deletions(dataset_id_list, table_list, delete_all_records, delete_record_list, tdr_url)              \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Compare Record Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     48,
     62
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def fetch_dataset_counts(dataset_id):\n",
    "    # Setup/refresh TDR clients\n",
    "    results = []\n",
    "    api_client = utils.refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    \n",
    "    # Get table list from dataset\n",
    "    table_set = set()\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\"]).to_dict()\n",
    "        for table_entry in dataset_details[\"schema\"][\"tables\"]:\n",
    "            table_set.add(table_entry[\"name\"])\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset: {str(e)}\"\n",
    "        results.append([dataset_id, \"All\", 0, \"Failure\", error_str])\n",
    "        return results\n",
    "    \n",
    "    # For each table in the table list, pull record counts from the two datasets and compare\n",
    "    payload = {\n",
    "      \"offset\": 0,\n",
    "      \"limit\": 10,\n",
    "      \"sort\": \"datarepo_row_id\",\n",
    "      \"direction\": \"asc\",\n",
    "      \"filter\": \"\"\n",
    "    }\n",
    "    for table in table_set:\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                record_count = record_results[\"total_row_count\"]\n",
    "                results.append([dataset_id, table, record_count, \"Success\", \"\"])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 2:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    error_str = f\"Error pulling record count: {str(e)}\"\n",
    "                    results.append([dataset_id, table, 0, \"Failure\", error_str])\n",
    "                    break\n",
    "    return results\n",
    "\n",
    "def fetch_workspace_counts(workspace):\n",
    "    # Pull workspace table counts\n",
    "    results = []\n",
    "    try:    \n",
    "        entity_types = utils.list_entity_types(\"anvil-datastorage\", workspace, \"https://api.firecloud.org\")\n",
    "        entity_json = json.loads(entity_types.text)\n",
    "        for table, table_details in entity_json.items():\n",
    "            results.append([workspace, table, table_details[\"count\"], \"Success\", \"\"])\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from workspace: {str(e)}\"\n",
    "        results.append([workspace, \"All\", 0, \"Failure\", error_str])\n",
    "        return results\n",
    "\n",
    "def compare_row_counts(object_1, object_2, table_ignore_list):\n",
    "    # Determine object types\n",
    "    uuid_pattern = re.compile(r'^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}$')\n",
    "    object_1_type = \"TDR\" if uuid_pattern.match(object_1) else \"WS\"\n",
    "    object_2_type = \"TDR\" if uuid_pattern.match(object_2) else \"WS\"\n",
    "\n",
    "    # Process object 1\n",
    "    if object_1_type == \"TDR\":\n",
    "        object_1_results = fetch_dataset_counts(object_1)\n",
    "    else:\n",
    "        object_1_results = fetch_workspace_counts(object_1)\n",
    "    df_obj_1_results = pd.DataFrame(object_1_results, columns = [\"source_object\", \"table\", \"record_count\", \"status\", \"message\"])\n",
    "\n",
    "    # Process object 2\n",
    "    if object_2_type == \"TDR\":\n",
    "        object_2_results = fetch_dataset_counts(object_2)\n",
    "    else:\n",
    "        object_2_results = fetch_workspace_counts(object_2)\n",
    "    df_obj_2_results = pd.DataFrame(object_2_results, columns = [\"source_object\", \"table\", \"record_count\", \"status\", \"message\"])\n",
    "\n",
    "    # Compare and return results\n",
    "    df_merged_results = pd.merge(df_obj_1_results, df_obj_2_results, on=\"table\", how=\"outer\", suffixes=(\"_1\", \"_2\"))\n",
    "    df_merged_results[\"match\"] = ((df_merged_results[\"record_count_1\"] == df_merged_results[\"record_count_2\"]) & df_merged_results[\"record_count_1\"].notna() & df_merged_results[\"record_count_2\"].notna())\n",
    "    conditions = [df_merged_results[\"record_count_1\"].isna(), df_merged_results[\"record_count_2\"].isna(), (df_merged_results[\"status_1\"] != \"Success\") | (df_merged_results[\"status_2\"] != \"Success\"), df_merged_results[\"match\"]]\n",
    "    choices = [\"Table only exists in source_object_2\", \"Table only exists in source_object_1\", \"Failure in record retrieval for one or more source objects\", None]\n",
    "    df_merged_results[\"match_detail\"] = np.select(conditions, choices, default=\"Table exists in both source objects but record counts don't match\")\n",
    "    if table_ignore_list:\n",
    "        df_merged_results = df_merged_results[~df_merged_results[\"table\"].isin(table_ignore_list)]\n",
    "    df_merged_results = df_merged_results[[\"table\", \"source_object_1\", \"record_count_1\", \"source_object_2\", \"record_count_2\", \"match\", \"match_detail\", \"status_1\", \"message_1\", \"status_2\", \"message_2\"]].where((pd.notnull(df_merged_results)), None)\n",
    "    return df_merged_results    \n",
    "    \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify tables to ignore\n",
    "table_ignore_list = [\"file_inventory\", \"workspace_attributes\", \"ingestion_reference\"]\n",
    "\n",
    "# Specify the list of dataset pairs to compare\n",
    "object_pairs_list = [\n",
    "    #[\"<dataset_id/workspace_name>\", \"<dataset_id/workspace_name>\"]\n",
    "    ['AnVIL_CMH_GAFK_R5_Staging', 'fa266e3c-d921-4555-a083-fb3803ae43d2'],\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Run validation\n",
    "df_agg_results = pd.DataFrame()\n",
    "for object_pair in object_pairs_list:\n",
    "    df_object_pair_results = compare_row_counts(object_pair[0], object_pair[1], table_ignore_list)\n",
    "    df_agg_results = pd.concat([df_agg_results, df_object_pair_results], ignore_index=True)\n",
    "\n",
    "# Display final results\n",
    "print(\"Failures Only:\")\n",
    "display(df_agg_results[df_agg_results[\"match\"] == False])\n",
    "\n",
    "print(\"Full Results:\")\n",
    "display(df_agg_results)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TDR Dataset and/or Snapshot Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to delete a specific TDR Snapshot\n",
    "def delete_snapshot(snapshot_id, tdr_url):\n",
    "    api_client = utils.refresh_tdr_api_client(tdr_url)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete snapshot = {}\".format(snapshot_id))\n",
    "    try:\n",
    "        delete_snapshot_result, job_id = utils.wait_for_tdr_job(snapshots_api.delete_snapshot(id=snapshot_id), run_env)\n",
    "        print(\"Result: {}\".format(delete_snapshot_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset\n",
    "def delete_dataset(dataset_id, tdr_url):\n",
    "    api_client = utils.refresh_tdr_api_client(tdr_url)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {}\".format(dataset_id))\n",
    "    try:\n",
    "        delete_dataset_result, job_id = utils.wait_for_tdr_job(datasets_api.delete_dataset(id=dataset_id), run_env)\n",
    "        print(\"Result: {}\".format(delete_dataset_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset and all of its Snapshots\n",
    "def delete_dataset_and_all_snapshots(dataset_id, tdr_url):\n",
    "    api_client = utils.refresh_tdr_api_client(tdr_url)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {} and all associated snapshots\".format(dataset_id))\n",
    "    dataset_id_list = [dataset_id]\n",
    "    # Delete snapshots\n",
    "    snapshot_list = snapshots_api.enumerate_snapshots(dataset_ids=dataset_id_list)\n",
    "    if snapshot_list.items:\n",
    "        for snapshot in snapshot_list.items:\n",
    "            snapshot_id = str(snapshot.id)\n",
    "            delete_snapshot(snapshot_id)\n",
    "            sleep(10)\n",
    "    # Delete dataset\n",
    "    delete_dataset(dataset_id, tdr_url)\n",
    "\n",
    "# # Delete snapshots\n",
    "# run_env = \"prod\"\n",
    "# tdr_url = \"https://data.terra.bio\"\n",
    "# snapshot_id_list = [\n",
    "# ]\n",
    "# for snapshot_id in snapshot_id_list:\n",
    "#     delete_snapshot(snapshot_id, tdr_url)\n",
    "\n",
    "# Delete datasets and all their associated snapshots\n",
    "run_env = \"prod\"\n",
    "tdr_url = \"https://data.terra.bio\"\n",
    "dataset_id_list = [\n",
    "    '500b7041-1338-48f3-b52f-82ae92c231f6'\n",
    "]\n",
    "for dataset_id in dataset_id_list:\n",
    "    delete_dataset_and_all_snapshots(dataset_id, tdr_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## AnVIL TDR Service Account Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Determine the Buckets a TDR Dataset SA Needs Access To"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "billing_profile = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "tdr_url = \"https://data.terra.bio\"\n",
    "\n",
    "# Establish API client\n",
    "api_client = utils.refresh_tdr_api_client(tdr_url)\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# Loop through enumerated datasets and create records for those related to AnVIL\n",
    "print(f\"Start time: {datetime.datetime.now()}\")\n",
    "records_list = []\n",
    "datasets_list = datasets_api.enumerate_datasets(limit=5000)\n",
    "for dataset_entry in datasets_list.items:\n",
    "    if dataset_entry.default_profile_id == billing_profile:\n",
    "        # Retrieve dataset details and pull source workspace(s)\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=dataset_entry.id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        \n",
    "        # Pull data file size sum from BigQuery\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        client = bigquery.Client()\n",
    "        source_bucket_query = f\"\"\"SELECT DISTINCT REGEXP_EXTRACT(source_name, r'^gs:\\/\\/([a-z0-9\\-]+)\\/') AS source_bucket FROM `{bq_project}.{bq_schema}.datarepo_load_history` WHERE state = 'succeeded'\"\"\"\n",
    "        try:\n",
    "            df_output = client.query(source_bucket_query).result().to_dataframe()\n",
    "            for i in range(0, len(df_output)):\n",
    "                source_bucket = df_output[\"source_bucket\"].values[i]\n",
    "                status = \"Success\"\n",
    "                record = [dataset_entry.id, source_bucket, status]\n",
    "                records_list.append(record)\n",
    "        except:\n",
    "            source_bucket = \"\"\n",
    "            status = \"Error\"\n",
    "            record = [dataset_entry.id, source_bucket, status]\n",
    "            records_list.append(record)\n",
    "        \n",
    "# Read records into a dataframe\n",
    "df = pd.DataFrame(records_list, columns =[\"Dataset UUID\", \"Source Bucket\", \"Retrieval Status\"])\n",
    "df_sorted = df.sort_values([\"Dataset UUID\", \"Source Bucket\"], ascending=[True, True], ignore_index=True)\n",
    "print(f\"End time: {datetime.datetime.now()}\")\n",
    "display(df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Clean Up Outdated AnVIL TDR Service Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "terra_url = \"https://api.firecloud.org\"\n",
    "valid_sa_list = [\n",
    "]\n",
    "\n",
    "# Establish credentials\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "\n",
    "# Get current anvil_tdr_ingest membership\n",
    "group = \"anvil_tdr_ingest\"\n",
    "group_members = requests.get(\n",
    "    url=f\"{terra_url}/api/groups/{group}\",\n",
    "    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    ").json()\n",
    "\n",
    "# Loop through anvil_tdr_ingest membership and remove outdated users\n",
    "user_cnt = 0\n",
    "success_cnt = 0\n",
    "for member in group_members[\"membersEmails\"]:\n",
    "    if \"tdr-ingest-sa\" in member and member not in valid_sa_list:\n",
    "        user_cnt += 1\n",
    "        response = requests.delete(\n",
    "            url=f\"{terra_url}/api/groups/{group}/member/{member}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "        if response.status_code == 204:\n",
    "            success_cnt += 1\n",
    "print(f\"Group ({group}) clean-up: \")\n",
    "print(f\"\\t- Users to remove: {user_cnt}\")\n",
    "print(f\"\\t- Users removed successfully: {success_cnt}\")\n",
    "\n",
    "# Get current workspace membership\n",
    "ws_members = requests.get(\n",
    "    url=f\"{terra_url}/api/workspaces/{ws_project}/{ws_name}/acl\",\n",
    "    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    ").json()\n",
    "\n",
    "# Loop through workspace membership and remove outdated users\n",
    "user_cnt = 0\n",
    "success_cnt = 0\n",
    "for member in ws_members[\"acl\"].keys():\n",
    "    if \"tdr-ingest-sa\" in member and member not in valid_sa_list:\n",
    "        user_cnt += 1\n",
    "        payload = [{\n",
    "            \"email\": member,\n",
    "            \"accessLevel\": \"NO ACCESS\",\n",
    "            \"canShare\": False,\n",
    "            \"canCompute\": False\n",
    "        }]\n",
    "        response = requests.patch(\n",
    "            url=f\"{terra_url}/api/workspaces/{ws_project}/{ws_name}/acl\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}, \n",
    "            json=payload\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            success_cnt += 1\n",
    "print(f\"Workspace ({ws_project}/{ws_name}) clean-up: \")\n",
    "print(f\"\\t- Users to remove: {user_cnt}\")\n",
    "print(f\"\\t- Users removed successfully: {success_cnt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Clean Up Processing Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!gsutil -u anvil-datastorage -m rm -r gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!gsutil -u anvil-datastorage -m rm -r gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!gsutil -u anvil-datastorage -m rm -r gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Other Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gsutil -u anvil-datastorage ls gs://fc-secure-33cad843-3453-42ea-bf50-0eda2b52171d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
