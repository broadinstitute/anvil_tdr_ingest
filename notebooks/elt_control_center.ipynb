{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print(\"Version 1.0.0: 09/23/2022 5:06pm - Nate Calvanese - First version created\")\n",
    "#print(\"Version 1.0.1: 09/26/2022 11:18m - Nate Calvanese - Fixed bug in default dataset naming\")\n",
    "#print(\"Version 1.0.2: 09/27/2022 2:43pm - Nate Calvanese - Added ability to aggregate multiple workspaces into one dataset\")\n",
    "#print(\"Version 1.0.3: 10/5/2022 1:32pm - Nate Calvanese - Added support for chunking up ingest requests\")\n",
    "#print(\"Version 1.0.4: 10/6/2022 10:35am - Nate Calvanese - Updated use of TDR utility functions\")\n",
    "print(\"Version 1.0.5: 10/13/2022 10:54am - Nate Calvanese - Parameter tweaks for latest changes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Common Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional modules (one time effort per cloud environment)\n",
    "!pip install --upgrade import_ipynb data_repo_client\n",
    "#!pip install data_repo_client==1.409.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace environment variables\n",
    "import os\n",
    "import re\n",
    "print(\"Recording workspace environment variables:\")\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "print(f\"Workspace name = {ws_name}\")\n",
    "print(f\"Workspace project = {ws_project}\")\n",
    "print(f\"Workspace bucket = {ws_bucket}\")\n",
    "print(f\"Workspace bucket name = {ws_bucket_name}\")\n",
    "\n",
    "# Copy latest version of the pipeline notebooks to the cloud environment (uncomment if any notebooks have changed since last run)\n",
    "# print(\"\\nCopying latest pipeline notebooks to the cloud environment:\")\n",
    "# !gsutil -m cp $ws_bucket/notebooks/*.ipynb .\n",
    "\n",
    "# Additional imports\n",
    "print(\"\\nRunning imports:\")\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "from firecloud import api as fapi\n",
    "import data_repo_client\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_mapping_query as bmq\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# Common pipeline variables (AnVIL)\n",
    "ws_attributes = utils.get_workspace_attributes(ws_project, ws_name)\n",
    "params = {}\n",
    "params[\"ws_name\"] = ws_name\n",
    "params[\"ws_project\"] = ws_project\n",
    "params[\"ws_bucket\"] = ws_bucket\n",
    "params[\"ws_bucket_name\"] = ws_bucket_name\n",
    "params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "params[\"google_project\"] = ws_attributes[\"googleProject\"]\n",
    "params[\"create_file_table\"] = True\n",
    "params[\"file_table_name\"] = \"file_inventory\"\n",
    "params[\"anvil_schema_version\"] = \"ANV2\"\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"EL\" Pipeline: Load Dataset to TDR in Source Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## >>> Run Variables <<<\n",
    "# For datasets split across multiple workspaces, set the staging area and target TDR dataset to the \n",
    "# same value to collect all of the source data and process it together.\n",
    "workspace_run_list = [\n",
    "    #[\"Workspace_Name\", \"Workspace_Project\", \"Staging Area (Leave empty for default)\", \"Target_TDR_Dataset_Name (Leave empty for default)\", Run (True/False)]\n",
    "    [\"ANVIL_1234\", \"anvil-datastorage\", \"\", \"\", False],\n",
    "    [\"AnVIL_5678\", \"anvil-datastorage\", \"\", \"\", False]\n",
    "]\n",
    "params[\"skip_source_files_creation\"] = False\n",
    "params[\"skip_file_inventory_creation\"] = False\n",
    "params[\"skip_table_data_processing\"] = False\n",
    "params[\"skip_ingests\"] = False\n",
    "params[\"skip_snapshot_creation\"] = False\n",
    "params[\"snapshot_readers_list\"] = [\"anvil_tdr_ingest@firecloud.org\", \"azul-anvil-prod@firecloud.org\"] # Auth domain groups are also added as readers automatically\n",
    "\n",
    "\n",
    "## >>> File Inventory Variables <<<\n",
    "# The GCS bucket associated with the source workspace will be automatically included in the file inventory build. To specify \n",
    "# additional GCS buckets to include in the file inventory build, add entries to the below dictionary.\n",
    "params[\"additional_file_inventory_sources\"] = {}\n",
    "# EXAMPLE:\n",
    "# params[\"additional_file_inventory_sources\"] = {\n",
    "#     \"staging_area\": {\n",
    "#         \"bucket_name\": {\n",
    "#             \"include_dirs\": [], # Leave empty to include all directories in bucket\n",
    "#             \"exclude_dirs\": [] # Exclusions will take precedence over inclusions\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "## >>> Ingest Variables <<<\n",
    "# For cases where you only want to ingest a subset of files, use the below dictionary to specify exactly what should be ingested.\n",
    "params[\"ingest_list_override\"] = {}\n",
    "# EXAMPLE:\n",
    "# params[\"ingest_list_override\"] = {\n",
    "#     \"ws_table\": [\"ws_table_0.json\"], # Leave empty to run ingest for every file for target table\n",
    "# }\n",
    "\n",
    "\n",
    "## >>> File Reference Variables <<<\n",
    "# Fields containing GCS links will be identified automatically by the pipeline. The below dict should contain any fields\n",
    "# that contain file references that aren't proper GCS links in the workspace tables.\n",
    "data_file_refs_dict = {   \n",
    "    \"sequencing\": [{\n",
    "        \"column\": \"sequencing_id\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": False, \n",
    "        \"match_regex\": None,\n",
    "        \"mode\": \"fileref_in_line\",\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"sequencing_id_fileref\"\n",
    "    }]\n",
    "}\n",
    "# Definitions:\n",
    "#    Required Fields: column, method, mode, create_new field\n",
    "#    Optional Fields: match_multiple_files (default to True), match_regex (default to None), new_field_name (default to None)\n",
    "#    Methods: \n",
    "#       file_path_match -- Field contains a full or partial file path, which can be matched to the file inventory to grab the file(s) referenced \n",
    "#       tdr_file_id -- Field contains file UUIDs of files already ingested into the target TDR dataset\n",
    "#    Modes:\n",
    "#       fileref_in_line -- Populates the field with a file reference object\n",
    "#       fileref_table_ref -- Populates the field with an ID that joins to a file table. If no file table built, falls back on fileref_in_line logic.\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "# Print variables\n",
    "print(\"Pipeline run variables set:\")\n",
    "print(\"Profile ID: \" + params[\"profile_id\"])\n",
    "print(\"Ingests to run: \")\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "for workspace in workspace_run_list:\n",
    "    if workspace[4] == True:\n",
    "        ws_attributes = utils.get_workspace_attributes(workspace[1], workspace[0])\n",
    "        params[\"phs_id\"] = utils.format_phs_id(ws_attributes[\"attributes\"][\"phs_id\"]) if ws_attributes[\"attributes\"].get(\"phs_id\") else \"\"\n",
    "        auth_list = ws_attributes[\"authorizationDomain\"] if ws_attributes.get(\"authorizationDomain\") else []\n",
    "        params[\"auth_domains\"] = [x[\"membersGroupName\"] for x in auth_list]\n",
    "        params[\"consent_name\"] = ws_attributes[\"attributes\"][\"library:dataUseRestriction\"] if ws_attributes[\"attributes\"].get(\"library:dataUseRestriction\") else \"\"\n",
    "        params[\"data_files_src_bucket\"] = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "        workspace[3] = workspace[3] if workspace[3] else utils.format_dataset_name(workspace[0])\n",
    "        workspace[2] = workspace[2] if workspace[2] else workspace[0]\n",
    "        print(\"- Workspace [\" + workspace[1] + \"/\" + workspace[0] + \"] to TDR dataset [\" + workspace[3] + \"] via Staging Area [\" + workspace[2] + \"]\")\n",
    "        print(\"\\t- PHS ID = \" + params[\"phs_id\"])\n",
    "        print(\"\\t- Consent Short Name = \" + params[\"consent_name\"])\n",
    "        print(\"\\t- Auth Domains = \" + str(params[\"auth_domains\"]))\n",
    "        print(\"\\t- Data Files Source Bucket = \" + params[\"data_files_src_bucket\"])\n",
    "print(\"Skip source files creation? \" + str(params[\"skip_source_files_creation\"]))\n",
    "print(\"Skip file inventory creation? \" + str(params[\"skip_file_inventory_creation\"]))\n",
    "print(\"Skip table data processing? \" + str(params[\"skip_table_data_processing\"]))\n",
    "print(\"Skip ingests? \" + str(params[\"skip_ingests\"]))\n",
    "print(\"Skip snapshot creation? \" + str(params[\"skip_snapshot_creation\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and execute workspace connector pipeline (\"E\") for listed workspaces\n",
    "if params[\"skip_source_files_creation\"] == True:\n",
    "    logging.info(\"Skipping source file creation, per user request.\")\n",
    "else:\n",
    "    for workspace in workspace_run_list:\n",
    "        if workspace[4] == True:\n",
    "            params[\"data_file_refs\"] = data_file_refs_dict  \n",
    "            utils.run_ws_connector_pipeline(workspace, params)\n",
    "\n",
    "# Aggregate staging area to target dataset combinations, loop through them, and execute ingest pipeline (\"L\")\n",
    "pipeline_run_list = []\n",
    "for workspace in workspace_run_list:\n",
    "    if workspace[4] == True:\n",
    "        temp_list = [workspace[2], workspace[3]]\n",
    "        if temp_list not in pipeline_run_list:\n",
    "            pipeline_run_list.append(temp_list)\n",
    "for pipeline in pipeline_run_list:\n",
    "    utils.run_el_pipeline(pipeline, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Development\n",
    "Work through the following steps for each dataset that needs to be processed through the transformation pipeline in Step 4. Note that you can use the logs or results_dict from the previous step to retrieve the dataset_id values of interest, or retrieve them directly from TDR via the UI or Swagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset for mapping development\n",
    "dataset_id = \"1234\"\n",
    "print(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Missing Relationships to TDR Dataset Schema\n",
    "Relationships are needed by the mapping query constructor to build appropriate joins between tables. If no joins are required between tables, this step is unnecessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record relationships to potentially add to source dataset. Note that there may be more relationships to add\n",
    "# than those listed below, so add to this list as necessary.\n",
    "potential_relationships = [\n",
    "#     [\"subject.family_id\", \"family.family_id\"],\n",
    "#     [\"sample.subject_id\", \"subject.subject_id\"],\n",
    "#     [\"sequencing.sample_id\", \"sample.sample_id\"],\n",
    "#     [\"sample.participant\", \"participant.participant_id\"],\n",
    "]\n",
    "\n",
    "# Retrieve source schema\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "src_schema_dict = {}\n",
    "try:\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "    src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "    src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "# Loop through potential relationships and add those present for the source dataset\n",
    "additional_relationships = []\n",
    "for rel in potential_relationships:\n",
    "    from_table = rel[0].split(\".\")[0] \n",
    "    from_column = rel[0].split(\".\")[1]\n",
    "    to_table = rel[1].split(\".\")[0]\n",
    "    to_column = rel[1].split(\".\")[1]\n",
    "    if bmq.confirm_column_exists(src_schema_dict, from_table, from_column) and bmq.confirm_column_exists(src_schema_dict, to_table, to_column):\n",
    "        relationship_found = False\n",
    "        for rel_entry in src_schema_dict[\"relationships\"]:\n",
    "            if rel_entry[\"_from\"][\"table\"] == from_table and rel_entry[\"_from\"][\"column\"] == from_column and rel_entry[\"to\"][\"table\"] == to_table and rel_entry[\"to\"][\"column\"] == to_column:\n",
    "                relationship_found = True\n",
    "            elif rel_entry[\"_from\"][\"table\"] == to_table and rel_entry[\"_from\"][\"column\"] == to_column and rel_entry[\"to\"][\"table\"] == from_table and rel_entry[\"to\"][\"column\"] == from_column:\n",
    "                relationship_found = True\n",
    "        if not relationship_found:\n",
    "            rel_dict = {\n",
    "                \"name\": from_table + \"_\" + from_column + \"__to__\" + to_table + \"_\" + to_column,\n",
    "                \"from\": {\"table\": from_table, \"column\": from_column},\n",
    "                \"to\": {\"table\": to_table, \"column\": to_column}\n",
    "            }\n",
    "            additional_relationships.append(rel_dict)\n",
    "\n",
    "# Submit the schema update request for the TDR dataset\n",
    "if additional_relationships:\n",
    "    schema_update_request = {\n",
    "        \"description\": \"Adding relationships to support query construction.\",\n",
    "        \"changes\": {\n",
    "            \"addRelationships\": additional_relationships\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        resp = utils.wait_for_tdr_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "        print(str(resp)[0:1000])\n",
    "    except Exception as e:\n",
    "        print(\"Error running schema update: \" + str(e))\n",
    "else:\n",
    "    print(\"No additional relationships to add to schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Mapping Artifacts and Run Query Construction\n",
    "Specify the target schema (\"mapping target\") and mapping specification (\"mapping_target_spec\") you would like to use to construct transformation queries for your dataset. These transformation queries will then be dynamically constructed based on the appropriate target schema, mapping specification, and source schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired mapping specification and retrieve artifacts needed for query construction\n",
    "mapping_target = \"anvil\"\n",
    "mapping_target_spec = \"cmg_ext_1\" \n",
    "\n",
    "# Retrieve source schema\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "src_schema_dict = {}\n",
    "try:\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "    src_schema_dict[\"name\"] = response[\"name\"]\n",
    "    src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "    src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "    bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "    bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "#print(src_schema_dict)\n",
    "\n",
    "# Set dataset_name parameter to substitute into transform queries ($DATASET_NAME)\n",
    "dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "\n",
    "# Retrieve target schema and mapping specification\n",
    "target_schema_dict = {}\n",
    "mapping_spec = {}\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "    target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "#print(target_schema_dict)\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "    blob_string = blob.download_as_text(client=None)\n",
    "    blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "    mapping_spec = json.loads(blob_string)\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "#print(mapping_spec)\n",
    "\n",
    "# Build queries from mapping specification\n",
    "query_dict = {}\n",
    "if target_schema_dict:\n",
    "    for target_table in target_schema_dict[\"tables\"]:\n",
    "        table_name = target_table[\"name\"]\n",
    "        missing_artifacts = False\n",
    "        if src_schema_dict and mapping_spec:\n",
    "            query_dict[table_name] = bmq.build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema)\n",
    "        else:\n",
    "            missing_artifacts = True\n",
    "            query_dict[table_name] = {\"query\": \"\", \"syntax_check\": \"\"} \n",
    "    if missing_artifacts == True:\n",
    "        print(\"Source schema dictionary and/or mapping specification missing. Unable to generate queries.\")\n",
    "else:\n",
    "    print(\"Target schema dictionary missing. Unable to generate queries.\")\n",
    "query_df = pd.DataFrame.from_dict(query_dict, orient=\"index\")\n",
    "query_df.index.name = \"target_table\"\n",
    "query_df.reset_index(inplace=True)\n",
    "display(query_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Review Generated Queries and Update/Override as Necessary\n",
    "Review the queries generated in the previous step. Any queries that have not passed the syntax check need to be remedied before they can be executed. Any other queries that do not align with expectations can be overridden by either A) Updating the mapping target specification, or B) Manually overriding the query below. Option B should only ideally only be used in one-off cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update the query definition for particular target table, input the target table and query below\n",
    "target_table = \"\"\n",
    "query = \"\"\n",
    "\n",
    "# Run syntax check\n",
    "query_dict[target_table][\"query\"] = query\n",
    "query_dict[target_table][\"syntax_check\"] = bmq.run_syntax_check(query)\n",
    "print(query_dict[target_table])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Vocabulary Mapping\n",
    "For target attributes leveraging the \"VOCAB_MAP\" transformation, evaluate whether the source values have a record in the dsp-data-ingest.transform_resources.vocab_map table. If additional mappings are needed, these should be put into place before the transformation queries are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vocab mapping and display results\n",
    "df = bmq.evaluate_vocab_mapping(mapping_spec, src_schema_dict, target_schema_dict, bq_project, bq_schema)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Missing mapped_value view:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(df[df[\"mapped_value\"].isnull() & df[\"source_value\"].notnull()])\n",
    "print(\"\\n-------------------------------------------\")\n",
    "print(\"Full view:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish Mapping Artifacts for use in \"T\" Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy target schema file to output folder for mapping target\n",
    "source_path = \"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target)\n",
    "destination_path = \"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(mapping_target, dataset_id)\n",
    "!gsutil cp $ws_bucket/$source_path $ws_bucket/$destination_path 2> stdout\n",
    "\n",
    "# Limit query dict to valid queries, write out, and copy to output folder for mapping target\n",
    "valid_query_dict = {}\n",
    "for target, val in query_dict.items():\n",
    "    if val[\"syntax_check\"] == \"Passed\":\n",
    "        valid_query_dict[target] = val\n",
    "final_query_dict = {\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"transforms\": valid_query_dict\n",
    "}\n",
    "query_dict_json = json.dumps(final_query_dict)\n",
    "query_output_file = \"transform_query_set.json\"\n",
    "with open(query_output_file, 'w') as outfile:\n",
    "    outfile.write(query_dict_json)\n",
    "destination_path = \"ingest_pipeline/output/transformed/{}/{}/queries\".format(mapping_target, dataset_id)\n",
    "!gsutil cp $query_output_file $ws_bucket/$destination_path/ 2> stdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"T\" Pipeline: Load Additional Transformed Tables to TDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Variables\n",
    "dataset_id_run_list = [\n",
    "    #[\"dataset_id\", Run (True/False)],\n",
    "    [\"1234\", False],\n",
    "    [\"5678\", False]\n",
    "]\n",
    "params[\"mapping_target\"] = \"anvil\"\n",
    "params[\"skip_transforms\"] = False\n",
    "params[\"transform_list_override\"] = [] # Leave empty to run transforms for all files, otherwise populate with target table names \n",
    "params[\"skip_schema_extension\"] = False\n",
    "params[\"skip_ingests\"] = False\n",
    "params[\"ingest_list_override\"] = [] # Leave empty to run ingests for all files, otherwise populate with target table names\n",
    "params[\"skip_snapshot_creation\"] = False\n",
    "params[\"snapshot_readers_list\"] = [\"anvil_tdr_ingest@firecloud.org\", \"azul-anvil-prod@firecloud.org\"] # Auth domain groups are also added as readers automatically\n",
    "params[\"skip_data_validation\"] = False\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Print variables\n",
    "print(\"Pipeline run variables set:\")\n",
    "print(\"Profile ID: \" + params[\"profile_id\"])\n",
    "print(\"Mapping Target: \" + params[\"mapping_target\"])\n",
    "print(\"Datasets to run: \")\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "dataset_skip_list = []\n",
    "for dataset in dataset_id_run_list:\n",
    "    if dataset[1]:\n",
    "        dataset_id = dataset[0]\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "            phs_id = dataset_info[\"phs_id\"]\n",
    "            consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "            auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "            src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "            dataset_skip_list.append(dataset_id)\n",
    "        if dataset_name:\n",
    "            dataset_id = dataset[0]\n",
    "            print(\"- \" + dataset_name + \" ({})\".format(dataset_id))\n",
    "            print(\"\\t- PHS ID = \" + phs_id)\n",
    "            print(\"\\t- Consent Short Name = \" + consent_name)\n",
    "            print(\"\\t- Auth Domains = \" + str(auth_domains))\n",
    "            print(\"\\t- Source Workspaces = \" + str(src_workspaces))\n",
    "if dataset_skip_list:\n",
    "    print(\"Datasets to skip (they either don't exist or aren't accessible to the current user): \")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(dataset_skip_list)) \n",
    "print(\"Skip transforms? \" + str(params[\"skip_transforms\"]))\n",
    "print(\"Skip schema extension? \" + str(params[\"skip_schema_extension\"]))\n",
    "print(\"Skip ingests? \" + str(params[\"skip_ingests\"]))\n",
    "print(\"Skip snapshot creation? \" + str(params[\"skip_snapshot_creation\"]))\n",
    "print(\"Skip data validation? \" + str(params[\"skip_data_validation\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and execute pipeline for listed workspaces\n",
    "for dataset in dataset_id_run_list:\n",
    "    if dataset[1]:\n",
    "        dataset_id = dataset[0]\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "            phs_id = dataset_info[\"phs_id\"]\n",
    "            consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "            auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "            src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "        if dataset_name:\n",
    "            params[\"dataset_id\"] = dataset_id\n",
    "            params[\"dataset_name\"] = dataset_name\n",
    "            params[\"phs_id\"] = phs_id\n",
    "            params[\"consent_name\"] = consent_name\n",
    "            params[\"auth_domains\"] = auth_domains\n",
    "            utils.run_t_pipeline(params)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Scripts\n",
    "Uncomment sections as necessary to accomplish various miscellaneous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Storage Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name, mapping target, and target dataset_id of the source workspace whose table data files should be removed \n",
    "clean_staging_area_data = True\n",
    "staging_area_name = \"\"\n",
    "clean_transformed_data = True\n",
    "mapping_target = \"anvil\"\n",
    "dataset_id = \"103c17df-ae7f-4f25-8441-a5eafaebdf01\"\n",
    "\n",
    "# gsutil commands to remove dataset table data and data file inventories where they live\n",
    "if clean_staging_area_data:\n",
    "    !gsutil -m rm -r $ws_bucket/ingest_pipeline/input/$workspace_name\n",
    "    !gsutil -m rm -r $ws_bucket/ingest_pipeline/output/source/$workspace_name/table_data\n",
    "if clean_transformed_data:\n",
    "    !gsutil -m rm -r $ws_bucket/ingest_pipeline/output/transformed/$mapping_target/$dataset_id/table_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDR Dataset and/or Snapshot Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Google creds and establish TDR clients\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "\n",
    "# Function to delete a specific TDR Snapshot\n",
    "def delete_snapshot(snapshot_id):\n",
    "    print(\"Attempting to delete snapshot = {}\".format(snapshot_id))\n",
    "    try:\n",
    "        delete_snapshot_result, job_id = utils.wait_for_tdr_job(snapshots_api.delete_snapshot(id=snapshot_id))\n",
    "        print(\"Result: {}\".format(delete_snapshot_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset\n",
    "def delete_dataset(dataset_id):\n",
    "    print(\"Attempting to delete dataset = {}\".format(dataset_id))\n",
    "    try:\n",
    "        delete_dataset_result, job_id = utils.wait_for_tdr_job(datasets_api.delete_dataset(id=dataset_id))\n",
    "        print(\"Result: {}\".format(delete_dataset_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset and all of its Snapshots\n",
    "def delete_dataset_and_all_snapshots(dataset_id):\n",
    "    print(\"Attempting to delete dataset = {} and all associated snapshots\".format(dataset_id))\n",
    "    dataset_id_list = [dataset_id]\n",
    "    # Delete snapshots\n",
    "    snapshot_list = snapshots_api.enumerate_snapshots(dataset_ids=dataset_id_list)\n",
    "    if snapshot_list.items:\n",
    "        for snapshot in snapshot_list.items:\n",
    "            snapshot_id = str(snapshot.id)\n",
    "            delete_snapshot(snapshot_id)\n",
    "    # Delete dataset\n",
    "    delete_dataset(dataset_id)\n",
    "\n",
    "# Delete a snapshot\n",
    "# snapshot_id = \"ce7f4b37-ce00-45d2-bc31-82eac2148d5f\"\n",
    "# delete_snapshot(snapshot_id)\n",
    "\n",
    "# Delete a dataset\n",
    "# dataset_id = \"26512be2-9ac0-4852-ac82-85fdc32bffea\"\n",
    "# delete_dataset(dataset_id)\n",
    "\n",
    "# Delete a dataset and all associated snapshots\n",
    "dataset_id = \"ee3cee29-241f-4f8d-9e73-31c15538c747\"\n",
    "delete_dataset_and_all_snapshots(dataset_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Deletion of TDR Dataset Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Google creds and establish TDR clients\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# Function to delete rows from a dataset\n",
    "def delete_datarepo_rows(dataset_id, table_name, datarepo_row_ids):\n",
    "    print(\"Attempting to delete specified rows from {} for dataset {}\".format(table_name, dataset_id))\n",
    "    if datarepo_row_ids:\n",
    "        data_deletion_payload = {\n",
    "            \"deleteType\": \"soft\",\n",
    "            \"specType\": \"jsonArray\",\n",
    "            \"tables\": [{\n",
    "              \"tableName\": table_name,\n",
    "              \"jsonArraySpec\": {\n",
    "                \"rowIds\": datarepo_row_ids\n",
    "              }\n",
    "            }]\n",
    "        }\n",
    "        try:\n",
    "            data_deletion_result, job_id = utils.wait_for_tdr_job(datasets_api.apply_dataset_data_deletion(id=dataset_id, data_deletion_request=data_deletion_payload))\n",
    "            print(\"Result: {}\".format(data_deletion_result))\n",
    "        except Exception as e:\n",
    "            print(\"Error: {}\".format(str(e)))\n",
    "    else:\n",
    "        print(\"No datarepo_row_ids specified for deletion.\")\n",
    "\n",
    "# Function to collect all datarepo rows for a particular table within a dataset\n",
    "def collect_all_datarepo_rows(dataset_id, table_name):\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving BQ project and schema: {}\".format(str(e)))\n",
    "    client = bigquery.Client()\n",
    "    query = \"SELECT datarepo_row_id FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = table_name)\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        results = [row[\"datarepo_row_id\"] for row in query_job]\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving datarepo_row_id list: {}\".format(str(e)))\n",
    "\n",
    "# Specify which rows should be deleted from which dataset and table                                  \n",
    "dataset_id = \"1234\"\n",
    "table_name = \"ws_table\"\n",
    "#datarepo_row_ids = collect_all_datarepo_rows(dataset_id, table_name) # Uncomment to pull all datarepo_row_ids in the specified table\n",
    "datarepo_row_ids = []\n",
    "delete_datarepo_rows(dataset_id, table_name, datarepo_row_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
