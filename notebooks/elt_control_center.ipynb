{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "print(\"Version 1.0.0: 09/23/2022 5:06pm - Nate Calvanese - First version created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Common Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional modules (one time effort per cloud environment)\n",
    "!pip install --upgrade import_ipynb data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace environment variables\n",
    "import os\n",
    "import re\n",
    "print(\"Recording workspace environment variables:\")\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "print(f\"Workspace name = {ws_name}\")\n",
    "print(f\"Workspace project = {ws_project}\")\n",
    "print(f\"Workspace bucket = {ws_bucket}\")\n",
    "print(f\"Workspace bucket name = {ws_bucket_name}\")\n",
    "\n",
    "# Copy latest version of the pipeline notebooks to the cloud environment (uncomment if any notebooks have changed since last run)\n",
    "# print(\"\\nCopying latest pipeline notebooks to the cloud environment:\")\n",
    "# !gsutil -m cp $ws_bucket/notebooks/*.ipynb .\n",
    "\n",
    "# Additional imports\n",
    "print(\"\\nRunning imports:\")\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "from firecloud import api as fapi\n",
    "import data_repo_client\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_mapping_query as bmq\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import logging\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "# Common pipeline variables (AnVIL)\n",
    "ws_attributes = utils.get_workspace_attributes(ws_project, ws_name)\n",
    "params = {}\n",
    "params[\"ws_name\"] = ws_name\n",
    "params[\"ws_project\"] = ws_project\n",
    "params[\"ws_bucket\"] = ws_bucket\n",
    "params[\"ws_bucket_name\"] = ws_bucket_name\n",
    "params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "params[\"google_project\"] = ws_attributes[\"googleProject\"]\n",
    "params[\"data_files_src_dirs\"] = []\n",
    "params[\"data_files_src_dirs_exclude\"] = []\n",
    "params[\"data_file_ref_mode\"] = \"fileref_table\"\n",
    "params[\"data_file_ref_table_name\"] = \"ws_file_inventory\"\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"EL\" Pipeline: Load Dataset to TDR in Source Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Variables\n",
    "workspace_run_list = [\n",
    "    #[\"Workspace_Name\", \"Workspace_Project\", \"Target_TDR_Dataset_Name (Leave empty for default logic)\", Run (True/False)]\n",
    "    [\"ANVIL_XXX\", \"anvil-datastorage\", \"\", True],\n",
    "    [\"ANVIL_XXX\", \"anvil-datastorage\", \"\", True]\n",
    "]\n",
    "params[\"skip_source_files_creation\"] = False\n",
    "params[\"skip_file_inventory_creation\"] = False\n",
    "params[\"skip_table_data_processing\"] = False\n",
    "params[\"skip_ingests\"] = False\n",
    "params[\"ingest_list_override\"] = [] # Leave empty to use the list generated by table data processing step, otherwise populate with target table names (e.g. \"ws_subject\")\n",
    "\n",
    "# Fields containing GCS links will be identified automatically by the pipeline. The below dict should contain any fields\n",
    "# that contain file references that aren't proper GCS links in the workspace tables.\n",
    "data_file_refs_dict = {   \n",
    "    \"ws_sequencing.tsv\": [{\n",
    "        \"column\": \"sequencing_id\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"sequencing_id_file_id\"\n",
    "    }]\n",
    "}\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "# Print variables\n",
    "print(\"Pipeline run variables set:\")\n",
    "print(\"Profile ID: \" + params[\"profile_id\"])\n",
    "print(\"Ingests to run: \")\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "for workspace in workspace_run_list:\n",
    "    if workspace[3] == True:\n",
    "        ws_attributes = utils.get_workspace_attributes(workspace[1], workspace[0])\n",
    "        params[\"phs_id\"] = utils.format_phs_id(ws_attributes[\"attributes\"][\"phs_id\"]) if ws_attributes[\"attributes\"].get(\"phs_id\") else \"\"\n",
    "        auth_list = ws_attributes[\"authorizationDomain\"] if ws_attributes.get(\"authorizationDomain\") else []\n",
    "        params[\"auth_domains\"] = [x[\"membersGroupName\"] for x in auth_list]\n",
    "        params[\"consent_name\"] = ws_attributes[\"attributes\"][\"library:dataUseRestriction\"] if ws_attributes[\"attributes\"].get(\"library:dataUseRestriction\") else \"\"\n",
    "        params[\"data_files_src_bucket\"] = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "        workspace[2] = workspace[2] if workspace[2] else \"ANVIL_\" + re.sub(\"^ANVIL[_]?\", \"\", workspace[0]) + \"_\" + current_date_string\n",
    "        print(\"\\t- Workspace [\" + workspace[1] + \"/\" + workspace[0] + \"] to TDR dataset [\" + workspace[2] + \"]\")\n",
    "        print(\"\\t\\t- PHS ID = \" + params[\"phs_id\"])\n",
    "        print(\"\\t\\t- Consent Short Name = \" + params[\"consent_name\"])\n",
    "        print(\"\\t\\t- Auth Domains = \" + str(params[\"auth_domains\"]))\n",
    "        print(\"\\t\\t- Data Files Source Bucket = \" + params[\"data_files_src_bucket\"])\n",
    "print(\"Skip source files creation? \" + str(params[\"skip_source_files_creation\"]))\n",
    "print(\"Skip file inventory creation? \" + str(params[\"skip_file_inventory_creation\"]))\n",
    "print(\"Skip table data processing? \" + str(params[\"skip_table_data_processing\"]))\n",
    "print(\"Skip ingests? \" + str(params[\"skip_ingests\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and execute pipeline for listed workspaces\n",
    "el_results_dict = {}\n",
    "for workspace in workspace_run_list:\n",
    "    if workspace[3] == True:\n",
    "        ws_attributes = utils.get_workspace_attributes(workspace[1], workspace[0])\n",
    "        params[\"phs_id\"] = ws_attributes[\"attributes\"][\"phs_id\"] if ws_attributes[\"attributes\"].get(\"phs_id\") else \"\"\n",
    "        auth_list = ws_attributes[\"authorizationDomain\"] if ws_attributes.get(\"authorizationDomain\") else []\n",
    "        params[\"auth_domains\"] = [x[\"membersGroupName\"] for x in auth_list]\n",
    "        params[\"consent_name\"] = ws_attributes[\"attributes\"][\"library:dataUseRestriction\"] if ws_attributes[\"attributes\"].get(\"library:dataUseRestriction\") else \"\"\n",
    "        params[\"data_files_src_bucket\"] = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "        params[\"data_file_refs\"] = data_file_refs_dict\n",
    "        dataset_id, pipeline_results = utils.run_el_pipeline(workspace, params)\n",
    "        el_results_dict[workspace[0]] = {}\n",
    "        el_results_dict[workspace[0]][\"dataset_id\"] = dataset_id\n",
    "        el_results_dict[workspace[0]][\"pipeline_results\"] = pipeline_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Development\n",
    "Work through the following steps for each dataset that needs to be processed through the transformation pipeline in Step 4. Note that you can use the logs or results_dict from the previous step to retrieve the dataset_id values of interest, or retrieve them directly from TDR via the UI or Swagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset for mapping development\n",
    "dataset_id = \"1234\"\n",
    "print(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Missing Relationships to TDR Dataset Schema\n",
    "Relationships are needed by the mapping query constructor to build appropriate joins between tables. If no joins are required between tables, this step is unnecessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record relationships to potentially add to source dataset\n",
    "potential_relationships = [\n",
    "    [\"ws_subject.family_id\", \"ws_family.family_id\"],\n",
    "    [\"ws_sample.subject_id\", \"ws_subject.subject_id\"],\n",
    "    [\"ws_sequencing.sample_id\", \"ws_sample.sample_id\"],\n",
    "]\n",
    "\n",
    "# Retrieve source schema\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "config = data_repo_client.Configuration()\n",
    "config.host = \"https://data.terra.bio\"\n",
    "config.access_token = creds.token\n",
    "api_client = data_repo_client.ApiClient(configuration=config)\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "jobs_api = data_repo_client.JobsApi(api_client=api_client)\n",
    "tdr_utils = utils.TdrUtils(jobs_api)\n",
    "src_schema_dict = {}\n",
    "try:\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "    src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "    src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "\n",
    "# Loop through potential relationships and add those present for the source dataset\n",
    "additional_relationships = []\n",
    "for rel in potential_relationships:\n",
    "    from_table = rel[0].split(\".\")[0] \n",
    "    from_column = rel[0].split(\".\")[1]\n",
    "    to_table = rel[1].split(\".\")[0]\n",
    "    to_column = rel[1].split(\".\")[1]\n",
    "    if bmq.confirm_column_exists(src_schema_dict, from_table, from_column) and bmq.confirm_column_exists(src_schema_dict, to_table, to_column):\n",
    "        relationship_found = False\n",
    "        for rel_entry in src_schema_dict[\"relationships\"]:\n",
    "            if rel_entry[\"_from\"][\"table\"] == from_table and rel_entry[\"_from\"][\"column\"] == from_column and rel_entry[\"to\"][\"table\"] == to_table and rel_entry[\"to\"][\"column\"] == to_column:\n",
    "                relationship_found = True\n",
    "            elif rel_entry[\"_from\"][\"table\"] == to_table and rel_entry[\"_from\"][\"column\"] == to_column and rel_entry[\"to\"][\"table\"] == from_table and rel_entry[\"to\"][\"column\"] == from_column:\n",
    "                relationship_found = True\n",
    "        if not relationship_found:\n",
    "            rel_dict = {\n",
    "                \"name\": from_table + \"_\" + from_column + \"__to__\" + to_table + \"_\" + to_column,\n",
    "                \"from\": {\"table\": from_table, \"column\": from_column},\n",
    "                \"to\": {\"table\": to_table, \"column\": to_column}\n",
    "            }\n",
    "            additional_relationships.append(rel_dict)\n",
    "\n",
    "# Submit the schema update request for the TDR dataset\n",
    "if additional_relationships:\n",
    "    schema_update_request = {\n",
    "        \"description\": \"Adding relationships to support query construction.\",\n",
    "        \"changes\": {\n",
    "            \"addRelationships\": additional_relationships\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        resp = tdr_utils.wait_for_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "        print(str(resp)[0:1000])\n",
    "    except Exception as e:\n",
    "        print(\"Error running schema update: \" + str(e))\n",
    "else:\n",
    "    print(\"No additional relationships to add to schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Mapping Artifacts and Run Query Construction\n",
    "Specify the target schema (\"mapping target\") and mapping specification (\"mapping_target_spec\") you would like to use to construct transformation queries for your dataset. These transformation queries will then be dynamically constructed based on the appropriate target schema, mapping specification, and source schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired mapping specification and retrieve artifacts needed for query construction\n",
    "mapping_target = \"anvil_fss\"\n",
    "mapping_target_spec = \"cmg_ext_1\" \n",
    "\n",
    "# Retrieve source schema\n",
    "src_schema_dict = {}\n",
    "try:\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "    src_schema_dict[\"name\"] = response[\"name\"]\n",
    "    src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "    src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "    bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "    bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "#print(src_schema_dict)\n",
    "\n",
    "# Set dataset_name parameter to substitute into transform queries ($DATASET_NAME)\n",
    "dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "\n",
    "# Retrieve target schema and mapping specification\n",
    "target_schema_dict = {}\n",
    "mapping_spec = {}\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "    target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "#print(target_schema_dict)\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "    blob_string = blob.download_as_text(client=None)\n",
    "    blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "    mapping_spec = json.loads(blob_string)\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "#print(mapping_spec)\n",
    "\n",
    "# Build queries from mapping specification\n",
    "query_dict = {}\n",
    "if target_schema_dict:\n",
    "    for target_table in target_schema_dict[\"tables\"]:\n",
    "        table_name = target_table[\"name\"]\n",
    "        missing_artifacts = False\n",
    "        if src_schema_dict and mapping_spec:\n",
    "            query_dict[table_name] = bmq.build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema)\n",
    "        else:\n",
    "            missing_artifacts = True\n",
    "            query_dict[table_name] = {\"query\": \"\", \"syntax_check\": \"\"} \n",
    "    if missing_artifacts == True:\n",
    "        print(\"Source schema dictionary and/or mapping specification missing. Unable to generate queries.\")\n",
    "else:\n",
    "    print(\"Target schema dictionary missing. Unable to generate queries.\")\n",
    "query_df = pd.DataFrame.from_dict(query_dict, orient=\"index\")\n",
    "query_df.index.name = \"target_table\"\n",
    "query_df.reset_index(inplace=True)\n",
    "display(query_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Review Generated Queries and Update/Override as Necessary\n",
    "Review the queries generated in the previous step. Any queries that have not passed the syntax check need to be remedied before they can be executed. Any other queries that do not align with expectations can be overridden by either A) Updating the mapping target specification, or B) Manually overriding the query below. Option B should only ideally only be used in one-off cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update the query definition for particular target table, input the target table and query below\n",
    "target_table = \"\"\n",
    "query = \"\"\n",
    "\n",
    "# Run syntax check\n",
    "query_dict[target_table][\"query\"] = query\n",
    "query_dict[target_table][\"syntax_check\"] = bmq.run_syntax_check(query)\n",
    "print(query_dict[target_table])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Vocabulary Mapping\n",
    "For target attributes leveraging the \"VOCAB_MAP\" transformation, evaluate whether the source values have a record in the dsp-data-ingest.transform_resources.vocab_map table. If additional mappings are needed, these should be put into place before the transformation queries are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vocab mapping and display results\n",
    "df = bmq.evaluate_vocab_mapping(mapping_spec, src_schema_dict, target_schema_dict, bq_project, bq_schema)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Missing mapped_value view:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(df[df[\"mapped_value\"].isnull() & df[\"source_value\"].notnull()])\n",
    "print(\"\\n-------------------------------------------\")\n",
    "print(\"Full view:\")\n",
    "print(\"-------------------------------------------\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish Mapping Artifacts for use in \"T\" Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy target schema file to output folder for mapping target\n",
    "source_path = \"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(mapping_target)\n",
    "destination_path = \"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(mapping_target, dataset_id)\n",
    "!gsutil cp $ws_bucket/$source_path $ws_bucket/$destination_path 2> stdout\n",
    "\n",
    "# Limit query dict to valid queries, write out, and copy to output folder for mapping target\n",
    "valid_query_dict = {}\n",
    "for target, val in query_dict.items():\n",
    "    if val[\"syntax_check\"] == \"Passed\":\n",
    "        valid_query_dict[target] = val\n",
    "final_query_dict = {\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"transforms\": valid_query_dict\n",
    "}\n",
    "query_dict_json = json.dumps(final_query_dict)\n",
    "query_output_file = \"transform_query_set.json\"\n",
    "with open(query_output_file, 'w') as outfile:\n",
    "    outfile.write(query_dict_json)\n",
    "destination_path = \"ingest_pipeline/output/transformed/{}/{}/queries\".format(mapping_target, dataset_id)\n",
    "!gsutil cp $query_output_file $ws_bucket/$destination_path/ 2> stdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"T\" Pipeline: Load Additional Transformed Tables to TDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Run Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Variables\n",
    "dataset_id_run_list = [\n",
    "    #[\"dataset_id\", Run (True/False)],\n",
    "    [\"1234\", True],\n",
    "    [\"5678\", True]\n",
    "]\n",
    "params[\"mapping_target\"] = \"anvil_fss\"\n",
    "params[\"skip_transforms\"] = False\n",
    "params[\"transform_list_override\"] = [] # Leave empty to run transforms for all files, otherwise populate with target table names \n",
    "params[\"skip_schema_extension\"] = False\n",
    "params[\"skip_ingests\"] = False\n",
    "params[\"ingest_list_override\"] = [] # Leave empty to run ingests for all files, otherwise populate with target table names\n",
    "params[\"skip_snapshot_creation\"] = False\n",
    "params[\"snapshot_readers_list\"] = [\"anvil_tdr_ingest@firecloud.org\", \"azul-anvil-prod@firecloud.org\"] # Auth domain groups are also added as readers automatically\n",
    "params[\"skip_data_validation\"] = False\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Print variables\n",
    "print(\"Pipeline run variables set:\")\n",
    "print(\"Profile ID: \" + params[\"profile_id\"])\n",
    "print(\"Mapping Target: \" + params[\"mapping_target\"])\n",
    "print(\"Datasets to run: \")\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "config = data_repo_client.Configuration()\n",
    "config.host = \"https://data.terra.bio\"\n",
    "config.access_token = creds.token\n",
    "api_client = data_repo_client.ApiClient(configuration=config)\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "dataset_skip_list = []\n",
    "for dataset in dataset_id_run_list:\n",
    "    if dataset[1]:\n",
    "        dataset_id = dataset[0]\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "            phs_id = dataset_info[\"phs_id\"]\n",
    "            consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "            auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "            src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "            dataset_skip_list.append(dataset_id)\n",
    "        if dataset_name:\n",
    "            dataset_id = dataset[0]\n",
    "            print(\"\\t- \" + dataset_name + \" ({})\".format(dataset_id))\n",
    "            print(\"\\t\\t- PHS ID = \" + phs_id)\n",
    "            print(\"\\t\\t- Consent Short Name = \" + consent_name)\n",
    "            print(\"\\t\\t- Auth Domains = \" + str(auth_domains))\n",
    "            print(\"\\t\\t- Source Workspaces = \" + str(src_workspaces))\n",
    "if dataset_skip_list:\n",
    "    print(\"Datasets to skip (they either don't exist or aren't accessible to the current user): \")\n",
    "    print(\"\\t- \" + \"\\n\\t- \".join(dataset_skip_list)) \n",
    "print(\"Skip transforms? \" + str(params[\"skip_transforms\"]))\n",
    "print(\"Skip schema extension? \" + str(params[\"skip_schema_extension\"]))\n",
    "print(\"Skip ingests? \" + str(params[\"skip_ingests\"]))\n",
    "print(\"Skip snapshot creation? \" + str(params[\"skip_snapshot_creation\"]))\n",
    "print(\"Skip data validation? \" + str(params[\"skip_data_validation\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and execute pipeline for listed workspaces\n",
    "t_results_dict = {}\n",
    "for dataset in dataset_id_run_list:\n",
    "    if dataset[1]:\n",
    "        dataset_id = dataset[0]\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "            dataset_name = dataset_info[\"name\"]\n",
    "            phs_id = dataset_info[\"phs_id\"]\n",
    "            consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "            auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "            src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "        except:\n",
    "            dataset_name = \"\"\n",
    "        if dataset_name:\n",
    "            params[\"dataset_id\"] = dataset_id\n",
    "            params[\"dataset_name\"] = dataset_name\n",
    "            params[\"phs_id\"] = phs_id\n",
    "            params[\"consent_name\"] = consent_name\n",
    "            params[\"auth_domains\"] = auth_domains\n",
    "            t_results_dict[dataset_id] = {}\n",
    "            t_results_dict[dataset_id][\"pipeline_results\"] = utils.run_t_pipeline(params)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Scripts\n",
    "Uncomment sections as necessary to accomplish various miscellaneous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Storage Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name, mapping target, and target dataset_id of the source workspace whose table data files should be removed \n",
    "workspace_name = \"ANVIL_CMG_UWASH_DS-BDIS\"\n",
    "mapping_target = \"anvil_fss\"\n",
    "dataset_id = \"103c17df-ae7f-4f25-8441-a5eafaebdf01\"\n",
    "\n",
    "# gsutil commands to remove dataset table data and data file inventories where they live\n",
    "!gsutil -m rm -r $ws_bucket/ingest_pipeline/input/$workspace_name\n",
    "!gsutil -m rm -r $ws_bucket/ingest_pipeline/output/source/$workspace_name/table_data\n",
    "!gsutil -m rm -r $ws_bucket/ingest_pipeline/output/transformed/$mapping_target/$dataset_id/table_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDR Dataset and/or Snapshot Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Google creds and establish TDR clients\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "config = data_repo_client.Configuration()\n",
    "config.host = \"https://data.terra.bio\"\n",
    "config.access_token = creds.token\n",
    "api_client = data_repo_client.ApiClient(configuration=config)\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "jobs_api = data_repo_client.JobsApi(api_client=api_client)\n",
    "snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "tdr_utils = utils.TdrUtils(jobs_api)\n",
    "\n",
    "# Function to delete a specific TDR Snapshot\n",
    "def delete_snapshot(snapshot_id):\n",
    "    print(\"Attempting to delete snapshot = {}\".format(snapshot_id))\n",
    "    try:\n",
    "        delete_snapshot_result, job_id = tdr_utils.wait_for_job(snapshots_api.delete_snapshot(id=snapshot_id))\n",
    "        print(\"Result: {}\".format(delete_snapshot_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset\n",
    "def delete_dataset(dataset_id):\n",
    "    print(\"Attempting to delete dataset = {}\".format(dataset_id))\n",
    "    try:\n",
    "        delete_dataset_result, job_id = tdr_utils.wait_for_job(datasets_api.delete_dataset(id=dataset_id))\n",
    "        print(\"Result: {}\".format(delete_dataset_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset and all of its Snapshots\n",
    "def delete_dataset_and_all_snapshots(dataset_id):\n",
    "    print(\"Attempting to delete dataset = {} and all associated snapshots\".format(dataset_id))\n",
    "    dataset_id_list = [dataset_id]\n",
    "    # Delete snapshots\n",
    "    snapshot_list = snapshots_api.enumerate_snapshots(dataset_ids=dataset_id_list)\n",
    "    if snapshot_list.items:\n",
    "        for snapshot in snapshot_list.items:\n",
    "            snapshot_id = str(snapshot.id)\n",
    "            delete_snapshot(snapshot_id)\n",
    "    # Delete dataset\n",
    "    delete_dataset(dataset_id)\n",
    "\n",
    "# Delete a snapshot\n",
    "# snapshot_id = \"ce7f4b37-ce00-45d2-bc31-82eac2148d5f\"\n",
    "# delete_snapshot(snapshot_id)\n",
    "\n",
    "# Delete a dataset\n",
    "# dataset_id = \"26512be2-9ac0-4852-ac82-85fdc32bffea\"\n",
    "# delete_dataset(dataset_id)\n",
    "\n",
    "# Delete a dataset and all associated snapshots\n",
    "dataset_id = \"a440f22b-3e60-43da-ba6f-eb0c57b6160c\"\n",
    "delete_dataset_and_all_snapshots(dataset_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
