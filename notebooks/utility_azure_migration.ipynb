{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import data_repo_client\n",
    "import google.auth\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from time import sleep\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Set up logging\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "# Function to refresh TDR API client\n",
    "def refresh_tdr_api_client(host):\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    config = data_repo_client.Configuration()\n",
    "    config.host = host\n",
    "    config.access_token = creds.token\n",
    "    api_client = data_repo_client.ApiClient(configuration=config)\n",
    "    api_client.client_side_validation = False\n",
    "    return api_client\n",
    "\n",
    "# Function to wait for TDR job completion\n",
    "def wait_for_tdr_job(job_model, host):\n",
    "    result = job_model\n",
    "    print(\"TDR Job ID: \" + job_model.id)\n",
    "    counter = 0\n",
    "    job_state = \"UNKNOWN\"\n",
    "    while True:\n",
    "        # Re-establish credentials and API clients every 30 minutes\n",
    "        if counter == 0 or counter%180 == 0:\n",
    "            api_client = refresh_tdr_api_client(host)\n",
    "            jobs_api = data_repo_client.JobsApi(api_client=api_client)\n",
    "        # Check for TDR connectivity issues and raise exception if the issue persists\n",
    "        conn_err_counter = 0\n",
    "        while job_state == \"UNKNOWN\":\n",
    "            conn_err_counter += 1\n",
    "            if conn_err_counter >= 10:\n",
    "                raise Exception(\"Error interacting with TDR: {}\".format(result.status_code)) \n",
    "            elif result == None or result.status_code in [\"500\", \"502\", \"503\", \"504\"]:\n",
    "                sleep(10)\n",
    "                counter += 1\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        result = jobs_api.retrieve_job(job_model.id)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 5:\n",
    "                            attempt_counter += 1\n",
    "                            sleep(10)\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "            else:\n",
    "                job_state = \"KNOWN\"\n",
    "        # Check if job is still running, and sleep/re-check if so\n",
    "        if job_state == \"KNOWN\" and result.job_status == \"running\":\n",
    "            sleep(10)\n",
    "            counter += 1\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    result = jobs_api.retrieve_job(job_model.id)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 5:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "        # If job has returned as failed, confirm this is the correct state and retrieve result if so\n",
    "        elif job_state == \"KNOWN\" and result.job_status == \"failed\":\n",
    "            fail_counter = 0\n",
    "            while True:\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        result = jobs_api.retrieve_job(job_model.id)\n",
    "                        if result.job_status == \"failed\":\n",
    "                            fail_counter += 1\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 5:\n",
    "                            sleep(10)\n",
    "                            attempt_counter += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "                if fail_counter >= 3:\n",
    "                    try:\n",
    "                        fail_result = jobs_api.retrieve_job_result(job_model.id)\n",
    "                        raise Exception(\"Job \" + job_model.id + \" failed: \" + fail_result)\n",
    "                    except Exception as e:\n",
    "                        raise Exception(\"Job \" + job_model.id + \" failed: \" + str(e))\n",
    "        # If a job has returned as succeeded, retrieve result\n",
    "        elif job_state == \"KNOWN\" and result.job_status == \"succeeded\":\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return jobs_api.retrieve_job_result(job_model.id), job_model.id\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 3:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        return \"Job succeeded, but error retrieving job result: {}\".format(str(e)), job_model.id\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized job state: {}\".format(result.job_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Step 1: Pre-Connector Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     5,
     14,
     133
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/06/2023 01:21:08 PM - INFO: Retrieving original dataset details from prod environment. UUID:  b12fb9be-2ce0-4bfd-8503-732fabba06ab\n",
      "12/06/2023 01:21:08 PM - INFO: Submitting dataset creation request.\n",
      "TDR Job ID: bdvEsOcVTBiS6028orP7jQ\n",
      "12/06/2023 01:22:19 PM - INFO: Dataset Creation succeeded: {'id': '744c85cc-13d2-4f90-9d2e-d3143cb01edb', 'name': 'ANVIL_1000G_high_coverage_2019_20231206', 'description': 'TDR Dataset for 1000G-high-coverage-2019\\n\\nCopy of dataset ANVIL_1000G_high_coverage_2019_20230517 from TDR prod.', 'defaultProfileId': '9ee23bed-b46c-4561-9103-d2a723113f7f', 'createdDate': '2023-12-06T13:22:05.650170Z', 'storage': [{'region': 'southcentralus', 'cloudResource': 'application_deployment', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'storage_account', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'synapse_workspace', 'cloudPlatform': 'azure'}], 'secureMonitoringEnabled': False, 'cloudPlatform': 'azure', 'dataProject': None, 'storageAccount': 'tdrcphagvxlmchocrlirvkzp', 'phsId': '', 'selfHosted': False, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None}}\n",
      "12/06/2023 01:22:19 PM - INFO: Preparing target BQ table (broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list).\n",
      "12/06/2023 01:22:21 PM - INFO: Fetching all rows from table 'file_inventory' in the original dataset (b12fb9be-2ce0-4bfd-8503-732fabba06ab). BQ Project = 'datarepo-79351e6c' and BQ Dataset = 'datarepo_ANVIL_1000G_high_coverage_2019_20230517'.\n",
      "12/06/2023 01:22:25 PM - INFO: Retrieving original dataset details from prod environment. UUID:  34c7cf76-024b-4711-baaa-5d21bf061ed2\n",
      "12/06/2023 01:22:25 PM - INFO: Submitting dataset creation request.\n",
      "TDR Job ID: KVQjo1VgSnOETsOqigIWYg\n",
      "12/06/2023 01:22:45 PM - INFO: Dataset Creation succeeded: {'id': 'da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'name': 'ANVIL_GTEx_public_data_20231206', 'description': 'TDR Dataset for AnVIL_GTEx_public_data\\n\\nCopy of dataset ANVIL_GTEx_public_data_20221115 from TDR prod.', 'defaultProfileId': '9ee23bed-b46c-4561-9103-d2a723113f7f', 'createdDate': '2023-12-06T13:22:27.962189Z', 'storage': [{'region': 'southcentralus', 'cloudResource': 'application_deployment', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'storage_account', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'synapse_workspace', 'cloudPlatform': 'azure'}], 'secureMonitoringEnabled': False, 'cloudPlatform': 'azure', 'dataProject': None, 'storageAccount': 'tdrcphagvxlmchocrlirvkzp', 'phsId': '', 'selfHosted': False, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None}}\n",
      "12/06/2023 01:22:45 PM - INFO: Preparing target BQ table (broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list).\n",
      "12/06/2023 01:22:48 PM - INFO: Fetching all rows from table 'file_inventory' in the original dataset (34c7cf76-024b-4711-baaa-5d21bf061ed2). BQ Project = 'datarepo-be60e9c7' and BQ Dataset = 'datarepo_ANVIL_GTEx_public_data_20221115'.\n",
      "12/06/2023 01:22:51 PM - INFO: Retrieving original dataset details from prod environment. UUID:  595b6755-e7ae-4e83-af2e-693c089aeec3\n",
      "12/06/2023 01:22:51 PM - INFO: Submitting dataset creation request.\n",
      "TDR Job ID: RlmvdwHKTXC6JMnswVMNIg\n",
      "12/06/2023 01:23:11 PM - INFO: Dataset Creation succeeded: {'id': '9fe49126-f4ac-4e46-a231-99820fb0d4c2', 'name': 'ANVIL_T2T_20231206', 'description': 'TDR Dataset for AnVIL_T2T\\n\\nCopy of dataset ANVIL_T2T_20230714 from TDR prod.', 'defaultProfileId': '9ee23bed-b46c-4561-9103-d2a723113f7f', 'createdDate': '2023-12-06T13:22:53.715393Z', 'storage': [{'region': 'southcentralus', 'cloudResource': 'application_deployment', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'storage_account', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'synapse_workspace', 'cloudPlatform': 'azure'}], 'secureMonitoringEnabled': False, 'cloudPlatform': 'azure', 'dataProject': None, 'storageAccount': 'tdrcphagvxlmchocrlirvkzp', 'phsId': '', 'selfHosted': False, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None}}\n",
      "12/06/2023 01:23:11 PM - INFO: Preparing target BQ table (broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list).\n",
      "12/06/2023 01:23:14 PM - INFO: Fetching all rows from table 'file_inventory' in the original dataset (595b6755-e7ae-4e83-af2e-693c089aeec3). BQ Project = 'datarepo-9e80499e' and BQ Dataset = 'datarepo_ANVIL_T2T_20230714'.\n",
      "12/06/2023 01:23:20 PM - INFO: Retrieving original dataset details from prod environment. UUID:  41812f8d-65b4-4bcc-a674-651661f3c70e\n",
      "12/06/2023 01:23:20 PM - INFO: Submitting dataset creation request.\n",
      "TDR Job ID: s4nmw_wPQcOmyp_MoS6Ukg\n",
      "12/06/2023 01:23:40 PM - INFO: Dataset Creation succeeded: {'id': '2b594d29-5d28-4037-a59e-214ccd387c73', 'name': 'ANVIL_ccdg_asc_ndd_daly_talkowski_CDCSEED_asd_exome_20231206', 'description': 'TDR Dataset for AnVIL_ccdg_asc_ndd_daly_talkowski_CDCSEED_asd_exome\\n\\nCopy of dataset ANVIL_ccdg_asc_ndd_daly_talkowski_CDCSEED_asd_exome_20221024 from TDR prod.', 'defaultProfileId': '9ee23bed-b46c-4561-9103-d2a723113f7f', 'createdDate': '2023-12-06T13:23:24.964578Z', 'storage': [{'region': 'southcentralus', 'cloudResource': 'application_deployment', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'storage_account', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'synapse_workspace', 'cloudPlatform': 'azure'}], 'secureMonitoringEnabled': True, 'cloudPlatform': 'azure', 'dataProject': None, 'storageAccount': 'tdrwefwykpmvobbjtdanzdur', 'phsId': 'phs000298', 'selfHosted': False, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None}}\n",
      "12/06/2023 01:23:40 PM - INFO: Preparing target BQ table (broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list).\n",
      "12/06/2023 01:23:43 PM - INFO: Fetching all rows from table 'file_inventory' in the original dataset (41812f8d-65b4-4bcc-a674-651661f3c70e). BQ Project = 'datarepo-134e1a05' and BQ Dataset = 'datarepo_ANVIL_ccdg_asc_ndd_daly_talkowski_CDCSEED_asd_exome_20221024'.\n",
      "12/06/2023 01:23:45 PM - INFO: Retrieving original dataset details from prod environment. UUID:  352a503b-41eb-4a84-b257-68d70e55337e\n",
      "12/06/2023 01:23:45 PM - INFO: Submitting dataset creation request.\n",
      "TDR Job ID: UdD0ISqmSjKgZaUCggMp0Q\n",
      "12/06/2023 01:24:06 PM - INFO: Dataset Creation succeeded: {'id': 'abfd61f2-9d46-4efe-81f2-0b608894dc60', 'name': 'ANVIL_CSER_KidsCanSeq_GRU_20231206', 'description': 'TDR Dataset for AnVIL_CSER_KidsCanSeq_GRU\\n\\nCopy of dataset ANVIL_CSER_KidsCanSeq_GRU_20221208 from TDR prod.', 'defaultProfileId': '9ee23bed-b46c-4561-9103-d2a723113f7f', 'createdDate': '2023-12-06T13:23:49.753941Z', 'storage': [{'region': 'southcentralus', 'cloudResource': 'application_deployment', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'storage_account', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'synapse_workspace', 'cloudPlatform': 'azure'}], 'secureMonitoringEnabled': True, 'cloudPlatform': 'azure', 'dataProject': None, 'storageAccount': 'tdrwefwykpmvobbjtdanzdur', 'phsId': 'phs002378', 'selfHosted': False, 'predictableFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None}}\n",
      "12/06/2023 01:24:06 PM - INFO: Preparing target BQ table (broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list).\n",
      "12/06/2023 01:24:08 PM - INFO: Fetching all rows from table 'file_inventory' in the original dataset (352a503b-41eb-4a84-b257-68d70e55337e). BQ Project = 'datarepo-0d47e8d1' and BQ Dataset = 'datarepo_ANVIL_CSER_KidsCanSeq_GRU_20221208'.\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to build default target TDR dataset name\n",
    "def format_dataset_name(input_str):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "    input_str = input_str[:-9]\n",
    "    output_str = \"ANVIL_\" + re.sub(\"^ANVIL[_]?\", \"\", input_str, flags=re.IGNORECASE) + \"_\" + current_date_string\n",
    "    output_str = re.sub(\"[^a-zA-Z0-9_]\", \"_\", output_str)\n",
    "    return output_str\n",
    "\n",
    "# Function to create a new TDR dataset from an existing TDR dataset\n",
    "def create_dataset_from_dataset(src_tdr_object_uuid, billing_profile):\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Retrieve original dataset details\n",
    "    logging.info(f\"Retrieving original dataset details from prod environment. UUID:  {src_tdr_object_uuid}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=src_tdr_object_uuid, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from {src_tdr_object_type} {src_tdr_object_uuid} in TDR {src_tdr_object_env} environment: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        return None, None, None\n",
    "\n",
    "    # Build new dataset schema\n",
    "    apply_anvil_transforms = True\n",
    "    new_schema_dict = {\"tables\": [], \"relationships\": [], \"assets\": []}\n",
    "    for table_entry in dataset_details[\"schema\"][\"tables\"]:\n",
    "        int_table_dict = table_entry.copy()\n",
    "        int_table_dict[\"primaryKey\"] = int_table_dict.pop(\"primary_key\")\n",
    "        for key in [\"partition_mode\", \"date_partition_options\", \"int_partition_options\", \"row_count\"]:\n",
    "            del int_table_dict[key]\n",
    "        for idx, column_entry in enumerate(table_entry[\"columns\"]):\n",
    "            if column_entry[\"datatype\"] == \"integer\":\n",
    "                table_entry[\"columns\"][idx][\"datatype\"] = \"int64\"\n",
    "        if apply_anvil_transforms:\n",
    "            if table_entry[\"name\"] == \"file_inventory\":\n",
    "                int_table_dict[\"columns\"].append({\"name\": \"orig_file_ref\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "                int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "            elif \"anvil_\" not in table_entry[\"name\"]:\n",
    "                int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "        new_schema_dict[\"tables\"].append(int_table_dict)\n",
    "    for rel_entry in dataset_details[\"schema\"][\"relationships\"]:\n",
    "        int_rel_dict = rel_entry.copy()\n",
    "        int_rel_dict[\"from\"] = int_rel_dict.pop(\"_from\")\n",
    "        new_schema_dict[\"relationships\"].append(int_rel_dict)\n",
    "    for asset_entry in dataset_details[\"schema\"][\"assets\"]:\n",
    "        int_asset_dict = asset_entry.copy()\n",
    "        int_asset_dict[\"rootTable\"] = int_asset_dict.pop(\"root_table\")\n",
    "        int_asset_dict[\"rootColumn\"] = int_asset_dict.pop(\"root_column\")\n",
    "        new_schema_dict[\"assets\"].append(int_asset_dict)\n",
    "\n",
    "    # Retrieve original dataset policies\n",
    "    try:\n",
    "        dataset_policies = datasets_api.retrieve_dataset_policies(id=src_tdr_object_uuid).to_dict()\n",
    "        for policy in dataset_policies[\"policies\"]:\n",
    "            if policy[\"name\"] == \"steward\":\n",
    "                stewards_list = policy[\"members\"]\n",
    "            elif policy[\"name\"] == \"custodian\":\n",
    "                custodians_list = policy[\"members\"]\n",
    "            elif policy[\"name\"] == \"snapshot_creator\":\n",
    "                snapshot_creators_list = policy[\"members\"]\n",
    "    except:\n",
    "        logging.info(\"Error retrieving original dataset policies. Skipping policy copy.\")\n",
    "        stewards_list = []\n",
    "        custodians_list = []\n",
    "        snapshot_creators_list = []\n",
    "    policies = {\n",
    "        \"stewards\": stewards_list,\n",
    "        \"custodians\": custodians_list,\n",
    "        \"snapshotCreators\": snapshot_creators_list\n",
    "    }\n",
    "\n",
    "    # Determine dataset properties\n",
    "    orig_object_name = dataset_details[\"name\"]\n",
    "    new_object_name = format_dataset_name(orig_object_name)\n",
    "    new_description = dataset_details[\"description\"] + f\"\\n\\nCopy of dataset {orig_object_name} from TDR prod.\"\n",
    "    self_hosted = False\n",
    "    dedicated_ingest_sa = False\n",
    "    phs_id = dataset_details[\"phs_id\"]\n",
    "    predictable_file_ids = dataset_details[\"predictable_file_ids\"]\n",
    "    secure_monitoring_enabled = dataset_details[\"secure_monitoring_enabled\"]\n",
    "    properties = dataset_details[\"properties\"]\n",
    "    tags = dataset_details[\"tags\"]\n",
    "\n",
    "    # Create new TDR dataset\n",
    "    logging.info(\"Submitting dataset creation request.\")\n",
    "    dataset_request = {\n",
    "        \"name\": new_object_name,\n",
    "        \"description\": new_description,\n",
    "        \"defaultProfileId\": billing_profile,\n",
    "        \"cloudPlatform\": \"azure\",\n",
    "        \"region\": \"southcentralus\",\n",
    "        \"phsId\": phs_id,\n",
    "        \"experimentalSelfHosted\": self_hosted,\n",
    "        \"experimentalPredictableFileIds\": predictable_file_ids,\n",
    "        \"dedicatedIngestServiceAccount\": dedicated_ingest_sa,\n",
    "        \"enableSecureMonitoring\": secure_monitoring_enabled,\n",
    "        \"properties\": properties,\n",
    "        \"tags\": tags,\n",
    "        \"policies\": policies,\n",
    "        \"schema\": new_schema_dict\n",
    "    }\n",
    "    attempt_counter = 1\n",
    "    while True:\n",
    "        try:\n",
    "            create_dataset_result, job_id = wait_for_tdr_job(datasets_api.create_dataset(dataset=dataset_request), \"https://data.terra.bio\")\n",
    "            logging.info(\"Dataset Creation succeeded: {}\".format(create_dataset_result))\n",
    "            new_dataset_id = create_dataset_result[\"id\"]\n",
    "            break\n",
    "        except Exception as e:\n",
    "            error_str = f\"Error on Dataset Creation: {str(e)}\"\n",
    "            logging.error(error_str)\n",
    "            if attempt_counter < 3:\n",
    "                logging.info(\"Retrying Dataset Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Exiting job.\")\n",
    "                return None, None, None\n",
    "        \n",
    "    # Exit function\n",
    "    return new_dataset_id, bq_project, bq_dataset\n",
    "\n",
    "# Function to create file transfer details\n",
    "def output_file_details(orig_dataset_id, new_dataset_id, bq_project, bq_dataset, target_bigquery_table):\n",
    "    \n",
    "    # Setup/refresh TDR clients (and BQ client)\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Clear records from target BQ table\n",
    "    logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "    delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE gcp_dataset_id = '{orig_dataset_id}'\"\"\"\n",
    "    try:\n",
    "        delete_query_job = client.query(delete_query)\n",
    "        delete_query_job.result()\n",
    "    except Exception as e:\n",
    "        logging.info(\"Error deleting records for the original dataset from the target BQ table.\")\n",
    "    \n",
    "    # Retrieve table data from the original dataset and write to target BQ table\n",
    "    logging.info(f\"Fetching all rows from table 'file_inventory' in the original dataset ({orig_dataset_id}). BQ Project = '{bq_project}' and BQ Dataset = '{bq_dataset}'.\")\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.destination = target_bigquery_table\n",
    "    job_config.write_disposition = \"WRITE_APPEND\"\n",
    "    query = f\"\"\"WITH drlh_deduped AS\n",
    "                        (\n",
    "                          SELECT DISTINCT file_id, target_path, source_name\n",
    "                          FROM `{bq_project}.{bq_dataset}.datarepo_load_history`\n",
    "                          WHERE state = \"succeeded\" \n",
    "                        )\n",
    "                        SELECT '{orig_dataset_id}' AS gcp_dataset_id, '{new_dataset_id}' AS az_dataset_id, source_name AS source_path, target_path, size_in_bytes, md5_hash, file_ref AS orig_tdr_file_id\n",
    "                        FROM `{bq_project}.{bq_dataset}.file_inventory` a\n",
    "                            LEFT JOIN drlh_deduped b\n",
    "                            ON a.uri = b.source_name\"\"\"\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            query_job = client.query(query, job_config=job_config)\n",
    "            query_job.result()\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error retrieving records for all rows of table 'file_inventory': {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                return\n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list\"\n",
    "azure_billing_profile = \"9ee23bed-b46c-4561-9103-d2a723113f7f\"\n",
    "dataset_id_list = [\n",
    "    'b12fb9be-2ce0-4bfd-8503-732fabba06ab',\n",
    "    '34c7cf76-024b-4711-baaa-5d21bf061ed2',\n",
    "    '595b6755-e7ae-4e83-af2e-693c089aeec3',\n",
    "    '41812f8d-65b4-4bcc-a674-651661f3c70e',\n",
    "    '352a503b-41eb-4a84-b257-68d70e55337e',\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "for dataset_id in dataset_id_list:\n",
    "    new_dataset_id, bq_project, bq_dataset = create_dataset_from_dataset(dataset_id, azure_billing_profile)\n",
    "    if new_dataset_id:\n",
    "        output_file_details(dataset_id, new_dataset_id, bq_project, bq_dataset, target_bigquery_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Post-Connector Processing (Migration Tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     97,
     101,
     132,
     161,
     273,
     356,
     543,
     802,
     1007,
     1188
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/18/2023 08:34:20 PM - INFO: Processing data from TDR dataset 34c7cf76-024b-4711-baaa-5d21bf061ed2 to TDR dataset da747f0d-950a-4cef-8fae-7e9bc3d92f67.\n",
      "12/18/2023 08:34:20 PM - INFO: Retrieving new dataset details from prod environment. UUID:  da747f0d-950a-4cef-8fae-7e9bc3d92f67\n",
      "12/18/2023 08:34:22 PM - INFO: Starting TDR object migration pipeline.\n",
      "12/18/2023 08:34:22 PM - INFO: Validating input parameters.\n",
      "12/18/2023 08:34:22 PM - WARNING: Parameter snapshot.recreate_snapshot set to false due to source.tdr_object_type not being 'snapshot'.\n",
      "12/18/2023 08:34:22 PM - INFO: Default region for AnVIL datasets on Azure is 'southcentralus', so setting the target TDR dataset region to this value.\n",
      "12/18/2023 08:34:22 PM - INFO: Dedicated dataset-specific SAs not currently supported for TDR-to-TDR ingestions, so setting 'dedicatedIngestServiceAccount' dataset property to False by default.\n",
      "12/18/2023 08:34:22 PM - INFO: Self-hosted functionality not available for Azure datasets, so setting 'experimentalSelfHosted' dataset property to False by default.\n",
      "12/18/2023 08:34:22 PM - INFO: Starting Dataset Creation step.\n",
      "12/18/2023 08:34:22 PM - INFO: Retrieving original dataset details from prod environment. UUID:  34c7cf76-024b-4711-baaa-5d21bf061ed2\n",
      "12/18/2023 08:34:22 PM - INFO: Attempting to leverage user-provided target dataset UUID (da747f0d-950a-4cef-8fae-7e9bc3d92f67) rather than creating a new dataset.\n",
      "12/18/2023 08:34:22 PM - INFO: Starting Dataset Ingestion step.\n",
      "12/18/2023 08:34:22 PM - INFO: Adding TDR SA to original dataset: 34c7cf76-024b-4711-baaa-5d21bf061ed2\n",
      "12/18/2023 08:34:22 PM - INFO: TDR SA to add: datarepo-jade-api@terra-datarepo-production.iam.gserviceaccount.com\n",
      "12/18/2023 08:34:23 PM - INFO: TDR SA added successfully.\n",
      "12/18/2023 08:34:23 PM - INFO: Processing dataset ingestion requests.\n",
      "12/18/2023 08:34:23 PM - INFO: Processing dataset ingestion for table 'file_inventory'.\n",
      "12/18/2023 08:34:24 PM - INFO: Table 'file_inventory' contains fileref columns. Will use a chunk size of 50000 rows per ingestion request, to keep the number of file references per chunk below 50000.\n",
      "12/18/2023 08:34:24 PM - INFO: Fetching rows 1-81 from table 'file_inventory' in the original dataset (34c7cf76-024b-4711-baaa-5d21bf061ed2).\n",
      "12/18/2023 08:34:27 PM - INFO: Submitting ingestion request to new dataset (da747f0d-950a-4cef-8fae-7e9bc3d92f67).\n",
      "TDR Job ID: th8p-FmOQLeX13NBzIoACg\n",
      "12/18/2023 08:35:08 PM - INFO: Ingest succeeded: {'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'dataset': 'ANVIL_GTEx_public_data_20231206', 'table': 'file_inventory', 'path': None, 'load_tag': 'Ingest for da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'row_count': 81, 'bad_row_count': 0, 'load_result': None}\n",
      "12/18/2023 08:35:11 PM - INFO: Validating table 'file_inventory' in new dataset vs. original dataset.\n",
      "12/18/2023 08:35:13 PM - INFO: Processing dataset ingestion for table 'workspace_attributes'.\n",
      "12/18/2023 08:35:14 PM - INFO: Table 'workspace_attributes' does not contain fileref columns. Will use a chunk size of 50000 rows per ingestion request.\n",
      "12/18/2023 08:35:15 PM - INFO: Fetching rows 1-37 from table 'workspace_attributes' in the original dataset (34c7cf76-024b-4711-baaa-5d21bf061ed2).\n",
      "12/18/2023 08:35:17 PM - INFO: Submitting ingestion request to new dataset (da747f0d-950a-4cef-8fae-7e9bc3d92f67).\n",
      "TDR Job ID: MIYAazVCSiyeMh2MyusjDw\n",
      "12/18/2023 08:35:39 PM - INFO: Ingest succeeded: {'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'dataset': 'ANVIL_GTEx_public_data_20231206', 'table': 'workspace_attributes', 'path': None, 'load_tag': 'Ingest for da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'row_count': 37, 'bad_row_count': 0, 'load_result': None}\n",
      "12/18/2023 08:35:41 PM - INFO: Validating table 'workspace_attributes' in new dataset vs. original dataset.\n",
      "12/18/2023 08:35:41 PM - INFO: Fetching ingested records and building datarepo_row_id lookup for use in AnVIL transforms.\n",
      "12/18/2023 08:35:43 PM - INFO: Processing dataset ingestion for table 'anvil_activity'.\n",
      "12/18/2023 08:35:46 PM - INFO: No records found for table in original dataset. Continuing to next table/record set.\n",
      "12/18/2023 08:35:46 PM - INFO: Processing dataset ingestion for table 'anvil_alignmentactivity'.\n",
      "12/18/2023 08:35:48 PM - INFO: No records found for table in original dataset. Continuing to next table/record set.\n",
      "12/18/2023 08:35:48 PM - INFO: Processing dataset ingestion for table 'anvil_antibody'.\n",
      "12/18/2023 08:35:49 PM - INFO: No records found for table in original dataset. Continuing to next table/record set.\n",
      "12/18/2023 08:35:49 PM - INFO: Processing dataset ingestion for table 'anvil_assayactivity'.\n",
      "12/18/2023 08:35:51 PM - INFO: No records found for table in original dataset. Continuing to next table/record set.\n",
      "12/18/2023 08:35:51 PM - INFO: Processing dataset ingestion for table 'anvil_biosample'.\n",
      "12/18/2023 08:35:53 PM - INFO: No records found for table in original dataset. Continuing to next table/record set.\n",
      "12/18/2023 08:35:53 PM - INFO: Processing dataset ingestion for table 'anvil_dataset'.\n",
      "12/18/2023 08:35:54 PM - INFO: Table 'anvil_dataset' does not contain fileref columns. Will use a chunk size of 50000 rows per ingestion request.\n",
      "12/18/2023 08:35:54 PM - INFO: Fetching rows 1-1 from table 'anvil_dataset' in the original dataset (34c7cf76-024b-4711-baaa-5d21bf061ed2).\n",
      "12/18/2023 08:35:56 PM - INFO: FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\n",
      "12/18/2023 08:35:56 PM - INFO: Submitting ingestion request to new dataset (da747f0d-950a-4cef-8fae-7e9bc3d92f67).\n",
      "TDR Job ID: F6Ijbi9iR6isip7X6ssnvw\n",
      "12/18/2023 08:36:28 PM - INFO: Ingest succeeded: {'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'dataset': 'ANVIL_GTEx_public_data_20231206', 'table': 'anvil_dataset', 'path': None, 'load_tag': 'Ingest for da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "12/18/2023 08:36:31 PM - INFO: Validating table 'anvil_dataset' in new dataset vs. original dataset.\n",
      "12/18/2023 08:36:31 PM - INFO: Processing dataset ingestion for table 'anvil_diagnosis'.\n",
      "12/18/2023 08:36:33 PM - INFO: No records found for table in original dataset. Continuing to next table/record set.\n",
      "12/18/2023 08:36:33 PM - INFO: Processing dataset ingestion for table 'anvil_donor'.\n",
      "12/18/2023 08:36:34 PM - INFO: No records found for table in original dataset. Continuing to next table/record set.\n",
      "12/18/2023 08:36:34 PM - INFO: Processing dataset ingestion for table 'anvil_file'.\n",
      "12/18/2023 08:36:35 PM - INFO: Table 'anvil_file' contains fileref columns. Will use a chunk size of 50000 rows per ingestion request, to keep the number of file references per chunk below 50000.\n",
      "12/18/2023 08:36:36 PM - INFO: Fetching rows 1-81 from table 'anvil_file' in the original dataset (34c7cf76-024b-4711-baaa-5d21bf061ed2).\n",
      "12/18/2023 08:36:38 PM - INFO: FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\n",
      "12/18/2023 08:36:38 PM - INFO: Submitting ingestion request to new dataset (da747f0d-950a-4cef-8fae-7e9bc3d92f67).\n",
      "TDR Job ID: 7iEX_x3VRDOnZ-ReKY804w\n",
      "12/18/2023 08:37:19 PM - INFO: Ingest succeeded: {'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'dataset': 'ANVIL_GTEx_public_data_20231206', 'table': 'anvil_file', 'path': None, 'load_tag': 'Ingest for da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'row_count': 81, 'bad_row_count': 0, 'load_result': None}\n",
      "12/18/2023 08:37:22 PM - INFO: Validating table 'anvil_file' in new dataset vs. original dataset.\n",
      "12/18/2023 08:37:22 PM - INFO: Processing dataset ingestion for table 'anvil_project'.\n",
      "12/18/2023 08:37:23 PM - INFO: Table 'anvil_project' does not contain fileref columns. Will use a chunk size of 50000 rows per ingestion request.\n",
      "12/18/2023 08:37:23 PM - INFO: Fetching rows 1-1 from table 'anvil_project' in the original dataset (34c7cf76-024b-4711-baaa-5d21bf061ed2).\n",
      "12/18/2023 08:37:25 PM - INFO: FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\n",
      "12/18/2023 08:37:25 PM - INFO: Submitting ingestion request to new dataset (da747f0d-950a-4cef-8fae-7e9bc3d92f67).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDR Job ID: _ovrDUYwSmarXfkie1bMhw\n",
      "12/18/2023 08:37:57 PM - INFO: Ingest succeeded: {'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'dataset': 'ANVIL_GTEx_public_data_20231206', 'table': 'anvil_project', 'path': None, 'load_tag': 'Ingest for da747f0d-950a-4cef-8fae-7e9bc3d92f67', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "12/18/2023 08:37:59 PM - INFO: Validating table 'anvil_project' in new dataset vs. original dataset.\n",
      "12/18/2023 08:37:59 PM - INFO: Processing dataset ingestion for table 'anvil_sequencingactivity'.\n",
      "12/18/2023 08:38:01 PM - INFO: No records found for table in original dataset. Continuing to next table/record set.\n",
      "12/18/2023 08:38:01 PM - INFO: Processing dataset ingestion for table 'anvil_variantcallingactivity'.\n",
      "12/18/2023 08:38:03 PM - INFO: No records found for table in original dataset. Continuing to next table/record set.\n",
      "12/18/2023 08:38:03 PM - INFO: Migration Pipeline Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dataset Creation</td>\n",
       "      <td>Dataset Creation</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>Attempting to leverage user-provided target da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: file_inventory -- Rows: 1-81</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: file_inventory</td>\n",
       "      <td>Success</td>\n",
       "      <td>81 records found in both new and original tabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: workspace_attributes -- Rows: 1-37</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: workspace_attributes</td>\n",
       "      <td>Success</td>\n",
       "      <td>37 records found in both new and original table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_activity</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table in original dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_alignmentactivity</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table in original dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_antibody</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table in original dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_assayactivity</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table in original dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_biosample</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table in original dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_dataset -- Rows: 1-1</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: anvil_dataset</td>\n",
       "      <td>Success</td>\n",
       "      <td>1 records found in both new and original table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_diagnosis</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table in original dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_donor</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table in original dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_file -- Rows: 1-81</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: anvil_file</td>\n",
       "      <td>Success</td>\n",
       "      <td>81 records found in both new and original table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_project -- Rows: 1-1</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: anvil_project</td>\n",
       "      <td>Success</td>\n",
       "      <td>1 records found in both new and original table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_sequencingactivity</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table in original dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_variantcallingactivity</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table in original dataset...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Task                                       Step   Status  \\\n",
       "0     Dataset Creation                           Dataset Creation  Skipped   \n",
       "1    Dataset Ingestion        Table: file_inventory -- Rows: 1-81  Success   \n",
       "2   Dataset Validation                      Table: file_inventory  Success   \n",
       "3    Dataset Ingestion  Table: workspace_attributes -- Rows: 1-37  Success   \n",
       "4   Dataset Validation                Table: workspace_attributes  Success   \n",
       "5    Dataset Ingestion                      Table: anvil_activity  Skipped   \n",
       "6    Dataset Ingestion             Table: anvil_alignmentactivity  Skipped   \n",
       "7    Dataset Ingestion                      Table: anvil_antibody  Skipped   \n",
       "8    Dataset Ingestion                 Table: anvil_assayactivity  Skipped   \n",
       "9    Dataset Ingestion                     Table: anvil_biosample  Skipped   \n",
       "10   Dataset Ingestion          Table: anvil_dataset -- Rows: 1-1  Success   \n",
       "11  Dataset Validation                       Table: anvil_dataset  Success   \n",
       "12   Dataset Ingestion                     Table: anvil_diagnosis  Skipped   \n",
       "13   Dataset Ingestion                         Table: anvil_donor  Skipped   \n",
       "14   Dataset Ingestion            Table: anvil_file -- Rows: 1-81  Success   \n",
       "15  Dataset Validation                          Table: anvil_file  Success   \n",
       "16   Dataset Ingestion          Table: anvil_project -- Rows: 1-1  Success   \n",
       "17  Dataset Validation                       Table: anvil_project  Success   \n",
       "18   Dataset Ingestion            Table: anvil_sequencingactivity  Skipped   \n",
       "19   Dataset Ingestion        Table: anvil_variantcallingactivity  Skipped   \n",
       "\n",
       "                                              Message  \n",
       "0   Attempting to leverage user-provided target da...  \n",
       "1   {'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3...  \n",
       "2   81 records found in both new and original tabl...  \n",
       "3   {'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3...  \n",
       "4    37 records found in both new and original table.  \n",
       "5   No records found for table in original dataset...  \n",
       "6   No records found for table in original dataset...  \n",
       "7   No records found for table in original dataset...  \n",
       "8   No records found for table in original dataset...  \n",
       "9   No records found for table in original dataset...  \n",
       "10  {'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3...  \n",
       "11    1 records found in both new and original table.  \n",
       "12  No records found for table in original dataset...  \n",
       "13  No records found for table in original dataset...  \n",
       "14  {'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3...  \n",
       "15   81 records found in both new and original table.  \n",
       "16  {'dataset_id': 'da747f0d-950a-4cef-8fae-7e9bc3...  \n",
       "17    1 records found in both new and original table.  \n",
       "18  No records found for table in original dataset...  \n",
       "19  No records found for table in original dataset...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/18/2023 08:38:03 PM - INFO: \n",
      "Pipeline finished with 0 failures.\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to recreate snapshot, if requested\n",
    "def recreate_snapshot(config, new_dataset_id):\n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source\"][\"tdr_object_uuid\"]\n",
    "    src_tdr_object_type = config[\"source\"][\"tdr_object_type\"]\n",
    "    src_tdr_object_env = config[\"source\"][\"tdr_object_env\"]\n",
    "    tdr_host = config[\"source\"][\"tdr_host\"]\n",
    "    tar_tdr_billing_profile = config[\"target\"][\"tdr_billing_profile\"]\n",
    "    recreate_snapshot = config[\"snapshot\"][\"recreate_snapshot\"]\n",
    "    new_snapshot_name = config[\"snapshot\"][\"new_snapshot_name\"]\n",
    "    copy_snapshot_policies = config[\"snapshot\"][\"copy_snapshot_policies\"]\n",
    "    \n",
    "    if recreate_snapshot:\n",
    "        \n",
    "        # Setup/refresh TDR clients\n",
    "        api_client = refresh_tdr_api_client(tdr_host)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "        \n",
    "        # Retrieve original dataset details\n",
    "        try:\n",
    "            snapshot_details = snapshots_api.retrieve_snapshot(id=src_tdr_object_uuid, include=[\"TABLES\", \"RELATIONSHIPS\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"SOURCES\"]).to_dict()\n",
    "            original_snapshot_name = snapshot_details[\"name\"]\n",
    "        except:\n",
    "            err_str = f\"Error retrieving details from original {src_tdr_object_type} {src_tdr_object_uuid} in TDR {src_tdr_object_env} environment.\"\n",
    "            logging.error(err_str)\n",
    "            config[\"migration_results\"].append([\"Snapshot Creation\", \"Snapshot Creation\", \"Failure\", error_str])\n",
    "            return\n",
    "        \n",
    "        # Retrieve new dataset name\n",
    "        try:\n",
    "            dataset_details = datasets_api.retrieve_dataset(id=new_dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "            new_dataset_name = dataset_details[\"name\"]\n",
    "            new_description = f\"Copy of {src_tdr_object_type} {original_snapshot_name} from TDR {src_tdr_object_env}. Original description below:\\n\\n\" + snapshot_details[\"description\"]\n",
    "        except:\n",
    "            err_str = f\"Error retrieving details from new dataset {new_dataset_id} in TDR {src_tdr_object_env} environment.\"\n",
    "            logging.error(err_str)\n",
    "            config[\"migration_results\"].append([\"Snapshot Creation\", \"Snapshot Creation\", \"Failure\", error_str])\n",
    "            return\n",
    "            \n",
    "        # Set desired snapshot policies\n",
    "        snapshot_stewards_list = []\n",
    "        snapshot_readers_list = []\n",
    "        if copy_snapshot_policies:\n",
    "            try:\n",
    "                snapshot_policies = snapshots_api.retrieve_snapshot_policies(id=src_tdr_object_uuid).to_dict()\n",
    "                for policy in snapshot_policies[\"policies\"]:\n",
    "                    if policy[\"name\"] == \"steward\":\n",
    "                        snapshot_stewards_list = policy[\"members\"]\n",
    "                    elif policy[\"name\"] == \"reader\":\n",
    "                        snapshot_readers_list = policy[\"members\"]\n",
    "            except:\n",
    "                logging.warning(\"Error retrieving policies from original snapshot. Check permissions and add manually as needed.\")\n",
    "                \n",
    "        # Create and submit snapshot creation request for Azure TDR dataset\n",
    "        logging.info(\"Submitting snapshot request.\")\n",
    "        snapshot_req = {\n",
    "            \"name\": new_snapshot_name,\n",
    "            \"description\": new_description,\n",
    "            \"consentCode\": snapshot_details[\"consent_code\"],\n",
    "            \"contents\": [{\n",
    "                \"datasetName\": new_dataset_name,\n",
    "                \"mode\": \"byFullView\"\n",
    "            }],\n",
    "            \"policies\": {\n",
    "                \"stewards\": snapshot_stewards_list,\n",
    "                \"readers\": snapshot_readers_list \n",
    "            },\n",
    "            \"profileId\": tar_tdr_billing_profile,\n",
    "            \"globalFileIds\": snapshot_details[\"global_file_ids\"],\n",
    "            \"compactIdPrefix\": snapshot_details[\"compact_id_prefix\"],\n",
    "            \"properties\": snapshot_details[\"properties\"],\n",
    "            \"tags\": snapshot_details[\"tags\"]\n",
    "        }\n",
    "        attempt_counter = 1\n",
    "        while True:\n",
    "            try:\n",
    "                create_snapshot_result, job_id = wait_for_tdr_job(snapshots_api.create_snapshot(snapshot=snapshot_req), tdr_host)\n",
    "                logging.info(\"Snapshot Creation succeeded: {}\".format(create_snapshot_result))\n",
    "                config[\"migration_results\"].append([\"Snapshot Creation\", \"Snapshot Creation\", \"Success\", str(create_snapshot_result)[0:1000]])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error on Snapshot Creation: {}\".format(str(e)))\n",
    "                if attempt_counter < 3:\n",
    "                    logging.info(\"Retrying Snapshot Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                    err_str = f\"Error on Snapshot Creation: {str(e)}\"\n",
    "                    config[\"migration_results\"].append([\"Snapshot Creation\", \"Snapshot Creation\", \"Failure\", err_str])\n",
    "                    break\n",
    "    else:\n",
    "        config[\"migration_results\"].append([\"Snapshot Creation\", \"Snapshot Creation\", \"Skipped\", \"\"])\n",
    "\n",
    "# Function to write records to specified GCP location\n",
    "def write_records_to_gcp(config, table, records_processed):\n",
    "    # Extract parameters from config\n",
    "    write_to_cloud_location = config[\"ingest\"][\"write_to_cloud_location\"]\n",
    "\n",
    "    # Write records to a file\n",
    "    records_cnt = len(records_processed)\n",
    "    destination_file = table + \".json\"\n",
    "    with open(destination_file, \"w\") as outfile:\n",
    "        for idx, val in enumerate(records_processed):\n",
    "            json.dump(val, outfile)\n",
    "            if idx < records_cnt:\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "    # Copy file to cloud\n",
    "    if write_to_cloud_location[-1] == \"/\":\n",
    "        target_cloud_path = write_to_cloud_location + destination_file\n",
    "    else:\n",
    "        target_cloud_path = write_to_cloud_location + \"/\" + destination_file\n",
    "    client = storage.Client()\n",
    "    target_bucket = target_cloud_path.split(\"/\")[2]\n",
    "    target_object = \"/\".join(target_cloud_path.split(\"/\")[3:])\n",
    "    bucket = client.bucket(target_bucket)\n",
    "    blob = bucket.blob(target_object)\n",
    "    blob.upload_from_filename(destination_file)\n",
    "    \n",
    "    # Remove local file\n",
    "    if os.path.exists(destination_file):\n",
    "        os.remove(destination_file)\n",
    "    return target_cloud_path \n",
    "\n",
    "# Function to write records to specified GCP location\n",
    "def write_records_to_azure(config, table, records_processed):\n",
    "    # Extract parameters from config\n",
    "    write_to_cloud_location = config[\"ingest\"][\"write_to_cloud_location\"]\n",
    "    write_to_cloud_sas_token = config[\"ingest\"][\"write_to_cloud_sas_token\"]\n",
    "\n",
    "    # Write records to a file\n",
    "    records_cnt = len(records_processed)\n",
    "    destination_file = table + \".json\"\n",
    "    with open(destination_file, \"w\") as outfile:\n",
    "        for idx, val in enumerate(records_processed):\n",
    "            json.dump(val, outfile)\n",
    "            if idx < records_cnt:\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "    # Copy file to cloud\n",
    "    if write_to_cloud_location[-1] == \"/\":\n",
    "        target_cloud_path = write_to_cloud_location + destination_file + \"?\" + write_to_cloud_sas_token\n",
    "    else:\n",
    "        target_cloud_path = write_to_cloud_location + \"/\" + destination_file + \"?\" + write_to_cloud_sas_token\n",
    "    blob = BlobClient.from_blob_url(target_cloud_path)\n",
    "    with open(destination_file, mode=\"rb\") as data:\n",
    "        blob.upload_blob(data=data, overwrite=True)\n",
    "    \n",
    "    # Remove local file\n",
    "    if os.path.exists(destination_file):\n",
    "        os.remove(destination_file)\n",
    "    return target_cloud_path\n",
    "                \n",
    "# Function to fetch data from BigQuery\n",
    "def fetch_source_records_bigquery(config, new_dataset_id, array_col_dict, table, start_row, end_row):\n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source\"][\"tdr_object_uuid\"]\n",
    "    src_tdr_object_type = config[\"source\"][\"tdr_object_type\"]\n",
    "    tdr_host = config[\"source\"][\"tdr_host\"]\n",
    "    files_already_ingested = config[\"ingest\"][\"files_already_ingested\"]\n",
    "    datarepo_row_ids_to_ingest = config[\"ingest\"][\"datarepo_row_ids_to_ingest\"]\n",
    "    apply_anvil_transforms = config[\"ingest\"][\"apply_anvil_transforms\"] \n",
    "    bq_project = config[\"source\"][\"bigquery_project\"]\n",
    "    bq_dataset = config[\"source\"][\"bigquery_dataset\"]\n",
    "    \n",
    "    # Setup/refresh TDR clients (and BQ client)\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Retrieve table data from the original dataset\n",
    "    logging.info(f\"Fetching rows {str(start_row)}-{str(end_row)} from table '{table}' in the original {src_tdr_object_type} ({src_tdr_object_uuid}).\")\n",
    "    table_recs_str = f\"Table: {table} -- Rows: {str(start_row)}-{str(end_row)}\"\n",
    "    final_records = []\n",
    "    if apply_anvil_transforms and \"anvil_\" not in table:\n",
    "        if table == \"file_inventory\":\n",
    "            if files_already_ingested == False:\n",
    "                file_ref_sql = \"TO_JSON_STRING(STRUCT(source_name AS sourcePath, target_path AS targetPath, 'Ingest of '||source_name AS description, COALESCE(content_type, 'application/octet-stream') AS mimeType))\"\n",
    "            else:\n",
    "                file_ref_sql = \"file_ref\"\n",
    "            rec_fetch_query = f\"\"\"WITH drlh_deduped AS\n",
    "                            (\n",
    "                              SELECT DISTINCT file_id, target_path, source_name\n",
    "                              FROM `{bq_project}.{bq_dataset}.datarepo_load_history`\n",
    "                              WHERE state = \"succeeded\" \n",
    "                            )\n",
    "                            SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT datarepo_row_id, datarepo_row_id AS orig_datarepo_row_id, a.file_id, name, path, uri, content_type, full_extension, size_in_bytes, crc32c, md5_hash, ingest_provenance,\n",
    "                              file_ref AS orig_file_ref, {file_ref_sql} AS file_ref,\n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}` a\n",
    "                                  LEFT JOIN drlh_deduped b\n",
    "                                  ON a.file_ref = b.file_id\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "        else:\n",
    "            rec_fetch_query = f\"\"\"SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT *, datarepo_row_id AS orig_datarepo_row_id,\n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}`\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "    else:\n",
    "        rec_fetch_query = f\"\"\"SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT *, \n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}`\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            df = client.query(rec_fetch_query).result().to_dataframe()\n",
    "            df = df.astype(object).where(pd.notnull(df),None)\n",
    "            for column in array_col_dict[table]:\n",
    "                df[column] = df[column].apply(lambda x: list(x))\n",
    "            if apply_anvil_transforms and table == \"file_inventory\" and files_already_ingested == False: \n",
    "                df[\"file_ref\"] = df.apply(lambda x: json.loads(x[\"file_ref\"].replace(\"\\'\", \"\\\"\")), axis=1)\n",
    "            final_records = df.to_dict(orient=\"records\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error retrieving records for rows {str(start_row)}-{str(end_row)} of table {table}: {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "                return {}\n",
    "    \n",
    "    # Filter retrieved data if necessary and return as dict of records\n",
    "    if final_records:\n",
    "        df_temp = pd.DataFrame.from_dict(final_records)\n",
    "        if datarepo_row_ids_to_ingest:\n",
    "            df_orig = df_temp[df_temp[\"datarepo_row_id\"].isin(datarepo_row_ids_to_ingest)].copy()\n",
    "        else:\n",
    "            df_orig = df_temp.copy()\n",
    "        del df_temp\n",
    "        df_orig.drop(columns=[\"datarepo_row_id\"], inplace=True, errors=\"ignore\")\n",
    "        df_orig = df_orig.astype(object).where(pd.notnull(df_orig),None)\n",
    "        records_orig = df_orig.to_dict(orient=\"records\")\n",
    "        if not records_orig:\n",
    "            msg_str = f\"No records found in rows {str(start_row)}-{str(end_row)} of table {table} after filtering based on datarepo_row_ids_to_ingest parameter. Continuing to next record set or table validation.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Skipped\", msg_str])\n",
    "            return records_orig\n",
    "        elif len(final_records) != len(records_orig):\n",
    "            logging.info(f\"Filtering records to ingest based on the datarepo_row_ids_to_ingest parameter. {str(len(records_orig))} of {str(len(final_records))} records to be ingested.\")\n",
    "            return records_orig\n",
    "        else:\n",
    "            return records_orig\n",
    "    else:\n",
    "        msg_str = f\"No records found for rows {str(start_row)}-{str(end_row)} of table {table} in original {src_tdr_object_type}. Continuing to next record set or table validation.\"\n",
    "        logging.info(msg_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Skipped\", msg_str])\n",
    "        return final_records\n",
    "        \n",
    "# Function to fetch data from TDR API\n",
    "def fetch_source_records_tdr_api(config, new_dataset_id, table, start_row, end_row):\n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source\"][\"tdr_object_uuid\"]\n",
    "    src_tdr_object_type = config[\"source\"][\"tdr_object_type\"]\n",
    "    tdr_host = config[\"source\"][\"tdr_host\"]\n",
    "    datarepo_row_ids_to_ingest = config[\"ingest\"][\"datarepo_row_ids_to_ingest\"]\n",
    "    \n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    \n",
    "    # Retrieve table data from the original dataset\n",
    "    logging.info(f\"Fetching rows {str(start_row)}-{str(end_row)} from table '{table}' in the original {src_tdr_object_type} ({src_tdr_object_uuid}).\")\n",
    "    table_recs_str = f\"Table: {table} -- Rows: {str(start_row)}-{str(end_row)}\"\n",
    "    max_page_size = 1000\n",
    "    total_records_fetched = start_row - 1\n",
    "    final_records = []\n",
    "    while True:\n",
    "        offset = total_records_fetched\n",
    "        page_size = min(max_page_size, end_row - total_records_fetched)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": offset,\n",
    "              \"limit\": page_size,\n",
    "              \"sort\": \"datarepo_row_id\",\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                if src_tdr_object_type == \"dataset\":\n",
    "                    record_results = datasets_api.query_dataset_data_by_id(id=src_tdr_object_uuid, table=table, query_data_request_model=payload).to_dict() \n",
    "                elif src_tdr_object_type == \"snapshot\":\n",
    "                    record_results = snapshots_api.query_snapshot_data_by_id(id=src_tdr_object_uuid, table=table, query_data_request_model=payload).to_dict() \n",
    "                else:\n",
    "                    raise Exception(\"Source TDR object type must be 'dataset' or 'snapshot'.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 5:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    err_str = f\"Error retrieving records for rows {str(start_row)}-{str(end_row)} of table {table}: {str(e)}.\"\n",
    "                    logging.error(err_str)\n",
    "                    config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "                    return {}\n",
    "        if record_results[\"result\"]:\n",
    "            final_records.extend(record_results[\"result\"])\n",
    "            total_records_fetched += len(record_results[\"result\"])\n",
    "        else:\n",
    "            break\n",
    "        if total_records_fetched >= end_row:\n",
    "            break\n",
    "    \n",
    "    # Filter retrieved data if necessary and return as dict of records\n",
    "    if final_records:\n",
    "        df_temp = pd.DataFrame.from_dict(final_records)\n",
    "        if datarepo_row_ids_to_ingest:\n",
    "            df_orig = df_temp[df_temp[\"datarepo_row_id\"].isin(datarepo_row_ids_to_ingest)].copy()\n",
    "        else:\n",
    "            df_orig = df_temp.copy()\n",
    "        del df_temp\n",
    "        df_orig.drop(columns=[\"datarepo_row_id\"], inplace=True, errors=\"ignore\")\n",
    "        records_orig = df_orig.to_dict(orient=\"records\")\n",
    "        if not records_orig:\n",
    "            msg_str = f\"No records found in rows {str(start_row)}-{str(end_row)} of table {table} after filtering based on datarepo_row_ids_to_ingest parameter. Continuing to next record set or table validation.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Skipped\", msg_str])\n",
    "            return records_orig\n",
    "        elif len(final_records) != len(records_orig):\n",
    "            logging.info(f\"Filtering records to ingest based on the datarepo_row_ids_to_ingest parameter. {str(len(records_orig))} of {str(len(final_records))} records to be ingested.\")\n",
    "            return records_orig\n",
    "        else:\n",
    "            return records_orig\n",
    "    else:\n",
    "        msg_str = f\"No records found for rows {str(start_row)}-{str(end_row)} of table {table} in original {src_tdr_object_type}. Continuing to next record set or table validation.\"\n",
    "        logging.info(msg_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Skipped\", msg_str])\n",
    "        return records_orig\n",
    "\n",
    "# Function to process ingests for specific table\n",
    "def ingest_table_data(config, new_dataset_id, fileref_col_dict, array_col_dict, table, start_row, end_row):\n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source\"][\"tdr_object_uuid\"]\n",
    "    src_tdr_object_type = config[\"source\"][\"tdr_object_type\"]\n",
    "    src_tdr_object_cloud = config[\"source\"][\"tdr_object_cloud\"]\n",
    "    tdr_host = config[\"source\"][\"tdr_host\"]\n",
    "    tar_tdr_billing_profile = config[\"target\"][\"tdr_billing_profile\"]\n",
    "    records_processing_method = config[\"ingest\"][\"records_processing_method\"]\n",
    "    write_to_cloud_platform = config[\"ingest\"][\"write_to_cloud_platform\"]\n",
    "    apply_anvil_transforms = config[\"ingest\"][\"apply_anvil_transforms\"] \n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    \n",
    "    # Retrieve table data from the original dataset\n",
    "    table_recs_str = f\"Table: {table} -- Rows: {str(start_row)}-{str(end_row)}\"\n",
    "    if src_tdr_object_cloud == \"gcp\":\n",
    "        records_orig = fetch_source_records_bigquery(config, new_dataset_id, array_col_dict, table, start_row, end_row)\n",
    "    else:\n",
    "        records_orig = fetch_source_records_tdr_api(config, new_dataset_id, table, start_row, end_row)\n",
    "    if not records_orig:\n",
    "        return\n",
    "\n",
    "    # Pre-process records before ingest\n",
    "    if fileref_col_dict[table] and not apply_anvil_transforms:\n",
    "        try:\n",
    "            # Pre-process records to include file reference objects\n",
    "            logging.info(\"File reference columns present. Pre-processing records before submitting ingestion request.\")\n",
    "            api_client = refresh_tdr_api_client(tdr_host)\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "            records_processed = []\n",
    "            for record in records_orig:\n",
    "                int_record = record.copy()\n",
    "                for fileref_col in fileref_col_dict[table]:\n",
    "                    if isinstance(int_record[fileref_col], list):\n",
    "                        fileref_obj_list = []\n",
    "                        for val in int_record[fileref_col]:\n",
    "                            attempt_counter = 0\n",
    "                            while True:\n",
    "                                try:\n",
    "                                    if src_tdr_object_type == \"dataset\":\n",
    "                                        file_results = datasets_api.lookup_file_by_id(id=src_tdr_object_uuid, fileid=val)\n",
    "                                    elif src_tdr_object_type == \"snapshot\":\n",
    "                                        file_results = snapshots_api.lookup_snapshot_file_by_id(id=src_tdr_object_uuid, fileid=val[-36:]) \n",
    "                                    else:\n",
    "                                        raise Exception(\"Source TDR object type must be 'dataset' or 'snapshot'.\") \n",
    "                                    fileref_obj = {\n",
    "                                        \"sourcePath\": file_results.file_detail.access_url,\n",
    "                                        \"targetPath\": file_results.path,\n",
    "                                        \"description\": file_results.description,\n",
    "                                        \"mimeType\": file_results.file_detail.mime_type\n",
    "                                    }\n",
    "                                    fileref_obj_list.append(fileref_obj)\n",
    "                                    break\n",
    "                                except Exception as e:\n",
    "                                    if attempt_counter < 5:\n",
    "                                        sleep(5)\n",
    "                                        attempt_counter += 1\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        break\n",
    "                        int_record[fileref_col] = fileref_obj_list\n",
    "                    elif int_record[fileref_col]:\n",
    "                        fileref_obj = {}\n",
    "                        attempt_counter = 0\n",
    "                        while True:\n",
    "                            try:\n",
    "                                if src_tdr_object_type == \"dataset\":\n",
    "                                    file_results = datasets_api.lookup_file_by_id(id=src_tdr_object_uuid, fileid=int_record[fileref_col])\n",
    "                                elif src_tdr_object_type == \"snapshot\":\n",
    "                                    file_results = snapshots_api.lookup_snapshot_file_by_id(id=src_tdr_object_uuid, fileid=int_record[fileref_col][-36:]) \n",
    "                                else:\n",
    "                                    raise Exception(\"Source TDR object type must be 'dataset' or 'snapshot'.\") \n",
    "                                fileref_obj = {\n",
    "                                    \"sourcePath\": file_results.file_detail.access_url,\n",
    "                                    \"targetPath\": file_results.path,\n",
    "                                    \"description\": file_results.description,\n",
    "                                    \"mimeType\": file_results.file_detail.mime_type\n",
    "                                }\n",
    "                                int_record[fileref_col] = fileref_obj\n",
    "                                break\n",
    "                            except Exception as e:\n",
    "                                if attempt_counter < 5:\n",
    "                                    sleep(5)\n",
    "                                    attempt_counter += 1\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    break\n",
    "                        int_record[fileref_col] = fileref_obj\n",
    "                records_processed.append(int_record)\n",
    "        except Exception as e:\n",
    "            err_str = f\"Failure in pre-processing: {str(e)}\"\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "            return\n",
    "    elif apply_anvil_transforms and \"anvil_\" in table:\n",
    "        try:\n",
    "            # Pre-process records in AnVIL_ records to use new datarepo_row_ids in the source_datarepo_row_ids field\n",
    "            logging.info(\"FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\")\n",
    "            records_processed = []\n",
    "            for record in records_orig:\n",
    "                int_record = record.copy()\n",
    "                new_dr_row_id_list = []\n",
    "                for row_id in int_record[\"source_datarepo_row_ids\"]:\n",
    "                    new_row_id = config[\"anvil\"][\"dr_row_id_xwalk\"].get(row_id)\n",
    "                    if new_row_id:\n",
    "                        new_dr_row_id_list.append(new_row_id)\n",
    "                int_record[\"source_datarepo_row_ids\"] = new_dr_row_id_list\n",
    "                records_processed.append(int_record)\n",
    "        except Exception as e:\n",
    "            err_str = f\"Failure in pre-processing: {str(e)}\"\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "            return\n",
    "    else:\n",
    "        records_processed = records_orig    \n",
    "    \n",
    "    # Write out records to cloud, if specified by user\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        logging.info(f\"Writing records to a control file in the cloud.\")\n",
    "        if write_to_cloud_platform == \"gcp\":\n",
    "            control_file_path = write_records_to_gcp(config, table, records_processed)\n",
    "        else:\n",
    "            control_file_path = write_records_to_azure(config, table, records_processed)\n",
    "\n",
    "    # Build, submit, and monitor ingest request\n",
    "    logging.info(f\"Submitting ingestion request to new dataset ({new_dataset_id}).\")\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        ingest_request = {\n",
    "            \"table\": table,\n",
    "            \"profile_id\": tar_tdr_billing_profile,\n",
    "            \"ignore_unknown_values\": True,\n",
    "            \"resolve_existing_files\": True,\n",
    "            \"updateStrategy\": \"append\",\n",
    "            \"format\": \"json\",\n",
    "            \"load_tag\": \"Ingest for {}\".format(new_dataset_id),\n",
    "            \"path\": control_file_path\n",
    "        }        \n",
    "    else:\n",
    "        ingest_request = {\n",
    "            \"table\": table,\n",
    "            \"profile_id\": tar_tdr_billing_profile,\n",
    "            \"ignore_unknown_values\": True,\n",
    "            \"resolve_existing_files\": True,\n",
    "            \"updateStrategy\": \"append\",\n",
    "            \"format\": \"array\",\n",
    "            \"load_tag\": \"Ingest for {}\".format(new_dataset_id),\n",
    "            \"records\": records_processed\n",
    "        }\n",
    "    attempt_counter = 1\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = refresh_tdr_api_client(tdr_host)\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = wait_for_tdr_job(datasets_api.ingest_dataset(id=new_dataset_id, ingest=ingest_request), tdr_host)\n",
    "            logging.info(\"Ingest succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Success\", str(ingest_request_result)[0:1000]])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on ingest: {}\".format(str(e)[0:2500]))\n",
    "            if attempt_counter < 3:\n",
    "                logging.info(\"Retrying ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Logging error.\")\n",
    "                err_str = f\"Error on ingest: {str(e)[0:2500]}\"\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])  \n",
    "                break\n",
    "\n",
    "    # Remove control file from cloud, if written out\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        logging.info(f\"Removing control file from the cloud.\")\n",
    "        if write_to_cloud_platform == \"gcp\":\n",
    "            client = storage.Client()\n",
    "            target_bucket = control_file_path.split(\"/\")[2]\n",
    "            target_object = \"/\".join(control_file_path.split(\"/\")[3:])\n",
    "            bucket = client.bucket(target_bucket)\n",
    "            blob = bucket.blob(target_object)\n",
    "            blob.delete()\n",
    "        else:\n",
    "            blob = BlobClient.from_blob_url(control_file_path)\n",
    "            blob.delete_blob()\n",
    "\n",
    "# Function to populate new TDR dataset\n",
    "def populate_new_dataset(config, new_dataset_id, fileref_col_dict, array_col_dict):\n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source\"][\"tdr_object_uuid\"]\n",
    "    src_tdr_object_type = config[\"source\"][\"tdr_object_type\"]\n",
    "    src_tdr_object_env = config[\"source\"][\"tdr_object_env\"]\n",
    "    tdr_host = config[\"source\"][\"tdr_host\"]\n",
    "    tar_tdr_billing_profile = config[\"target\"][\"tdr_billing_profile\"]\n",
    "    tdr_general_sa = config[\"tdr_general_sa\"]\n",
    "    chunk_size = config[\"ingest\"][\"max_records_per_ingest_request\"]\n",
    "    max_combined_rec_ref_size = config[\"ingest\"][\"max_filerefs_per_ingest_request\"]\n",
    "    tables_to_ingest = config[\"ingest\"][\"tables_to_ingest\"]\n",
    "    datarepo_row_ids_to_ingest = config[\"ingest\"][\"datarepo_row_ids_to_ingest\"]\n",
    "    apply_anvil_transforms = config[\"ingest\"][\"apply_anvil_transforms\"] \n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    \n",
    "    # Retrieve TDR SA to add from new dataset and add to original object\n",
    "    logging.info(f\"Adding TDR SA to original {src_tdr_object_type}: {src_tdr_object_uuid}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=new_dataset_id).to_dict()\n",
    "        if dataset_details[\"ingest_service_account\"]:\n",
    "            tdr_sa_to_use = dataset_details[\"ingest_service_account\"]\n",
    "        else:\n",
    "            tdr_sa_to_use = tdr_general_sa\n",
    "    except:\n",
    "        error_str = f\"Error retrieving details from dataset {new_dataset_id} in TDR {src_tdr_object_env} environment.\"\n",
    "        logging.error(error_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "        return\n",
    "    logging.info(f\"TDR SA to add: {tdr_sa_to_use}\")\n",
    "    try:\n",
    "        if src_tdr_object_type == \"dataset\":\n",
    "            resp = datasets_api.add_dataset_policy_member(id=src_tdr_object_uuid, policy_name=\"steward\", policy_member={\"email\": tdr_sa_to_use}) \n",
    "        elif src_tdr_object_type == \"snapshot\":\n",
    "            resp = snapshots_api.add_snapshot_policy_member(id=src_tdr_object_uuid, policy_name=\"steward\", policy_member={\"email\": tdr_sa_to_use}) \n",
    "        else:\n",
    "            raise Exception(\"Source TDR object type must be 'dataset' or 'snapshot'.\")\n",
    "        logging.info(\"TDR SA added successfully.\")\n",
    "    except:\n",
    "        error_str = f\"Error adding TDR SA to {src_tdr_object_type} {src_tdr_object_uuid} in TDR {src_tdr_object_env} environment: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "        return\n",
    "    \n",
    "    # Loop through and process tables for ingestion\n",
    "    logging.info(\"Processing dataset ingestion requests.\")\n",
    "    if apply_anvil_transforms:\n",
    "        config[\"anvil\"] = {}\n",
    "        config[\"anvil\"][\"dr_row_id_xwalk\"] = {}\n",
    "        table_rank_dict = {}\n",
    "        for table in fileref_col_dict.keys():\n",
    "            if table == \"file_inventory\":\n",
    "                table_rank_dict[table] = 1\n",
    "            elif \"anvil_\" not in table:\n",
    "                table_rank_dict[table] = 2\n",
    "            else:\n",
    "                table_rank_dict[table] = 3\n",
    "        ordered_table_list = sorted(table_rank_dict, key= lambda key: table_rank_dict[key])\n",
    "    else:\n",
    "        ordered_table_list = sorted(fileref_col_dict, key=lambda key: (len(fileref_col_dict[key]), key))\n",
    "    for table in ordered_table_list:\n",
    "        \n",
    "        # Determine whether table should be processed, and skip if not\n",
    "        logging.info(f\"Processing dataset ingestion for table '{table}'.\")\n",
    "        if tables_to_ingest and table not in tables_to_ingest:\n",
    "            msg_str = f\"Table '{table}' not listed in the ingest.tables_to_ingest parameter. Skipping.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "            continue\n",
    "        \n",
    "        # Fetch total record count for table\n",
    "        api_client = refresh_tdr_api_client(tdr_host)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": 0,\n",
    "              \"limit\": 10,\n",
    "              \"sort\": \"datarepo_row_id\",\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                if src_tdr_object_type == \"dataset\":\n",
    "                    record_results = datasets_api.query_dataset_data_by_id(id=src_tdr_object_uuid, table=table, query_data_request_model=payload).to_dict()\n",
    "                elif src_tdr_object_type == \"snapshot\":\n",
    "                    record_results = snapshots_api.query_snapshot_data_by_id(id=src_tdr_object_uuid, table=table, query_data_request_model=payload).to_dict() \n",
    "                else:\n",
    "                    raise Exception(\"Source TDR object type must be 'dataset' or 'snapshot'.\")\n",
    "                total_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 5:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    total_record_count = -1\n",
    "                    break\n",
    "        if total_record_count == -1:\n",
    "            err_str = f\"Error retrieving record count for table '{table}' in original {src_tdr_object_type}. Continuing to next table.\"\n",
    "            logging.error(err_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Failure\", err_str])\n",
    "            continue \n",
    "        elif total_record_count == 0:\n",
    "            msg_str = f\"No records found for table in original {src_tdr_object_type}. Continuing to next table/record set.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "            continue\n",
    "        \n",
    "        # Chunk table records as necessary, then loop through and process each chunk\n",
    "        if fileref_col_dict[table]:\n",
    "            ref_chunk_size = math.floor(max_combined_rec_ref_size / len(fileref_col_dict[table]))\n",
    "            chunk_size = min(chunk_size, ref_chunk_size)\n",
    "            logging.info(f\"Table '{table}' contains fileref columns. Will use a chunk size of {chunk_size} rows per ingestion request, to keep the number of file references per chunk below {max_combined_rec_ref_size}.\")\n",
    "        else:\n",
    "            logging.info(f\"Table '{table}' does not contain fileref columns. Will use a chunk size of {chunk_size} rows per ingestion request.\")\n",
    "        start_row = 1\n",
    "        end_row = min((chunk_size), total_record_count)\n",
    "        while start_row <= total_record_count:\n",
    "            if end_row > total_record_count:\n",
    "                end_row = total_record_count\n",
    "            ingest_table_data(config, new_dataset_id, fileref_col_dict, array_col_dict, table, start_row, end_row)    \n",
    "            start_row += chunk_size\n",
    "            end_row += chunk_size\n",
    "            \n",
    "        # Fetch total record count for the new table\n",
    "        api_client = refresh_tdr_api_client(tdr_host)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": 0,\n",
    "              \"limit\": 10,\n",
    "              \"sort\": \"datarepo_row_id\",\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=new_dataset_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                new_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 5:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    new_record_count = -1\n",
    "                    break\n",
    "        if new_record_count == -1:\n",
    "            err_str = f\"Error retrieving record count for table '{table}' in new dataset. Skipping validation and continuing to next table.\"\n",
    "            logging.error(err_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Failure\", err_str])\n",
    "            continue \n",
    "        \n",
    "        # Validate the new table against the old table, with extra scrutiny given to the file_inventory table for AnVIL migrations\n",
    "        logging.info(f\"Validating table '{table}' in new dataset vs. original {src_tdr_object_type}.\")\n",
    "        if apply_anvil_transforms and table == \"file_inventory\":\n",
    "            err_msg = f\"Validation error with file_inventory table for job with ingest.apply_anvil_transforms parameter set to 'True'. Due to downstream dependencies on this table, skipping remaining tables and failing job.\"\n",
    "            if new_record_count != total_record_count:\n",
    "                config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Failure\", f\"{new_record_count} records found in new table doesn't match {total_record_count} records in original table.\"])\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Remaining Tables\", \"Skipped\", err_msg])\n",
    "                logging.error(err_msg)\n",
    "                return\n",
    "            else:\n",
    "                api_client = refresh_tdr_api_client(tdr_host)\n",
    "                datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "                max_page_size = 1000\n",
    "                records_fetched = 0\n",
    "                errors_found = []\n",
    "                retrieval_error = False\n",
    "                while records_fetched < total_record_count and not retrieval_error:\n",
    "                    row_start = records_fetched\n",
    "                    attempt_counter = 0\n",
    "                    while True:\n",
    "                        payload = {\n",
    "                          \"offset\": row_start,\n",
    "                          \"limit\": max_page_size,\n",
    "                          \"sort\": \"datarepo_row_id\",\n",
    "                          \"direction\": \"asc\",\n",
    "                          \"filter\": \"\"\n",
    "                        }\n",
    "                        try:\n",
    "                            dataset_results = datasets_api.query_dataset_data_by_id(id=new_dataset_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                            for record in dataset_results[\"result\"]:\n",
    "                                key = table + \":\" + record[\"orig_datarepo_row_id\"]\n",
    "                                val = table + \":\" + record[\"datarepo_row_id\"]\n",
    "                                config[\"anvil\"][\"dr_row_id_xwalk\"][key] = val\n",
    "                                records_fetched += 1\n",
    "                                if record[\"file_ref\"] != record[\"orig_file_ref\"] and len(errors_found) < 5:\n",
    "                                    errors_found.append(record)\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            if attempt_counter < 5:\n",
    "                                sleep(10)\n",
    "                                attempt_counter += 1\n",
    "                                continue\n",
    "                            else:\n",
    "                                warn_str = \"Error retrieving records for 'file_inventory' table for job with ingest.apply_anvil_transforms parameter set to 'True'. Skipping comparison of file_ref and orig_file_ref fields. Note that mismatches between these fields may cause issues with ingest jobs downstream.\"\n",
    "                                logging.warning(warn_str)\n",
    "                                config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Skipped\", warn_str])\n",
    "                                retrieval_error = True\n",
    "                                break\n",
    "                if errors_found:\n",
    "                    config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Failure\", f\"Records exist with mismatching file_ref and orig_file_ref_values. Sample records: {str(errors_found)}\"])\n",
    "                    config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Remaining Tables\", \"Skipped\", err_msg])\n",
    "                    logging.error(err_msg)\n",
    "                    return\n",
    "                else:\n",
    "                    config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Success\", f\"{new_record_count} records found in both new and original table. No mismatches between file_ref and orig_file_ref found.\"])\n",
    "        else:\n",
    "            if new_record_count == total_record_count:\n",
    "                config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Success\", f\"{new_record_count} records found in both new and original table.\"])\n",
    "            else:\n",
    "                config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Failure\", f\"{new_record_count} records found in new table doesn't match {total_record_count} records in original table.\"])\n",
    "        \n",
    "        # Build datarepo_row_id crosswalk for use in AnVIL migrations\n",
    "        if apply_anvil_transforms and table != \"file_inventory\" and \"anvil_\" not in table: \n",
    "            logging.info(\"Fetching ingested records and building datarepo_row_id lookup for use in AnVIL transforms.\")\n",
    "            api_client = refresh_tdr_api_client(tdr_host)\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            max_page_size = 1000\n",
    "            records_fetched = 0\n",
    "            retrieval_error = False\n",
    "            while records_fetched < total_record_count and not retrieval_error:\n",
    "                row_start = records_fetched\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    payload = {\n",
    "                      \"offset\": row_start,\n",
    "                      \"limit\": max_page_size,\n",
    "                      \"sort\": \"datarepo_row_id\",\n",
    "                      \"direction\": \"asc\",\n",
    "                      \"filter\": \"\"\n",
    "                    }\n",
    "                    try:\n",
    "                        dataset_results = datasets_api.query_dataset_data_by_id(id=new_dataset_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                        for record in dataset_results[\"result\"]:\n",
    "                            key = table + \":\" + record[\"orig_datarepo_row_id\"]\n",
    "                            val = table + \":\" + record[\"datarepo_row_id\"]\n",
    "                            config[\"anvil\"][\"dr_row_id_xwalk\"][key] = val\n",
    "                            records_fetched += 1\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 5:\n",
    "                            sleep(10)\n",
    "                            attempt_counter += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            warn_str = f\"Error retrieving records for '{table}' table for job with ingest.apply_anvil_transforms parameter set to 'True'. Note that this may cause issues with datarepo_row_id look-ups downstream.\"\n",
    "                            logging.warning(warn_str)\n",
    "                            retrieval_error = True\n",
    "                            break\n",
    "        \n",
    "# Function to create a new TDR dataset from an existing TDR dataset\n",
    "def create_dataset_from_dataset(config):\n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source\"][\"tdr_object_uuid\"]\n",
    "    src_tdr_object_type = config[\"source\"][\"tdr_object_type\"]\n",
    "    src_tdr_object_env = config[\"source\"][\"tdr_object_env\"]\n",
    "    tdr_host = config[\"source\"][\"tdr_host\"]\n",
    "    tar_tdr_billing_profile = config[\"target\"][\"tdr_billing_profile\"]\n",
    "    tar_tdr_dataset_uuid = config[\"target\"][\"tdr_dataset_uuid\"]\n",
    "    tar_tdr_dataset_name = config[\"target\"][\"tdr_dataset_name\"]\n",
    "    tar_tdr_dataset_cloud = config[\"target\"][\"tdr_dataset_cloud\"]\n",
    "    tar_tdr_dataset_props = config[\"target\"][\"tdr_dataset_properties\"]\n",
    "    copy_policies = config[\"target\"][\"copy_policies\"] \n",
    "    apply_anvil_transforms = config[\"ingest\"][\"apply_anvil_transforms\"] \n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Retrieve original dataset details\n",
    "    logging.info(f\"Retrieving original {src_tdr_object_type} details from {src_tdr_object_env} environment. UUID:  {src_tdr_object_uuid}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=src_tdr_object_uuid, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from {src_tdr_object_type} {src_tdr_object_uuid} in TDR {src_tdr_object_env} environment: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Creation\", \"Dataset Creation\", \"Failure\", error_str])\n",
    "        return None, {}, {}\n",
    "    \n",
    "    # Validate source cloud platform\n",
    "    #config[\"source\"][\"tdr_object_cloud\"] = dataset_details[\"cloud_platform\"] # This is null in the API endpoint at the moment\n",
    "    config[\"source\"][\"tdr_object_cloud\"] = dataset_details[\"storage\"][0][\"cloud_platform\"]\n",
    "    if config[\"source\"][\"tdr_object_cloud\"] == \"azure\":\n",
    "        config[\"migration_results\"].append([\"Dataset Creation\", \"Dataset Creation\", \"Failure\", \"Migrate of Azure TDR objects is not yet supported. Try again with a GCP TDR object.\"])\n",
    "        return None, {}, {}\n",
    "    else:\n",
    "        config[\"source\"][\"bigquery_project\"] = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        config[\"source\"][\"bigquery_dataset\"] = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "\n",
    "    # Build new dataset schema\n",
    "    new_schema_dict = {\"tables\": [], \"relationships\": [], \"assets\": []}\n",
    "    fileref_col_dict = {}\n",
    "    array_col_dict = {}\n",
    "    for table_entry in dataset_details[\"schema\"][\"tables\"]:\n",
    "        int_table_dict = table_entry.copy()\n",
    "        int_table_dict[\"primaryKey\"] = int_table_dict.pop(\"primary_key\")\n",
    "        for key in [\"partition_mode\", \"date_partition_options\", \"int_partition_options\", \"row_count\"]:\n",
    "            del int_table_dict[key]\n",
    "        fileref_list = []\n",
    "        array_list = []\n",
    "        for idx, column_entry in enumerate(table_entry[\"columns\"]):\n",
    "            if column_entry[\"datatype\"] == \"fileref\":\n",
    "                fileref_list.append(column_entry[\"name\"])\n",
    "            if column_entry[\"array_of\"] == True:\n",
    "                array_list.append(column_entry[\"name\"])\n",
    "            if tar_tdr_dataset_cloud == \"azure\" and column_entry[\"datatype\"] == \"integer\":\n",
    "                table_entry[\"columns\"][idx][\"datatype\"] = \"int64\"\n",
    "        fileref_col_dict[table_entry[\"name\"]] = fileref_list\n",
    "        array_col_dict[table_entry[\"name\"]] = array_list\n",
    "        if apply_anvil_transforms:\n",
    "            if table_entry[\"name\"] == \"file_inventory\":\n",
    "                int_table_dict[\"columns\"].append({\"name\": \"orig_file_ref\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "                int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "            elif \"anvil_\" not in table_entry[\"name\"]:\n",
    "                int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "        new_schema_dict[\"tables\"].append(int_table_dict)\n",
    "    for rel_entry in dataset_details[\"schema\"][\"relationships\"]:\n",
    "        int_rel_dict = rel_entry.copy()\n",
    "        int_rel_dict[\"from\"] = int_rel_dict.pop(\"_from\")\n",
    "        new_schema_dict[\"relationships\"].append(int_rel_dict)\n",
    "    for asset_entry in dataset_details[\"schema\"][\"assets\"]:\n",
    "        int_asset_dict = asset_entry.copy()\n",
    "        int_asset_dict[\"rootTable\"] = int_asset_dict.pop(\"root_table\")\n",
    "        int_asset_dict[\"rootColumn\"] = int_asset_dict.pop(\"root_column\")\n",
    "        new_schema_dict[\"assets\"].append(int_asset_dict)\n",
    "\n",
    "    # Create a new dataset, unless a target dataset UUID has been provided\n",
    "    if tar_tdr_dataset_uuid:\n",
    "        new_dataset_id = tar_tdr_dataset_uuid\n",
    "        msg_str = f\"Attempting to leverage user-provided target dataset UUID ({tar_tdr_dataset_uuid}) rather than creating a new dataset.\"\n",
    "        logging.info(msg_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Creation\", \"Dataset Creation\", \"Skipped\", msg_str])\n",
    "    else:\n",
    "        # Retrieve original dataset policies\n",
    "        if copy_policies:\n",
    "            try:\n",
    "                dataset_policies = datasets_api.retrieve_dataset_policies(id=src_tdr_object_uuid).to_dict()\n",
    "                for policy in dataset_policies[\"policies\"]:\n",
    "                    if policy[\"name\"] == \"steward\":\n",
    "                        stewards_list = policy[\"members\"]\n",
    "                    elif policy[\"name\"] == \"custodian\":\n",
    "                        custodians_list = policy[\"members\"]\n",
    "                    elif policy[\"name\"] == \"snapshot_creator\":\n",
    "                        snapshot_creators_list = policy[\"members\"]\n",
    "            except:\n",
    "                logging.info(\"Error retrieving original dataset policies. Skipping policy copy.\")\n",
    "                stewards_list = []\n",
    "                custodians_list = []\n",
    "                snapshot_creators_list = []\n",
    "        else:\n",
    "            stewards_list = []\n",
    "            custodians_list = []\n",
    "            snapshot_creators_list = []\n",
    "\n",
    "        # Determine dataset properties - Description\n",
    "        orig_object_name = dataset_details[\"name\"]\n",
    "        new_description = dataset_details[\"description\"] + f\"\\n\\nCopy of {src_tdr_object_type} {orig_object_name} from TDR {src_tdr_object_env}.\"\n",
    "        description = tar_tdr_dataset_props[\"description\"] if tar_tdr_dataset_props.get(\"description\") else new_description\n",
    "\n",
    "        # Determine dataset properties - Region\n",
    "        for storage_entry in dataset_details[\"storage\"]:\n",
    "            if storage_entry[\"cloud_resource\"] == \"bucket\":\n",
    "                orig_region = storage_entry[\"region\"]\n",
    "                break\n",
    "        if tar_tdr_dataset_props.get(\"region\"):\n",
    "            dataset_region = tar_tdr_dataset_props[\"region\"]\n",
    "        elif tar_tdr_dataset_cloud == \"gcp\" and orig_region:\n",
    "            dataset_region = orig_region\n",
    "        else:\n",
    "            dataset_region = None\n",
    "\n",
    "        # Determine dataset properties - Dedicated Ingest SA\n",
    "        dedicated_ingest_sa = tar_tdr_dataset_props.get(\"dedicatedIngestServiceAccount\")\n",
    "        if dedicated_ingest_sa == None:\n",
    "            dedicated_ingest_sa = True if dataset_details[\"ingest_service_account\"] else False\n",
    "\n",
    "        # Determine dataset properties - Self-Hosted\n",
    "        self_hosted = False\n",
    "        if tar_tdr_dataset_cloud == \"azure\":\n",
    "            self_hosted = False\n",
    "        elif tar_tdr_dataset_props.get(\"experimentalSelfHosted\"):\n",
    "            self_hosted = tar_tdr_dataset_props[\"experimentalSelfHosted\"]\n",
    "        else:\n",
    "            self_hosted = dataset_details[\"self_hosted\"]\n",
    "\n",
    "        # Determine dataset properties - Policies\n",
    "        policies = {}\n",
    "        if tar_tdr_dataset_props.get(\"policies\"):\n",
    "            if tar_tdr_dataset_props[\"policies\"].get(\"stewards\"):\n",
    "                for user in tar_tdr_dataset_props[\"policies\"][\"stewards\"]:\n",
    "                    if user not in stewards_list:\n",
    "                        stewards_list.append(user)\n",
    "            if tar_tdr_dataset_props[\"policies\"].get(\"custodians\"):\n",
    "                for user in tar_tdr_dataset_props[\"policies\"][\"custodians\"]:\n",
    "                    if user not in custodians_list:\n",
    "                        custodians_list.append(user)\n",
    "            if tar_tdr_dataset_props[\"policies\"].get(\"snapshotCreators\"):\n",
    "                for user in tar_tdr_dataset_props[\"policies\"][\"snapshotCreators\"]:\n",
    "                    if user not in snapshot_creators_list:\n",
    "                        snapshot_creators_list.append(user)\n",
    "        policies = {\n",
    "            \"stewards\": stewards_list,\n",
    "            \"custodians\": custodians_list,\n",
    "            \"snapshotCreators\": snapshot_creators_list\n",
    "        }\n",
    "\n",
    "        # Determine dataset properties - Other\n",
    "        phs_id = tar_tdr_dataset_props[\"phsId\"] if tar_tdr_dataset_props.get(\"phsId\") else dataset_details[\"phs_id\"]\n",
    "        predictable_file_ids = tar_tdr_dataset_props[\"experimentalPredictableFileIds\"] if tar_tdr_dataset_props.get(\"experimentalPredictableFileIds\") else dataset_details[\"predictable_file_ids\"]\n",
    "        secure_monitoring_enabled = tar_tdr_dataset_props[\"enableSecureMonitoring\"] if tar_tdr_dataset_props.get(\"enableSecureMonitoring\") else dataset_details[\"secure_monitoring_enabled\"]\n",
    "        properties = tar_tdr_dataset_props[\"properties\"] if tar_tdr_dataset_props.get(\"properties\") else dataset_details[\"properties\"]\n",
    "        tags = tar_tdr_dataset_props[\"tags\"] if tar_tdr_dataset_props.get(\"tags\") else dataset_details[\"tags\"]\n",
    "\n",
    "        # Create new TDR dataset\n",
    "        logging.info(\"Submitting dataset creation request.\")\n",
    "        dataset_request = {\n",
    "            \"name\": tar_tdr_dataset_name,\n",
    "            \"description\": description,\n",
    "            \"defaultProfileId\": tar_tdr_billing_profile,\n",
    "            \"cloudPlatform\": tar_tdr_dataset_cloud,\n",
    "            \"region\": dataset_region,\n",
    "            \"phsId\": phs_id,\n",
    "            \"experimentalSelfHosted\": self_hosted,\n",
    "            \"experimentalPredictableFileIds\": predictable_file_ids,\n",
    "            \"dedicatedIngestServiceAccount\": dedicated_ingest_sa,\n",
    "            \"enableSecureMonitoring\": secure_monitoring_enabled,\n",
    "            \"properties\": properties,\n",
    "            \"tags\": tags,\n",
    "            \"policies\": policies,\n",
    "            \"schema\": new_schema_dict\n",
    "        }\n",
    "        attempt_counter = 1\n",
    "        while True:\n",
    "            try:\n",
    "                create_dataset_result, job_id = wait_for_tdr_job(datasets_api.create_dataset(dataset=dataset_request), tdr_host)\n",
    "                logging.info(\"Dataset Creation succeeded: {}\".format(create_dataset_result))\n",
    "                new_dataset_id = create_dataset_result[\"id\"]\n",
    "                config[\"migration_results\"].append([\"Dataset Creation\", \"Dataset Creation\", \"Success\", create_dataset_result])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = f\"Error on Dataset Creation: {str(e)}\"\n",
    "                logging.error(error_str)\n",
    "                if attempt_counter < 3:\n",
    "                    logging.info(\"Retrying Dataset Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Exiting job.\")\n",
    "                    config[\"migration_results\"].append([\"Dataset Creation\", \"Dataset Creation\", \"Failure\", error_str])\n",
    "                    return None, {}, {}\n",
    "        \n",
    "    # Exit function\n",
    "    return new_dataset_id, fileref_col_dict, array_col_dict\n",
    "    \n",
    "# Function to create a new TDR dataset from an existing TDR snapshot  \n",
    "def create_dataset_from_snapshot(config):\n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source\"][\"tdr_object_uuid\"]\n",
    "    src_tdr_object_type = config[\"source\"][\"tdr_object_type\"]\n",
    "    src_tdr_object_env = config[\"source\"][\"tdr_object_env\"]\n",
    "    tdr_host = config[\"source\"][\"tdr_host\"]\n",
    "    tar_tdr_billing_profile = config[\"target\"][\"tdr_billing_profile\"]\n",
    "    tar_tdr_dataset_uuid = config[\"target\"][\"tdr_dataset_uuid\"]\n",
    "    tar_tdr_dataset_name = config[\"target\"][\"tdr_dataset_name\"]\n",
    "    tar_tdr_dataset_cloud = config[\"target\"][\"tdr_dataset_cloud\"]\n",
    "    tar_tdr_dataset_props = config[\"target\"][\"tdr_dataset_properties\"]\n",
    "    copy_policies = config[\"target\"][\"copy_policies\"] \n",
    "    apply_anvil_transforms = config[\"ingest\"][\"apply_anvil_transforms\"]\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "\n",
    "    # Retrieve original dataset details\n",
    "    logging.info(f\"Retrieving original {src_tdr_object_type} details from {src_tdr_object_env} environment. UUID:  {src_tdr_object_uuid}\")\n",
    "    try:\n",
    "        snapshot_details = snapshots_api.retrieve_snapshot(id=src_tdr_object_uuid, include=[\"TABLES\", \"RELATIONSHIPS\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"SOURCES\"]).to_dict()\n",
    "    except:\n",
    "        error_str = f\"Error retrieving details from {src_tdr_object_type} {src_tdr_object_uuid} in TDR {src_tdr_object_env} environment: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Creation\", \"Dataset Creation\", \"Failure\", error_str])\n",
    "        return None, {}, {}\n",
    "    \n",
    "    # Validate source cloud platform\n",
    "    config[\"source\"][\"tdr_object_cloud\"] = snapshot_details[\"cloud_platform\"]\n",
    "    if config[\"source\"][\"tdr_object_cloud\"] == \"azure\":\n",
    "        config[\"migration_results\"].append([\"Dataset Creation\", \"Dataset Creation\", \"Failure\", \"Migrate of Azure TDR objects is not yet supported. Try again with a GCP TDR object.\"])\n",
    "        return None, {}, {}\n",
    "    else:\n",
    "        config[\"source\"][\"bigquery_project\"] = snapshot_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        config[\"source\"][\"bigquery_dataset\"] = snapshot_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "\n",
    "    # Build new dataset schema\n",
    "    new_schema_dict = {\"tables\": [], \"relationships\": [], \"assets\": []}\n",
    "    fileref_col_dict = {}\n",
    "    array_col_dict = {}\n",
    "    for table_entry in snapshot_details[\"tables\"]:\n",
    "        int_table_dict = table_entry.copy()\n",
    "        int_table_dict[\"primaryKey\"] = int_table_dict.pop(\"primary_key\")\n",
    "        for key in [\"partition_mode\", \"date_partition_options\", \"int_partition_options\", \"row_count\"]:\n",
    "            del int_table_dict[key]\n",
    "        fileref_list = []\n",
    "        array_list = []\n",
    "        for idx, column_entry in enumerate(table_entry[\"columns\"]):\n",
    "            if column_entry[\"datatype\"] == \"fileref\":\n",
    "                fileref_list.append(column_entry[\"name\"])\n",
    "            if column_entry[\"array_of\"] == True:\n",
    "                array_list.append(column_entry[\"name\"])\n",
    "            if tar_tdr_dataset_cloud == \"azure\" and column_entry[\"datatype\"] == \"integer\":\n",
    "                table_entry[\"columns\"][idx][\"datatype\"] = \"int64\"\n",
    "        fileref_col_dict[table_entry[\"name\"]] = fileref_list\n",
    "        array_col_dict[table_entry[\"name\"]] = array_list\n",
    "        if apply_anvil_transforms:\n",
    "            if table_entry[\"name\"] == \"file_inventory\":\n",
    "                int_table_dict[\"columns\"].append({\"name\": \"orig_file_ref\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "                int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "            elif \"anvil_\" not in table_entry[\"name\"]:\n",
    "                int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "        new_schema_dict[\"tables\"].append(int_table_dict)\n",
    "    for rel_entry in snapshot_details[\"relationships\"]:\n",
    "        int_rel_dict = rel_entry.copy()\n",
    "        int_rel_dict[\"from\"] = int_rel_dict.pop(\"_from\")\n",
    "        new_schema_dict[\"relationships\"].append(int_rel_dict)\n",
    "\n",
    "    # Create a new dataset, unless a target dataset UUID has been provided\n",
    "    if tar_tdr_dataset_uuid:\n",
    "        new_dataset_id = tar_tdr_dataset_uuid\n",
    "        msg_str = f\"Attempting to leverage user-provided target dataset UUID ({tar_tdr_dataset_uuid}) rather than creating a new dataset.\"\n",
    "        logging.info(msg_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Creation\", \"Dataset Creation\", \"Skipped\", msg_str])\n",
    "    else:\n",
    "\n",
    "        # Determine dataset properties - Description\n",
    "        orig_object_name = snapshot_details[\"name\"]\n",
    "        new_description = snapshot_details[\"description\"] + f\"\\n\\nCopy of {src_tdr_object_type} {orig_object_name} from TDR {src_tdr_object_env}.\"\n",
    "        description = tar_tdr_dataset_props[\"description\"] if tar_tdr_dataset_props.get(\"description\") else new_description\n",
    "\n",
    "        # Determine dataset properties - Region\n",
    "        for storage_entry in snapshot_details[\"source\"][0][\"dataset\"][\"storage\"]:\n",
    "            if storage_entry[\"cloud_resource\"] == \"bucket\":\n",
    "                orig_region = storage_entry[\"region\"]\n",
    "                break\n",
    "        if tar_tdr_dataset_props.get(\"region\"):\n",
    "            dataset_region = tar_tdr_dataset_props[\"region\"]\n",
    "        elif tar_tdr_dataset_cloud == \"gcp\" and orig_region:\n",
    "            dataset_region = orig_region\n",
    "        else:\n",
    "            dataset_region = None\n",
    "\n",
    "        # Determine dataset properties - Dedicated Ingest SA\n",
    "        dedicated_ingest_sa = tar_tdr_dataset_props[\"dedicatedIngestServiceAccount\"] if tar_tdr_dataset_props.get(\"dedicatedIngestServiceAccount\") else False\n",
    "\n",
    "        # Determine dataset properties - Self-Hosted\n",
    "        self_hosted = False\n",
    "        if tar_tdr_dataset_cloud == \"azure\":\n",
    "            self_hosted = False\n",
    "        elif tar_tdr_dataset_props.get(\"experimentalSelfHosted\"):\n",
    "            self_hosted = tar_tdr_dataset_props[\"experimentalSelfHosted\"]\n",
    "        else:\n",
    "            self_hosted = snapshot_details[\"source\"][0][\"dataset\"][\"self_hosted\"]\n",
    "        \n",
    "        # Determine dataset properties - Policies\n",
    "        policies = {}\n",
    "        stewards_list = []\n",
    "        custodians_list = []\n",
    "        snapshot_creators_list = []\n",
    "        if tar_tdr_dataset_props.get(\"policies\"):\n",
    "            if tar_tdr_dataset_props[\"policies\"].get(\"stewards\"):\n",
    "                for user in tar_tdr_dataset_props[\"policies\"][\"stewards\"]:\n",
    "                    if user not in stewards_list:\n",
    "                        stewards_list.append(user)\n",
    "            if tar_tdr_dataset_props[\"policies\"].get(\"custodians\"):\n",
    "                for user in tar_tdr_dataset_props[\"policies\"][\"custodians\"]:\n",
    "                    if user not in custodians_list:\n",
    "                        custodians_list.append(user)\n",
    "            if tar_tdr_dataset_props[\"policies\"].get(\"snapshotCreators\"):\n",
    "                for user in tar_tdr_dataset_props[\"policies\"][\"snapshotCreators\"]:\n",
    "                    if user not in snapshot_creators_list:\n",
    "                        snapshot_creators_list.append(user)\n",
    "        policies = {\n",
    "            \"stewards\": stewards_list,\n",
    "            \"custodians\": custodians_list,\n",
    "            \"snapshotCreators\": snapshot_creators_list\n",
    "        }\n",
    "\n",
    "        # Determine dataset properties - Other\n",
    "        phs_id = tar_tdr_dataset_props[\"phsId\"] if tar_tdr_dataset_props.get(\"phsId\") else snapshot_details[\"source\"][0][\"dataset\"][\"phs_id\"]\n",
    "        predictable_file_ids = tar_tdr_dataset_props[\"experimentalPredictableFileIds\"] if tar_tdr_dataset_props.get(\"experimentalPredictableFileIds\") else snapshot_details[\"source\"][0][\"dataset\"][\"predictable_file_ids\"]\n",
    "        secure_monitoring_enabled = tar_tdr_dataset_props[\"enableSecureMonitoring\"] if tar_tdr_dataset_props.get(\"enableSecureMonitoring\") else snapshot_details[\"source\"][0][\"dataset\"][\"secure_monitoring_enabled\"]\n",
    "        properties = tar_tdr_dataset_props[\"properties\"] if tar_tdr_dataset_props.get(\"properties\") else snapshot_details[\"properties\"]\n",
    "        tags = tar_tdr_dataset_props[\"tags\"] if tar_tdr_dataset_props.get(\"tags\") else snapshot_details[\"tags\"]\n",
    "\n",
    "        # Create new TDR dataset\n",
    "        logging.info(\"Submitting dataset creation request.\")\n",
    "        dataset_request = {\n",
    "            \"name\": tar_tdr_dataset_name,\n",
    "            \"description\": description,\n",
    "            \"defaultProfileId\": tar_tdr_billing_profile,\n",
    "            \"cloudPlatform\": tar_tdr_dataset_cloud,\n",
    "            \"region\": dataset_region,\n",
    "            \"phsId\": phs_id,\n",
    "            \"experimentalSelfHosted\": self_hosted,\n",
    "            \"experimentalPredictableFileIds\": predictable_file_ids,\n",
    "            \"dedicatedIngestServiceAccount\": dedicated_ingest_sa,\n",
    "            \"enableSecureMonitoring\": secure_monitoring_enabled,\n",
    "            \"properties\": properties,\n",
    "            \"tags\": tags,\n",
    "            \"policies\": policies,\n",
    "            \"schema\": new_schema_dict\n",
    "        }\n",
    "        attempt_counter = 1\n",
    "        while True:\n",
    "            try:\n",
    "                create_dataset_result, job_id = wait_for_tdr_job(datasets_api.create_dataset(dataset=dataset_request), tdr_host)\n",
    "                logging.info(\"Dataset Creation succeeded: {}\".format(create_dataset_result))\n",
    "                new_dataset_id = create_dataset_result[\"id\"]\n",
    "                config[\"migration_results\"].append([\"Dataset Creation\", \"Dataset Creation\", \"Success\", create_dataset_result])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = f\"Error on Dataset Creation: {str(e)}\"\n",
    "                logging.error(error_str)\n",
    "                if attempt_counter < 3:\n",
    "                    logging.info(\"Retrying Dataset Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Exiting job.\")\n",
    "                    config[\"migration_results\"].append([\"Dataset Creation\", \"Dataset Creation\", \"Failure\", error_str])\n",
    "                    return None, {}, {}\n",
    "    \n",
    "    # Exit function\n",
    "    return new_dataset_id, fileref_col_dict, array_col_dict\n",
    "    \n",
    "# Function to create a new dataset from an existing TDR object\n",
    "def create_dataset(config):\n",
    "    if config[\"source\"][\"tdr_object_type\"] == \"dataset\":\n",
    "        new_dataset_id, fileref_col_dict, array_col_dict = create_dataset_from_dataset(config)\n",
    "    elif config[\"source\"][\"tdr_object_type\"] == \"snapshot\":\n",
    "        new_dataset_id, fileref_col_dict, array_col_dict = create_dataset_from_snapshot(config) \n",
    "    else:\n",
    "        raise Exception(\"Source TDR object type must be 'dataset' or 'snapshot'.\")\n",
    "    return new_dataset_id, fileref_col_dict, array_col_dict\n",
    "        \n",
    "# Main function to migrate a TDR object\n",
    "def migrate_object(config):\n",
    "    \n",
    "    # Set up logging\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    logs_stream_file_path = \"migration_results_\" + current_datetime_string + \".log\"\n",
    "    while logging.root.handlers:\n",
    "        logging.root.removeHandler(logging.root.handlers[-1])\n",
    "    logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.FileHandler(logs_stream_file_path), logging.StreamHandler(sys.stdout)])\n",
    "    logging.getLogger(\"azure\").setLevel(logging.WARNING)\n",
    "\n",
    "    # Start pipeline\n",
    "    logging.info(\"Starting TDR object migration pipeline.\")\n",
    "    logging.info(\"Validating input parameters.\")\n",
    "\n",
    "#     # Parse arguments and collect configuration\n",
    "#     argParser = create_arg_parser()\n",
    "#     parsedArgs = argParser.parse_args(sys.argv[1:])\n",
    "#     with open(parsedArgs.config_path) as config_file:\n",
    "#         config = json.load(config_file)\n",
    "    \n",
    "    # Validate object type\n",
    "    if config[\"source\"][\"tdr_object_type\"] not in [\"dataset\", \"snapshot\"]:\n",
    "        logging.error(\"Please set source.tdr_object_type to either 'dataset' or 'snapshot'.\")\n",
    "        return\n",
    "\n",
    "    # Determine Source TDR host based on environment:\n",
    "    if config[\"source\"][\"tdr_object_env\"] == \"prod\":\n",
    "        config[\"source\"][\"tdr_host\"] = \"https://data.terra.bio\"\n",
    "        config[\"tdr_general_sa\"] = \"datarepo-jade-api@terra-datarepo-production.iam.gserviceaccount.com\"\n",
    "    elif config[\"source\"][\"tdr_object_env\"] == \"dev\":\n",
    "        config[\"source\"][\"tdr_host\"] = \"https://jade.datarepo-dev.broadinstitute.org\"\n",
    "        config[\"tdr_general_sa\"] = \"jade-k8-sa@broad-jade-dev.iam.gserviceaccount.com\"\n",
    "    else:\n",
    "        logging.error(\"Please set source.tdr_object_env to one of 'dev' or 'prod'.\")\n",
    "        return \n",
    "        \n",
    "    # Validate Target Dataset Cloud\n",
    "    if config[\"target\"][\"tdr_dataset_cloud\"] not in [\"gcp\", \"azure\"]:\n",
    "        logging.error(\"Please set target.tdr_dataset_cloud to either 'gcp' or 'azure'.\")\n",
    "        return\n",
    "\n",
    "    # Validate ingest options\n",
    "    if config[\"ingest\"][\"records_processing_method\"] not in [\"in_memory\", \"write_to_cloud\"]:\n",
    "        logging.error(\"Please set ingest.records_processing_method to either 'in_memory' or 'write_to_cloud'.\")\n",
    "        return\n",
    "    if config[\"ingest\"][\"records_processing_method\"] == \"write_to_cloud\" and config[\"ingest\"][\"write_to_cloud_platform\"] not in [\"gcp\", \"azure\"]:\n",
    "        logging.error(\"For 'write_to_cloud' records processing method, please set ingest.write_to_cloud_platform to either 'gcp' or 'azure'.\")\n",
    "        return\n",
    "    if config[\"ingest\"][\"records_processing_method\"] == \"write_to_cloud\" and not config[\"ingest\"][\"write_to_cloud_location\"]:\n",
    "        logging.error(\"For 'write_to_cloud' records processing method, please ensure a cloud location is provided in ingest.write_to_cloud_location.\")\n",
    "        return\n",
    "    if config[\"ingest\"][\"records_processing_method\"] == \"write_to_cloud\" and config[\"ingest\"][\"write_to_cloud_platform\"] == \"azure\" and not config[\"ingest\"][\"write_to_cloud_sas_token\"]:\n",
    "        logging.error(\"For 'write_to_cloud' records processing method with an ingest.write_to_cloud_platform value of 'azure', please ensure a cloud SAS token is provided in ingest.write_to_cloud_sas_token.\")\n",
    "        return\n",
    "    if config[\"ingest\"][\"records_processing_method\"] == \"write_to_cloud\" and config[\"ingest\"][\"write_to_cloud_platform\"] != config[\"target\"][\"tdr_dataset_cloud\"]:\n",
    "        logging.error(\"For 'write_to_cloud' records processing method, the ingest.write_to_cloud_platform parameter must have the same value as target.tdr_dataset_cloud.\")\n",
    "        return\n",
    "\n",
    "    # Determine record/fileref limits\n",
    "    if not config[\"ingest\"][\"max_records_per_ingest_request\"]:\n",
    "        config[\"ingest\"][\"max_records_per_ingest_request\"] = 1000000\n",
    "    if not config[\"ingest\"][\"max_filerefs_per_ingest_request\"]:\n",
    "        config[\"ingest\"][\"max_filerefs_per_ingest_request\"] = 50000 \n",
    "    elif config[\"ingest\"][\"max_filerefs_per_ingest_request\"] > 50000:\n",
    "        logging.warning(\"Parameter ingest.max_filerefs_per_ingest_request set to value above recommended max of 50000. If errors occur in ingestion, try reducing to below this threshold.\")\n",
    "        \n",
    "    # Validate snapshot recreation\n",
    "    if config[\"snapshot\"][\"recreate_snapshot\"] == True and config[\"source\"][\"tdr_object_type\"] != \"snapshot\":\n",
    "        logging.warning(\"Parameter snapshot.recreate_snapshot set to false due to source.tdr_object_type not being 'snapshot'.\")\n",
    "        config[\"snapshot\"][\"recreate_snapshot\"] = False\n",
    "        \n",
    "    # Enforce AnVIL restrictions\n",
    "    if config[\"ingest\"][\"apply_anvil_transforms\"] == True and config[\"target\"][\"tdr_dataset_cloud\"] == \"azure\":\n",
    "        logging.info(\"Default region for AnVIL datasets on Azure is 'southcentralus', so setting the target TDR dataset region to this value.\")\n",
    "        config[\"target\"][\"tdr_dataset_properties\"][\"region\"] = \"southcentralus\"\n",
    "\n",
    "    # Enforce tool/TDR limitations\n",
    "    logging.info(\"Dedicated dataset-specific SAs not currently supported for TDR-to-TDR ingestions, so setting 'dedicatedIngestServiceAccount' dataset property to False by default.\")\n",
    "    config[\"target\"][\"tdr_dataset_properties\"][\"dedicatedIngestServiceAccount\"] = False\n",
    "    if config[\"target\"][\"tdr_dataset_cloud\"] == \"azure\":\n",
    "        logging.info(\"Self-hosted functionality not available for Azure datasets, so setting 'experimentalSelfHosted' dataset property to False by default.\")\n",
    "        config[\"target\"][\"tdr_dataset_properties\"][\"experimentalSelfHosted\"] = False\n",
    "\n",
    "    # Initiate migration pipeline\n",
    "    config[\"migration_results\"] = []\n",
    "    logging.info(\"Starting Dataset Creation step.\")\n",
    "    new_dataset_id, fileref_col_dict, array_col_dict = create_dataset(config)\n",
    "    if new_dataset_id and fileref_col_dict:\n",
    "        logging.info(\"Starting Dataset Ingestion step.\")\n",
    "        populate_new_dataset(config, new_dataset_id, fileref_col_dict, array_col_dict)\n",
    "    else:\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Skipped\", \"Skipped due to upstream failures.\"])\n",
    "    if new_dataset_id and config[\"snapshot\"][\"recreate_snapshot\"]:\n",
    "        logging.info(\"Starting Snapshot Creation step.\")\n",
    "        recreate_snapshot(config, new_dataset_id)\n",
    "    \n",
    "    # Display migration pipeline results\n",
    "    pipeline_results = pd.DataFrame(config[\"migration_results\"], columns = [\"Task\", \"Step\", \"Status\", \"Message\"])\n",
    "    failures = pipeline_results[pipeline_results[\"Status\"].str.contains(\"Failure\")]\n",
    "    #results_formatted = pprint.pformat(pipeline_results.to_dict('index'), indent=4)\n",
    "    #logging.info(\"\\n-----------------------------------------------------------------------------------------------------\\nMigration Pipeline Results:\\n-----------------------------------------------------------------------------------------------------\\n\" + results_formatted)\n",
    "    logging.info(\"Migration Pipeline Results:\") # Remove from standalone script\n",
    "    display(pipeline_results) # Remove from standalone script\n",
    "    logging.info(f\"\\nPipeline finished with {len(failures)} failures.\")\n",
    "    \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "azure_billing_profile = \"9ee23bed-b46c-4561-9103-d2a723113f7f\"\n",
    "anvil_schema = \"ANV5\"\n",
    "dataset_id_pairs_list = [\n",
    "    #[\"gcp_dataset_id\", \"az_dataset_id\"]\n",
    "#     [\"b12fb9be-2ce0-4bfd-8503-732fabba06ab\", \"744c85cc-13d2-4f90-9d2e-d3143cb01edb\"],\n",
    "    [\"34c7cf76-024b-4711-baaa-5d21bf061ed2\", \"da747f0d-950a-4cef-8fae-7e9bc3d92f67\"]\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "for dataset_id_pair in dataset_id_pairs_list:\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    logging.info(f\"Processing data from TDR dataset {dataset_id_pair[0]} to TDR dataset {dataset_id_pair[1]}.\")\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Retrieve original dataset details\n",
    "    logging.info(f\"Retrieving new dataset details from prod environment. UUID:  {dataset_id_pair[1]}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=dataset_id_pair[1], include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        dataset_name = dataset_details[\"name\"]\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "        snapshot_name = dataset_name + \"_\" + anvil_schema + \"_\" + current_datetime_string\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        \n",
    "    # Build config and submit migration job\n",
    "    config = {\n",
    "        \"source\": {\n",
    "            \"tdr_object_uuid\": dataset_id_pair[0],\n",
    "            \"tdr_object_type\": \"dataset\",\n",
    "            \"tdr_object_env\": \"prod\"\n",
    "        },\n",
    "        \"target\": {\n",
    "            \"tdr_billing_profile\": azure_billing_profile,\n",
    "            \"tdr_dataset_uuid\": dataset_id_pair[1],\n",
    "            \"tdr_dataset_name\": \"N/A\",\n",
    "            \"tdr_dataset_cloud\": \"azure\",\n",
    "            \"tdr_dataset_properties\": {},\n",
    "            \"copy_policies\": True\n",
    "        },\n",
    "        \"ingest\": {\n",
    "            \"records_processing_method\": \"in_memory\", \n",
    "            \"write_to_cloud_platform\": \"\",\n",
    "            \"write_to_cloud_location\": \"\",\n",
    "            \"write_to_cloud_sas_token\": \"\",\n",
    "            \"max_records_per_ingest_request\": 250000,\n",
    "            \"max_filerefs_per_ingest_request\": 50000,\n",
    "            \"files_already_ingested\": True,\n",
    "            \"tables_to_ingest\": [],\n",
    "            \"datarepo_row_ids_to_ingest\": [],\n",
    "            \"apply_anvil_transforms\": True\n",
    "        },\n",
    "        \"snapshot\": {\n",
    "            \"recreate_snapshot\": True,\n",
    "            \"new_snapshot_name\": snapshot_name,\n",
    "            \"copy_snapshot_policies\": True\n",
    "        }\n",
    "    }\n",
    "    migrate_object(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to delete dataset = 847d16f5-5664-47d4-936d-563db2543955 and all associated snapshots\n",
      "Attempting to delete dataset = 847d16f5-5664-47d4-936d-563db2543955\n",
      "TDR Job ID: -G13xlP1QI-5u2kVmFMROw\n",
      "Result: {'objectState': 'deleted'}\n"
     ]
    }
   ],
   "source": [
    "# Function to delete a specific TDR Snapshot\n",
    "def delete_snapshot(snapshot_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete snapshot = {}\".format(snapshot_id))\n",
    "    try:\n",
    "        delete_snapshot_result, job_id = wait_for_tdr_job(snapshots_api.delete_snapshot(id=snapshot_id), \"https://data.terra.bio\")\n",
    "        print(\"Result: {}\".format(delete_snapshot_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset\n",
    "def delete_dataset(dataset_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {}\".format(dataset_id))\n",
    "    try:\n",
    "        delete_dataset_result, job_id = wait_for_tdr_job(datasets_api.delete_dataset(id=dataset_id), \"https://data.terra.bio\")\n",
    "        print(\"Result: {}\".format(delete_dataset_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset and all of its Snapshots\n",
    "def delete_dataset_and_all_snapshots(dataset_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {} and all associated snapshots\".format(dataset_id))\n",
    "    dataset_id_list = [dataset_id]\n",
    "    # Delete snapshots\n",
    "    snapshot_list = snapshots_api.enumerate_snapshots(dataset_ids=dataset_id_list)\n",
    "    if snapshot_list.items:\n",
    "        for snapshot in snapshot_list.items:\n",
    "            snapshot_id = str(snapshot.id)\n",
    "            delete_snapshot(snapshot_id)\n",
    "            sleep(10)\n",
    "    # Delete dataset\n",
    "    delete_dataset(dataset_id)\n",
    "\n",
    "# Delete snapshots\n",
    "# snapshot_id_list = [\n",
    "# '1234',\n",
    "# ]\n",
    "# for snapshot_id in snapshot_id_list:\n",
    "#     delete_snapshot(snapshot_id)\n",
    "\n",
    "# Delete datasets and all their associated snapshots\n",
    "dataset_id_list = [\n",
    "'847d16f5-5664-47d4-936d-563db2543955',\n",
    "]\n",
    "for dataset_id in dataset_id_list:\n",
    "    delete_dataset_and_all_snapshots(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BQ Validation Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT gcp_dataset_id, COUNT(DISTINCT az_dataset_id) AS azure_dataset_count, \n",
    "# COUNT(*) AS total_record_count,\n",
    "# COUNT(source_path) AS source_path_populated_count,\n",
    "# COUNT(target_path) AS target_path_populated_count, \n",
    "# COUNT(size_in_bytes) AS size_in_bytes_populated_count,\n",
    "# COUNT(md5_hash) AS md5_hash_populated_count,\n",
    "# COUNT(orig_tdr_file_id) AS orig_tdr_file_id_populated_count\n",
    "# FROM `broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list` \n",
    "# GROUP BY gcp_dataset_id\n",
    "# ORDER BY gcp_dataset_id\n",
    "# ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdr_host = \"https://data.terra.bio\"\n",
    "api_client = refresh_tdr_api_client(tdr_host)\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SnapshotsApi in module data_repo_client.api.snapshots_api object:\n",
      "\n",
      "class SnapshotsApi(builtins.object)\n",
      " |  SnapshotsApi(api_client=None)\n",
      " |  \n",
      " |  NOTE: This class is auto generated by OpenAPI Generator\n",
      " |  Ref: https://openapi-generator.tech\n",
      " |  \n",
      " |  Do not edit the class manually.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, api_client=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add_snapshot_auth_domain(self, id, **kwargs)\n",
      " |      add_snapshot_auth_domain  # noqa: E501\n",
      " |      \n",
      " |      Add an auth domain to the snapshot. *** WARNING - Once this is set it cannot be modified *** A Sam auth domain consists of a collection of user groups. If a snapshot has an auth domain, users must have access to the resource directly (via a policy) and belong to all user groups in the auth domain in order to view and export the data. For example, if a user is a snapshot steward but is not included in the auth domain, Sam will prevent that user from accessing the resource.  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.add_snapshot_auth_domain(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param list[str] request_body:\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: AddAuthDomainResponseModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  add_snapshot_auth_domain_with_http_info(self, id, **kwargs)\n",
      " |      add_snapshot_auth_domain  # noqa: E501\n",
      " |      \n",
      " |      Add an auth domain to the snapshot. *** WARNING - Once this is set it cannot be modified *** A Sam auth domain consists of a collection of user groups. If a snapshot has an auth domain, users must have access to the resource directly (via a policy) and belong to all user groups in the auth domain in order to view and export the data. For example, if a user is a snapshot steward but is not included in the auth domain, Sam will prevent that user from accessing the resource.  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.add_snapshot_auth_domain_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param list[str] request_body:\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(AddAuthDomainResponseModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  add_snapshot_policy_member(self, id, policy_name, **kwargs)\n",
      " |      add_snapshot_policy_member  # noqa: E501\n",
      " |      \n",
      " |      Adds a member to the specified policy for the snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.add_snapshot_policy_member(id, policy_name, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str policy_name: The relevant policy (required)\n",
      " |      :param PolicyMemberRequest policy_member: Snapshot to change the policy of\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: PolicyResponse\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  add_snapshot_policy_member_with_http_info(self, id, policy_name, **kwargs)\n",
      " |      add_snapshot_policy_member  # noqa: E501\n",
      " |      \n",
      " |      Adds a member to the specified policy for the snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.add_snapshot_policy_member_with_http_info(id, policy_name, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str policy_name: The relevant policy (required)\n",
      " |      :param PolicyMemberRequest policy_member: Snapshot to change the policy of\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(PolicyResponse, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  create_search_index(self, id, search_index_request, **kwargs)\n",
      " |      create_search_index  # noqa: E501\n",
      " |      \n",
      " |      Create a new search index for a snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.create_search_index(id, search_index_request, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param SearchIndexRequest search_index_request: Index to create from SQL (required)\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SearchIndexModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  create_search_index_with_http_info(self, id, search_index_request, **kwargs)\n",
      " |      create_search_index  # noqa: E501\n",
      " |      \n",
      " |      Create a new search index for a snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.create_search_index_with_http_info(id, search_index_request, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param SearchIndexRequest search_index_request: Index to create from SQL (required)\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SearchIndexModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  create_snapshot(self, **kwargs)\n",
      " |      create_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Create a new snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.create_snapshot(async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param SnapshotRequestModel snapshot: Snapshot to create\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: JobModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  create_snapshot_with_http_info(self, **kwargs)\n",
      " |      create_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Create a new snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.create_snapshot_with_http_info(async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param SnapshotRequestModel snapshot: Snapshot to create\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(JobModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  delete_snapshot(self, id, **kwargs)\n",
      " |      delete_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Delete a snapshot by id  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.delete_snapshot(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: JobModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  delete_snapshot_policy_member(self, id, policy_name, member_email, **kwargs)\n",
      " |      delete_snapshot_policy_member  # noqa: E501\n",
      " |      \n",
      " |      Removes a member from the specified policy for the snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.delete_snapshot_policy_member(id, policy_name, member_email, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str policy_name: The relevant policy (required)\n",
      " |      :param str member_email: The email of the user to remove (required)\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: PolicyResponse\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  delete_snapshot_policy_member_with_http_info(self, id, policy_name, member_email, **kwargs)\n",
      " |      delete_snapshot_policy_member  # noqa: E501\n",
      " |      \n",
      " |      Removes a member from the specified policy for the snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.delete_snapshot_policy_member_with_http_info(id, policy_name, member_email, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str policy_name: The relevant policy (required)\n",
      " |      :param str member_email: The email of the user to remove (required)\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(PolicyResponse, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  delete_snapshot_with_http_info(self, id, **kwargs)\n",
      " |      delete_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Delete a snapshot by id  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.delete_snapshot_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(JobModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  enumerate_snapshots(self, **kwargs)\n",
      " |      enumerate_snapshots  # noqa: E501\n",
      " |      \n",
      " |      Returns a list of all of the snapshots to which the caller has access. Access may be granted directly via SAM and/or indirectly via a user's linked RAS passport in Terra.  Snapshot accessibility derived from a linked RAS passport will be attributed to the \"reader\" role.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.enumerate_snapshots(async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param int offset: The number of items to skip before starting to collect the result set.\n",
      " |      :param int limit: The numbers of items to return.\n",
      " |      :param EnumerateSortByParam sort: The field to use for sorting.\n",
      " |      :param SqlSortDirection direction: The direction to sort.\n",
      " |      :param str filter: Filter the results where this string is a case insensitive match in the name or description.\n",
      " |      :param str region: Filter the results where this string is a case insensitive match in any of the cloud storage regions used by the source datasets.\n",
      " |      :param list[str] dataset_ids: Filter the results where these datasetIds are source datasets.\n",
      " |      :param list[str] tags: Filter the results where these case sensitive tags are applied to the snapshots \n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: EnumerateSnapshotModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  enumerate_snapshots_with_http_info(self, **kwargs)\n",
      " |      enumerate_snapshots  # noqa: E501\n",
      " |      \n",
      " |      Returns a list of all of the snapshots to which the caller has access. Access may be granted directly via SAM and/or indirectly via a user's linked RAS passport in Terra.  Snapshot accessibility derived from a linked RAS passport will be attributed to the \"reader\" role.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.enumerate_snapshots_with_http_info(async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param int offset: The number of items to skip before starting to collect the result set.\n",
      " |      :param int limit: The numbers of items to return.\n",
      " |      :param EnumerateSortByParam sort: The field to use for sorting.\n",
      " |      :param SqlSortDirection direction: The direction to sort.\n",
      " |      :param str filter: Filter the results where this string is a case insensitive match in the name or description.\n",
      " |      :param str region: Filter the results where this string is a case insensitive match in any of the cloud storage regions used by the source datasets.\n",
      " |      :param list[str] dataset_ids: Filter the results where these datasetIds are source datasets.\n",
      " |      :param list[str] tags: Filter the results where these case sensitive tags are applied to the snapshots \n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(EnumerateSnapshotModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  export_snapshot(self, id, **kwargs)\n",
      " |      export_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Export a snapshot by id  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.export_snapshot(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param bool export_gs_paths: Convert DRS urls to GS paths in the output parquet file. Note, GS paths could change over time.\n",
      " |      :param bool validate_primary_key_uniqueness: Verify that primary keys are unique in all exported tables. Required for proper Terra workspace integration\n",
      " |      :param bool sign_urls: If true, then the export will generate signed URLs for the exported files and manifest. If false, then the export will return URLs native to the cloud backing the snapshot.  Note that this has no effect on Azure backed snapshots\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: JobModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  export_snapshot_with_http_info(self, id, **kwargs)\n",
      " |      export_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Export a snapshot by id  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.export_snapshot_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param bool export_gs_paths: Convert DRS urls to GS paths in the output parquet file. Note, GS paths could change over time.\n",
      " |      :param bool validate_primary_key_uniqueness: Verify that primary keys are unique in all exported tables. Required for proper Terra workspace integration\n",
      " |      :param bool sign_urls: If true, then the export will generate signed URLs for the exported files and manifest. If false, then the export will return URLs native to the cloud backing the snapshot.  Note that this has no effect on Azure backed snapshots\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(JobModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  get_snapshot_ids_and_roles(self, **kwargs)\n",
      " |      get_snapshot_ids_and_roles  # noqa: E501\n",
      " |      \n",
      " |      Get accessible snapshot IDs mapped to the roles which confer access. Access may be granted directly via Sam and/or indirectly via a user's linked RAS passport in Terra.  Snapshot accessibility derived from a linked RAS passport will be attributed to the \"reader\" role.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.get_snapshot_ids_and_roles(async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SnapshotIdsAndRolesModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  get_snapshot_ids_and_roles_with_http_info(self, **kwargs)\n",
      " |      get_snapshot_ids_and_roles  # noqa: E501\n",
      " |      \n",
      " |      Get accessible snapshot IDs mapped to the roles which confer access. Access may be granted directly via Sam and/or indirectly via a user's linked RAS passport in Terra.  Snapshot accessibility derived from a linked RAS passport will be attributed to the \"reader\" role.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.get_snapshot_ids_and_roles_with_http_info(async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SnapshotIdsAndRolesModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  get_snapshot_tags(self, **kwargs)\n",
      " |      get_snapshot_tags  # noqa: E501\n",
      " |      \n",
      " |      Get accessible snapshot tags. Access may be granted directly via Sam and/or indirectly via a user's linked RAS passport in Terra.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.get_snapshot_tags(async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str filter: Filter to tags with this string as a case-insensitive match.\n",
      " |      :param int limit: The maximum number of tags to return, in descending order of occurrence count. All accessible tags will be returned if unspecified. \n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: TagCountResultModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  get_snapshot_tags_with_http_info(self, **kwargs)\n",
      " |      get_snapshot_tags  # noqa: E501\n",
      " |      \n",
      " |      Get accessible snapshot tags. Access may be granted directly via Sam and/or indirectly via a user's linked RAS passport in Terra.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.get_snapshot_tags_with_http_info(async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str filter: Filter to tags with this string as a case-insensitive match.\n",
      " |      :param int limit: The maximum number of tags to return, in descending order of occurrence count. All accessible tags will be returned if unspecified. \n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(TagCountResultModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  link_duos_dataset_to_snapshot(self, id, duos_id, **kwargs)\n",
      " |      link_duos_dataset_to_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Link the DUOS dataset to the snapshot so that its authorized users are synced as snapshot readers. NOTE: This is an experimental feature and its response body may change.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.link_duos_dataset_to_snapshot(id, duos_id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str duos_id: (required)\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SnapshotLinkDuosDatasetResponse\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  link_duos_dataset_to_snapshot_with_http_info(self, id, duos_id, **kwargs)\n",
      " |      link_duos_dataset_to_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Link the DUOS dataset to the snapshot so that its authorized users are synced as snapshot readers. NOTE: This is an experimental feature and its response body may change.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.link_duos_dataset_to_snapshot_with_http_info(id, duos_id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str duos_id: (required)\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SnapshotLinkDuosDatasetResponse, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  list_files(self, id, **kwargs)\n",
      " |      list_files  # noqa: E501\n",
      " |      \n",
      " |      List metadata for all files in a snapshot.  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.list_files(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param int offset: The number of items to skip before starting to collect the result set.\n",
      " |      :param int limit: The numbers of items to return.\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: list[FileModel]\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  list_files_with_http_info(self, id, **kwargs)\n",
      " |      list_files  # noqa: E501\n",
      " |      \n",
      " |      List metadata for all files in a snapshot.  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.list_files_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param int offset: The number of items to skip before starting to collect the result set.\n",
      " |      :param int limit: The numbers of items to return.\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(list[FileModel], status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  lookup_snapshot_file_by_id(self, id, fileid, **kwargs)\n",
      " |      lookup_snapshot_file_by_id  # noqa: E501\n",
      " |      \n",
      " |      Lookup metadata for one file  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.lookup_snapshot_file_by_id(id, fileid, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str fileid: A file id (required)\n",
      " |      :param int depth: Enumeration depth; -1 means fully expand; 0 means no expansion; 1..N expands that many subdirectories\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: FileModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  lookup_snapshot_file_by_id_with_http_info(self, id, fileid, **kwargs)\n",
      " |      lookup_snapshot_file_by_id  # noqa: E501\n",
      " |      \n",
      " |      Lookup metadata for one file  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.lookup_snapshot_file_by_id_with_http_info(id, fileid, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str fileid: A file id (required)\n",
      " |      :param int depth: Enumeration depth; -1 means fully expand; 0 means no expansion; 1..N expands that many subdirectories\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(FileModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  lookup_snapshot_file_by_path(self, id, path, **kwargs)\n",
      " |      lookup_snapshot_file_by_path  # noqa: E501\n",
      " |      \n",
      " |      Lookup metadata for one file  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.lookup_snapshot_file_by_path(id, path, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str path: URL-encoded full path to a file or directory (required)\n",
      " |      :param int depth: Enumeration depth; -1 means fully expand; 0 means no expansion; 1..N expands that many subdirectories\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: FileModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  lookup_snapshot_file_by_path_with_http_info(self, id, path, **kwargs)\n",
      " |      lookup_snapshot_file_by_path  # noqa: E501\n",
      " |      \n",
      " |      Lookup metadata for one file  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.lookup_snapshot_file_by_path_with_http_info(id, path, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str path: URL-encoded full path to a file or directory (required)\n",
      " |      :param int depth: Enumeration depth; -1 means fully expand; 0 means no expansion; 1..N expands that many subdirectories\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(FileModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  lookup_snapshot_preview_by_id(self, id, table, **kwargs)\n",
      " |      lookup_snapshot_preview_by_id  # noqa: E501\n",
      " |      \n",
      " |      Retrieve data for a table in a snapshot. This endpoint is deprecated, please use the POST version.  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.lookup_snapshot_preview_by_id(id, table, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str table: Name of table to get data from (required)\n",
      " |      :param int offset: The number of rows to skip when retrieving the next page\n",
      " |      :param int limit: The number of rows to return for the data\n",
      " |      :param str sort: The table column to sort by\n",
      " |      :param SqlSortDirection direction: The direction to sort.\n",
      " |      :param str filter: A SQL WHERE clause to filter the table results.\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SnapshotPreviewModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  lookup_snapshot_preview_by_id_with_http_info(self, id, table, **kwargs)\n",
      " |      lookup_snapshot_preview_by_id  # noqa: E501\n",
      " |      \n",
      " |      Retrieve data for a table in a snapshot. This endpoint is deprecated, please use the POST version.  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.lookup_snapshot_preview_by_id_with_http_info(id, table, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str table: Name of table to get data from (required)\n",
      " |      :param int offset: The number of rows to skip when retrieving the next page\n",
      " |      :param int limit: The number of rows to return for the data\n",
      " |      :param str sort: The table column to sort by\n",
      " |      :param SqlSortDirection direction: The direction to sort.\n",
      " |      :param str filter: A SQL WHERE clause to filter the table results.\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SnapshotPreviewModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  patch_snapshot(self, id, **kwargs)\n",
      " |      patch_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Update supported fields of the specified snapshot.  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.patch_snapshot(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param SnapshotPatchRequestModel snapshot_patch_request_model: A 'lite' snapshot definition (used to modify supported fields of a snapshot). Null assignments will be ignored.\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SnapshotSummaryModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  patch_snapshot_with_http_info(self, id, **kwargs)\n",
      " |      patch_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Update supported fields of the specified snapshot.  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.patch_snapshot_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param SnapshotPatchRequestModel snapshot_patch_request_model: A 'lite' snapshot definition (used to modify supported fields of a snapshot). Null assignments will be ignored.\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SnapshotSummaryModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  query_search_indices(self, search_query_request, **kwargs)\n",
      " |      query_search_indices  # noqa: E501\n",
      " |      \n",
      " |      Run a query on the metadata of one or more snapshots  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.query_search_indices(search_query_request, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param SearchQueryRequest search_query_request: Search query to run against one or more snapshots (required)\n",
      " |      :param int offset: The number of results to skip before retrieving the next page\n",
      " |      :param int limit: The number of search results to return\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SearchQueryResultModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  query_search_indices_with_http_info(self, search_query_request, **kwargs)\n",
      " |      query_search_indices  # noqa: E501\n",
      " |      \n",
      " |      Run a query on the metadata of one or more snapshots  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.query_search_indices_with_http_info(search_query_request, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param SearchQueryRequest search_query_request: Search query to run against one or more snapshots (required)\n",
      " |      :param int offset: The number of results to skip before retrieving the next page\n",
      " |      :param int limit: The number of search results to return\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SearchQueryResultModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  query_snapshot_data_by_id(self, id, table, **kwargs)\n",
      " |      query_snapshot_data_by_id  # noqa: E501\n",
      " |      \n",
      " |      Retrieve data for a table in a snapshot.  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.query_snapshot_data_by_id(id, table, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str table: Name of table to get data from (required)\n",
      " |      :param QueryDataRequestModel query_data_request_model: Parameters to filter results\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SnapshotPreviewModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  query_snapshot_data_by_id_with_http_info(self, id, table, **kwargs)\n",
      " |      query_snapshot_data_by_id  # noqa: E501\n",
      " |      \n",
      " |      Retrieve data for a table in a snapshot.  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.query_snapshot_data_by_id_with_http_info(id, table, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param str table: Name of table to get data from (required)\n",
      " |      :param QueryDataRequestModel query_data_request_model: Parameters to filter results\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SnapshotPreviewModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  retrieve_snapshot(self, id, **kwargs)\n",
      " |      retrieve_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Retrieve a snapshot by id  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.retrieve_snapshot(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param list[SnapshotRetrieveIncludeModel] include: A list of what to include with the snapshot object\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SnapshotModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  retrieve_snapshot_policies(self, id, **kwargs)\n",
      " |      retrieve_snapshot_policies  # noqa: E501\n",
      " |      \n",
      " |      Retrieve the read and discover policies for the snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.retrieve_snapshot_policies(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: PolicyResponse\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  retrieve_snapshot_policies_with_http_info(self, id, **kwargs)\n",
      " |      retrieve_snapshot_policies  # noqa: E501\n",
      " |      \n",
      " |      Retrieve the read and discover policies for the snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.retrieve_snapshot_policies_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(PolicyResponse, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  retrieve_snapshot_summary(self, id, **kwargs)\n",
      " |      retrieve_snapshot_summary  # noqa: E501\n",
      " |      \n",
      " |      Retrieve a snapshot summary by id.  If the caller has permission to list the snapshot in an enumeration, they will have permission to retrieve its summary, whereas they may lack necessary permissions to retrieve the full snapshot object.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.retrieve_snapshot_summary(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SnapshotSummaryModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  retrieve_snapshot_summary_with_http_info(self, id, **kwargs)\n",
      " |      retrieve_snapshot_summary  # noqa: E501\n",
      " |      \n",
      " |      Retrieve a snapshot summary by id.  If the caller has permission to list the snapshot in an enumeration, they will have permission to retrieve its summary, whereas they may lack necessary permissions to retrieve the full snapshot object.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.retrieve_snapshot_summary_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SnapshotSummaryModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  retrieve_snapshot_with_http_info(self, id, **kwargs)\n",
      " |      retrieve_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Retrieve a snapshot by id  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.retrieve_snapshot_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param list[SnapshotRetrieveIncludeModel] include: A list of what to include with the snapshot object\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SnapshotModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  retrieve_user_snapshot_roles(self, id, **kwargs)\n",
      " |      retrieve_user_snapshot_roles  # noqa: E501\n",
      " |      \n",
      " |      Retrieve the roles the calling user has on the snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.retrieve_user_snapshot_roles(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: list[str]\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  retrieve_user_snapshot_roles_with_http_info(self, id, **kwargs)\n",
      " |      retrieve_user_snapshot_roles  # noqa: E501\n",
      " |      \n",
      " |      Retrieve the roles the calling user has on the snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.retrieve_user_snapshot_roles_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(list[str], status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  unlink_duos_dataset_from_snapshot(self, id, **kwargs)\n",
      " |      unlink_duos_dataset_from_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Unlink the DUOS dataset from the snapshot so that its authorized users are no longer synced as snapshot readers. NOTE: This is an experimental feature and its response body may change.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.unlink_duos_dataset_from_snapshot(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SnapshotLinkDuosDatasetResponse\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  unlink_duos_dataset_from_snapshot_with_http_info(self, id, **kwargs)\n",
      " |      unlink_duos_dataset_from_snapshot  # noqa: E501\n",
      " |      \n",
      " |      Unlink the DUOS dataset from the snapshot so that its authorized users are no longer synced as snapshot readers. NOTE: This is an experimental feature and its response body may change.   # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.unlink_duos_dataset_from_snapshot_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SnapshotLinkDuosDatasetResponse, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  update_snapshot_tags(self, id, **kwargs)\n",
      " |      update_snapshot_tags  # noqa: E501\n",
      " |      \n",
      " |      Update tags on snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.update_snapshot_tags(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param TagUpdateRequestModel tag_update_request_model:\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: SnapshotSummaryModel\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  update_snapshot_tags_with_http_info(self, id, **kwargs)\n",
      " |      update_snapshot_tags  # noqa: E501\n",
      " |      \n",
      " |      Update tags on snapshot  # noqa: E501\n",
      " |      This method makes a synchronous HTTP request by default. To make an\n",
      " |      asynchronous HTTP request, please pass async_req=True\n",
      " |      >>> thread = api.update_snapshot_tags_with_http_info(id, async_req=True)\n",
      " |      >>> result = thread.get()\n",
      " |      \n",
      " |      :param async_req bool: execute request asynchronously\n",
      " |      :param str id: A UUID to used to identify an object in the repository (required)\n",
      " |      :param TagUpdateRequestModel tag_update_request_model:\n",
      " |      :param _return_http_data_only: response data without head status code\n",
      " |                                     and headers\n",
      " |      :param _preload_content: if False, the urllib3.HTTPResponse object will\n",
      " |                               be returned without reading/decoding response\n",
      " |                               data. Default is True.\n",
      " |      :param _request_timeout: timeout setting for this request. If one\n",
      " |                               number provided, it will be total request\n",
      " |                               timeout. It can also be a pair (tuple) of\n",
      " |                               (connection, read) timeouts.\n",
      " |      :return: tuple(SnapshotSummaryModel, status_code(int), headers(HTTPHeaderDict))\n",
      " |               If the method is called asynchronously,\n",
      " |               returns the request thread.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(snapshots_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
