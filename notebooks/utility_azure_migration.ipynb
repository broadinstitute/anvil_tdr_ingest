{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade data_repo_client\n",
    "# !wget https://aka.ms/downloadazcopy-v10-linux\n",
    "# !tar -xvf downloadazcopy-v10-linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     31,
     43
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import import_ipynb\n",
    "import data_repo_client\n",
    "import google.auth\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from time import sleep\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import ingest_pipeline_utilities as utils\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import subprocess\n",
    "\n",
    "# Configure logging format\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Function to refresh TDR API client\n",
    "def refresh_tdr_api_client(host):\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    config = data_repo_client.Configuration()\n",
    "    config.host = host\n",
    "    config.access_token = creds.token\n",
    "    api_client = data_repo_client.ApiClient(configuration=config)\n",
    "    api_client.client_side_validation = False\n",
    "    return api_client\n",
    "\n",
    "# Function to wait for TDR job completion\n",
    "def wait_for_tdr_job(job_model, host):\n",
    "    result = job_model\n",
    "    print(\"TDR Job ID: \" + job_model.id)\n",
    "    counter = 0\n",
    "    job_state = \"UNKNOWN\"\n",
    "    while True:\n",
    "        # Re-establish credentials and API clients every 30 minutes\n",
    "        if counter == 0 or counter%180 == 0:\n",
    "            api_client = refresh_tdr_api_client(host)\n",
    "            jobs_api = data_repo_client.JobsApi(api_client=api_client)\n",
    "        # Check for TDR connectivity issues and raise exception if the issue persists\n",
    "        conn_err_counter = 0\n",
    "        while job_state == \"UNKNOWN\":\n",
    "            conn_err_counter += 1\n",
    "            if conn_err_counter >= 10:\n",
    "                raise Exception(\"Error interacting with TDR: {}\".format(result.status_code)) \n",
    "            elif result == None or result.status_code in [\"500\", \"502\", \"503\", \"504\"]:\n",
    "                sleep(10)\n",
    "                counter += 1\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        result = jobs_api.retrieve_job(job_model.id)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 5:\n",
    "                            attempt_counter += 1\n",
    "                            sleep(10)\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "            else:\n",
    "                job_state = \"KNOWN\"\n",
    "        # Check if job is still running, and sleep/re-check if so\n",
    "        if job_state == \"KNOWN\" and result.job_status == \"running\":\n",
    "            sleep(10)\n",
    "            counter += 1\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    result = jobs_api.retrieve_job(job_model.id)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 5:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "        # If job has returned as failed, confirm this is the correct state and retrieve result if so\n",
    "        elif job_state == \"KNOWN\" and result.job_status == \"failed\":\n",
    "            fail_counter = 0\n",
    "            while True:\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        result = jobs_api.retrieve_job(job_model.id)\n",
    "                        if result.job_status == \"failed\":\n",
    "                            fail_counter += 1\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 5:\n",
    "                            sleep(10)\n",
    "                            attempt_counter += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "                if fail_counter >= 3:\n",
    "                    try:\n",
    "                        fail_result = jobs_api.retrieve_job_result(job_model.id)\n",
    "                        raise Exception(\"Job \" + job_model.id + \" failed: \" + fail_result)\n",
    "                    except Exception as e:\n",
    "                        raise Exception(\"Job \" + job_model.id + \" failed: \" + str(e))\n",
    "        # If a job has returned as succeeded, retrieve result\n",
    "        elif job_state == \"KNOWN\" and result.job_status == \"succeeded\":\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return jobs_api.retrieve_job_result(job_model.id), job_model.id\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 3:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        return \"Job succeeded, but error retrieving job result: {}\".format(str(e)), job_model.id\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized job state: {}\".format(result.job_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrating TDR Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 1: Pre-Connector Processing\n",
    "For the list of GCP TDR datasets provided:\n",
    "1. Extract the schema\n",
    "2. Create an Azure TDR dataset using the extracted schema\n",
    "3. Build a manifest of files to be copied from the GCP dataset to the Azure dataset and write to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     14,
     145
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to build default target TDR dataset name\n",
    "def format_dataset_name(input_str):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "    input_str = input_str[:-9]\n",
    "    output_str = \"ANVIL_\" + re.sub(\"^ANVIL[_]?\", \"\", input_str, flags=re.IGNORECASE) + \"_\" + current_date_string\n",
    "    output_str = re.sub(\"[^a-zA-Z0-9_]\", \"_\", output_str)\n",
    "    return output_str\n",
    "\n",
    "# Function to create a new TDR dataset from an existing TDR dataset\n",
    "def create_dataset_from_dataset(src_tdr_object_uuid, tar_tdr_object_uuid, billing_profile):\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Retrieve original dataset details\n",
    "    logging.info(f\"Retrieving original dataset details from prod environment. UUID:  {src_tdr_object_uuid}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=src_tdr_object_uuid, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        orig_object_name = dataset_details[\"name\"]\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset {src_tdr_object_uuid} in TDR prod environment: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # If target dataset specified, retrieve name\n",
    "    if tar_tdr_object_uuid:\n",
    "        new_dataset_id = tar_tdr_object_uuid\n",
    "        logging.info(f\"Retrieving new dataset details from prod environment. UUID:  {tar_tdr_object_uuid}\")\n",
    "        try:\n",
    "            dataset_details = datasets_api.retrieve_dataset(id=tar_tdr_object_uuid, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "            new_object_name = dataset_details[\"name\"]\n",
    "        except Exception as e:\n",
    "            error_str = f\"Error retrieving details from dataset {tar_tdr_object_uuid} in TDR prod environment: {str(e)}\"\n",
    "            logging.error(error_str)\n",
    "            return None, None, None, None, None \n",
    "    else:\n",
    "        # Build new dataset schema\n",
    "        apply_anvil_transforms = True\n",
    "        new_schema_dict = {\"tables\": [], \"relationships\": [], \"assets\": []}\n",
    "        for table_entry in dataset_details[\"schema\"][\"tables\"]:\n",
    "            int_table_dict = table_entry.copy()\n",
    "            int_table_dict[\"primaryKey\"] = int_table_dict.pop(\"primary_key\")\n",
    "            for key in [\"partition_mode\", \"date_partition_options\", \"int_partition_options\", \"row_count\"]:\n",
    "                del int_table_dict[key]\n",
    "            for idx, column_entry in enumerate(table_entry[\"columns\"]):\n",
    "                if column_entry[\"datatype\"] == \"integer\":\n",
    "                    table_entry[\"columns\"][idx][\"datatype\"] = \"int64\"\n",
    "            if apply_anvil_transforms:\n",
    "                if table_entry[\"name\"] == \"file_inventory\":\n",
    "                    int_table_dict[\"columns\"].append({\"name\": \"orig_file_ref\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "                    int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "                elif \"anvil_\" not in table_entry[\"name\"]:\n",
    "                    int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "            new_schema_dict[\"tables\"].append(int_table_dict)\n",
    "        for rel_entry in dataset_details[\"schema\"][\"relationships\"]:\n",
    "            int_rel_dict = rel_entry.copy()\n",
    "            int_rel_dict[\"from\"] = int_rel_dict.pop(\"_from\")\n",
    "            new_schema_dict[\"relationships\"].append(int_rel_dict)\n",
    "        for asset_entry in dataset_details[\"schema\"][\"assets\"]:\n",
    "            int_asset_dict = asset_entry.copy()\n",
    "            int_asset_dict[\"rootTable\"] = int_asset_dict.pop(\"root_table\")\n",
    "            int_asset_dict[\"rootColumn\"] = int_asset_dict.pop(\"root_column\")\n",
    "            new_schema_dict[\"assets\"].append(int_asset_dict)\n",
    "\n",
    "        # Retrieve original dataset policies\n",
    "        try:\n",
    "            dataset_policies = datasets_api.retrieve_dataset_policies(id=src_tdr_object_uuid).to_dict()\n",
    "            for policy in dataset_policies[\"policies\"]:\n",
    "                if policy[\"name\"] == \"steward\":\n",
    "                    stewards_list = policy[\"members\"]\n",
    "                elif policy[\"name\"] == \"custodian\":\n",
    "                    custodians_list = policy[\"members\"]\n",
    "                elif policy[\"name\"] == \"snapshot_creator\":\n",
    "                    snapshot_creators_list = policy[\"members\"]\n",
    "        except:\n",
    "            logging.info(\"Error retrieving original dataset policies. Skipping policy copy.\")\n",
    "            stewards_list = []\n",
    "            custodians_list = []\n",
    "            snapshot_creators_list = []\n",
    "        policies = {\n",
    "            \"stewards\": stewards_list,\n",
    "            \"custodians\": custodians_list,\n",
    "            \"snapshotCreators\": snapshot_creators_list\n",
    "        }\n",
    "\n",
    "        # Determine dataset properties\n",
    "        new_object_name = format_dataset_name(orig_object_name)\n",
    "        new_description = dataset_details[\"description\"] + f\"\\n\\nCopy of dataset {orig_object_name} from TDR prod.\"\n",
    "        self_hosted = False\n",
    "        dedicated_ingest_sa = False\n",
    "        phs_id = dataset_details[\"phs_id\"]\n",
    "        predictable_file_ids = dataset_details[\"predictable_file_ids\"]\n",
    "        secure_monitoring_enabled = dataset_details[\"secure_monitoring_enabled\"]\n",
    "        properties = dataset_details[\"properties\"]\n",
    "        tags = dataset_details[\"tags\"]\n",
    "\n",
    "        # Create new TDR dataset\n",
    "        logging.info(\"Submitting dataset creation request.\")\n",
    "        dataset_request = {\n",
    "            \"name\": new_object_name,\n",
    "            \"description\": new_description,\n",
    "            \"defaultProfileId\": billing_profile,\n",
    "            \"cloudPlatform\": \"azure\",\n",
    "            \"region\": \"southcentralus\",\n",
    "            \"phsId\": phs_id,\n",
    "            \"experimentalSelfHosted\": self_hosted,\n",
    "            \"experimentalPredictableFileIds\": predictable_file_ids,\n",
    "            \"dedicatedIngestServiceAccount\": dedicated_ingest_sa,\n",
    "            \"enableSecureMonitoring\": secure_monitoring_enabled,\n",
    "            \"properties\": properties,\n",
    "            \"tags\": tags,\n",
    "            \"policies\": policies,\n",
    "            \"schema\": new_schema_dict\n",
    "        }\n",
    "        attempt_counter = 1\n",
    "        while True:\n",
    "            try:\n",
    "                create_dataset_result, job_id = wait_for_tdr_job(datasets_api.create_dataset(dataset=dataset_request), \"https://data.terra.bio\")\n",
    "                logging.info(\"Dataset Creation succeeded: {}\".format(create_dataset_result))\n",
    "                new_dataset_id = create_dataset_result[\"id\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = f\"Error on Dataset Creation: {str(e)}\"\n",
    "                logging.error(error_str)\n",
    "                if attempt_counter < 3:\n",
    "                    logging.info(\"Retrying Dataset Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Exiting job.\")\n",
    "                    return None, None, None, None, None\n",
    "        \n",
    "    # Exit function\n",
    "    return orig_object_name, new_dataset_id, new_object_name, bq_project, bq_dataset\n",
    "\n",
    "# Function to create file transfer details\n",
    "def output_file_details(orig_dataset_id, orig_dataset_name, new_dataset_id, new_dataset_name, bq_project, bq_dataset, public_flag, target_bigquery_table, delete_existing_records):\n",
    "    \n",
    "    # Setup/refresh TDR clients (and BQ client)\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Clear records from target BQ table\n",
    "    if delete_existing_records:\n",
    "        logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "        delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE gcp_dataset_id = '{orig_dataset_id}'\"\"\"\n",
    "        try:\n",
    "            delete_query_job = client.query(delete_query)\n",
    "            delete_query_job.result()\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error deleting records for the original dataset from the target BQ table.\")\n",
    "    \n",
    "    # Retrieve table data from the original dataset and write to target BQ table\n",
    "    logging.info(f\"Fetching and recording all rows from table 'file_inventory' in the original dataset ({orig_dataset_id}). BQ Project = '{bq_project}' and BQ Dataset = '{bq_dataset}'.\")\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.destination = target_bigquery_table\n",
    "    job_config.write_disposition = \"WRITE_APPEND\"\n",
    "    query = f\"\"\"WITH drlh_deduped AS\n",
    "                (\n",
    "                  SELECT DISTINCT file_id, target_path, source_name \n",
    "                  FROM \n",
    "                  (\n",
    "                    SELECT *, ROW_NUMBER() OVER (PARTITION BY source_name ORDER BY load_time DESC) AS rn\n",
    "                    --SELECT *, ROW_NUMBER() OVER (PARTITION BY source_name, target_path ORDER BY load_time DESC) AS rn\n",
    "                    FROM `{bq_project}.{bq_dataset}.datarepo_load_history`\n",
    "                    WHERE state = \"succeeded\" \n",
    "                  )\n",
    "                  WHERE rn = 1\n",
    "                ),\n",
    "                file_records AS\n",
    "                (\n",
    "                  SELECT '{orig_dataset_id}' AS gcp_dataset_id, '{orig_dataset_name}' AS gcp_dataset_name, \n",
    "                  '{new_dataset_id}' AS az_dataset_id, '{new_dataset_name}' AS az_dataset_name, \n",
    "                  b.source_name AS source_path, b.target_path, a.size_in_bytes, a.md5_hash, a.file_ref AS orig_tdr_file_id,\n",
    "                  '{current_datetime_string}' AS date_added, '{public_flag}' AS public_flag, ROW_NUMBER() OVER (PARTITION BY a.file_ref ORDER BY b.source_name) AS rn\n",
    "                  FROM `{bq_project}.{bq_dataset}.file_inventory` a\n",
    "                      LEFT JOIN drlh_deduped b\n",
    "                      ON a.uri = b.source_name\n",
    "                      LEFT JOIN `broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list` c\n",
    "                      ON a.file_ref = c.orig_tdr_file_id AND c.az_dataset_id = '{new_dataset_id}'\n",
    "                  WHERE c.source_path IS NULL\n",
    "                )\n",
    "                SELECT * EXCEPT(rn)\n",
    "                FROM file_records\n",
    "                WHERE rn = 1\"\"\"\n",
    "    #print(query)\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            query_job = client.query(query, job_config=job_config)\n",
    "            query_job.result()\n",
    "            logging.info(\"Records recorded successfully.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error recording records for all rows of table 'file_inventory': {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                return\n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# General parameters\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list\"\n",
    "azure_billing_profile = \"9ee23bed-b46c-4561-9103-d2a723113f7f\"\n",
    "\n",
    "# Specify the list of datasets to process, leaving the target Azure dataset ID empty to create a new one\n",
    "migration_list = [\n",
    "    #[\"src_gcp_dataset_id\", \"tar_az_dataset_id\", \"open_access (Y/N)\"]\n",
    "    ['902596ce-714e-49b3-8271-f3dfece52309', 'e091028e-a6b1-4989-9477-498e7ea206f0', 'N'],\n",
    "]\n",
    "\n",
    "# Specify whether existing records in the azure_migration_file_list table should be deleted before running\n",
    "delete_existing_records = False\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for entry in migration_list:\n",
    "    logging.info(f\"Processing Migration List Entry: {str(entry)}\")\n",
    "    dataset_name, new_dataset_id, new_dataset_name, bq_project, bq_dataset = create_dataset_from_dataset(entry[0], entry[1], azure_billing_profile)\n",
    "    if new_dataset_id:\n",
    "        output_file_details(entry[0], dataset_name, new_dataset_id, new_dataset_name, bq_project, bq_dataset, entry[2], target_bigquery_table, delete_existing_records)\n",
    "        results.append([entry[0], dataset_name, \"Success\", new_dataset_id, new_dataset_name])\n",
    "    else:\n",
    "        results.append([entry[0], dataset_name, \"Failure\", new_dataset_id, new_dataset_name])\n",
    "        \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Source Dataset ID\", \"Source Dataset Name\", \"Status\", \"New Dataset ID\", \"New Dataset Name\"])\n",
    "display(results_df)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Post-Connector Processing\n",
    "For each GCP Dataset - Azure Dataset pair:\n",
    "1. Retrieve the source GCP Dataset for the Snapshot\n",
    "2. Extract, pre-process, and ingest tabular data from the GCP Dataset to the Azure Dataset\n",
    "3. Create a new Azure snapshot based on the GCP snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     5,
     117,
     252,
     532
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2024 05:37:37 PM - INFO: Creating a snapshot for TDR dataset d28cb1d4-2300-4cd7-882b-99ce59305ce0.\n",
      "11/14/2024 05:37:37 PM - INFO: Retrieving dataset details from prod environment. UUID:  d28cb1d4-2300-4cd7-882b-99ce59305ce0\n",
      "11/14/2024 05:37:39 PM - INFO: Creating full-view snapshot.\n",
      "11/14/2024 05:37:39 PM - WARNING: Unable to lookup consent code. Consent name and/or PHS ID missing for lookup.\n",
      "11/14/2024 05:37:39 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: KRe2SAG2RxWhOlImy46kaQ\n",
      "11/14/2024 05:41:51 PM - INFO: Snapshot Creation succeeded: {'id': '6fda69f9-9d74-4c1b-8931-68d7a47aa000', 'name': 'ANVIL_CCDG_Broad_CVD_AF_PEGASUS_HMB_WES_20240313_ANV5_202411141737', 'description': 'Full view snapshot of ANVIL_CCDG_Broad_CVD_AF_PEGASUS_HMB_WES_20240313', 'createdDate': '2024-11-14T17:37:40.039901Z', 'profileId': '9ee23bed-b46c-4561-9103-d2a723113f7f', 'storage': [{'region': 'southcentralus', 'cloudResource': 'application_deployment', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'storage_account', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'synapse_workspace', 'cloudPlatform': 'azure'}], 'secureMonitoringEnabled': True, 'consentCode': 'NA', 'phsId': '', 'cloudPlatform': 'azure', 'dataProject': None, 'storageAccount': 'tdrwefwykpmvobbjtdanzdur', 'selfHosted': False, 'globalFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': None}, 'duosId': None}\n",
      "11/14/2024 05:41:51 PM - INFO: Creating a snapshot for TDR dataset 183ec762-f867-46c5-bb19-8b2b3417f7b2.\n",
      "11/14/2024 05:41:51 PM - INFO: Retrieving dataset details from prod environment. UUID:  183ec762-f867-46c5-bb19-8b2b3417f7b2\n",
      "11/14/2024 05:41:53 PM - INFO: Creating full-view snapshot.\n",
      "11/14/2024 05:41:53 PM - INFO: Attempting to lookup consent code using PHS: 2242 and Consent Name: TBD.\n",
      "11/14/2024 05:41:54 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: k2HeeDj4TiOiycZ2usZELQ\n",
      "11/14/2024 05:45:06 PM - INFO: Snapshot Creation succeeded: {'id': '75c6d302-ed72-475f-bbbf-70eec8f36b5c', 'name': 'ANVIL_CCDG_Broad_CVD_AF_Swiss_Cases_DS_MDS_Arrays_20240220_ANV5_202411141741', 'description': 'Full view snapshot of ANVIL_CCDG_Broad_CVD_AF_Swiss_Cases_DS_MDS_Arrays_20240220', 'createdDate': '2024-11-14T17:41:55.054760Z', 'profileId': '9ee23bed-b46c-4561-9103-d2a723113f7f', 'storage': [{'region': 'southcentralus', 'cloudResource': 'application_deployment', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'storage_account', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'synapse_workspace', 'cloudPlatform': 'azure'}], 'secureMonitoringEnabled': True, 'consentCode': 'TBD', 'phsId': 'phs002242', 'cloudPlatform': 'azure', 'dataProject': None, 'storageAccount': 'tdrwefwykpmvobbjtdanzdur', 'selfHosted': False, 'globalFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': None}, 'duosId': None}\n",
      "11/14/2024 05:45:06 PM - INFO: Creating a snapshot for TDR dataset 933d1603-8c61-4ff2-8489-7f774ac15e97.\n",
      "11/14/2024 05:45:06 PM - INFO: Retrieving dataset details from prod environment. UUID:  933d1603-8c61-4ff2-8489-7f774ac15e97\n",
      "11/14/2024 05:45:09 PM - INFO: Creating full-view snapshot.\n",
      "11/14/2024 05:45:09 PM - WARNING: Unable to lookup consent code. Consent name and/or PHS ID missing for lookup.\n",
      "11/14/2024 05:45:09 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: _nSh3AFVSyC2_dVh24a9bw\n",
      "11/14/2024 05:48:10 PM - INFO: Snapshot Creation succeeded: {'id': '0f37c020-b9fa-48a1-a9d8-107953edda83', 'name': 'ANVIL_CCDG_Broad_CVD_AF_Swiss_Cases_DS_MDS_WES_20240307_ANV5_202411141745', 'description': 'Full view snapshot of ANVIL_CCDG_Broad_CVD_AF_Swiss_Cases_DS_MDS_WES_20240307', 'createdDate': '2024-11-14T17:45:09.482571Z', 'profileId': '9ee23bed-b46c-4561-9103-d2a723113f7f', 'storage': [{'region': 'southcentralus', 'cloudResource': 'application_deployment', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'storage_account', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'synapse_workspace', 'cloudPlatform': 'azure'}], 'secureMonitoringEnabled': True, 'consentCode': 'NA', 'phsId': '', 'cloudPlatform': 'azure', 'dataProject': None, 'storageAccount': 'tdrwefwykpmvobbjtdanzdur', 'selfHosted': False, 'globalFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': None}, 'duosId': None}\n",
      "11/14/2024 05:48:10 PM - INFO: Creating a snapshot for TDR dataset 757191b0-9db3-4d18-b4ad-97bead5f3221.\n",
      "11/14/2024 05:48:10 PM - INFO: Retrieving dataset details from prod environment. UUID:  757191b0-9db3-4d18-b4ad-97bead5f3221\n",
      "11/14/2024 05:48:11 PM - INFO: Creating full-view snapshot.\n",
      "11/14/2024 05:48:11 PM - INFO: Attempting to lookup consent code using PHS: 997 and Consent Name: NA.\n",
      "11/14/2024 05:48:12 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: tAC8ZHADTk6rqOLCPOIijw\n",
      "11/14/2024 05:49:43 PM - INFO: Snapshot Creation succeeded: {'id': 'e6582b4b-2eb6-4980-b1fc-e65682d5ea54', 'name': 'ANVIL_CCDG_Broad_CVD_AF_VAFAR_HMB_WES_20240220_ANV5_202411141748', 'description': 'Full view snapshot of ANVIL_CCDG_Broad_CVD_AF_VAFAR_HMB_WES_20240220', 'createdDate': '2024-11-14T17:48:12.392765Z', 'profileId': '9ee23bed-b46c-4561-9103-d2a723113f7f', 'storage': [{'region': 'southcentralus', 'cloudResource': 'application_deployment', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'storage_account', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'synapse_workspace', 'cloudPlatform': 'azure'}], 'secureMonitoringEnabled': True, 'consentCode': 'NA', 'phsId': 'phs000997', 'cloudPlatform': 'azure', 'dataProject': None, 'storageAccount': 'tdrwefwykpmvobbjtdanzdur', 'selfHosted': False, 'globalFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': None}, 'duosId': None}\n",
      "11/14/2024 05:49:43 PM - INFO: Creating a snapshot for TDR dataset 7a9eee5d-95c2-4947-93a8-e31d53a2a09a.\n",
      "11/14/2024 05:49:43 PM - INFO: Retrieving dataset details from prod environment. UUID:  7a9eee5d-95c2-4947-93a8-e31d53a2a09a\n",
      "11/14/2024 05:49:45 PM - INFO: Creating full-view snapshot.\n",
      "11/14/2024 05:49:45 PM - INFO: Attempting to lookup consent code using PHS: 1398 and Consent Name: GRU.\n",
      "11/14/2024 05:49:45 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: __rqLfT-RGKHvi6SuIyG1w\n",
      "11/14/2024 05:50:56 PM - INFO: Snapshot Creation succeeded: {'id': '172d484c-1336-427e-ac0b-37bb9806114d', 'name': 'ANVIL_CCDG_Broad_CVD_Stroke_BRAVE_WGS_20240307_ANV5_202411141749', 'description': 'Full view snapshot of ANVIL_CCDG_Broad_CVD_Stroke_BRAVE_WGS_20240307', 'createdDate': '2024-11-14T17:49:45.702965Z', 'profileId': '9ee23bed-b46c-4561-9103-d2a723113f7f', 'storage': [{'region': 'southcentralus', 'cloudResource': 'application_deployment', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'storage_account', 'cloudPlatform': 'azure'}, {'region': 'southcentralus', 'cloudResource': 'synapse_workspace', 'cloudPlatform': 'azure'}], 'secureMonitoringEnabled': True, 'consentCode': 'c1', 'phsId': 'phs001398', 'cloudPlatform': 'azure', 'dataProject': None, 'storageAccount': 'tdrwefwykpmvobbjtdanzdur', 'selfHosted': False, 'globalFileIds': True, 'tags': [], 'resourceLocks': {'exclusive': None, 'shared': None}, 'duosId': None}\n",
      "\n",
      "Final Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Source Dataset ID</th>\n",
       "      <th>Target Dataset ID</th>\n",
       "      <th>Processing Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "      <th>Snapshot ID</th>\n",
       "      <th>Snapshot Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>Data Snapshotting</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>6fda69f9-9d74-4c1b-8931-68d7a47aa000</td>\n",
       "      <td>ANVIL_CCDG_Broad_CVD_AF_PEGASUS_HMB_WES_20240313_ANV5_202411141737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>Data Snapshotting</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>75c6d302-ed72-475f-bbbf-70eec8f36b5c</td>\n",
       "      <td>ANVIL_CCDG_Broad_CVD_AF_Swiss_Cases_DS_MDS_Arrays_20240220_ANV5_202411141741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>Data Snapshotting</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>0f37c020-b9fa-48a1-a9d8-107953edda83</td>\n",
       "      <td>ANVIL_CCDG_Broad_CVD_AF_Swiss_Cases_DS_MDS_WES_20240307_ANV5_202411141745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>Data Snapshotting</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>e6582b4b-2eb6-4980-b1fc-e65682d5ea54</td>\n",
       "      <td>ANVIL_CCDG_Broad_CVD_AF_VAFAR_HMB_WES_20240220_ANV5_202411141748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>Data Snapshotting</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>172d484c-1336-427e-ac0b-37bb9806114d</td>\n",
       "      <td>ANVIL_CCDG_Broad_CVD_Stroke_BRAVE_WGS_20240307_ANV5_202411141749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Source Dataset ID                     Target Dataset ID            Processing Step    Status  Message              Snapshot ID                                              Snapshot Name                                \n",
       "0  2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0  Data Snapshotting  Success          6fda69f9-9d74-4c1b-8931-68d7a47aa000            ANVIL_CCDG_Broad_CVD_AF_PEGASUS_HMB_WES_20240313_ANV5_202411141737\n",
       "1  65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2  Data Snapshotting  Success          75c6d302-ed72-475f-bbbf-70eec8f36b5c  ANVIL_CCDG_Broad_CVD_AF_Swiss_Cases_DS_MDS_Arrays_20240220_ANV5_202411141741\n",
       "2  36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97  Data Snapshotting  Success          0f37c020-b9fa-48a1-a9d8-107953edda83     ANVIL_CCDG_Broad_CVD_AF_Swiss_Cases_DS_MDS_WES_20240307_ANV5_202411141745\n",
       "3  b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221  Data Snapshotting  Success          e6582b4b-2eb6-4980-b1fc-e65682d5ea54              ANVIL_CCDG_Broad_CVD_AF_VAFAR_HMB_WES_20240220_ANV5_202411141748\n",
       "4  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a  Data Snapshotting  Success          172d484c-1336-427e-ac0b-37bb9806114d              ANVIL_CCDG_Broad_CVD_Stroke_BRAVE_WGS_20240307_ANV5_202411141749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to fetch data from BigQuery\n",
    "def fetch_source_records_bigquery(config, new_dataset_id, array_col_dict, table, start_row, end_row):\n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source_dataset_id\"]\n",
    "    src_tdr_object_type = \"dataset\"\n",
    "    tdr_host = config[\"tdr_host\"]\n",
    "    files_already_ingested = True\n",
    "    datarepo_row_ids_to_ingest = []\n",
    "    apply_anvil_transforms = True\n",
    "    bq_project = config[\"bigquery_project\"]\n",
    "    bq_dataset = config[\"bigquery_dataset\"]\n",
    "    \n",
    "    # Setup/refresh TDR clients (and BQ client)\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Retrieve table data from the original dataset\n",
    "    logging.info(f\"Fetching rows {str(start_row)}-{str(end_row)} from table '{table}' in the original {src_tdr_object_type} ({src_tdr_object_uuid}).\")\n",
    "    table_recs_str = f\"Table: {table} -- Rows: {str(start_row)}-{str(end_row)}\"\n",
    "    final_records = []\n",
    "    if apply_anvil_transforms and \"anvil_\" not in table:\n",
    "        if table == \"file_inventory\":\n",
    "            if files_already_ingested == False:\n",
    "                file_ref_sql = \"TO_JSON_STRING(STRUCT(source_name AS sourcePath, target_path AS targetPath, 'Ingest of '||source_name AS description, COALESCE(content_type, 'application/octet-stream') AS mimeType))\"\n",
    "            else:\n",
    "                file_ref_sql = \"file_ref\"\n",
    "            rec_fetch_query = f\"\"\"WITH drlh_deduped AS\n",
    "                            (\n",
    "                              SELECT DISTINCT file_id, target_path, source_name\n",
    "                              FROM `{bq_project}.{bq_dataset}.datarepo_load_history`\n",
    "                              WHERE state = \"succeeded\" \n",
    "                            )\n",
    "                            SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT datarepo_row_id, datarepo_row_id AS orig_datarepo_row_id, a.file_id, name, path, target_path AS uri, content_type, full_extension, size_in_bytes, crc32c, md5_hash, ingest_provenance,\n",
    "                              file_ref AS orig_file_ref, {file_ref_sql} AS file_ref,\n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}` a\n",
    "                                  LEFT JOIN drlh_deduped b\n",
    "                                  ON a.file_ref = b.file_id\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "        else:\n",
    "            rec_fetch_query = f\"\"\"SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT *, datarepo_row_id AS orig_datarepo_row_id,\n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}`\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "    else:\n",
    "        rec_fetch_query = f\"\"\"SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT *, \n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}`\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            df = client.query(rec_fetch_query).result().to_dataframe()\n",
    "            df = df.astype(object).where(pd.notnull(df),None)\n",
    "            for column in array_col_dict[table]:\n",
    "                df[column] = df[column].apply(lambda x: list(x))\n",
    "            if apply_anvil_transforms and table == \"file_inventory\" and files_already_ingested == False: \n",
    "                df[\"file_ref\"] = df.apply(lambda x: json.loads(x[\"file_ref\"].replace(\"\\'\", \"\\\"\")), axis=1)\n",
    "            final_records = df.to_dict(orient=\"records\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error retrieving records for rows {str(start_row)}-{str(end_row)} of table {table}: {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "                return {}\n",
    "    \n",
    "    # Filter retrieved data if necessary and return as dict of records\n",
    "    if final_records:\n",
    "        df_temp = pd.DataFrame.from_dict(final_records)\n",
    "        if datarepo_row_ids_to_ingest:\n",
    "            df_orig = df_temp[df_temp[\"datarepo_row_id\"].isin(datarepo_row_ids_to_ingest)].copy()\n",
    "        else:\n",
    "            df_orig = df_temp.copy()\n",
    "        del df_temp\n",
    "        df_orig.drop(columns=[\"datarepo_row_id\"], inplace=True, errors=\"ignore\")\n",
    "        df_orig = df_orig.astype(object).where(pd.notnull(df_orig),None)\n",
    "        records_orig = df_orig.to_dict(orient=\"records\")\n",
    "        if not records_orig:\n",
    "            msg_str = f\"No records found in rows {str(start_row)}-{str(end_row)} of table {table} after filtering based on datarepo_row_ids_to_ingest parameter. Continuing to next record set or table validation.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Skipped\", msg_str])\n",
    "            return records_orig\n",
    "        elif len(final_records) != len(records_orig):\n",
    "            logging.info(f\"Filtering records to ingest based on the datarepo_row_ids_to_ingest parameter. {str(len(records_orig))} of {str(len(final_records))} records to be ingested.\")\n",
    "            return records_orig\n",
    "        else:\n",
    "            return records_orig\n",
    "    else:\n",
    "        msg_str = f\"No records found for rows {str(start_row)}-{str(end_row)} of table {table} in original {src_tdr_object_type}. Continuing to next record set or table validation.\"\n",
    "        logging.info(msg_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Skipped\", msg_str])\n",
    "        return final_records\n",
    "\n",
    "# Function to process ingests for specific table\n",
    "def ingest_table_data(config, new_dataset_id, array_col_dict, data_type_col_dict, table, start_row, end_row):\n",
    "    \n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source_dataset_id\"]\n",
    "    src_tdr_object_type = \"dataset\"\n",
    "    tdr_host = config[\"tdr_host\"]\n",
    "    tar_tdr_billing_profile = config[\"tar_tdr_billing_profile\"]\n",
    "    records_processing_method = \"in_memory\"\n",
    "    write_to_cloud_platform = \"\"\n",
    "    apply_anvil_transforms = True\n",
    "    dr_row_id_xwalk = config[\"dr_row_id_xwalk\"]\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    \n",
    "    # Retrieve table data from the original dataset\n",
    "    table_recs_str = f\"Table: {table} -- Rows: {str(start_row)}-{str(end_row)}\"\n",
    "    records_orig = fetch_source_records_bigquery(config, new_dataset_id, array_col_dict, table, start_row, end_row)\n",
    "    if not records_orig:\n",
    "        return\n",
    "\n",
    "    # Pre-process records before ingest\n",
    "    if \"anvil_\" in table:\n",
    "        try:\n",
    "            # Pre-process records in AnVIL_ records to use new datarepo_row_ids in the source_datarepo_row_ids field\n",
    "            logging.info(\"FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\")\n",
    "            records_processed = []\n",
    "            for record in records_orig:\n",
    "                int_record = record.copy()\n",
    "                new_dr_row_id_list = []\n",
    "                for row_id in int_record[\"source_datarepo_row_ids\"]:\n",
    "                    new_row_id = dr_row_id_xwalk.get(row_id)\n",
    "                    if new_row_id:\n",
    "                        new_dr_row_id_list.append(new_row_id)\n",
    "                    else:\n",
    "                        err_str = f\"Failure in pre-processing: row_id '{row_id}'' not found in datarepo_row_id crosswalk.\"\n",
    "                        logging.error(err_str)\n",
    "                        config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "                        return   \n",
    "                int_record[\"source_datarepo_row_ids\"] = new_dr_row_id_list\n",
    "                for fcol in data_type_col_dict[\"float\"][table]:\n",
    "                    if int_record[fcol]:\n",
    "                        int_record[fcol] = float(int_record[fcol])\n",
    "                for icol in data_type_col_dict[\"int\"][table]:\n",
    "                    if int_record[icol]:\n",
    "                        int_record[icol] = int(int_record[icol])\n",
    "                records_processed.append(int_record)\n",
    "        except Exception as e:\n",
    "            err_str = f\"Failure in pre-processing: {str(e)}\"\n",
    "            logging.error(err_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "            return\n",
    "    else:\n",
    "        records_processed = []\n",
    "        for record in records_orig:\n",
    "            int_record = record.copy()\n",
    "            for fcol in data_type_col_dict[\"float\"][table]:\n",
    "                if int_record[fcol]:\n",
    "                    int_record[fcol] = float(int_record[fcol])\n",
    "            for icol in data_type_col_dict[\"int\"][table]:\n",
    "                if int_record[icol]:\n",
    "                    int_record[icol] = int(int_record[icol])\n",
    "            records_processed.append(int_record)\n",
    "    \n",
    "    # Write out records to cloud, if specified by user\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        logging.info(f\"Writing records to a control file in the cloud.\")\n",
    "        if write_to_cloud_platform == \"gcp\":\n",
    "            control_file_path = write_records_to_gcp(config, table, records_processed)\n",
    "        else:\n",
    "            control_file_path = write_records_to_azure(config, table, records_processed)\n",
    "\n",
    "    # Build, submit, and monitor ingest request\n",
    "    logging.info(f\"Submitting ingestion request to new dataset ({new_dataset_id}).\")\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        ingest_request = {\n",
    "            \"table\": table,\n",
    "            \"profile_id\": tar_tdr_billing_profile,\n",
    "            \"ignore_unknown_values\": True,\n",
    "            \"resolve_existing_files\": True,\n",
    "            \"updateStrategy\": \"append\",\n",
    "            \"format\": \"json\",\n",
    "            \"load_tag\": \"Ingest for {}\".format(new_dataset_id),\n",
    "            \"path\": control_file_path\n",
    "        }        \n",
    "    else:\n",
    "        ingest_request = {\n",
    "            \"table\": table,\n",
    "            \"profile_id\": tar_tdr_billing_profile,\n",
    "            \"ignore_unknown_values\": True,\n",
    "            \"resolve_existing_files\": True,\n",
    "            \"updateStrategy\": \"append\",\n",
    "            \"format\": \"array\",\n",
    "            \"load_tag\": \"Ingest for {}\".format(new_dataset_id),\n",
    "            \"records\": records_processed\n",
    "        }\n",
    "    attempt_counter = 1\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = refresh_tdr_api_client(tdr_host)\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = wait_for_tdr_job(datasets_api.ingest_dataset(id=new_dataset_id, ingest=ingest_request), tdr_host)\n",
    "            logging.info(\"Ingest succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Success\", str(ingest_request_result)[0:1000]])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on ingest: {}\".format(str(e)[0:2500]))\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Logging error.\")\n",
    "                err_str = f\"Error on ingest: {str(e)[0:2500]}\"\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])  \n",
    "                break\n",
    "\n",
    "    # Remove control file from cloud, if written out\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        logging.info(f\"Removing control file from the cloud.\")\n",
    "        if write_to_cloud_platform == \"gcp\":\n",
    "            client = storage.Client()\n",
    "            target_bucket = control_file_path.split(\"/\")[2]\n",
    "            target_object = \"/\".join(control_file_path.split(\"/\")[3:])\n",
    "            bucket = client.bucket(target_bucket)\n",
    "            blob = bucket.blob(target_object)\n",
    "            blob.delete()\n",
    "        else:\n",
    "            blob = BlobClient.from_blob_url(control_file_path)\n",
    "            blob.delete_blob()\n",
    "\n",
    "# Function to orchestration the migration of tabular data\n",
    "def migrate_tabular_data(config):\n",
    "\n",
    "    # Extract parameters from config\n",
    "    source_dataset_id = config[\"source_dataset_id\"]\n",
    "    target_dataset_id = config[\"target_dataset_id\"] \n",
    "    tables_to_ingest = config[\"tables_to_ingest\"] \n",
    "    tdr_host = config[\"tdr_host\"] \n",
    "    tdr_sa_to_use = config[\"tdr_sa_to_use\"] \n",
    "    chunk_size = config[\"chunk_size\"] \n",
    "    max_combined_rec_ref_size = config[\"max_combined_rec_ref_size\"] \n",
    "    skip_ingests = config[\"skip_ingests\"]\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "\n",
    "    # Add TDR SA to original dataset\n",
    "    logging.info(f\"Adding TDR general SA ({tdr_sa_to_use}) to original dataset: {source_dataset_id}\")\n",
    "    try:\n",
    "        resp = datasets_api.add_dataset_policy_member(id=source_dataset_id, policy_name=\"steward\", policy_member={\"email\": tdr_sa_to_use}) \n",
    "        logging.info(\"TDR SA added successfully.\")\n",
    "    except:\n",
    "        error_str = f\"Error adding TDR SA to dataset {source_dataset_id}: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "        return\n",
    "\n",
    "    # Collect details from original dataset to build inventory of tables to migrate\n",
    "    logging.info(f\"Retrieving dataset details from original dataset: {source_dataset_id}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=source_dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        config[\"bigquery_project\"] = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        config[\"bigquery_dataset\"] = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        fileref_col_dict = {}\n",
    "        array_col_dict = {}\n",
    "        data_type_col_dict = {}\n",
    "        float_col_dict = {}\n",
    "        int_col_dict = {}\n",
    "        for table_entry in dataset_details[\"schema\"][\"tables\"]:\n",
    "            fileref_list = []\n",
    "            array_list = []\n",
    "            float_list = []\n",
    "            int_list = []\n",
    "            for idx, column_entry in enumerate(table_entry[\"columns\"]):\n",
    "                if column_entry[\"datatype\"] == \"fileref\":\n",
    "                    fileref_list.append(column_entry[\"name\"])\n",
    "                elif column_entry[\"datatype\"] == \"float\":\n",
    "                    float_list.append(column_entry[\"name\"])\n",
    "                elif column_entry[\"datatype\"] == \"integer\":\n",
    "                    int_list.append(column_entry[\"name\"])\n",
    "                if column_entry[\"array_of\"] == True:\n",
    "                    array_list.append(column_entry[\"name\"])\n",
    "            fileref_col_dict[table_entry[\"name\"]] = fileref_list\n",
    "            array_col_dict[table_entry[\"name\"]] = array_list\n",
    "            float_col_dict[table_entry[\"name\"]] = float_list\n",
    "            data_type_col_dict[\"float\"] = float_col_dict\n",
    "            int_col_dict[table_entry[\"name\"]] = int_list\n",
    "            data_type_col_dict[\"int\"] = int_col_dict\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset {source_dataset_id}: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "        return\n",
    "\n",
    "    # Read in existing datarepo_row_id crosswalk, if one exists\n",
    "    logging.info(\"Fetching existing datarepo_row_id crosswalk (if one exists).\")\n",
    "    xwalk_json_file_name = f\"{source_dataset_id}_{target_dataset_id}_rowid_xwalk.json\"\n",
    "    try:\n",
    "        with open(xwalk_json_file_name,\"r\") as file:\n",
    "            datarepo_row_id_xwalk = json.load(file)\n",
    "    except:\n",
    "        datarepo_row_id_xwalk = {}\n",
    "        logging.warning(f\"No datarepo_row_id crosswalk file name '{xwalk_json_file_name}' found.\")\n",
    "\n",
    "    # Order tables for ingestion\n",
    "    logging.info(\"Ordering tables and pulling current record counts for validation.\")\n",
    "    table_rank_dict = {}\n",
    "    for table in fileref_col_dict.keys():\n",
    "        if table == \"file_inventory\":\n",
    "            table_rank_dict[table] = 1\n",
    "        elif \"anvil_\" not in table:\n",
    "            table_rank_dict[table] = 2\n",
    "        else:\n",
    "            table_rank_dict[table] = 3\n",
    "    ordered_table_list = sorted(table_rank_dict, key= lambda key: table_rank_dict[key])\n",
    "\n",
    "    # Fetch total record counts for all tables\n",
    "    populated_table_dict = {}\n",
    "    for table in ordered_table_list:\n",
    "        api_client = refresh_tdr_api_client(tdr_host)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": 0,\n",
    "              \"limit\": 10,\n",
    "              \"sort\": \"datarepo_row_id\",\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=source_dataset_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                total_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 2:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    total_record_count = -1\n",
    "                    break\n",
    "        if total_record_count == -1:\n",
    "            error_str = f\"Error retrieving current record counts for tables in dataset {source_dataset_id}: {str(e)}\"\n",
    "            logging.error(error_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "            return\n",
    "        elif total_record_count > 0:\n",
    "            populated_table_dict[table] = total_record_count\n",
    "\n",
    "    # Loop through and process tables for ingestion\n",
    "    logging.info(\"Processing dataset ingestion requests.\")\n",
    "    pop_fss_table_cnt = 0\n",
    "    for table in ordered_table_list:\n",
    "\n",
    "        # Determine whether table should be processed, and skip if not\n",
    "        logging.info(f\"Processing dataset ingestion for table '{table}'.\")\n",
    "        total_record_count = 0\n",
    "        if tables_to_ingest and table not in tables_to_ingest:\n",
    "            msg_str = f\"Table '{table}' not listed in the tables_to_ingest parameter. Skipping.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "            continue\n",
    "        elif table not in populated_table_dict.keys():\n",
    "            msg_str = f\"No records found for table '{table}' in original dataset. Continuing to next table/record set.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "            continue\n",
    "        elif \"anvil_\" in table:\n",
    "            # Confirm all non-FSS tables are present in datarepo_row_id_xwalk\n",
    "            pop_fss_table_cnt += 1\n",
    "            missing_tab_list = []\n",
    "            for tab in populated_table_dict.keys():\n",
    "                if \"anvil_\" not in tab and tab not in datarepo_row_id_xwalk.keys():\n",
    "                    missing_tab_list.append(tab)\n",
    "            if len(missing_tab_list) > 0:\n",
    "                missing_tab_string = \", \".join(missing_tab_list)\n",
    "                msg_str = f\"Populated non-FSS tables missing from datarepo_row_id crosswalk: {missing_tab_string}. Skipping FSS table '{table}'.\"\n",
    "                logging.info(msg_str)\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "                continue\n",
    "        \n",
    "        # Aggregate datarepo_row_id crosswalk information for use in FSS table processing\n",
    "        if pop_fss_table_cnt == 1:\n",
    "            dr_row_id_xwalk = {}\n",
    "            for key in datarepo_row_id_xwalk.keys():\n",
    "                dr_row_id_xwalk.update(datarepo_row_id_xwalk[key])\n",
    "            config[\"dr_row_id_xwalk\"] = dr_row_id_xwalk \n",
    "            \n",
    "        # Chunk table records as necessary, then loop through and process each chunk\n",
    "        total_record_count = populated_table_dict.get(table)\n",
    "        if skip_ingests:\n",
    "            msg_str = f\"Parameter 'skip_ingests' set to true. Skipping ingestion for table '{table}'.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "        else:\n",
    "            if fileref_col_dict[table]:\n",
    "                ref_chunk_size = math.floor(max_combined_rec_ref_size / len(fileref_col_dict[table]))\n",
    "                table_chunk_size = min(chunk_size, ref_chunk_size)\n",
    "                logging.info(f\"Table '{table}' contains fileref columns. Will use a chunk size of {table_chunk_size} rows per ingestion request, to keep the number of file references per chunk below {max_combined_rec_ref_size}.\")\n",
    "            else:\n",
    "                table_chunk_size = chunk_size\n",
    "                logging.info(f\"Table '{table}' does not contain fileref columns. Will use a chunk size of {table_chunk_size} rows per ingestion request.\")\n",
    "            start_row = 1\n",
    "            end_row = min((table_chunk_size), total_record_count)\n",
    "            while start_row <= total_record_count:\n",
    "                if end_row > total_record_count:\n",
    "                    end_row = total_record_count\n",
    "                ingest_table_data(config, target_dataset_id, array_col_dict, data_type_col_dict, table, start_row, end_row)    \n",
    "                start_row += table_chunk_size\n",
    "                end_row += table_chunk_size\n",
    "\n",
    "        # Build datarepo_row_id crosswalk for the table, add to datarepo_row_id_xwalk dict, and write out updated dict to file\n",
    "        if \"anvil_\" not in table: \n",
    "            logging.info(\"Fetching ingested records and building datarepo_row_id lookup for use in AnVIL transforms.\")\n",
    "            temp_dr_xwalk = {}\n",
    "            api_client = refresh_tdr_api_client(tdr_host)\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            max_page_size = 1000\n",
    "            records_fetched = 0\n",
    "            retrieval_error = False\n",
    "            while records_fetched < total_record_count and not retrieval_error:\n",
    "                row_start = records_fetched\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    payload = {\n",
    "                      \"offset\": row_start,\n",
    "                      \"limit\": max_page_size,\n",
    "                      \"sort\": \"datarepo_row_id\",\n",
    "                      \"direction\": \"asc\",\n",
    "                      \"filter\": \"\"\n",
    "                    }\n",
    "                    try:\n",
    "                        dataset_results = datasets_api.query_dataset_data_by_id(id=target_dataset_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                        if len(dataset_results[\"result\"]) == 0:\n",
    "                            warn_str = f\"No records found for '{table}' table, which prevents the proper building of the datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream.\"\n",
    "                            logging.warning(warn_str)\n",
    "                            retrieval_error = True\n",
    "                            break  \n",
    "                        else:\n",
    "                            for record in dataset_results[\"result\"]:\n",
    "                                key = table + \":\" + record[\"orig_datarepo_row_id\"]\n",
    "                                val = table + \":\" + record[\"datarepo_row_id\"]\n",
    "                                temp_dr_xwalk[key] = val\n",
    "                                records_fetched += 1\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 2:\n",
    "                            sleep(10)\n",
    "                            attempt_counter += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            warn_str = f\"Error retrieving records for '{table}' table to build datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream. Error: {str(e)}\"\n",
    "                            logging.warning(warn_str)\n",
    "                            retrieval_error = True\n",
    "                            break\n",
    "            if not retrieval_error:\n",
    "                datarepo_row_id_xwalk[table] = temp_dr_xwalk\n",
    "                with open(xwalk_json_file_name, 'w') as file:\n",
    "                    json.dump(datarepo_row_id_xwalk, file)\n",
    "        \n",
    "        # Fetch total record count for the new table\n",
    "        api_client = refresh_tdr_api_client(tdr_host)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": 0,\n",
    "              \"limit\": 10,\n",
    "              \"sort\": \"datarepo_row_id\",\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=target_dataset_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                new_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 2:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    new_record_count = -1\n",
    "                    break\n",
    "        if new_record_count == -1:\n",
    "            err_str = f\"Error retrieving record count for table '{table}' in new dataset. Skipping validation and continuing to next table.\"\n",
    "            logging.error(err_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Failure\", err_str])\n",
    "            continue \n",
    "\n",
    "        # Validate the new table against the old table, with extra scrutiny given to the file_inventory table for AnVIL migrations\n",
    "        logging.info(f\"Validating table '{table}' in new dataset vs. original dataset.\")\n",
    "        if new_record_count == total_record_count:\n",
    "            config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Success\", f\"{new_record_count} records found in both new and original table.\"])\n",
    "        else:\n",
    "            config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Failure\", f\"{new_record_count} records found in new table doesn't match {total_record_count} records in original table.\"])\n",
    "\n",
    "    # Display results\n",
    "    pipeline_results = pd.DataFrame(config[\"migration_results\"], columns = [\"Task\", \"Step\", \"Status\", \"Message\"])\n",
    "    failures = pipeline_results[pipeline_results[\"Status\"].str.contains(\"Failure\")]\n",
    "    logging.info(\"Migration Pipeline Results:\")\n",
    "    display(pipeline_results)\n",
    "    logging.info(f\"\\nPipeline finished with {len(failures)} failures.\")\n",
    "    return len(failures)\n",
    "\n",
    "# Function for creating a snapshot for the new dataset\n",
    "def recreate_snapshot(config):\n",
    "    \n",
    "    # Extract parameters from config\n",
    "    target_dataset_id = config[\"target_dataset_id\"] \n",
    "    azure_billing_profile = config[\"azure_billing_profile\"] \n",
    "    tdr_host = config[\"tdr_host\"] \n",
    "    anvil_schema = config[\"anvil_schema\"] \n",
    "    \n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    \n",
    "    # Retrieve new dataset details\n",
    "    logging.info(f\"Retrieving dataset details from prod environment. UUID:  {target_dataset_id}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=target_dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        dataset_name = dataset_details[\"name\"]\n",
    "        phs_id = dataset_details[\"phs_id\"]\n",
    "        consent_name = dataset_details[\"properties\"][\"consent_name\"]\n",
    "        auth_domains = dataset_details[\"properties\"][\"auth_domains\"]\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "        snapshot_name = dataset_name + \"_\" + anvil_schema + \"_\" + current_datetime_string\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "    # Build config and submit snapshot job\n",
    "    snapshot_config = {\n",
    "        \"profile_id\": azure_billing_profile,\n",
    "        \"snapshot_readers_list\": [\"azul-anvil-prod@firecloud.org\", \"auth-domain\"],\n",
    "        \"anvil_schema_versin\": anvil_schema,\n",
    "        \"ws_bucket\": os.environ[\"WORKSPACE_BUCKET\"],\n",
    "        \"dataset_id\": entry[1],\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"phs_id\": phs_id,\n",
    "        \"consent_name\": consent_name,\n",
    "        \"auth_domains\": auth_domains,\n",
    "        \"pipeline_results\": [],\n",
    "        \"snapshot_name\": snapshot_name\n",
    "    }\n",
    "    utils.create_and_share_snapshot(snapshot_config)\n",
    "    int_df_results = pd.DataFrame(snapshot_config[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.error(\"Errors reported in snapshotting. See logs for details.\")\n",
    "        status = \"Failure\"\n",
    "        message = f\"{len(errors)} failures reported. See log for details.\"\n",
    "        snapshot_id = \"\"\n",
    "        snapshot_name = \"\"\n",
    "    else:\n",
    "        status = \"Success\"\n",
    "        message = \"\"\n",
    "        snapshot_id = re.search(\"{'id': '([a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12})'\", str(int_df_results[int_df_results[\"Task\"]==\"Create and Share Snapshot\"][\"Message\"]))[1]\n",
    "        snapshot_name = re.search(\"'name': '([a-zA-Z0-9_\\-]+)'\", str(int_df_results[int_df_results[\"Task\"]==\"Create and Share Snapshot\"][\"Message\"]))[1]\n",
    "    return status, message, snapshot_id, snapshot_name\n",
    "        \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify migration pairs: [Source GCP Dataset, Target Azure Dataset]\n",
    "migration_list = [\n",
    "    #[\"gcp_dataset_id\", \"az_dataset_id\"]\n",
    "    ['2ef4530a-cc36-4f32-9a1a-63a555346587', 'd28cb1d4-2300-4cd7-882b-99ce59305ce0'],\n",
    "    ['65793118-3c88-4185-9172-2354850e6056', '183ec762-f867-46c5-bb19-8b2b3417f7b2'],\n",
    "    ['36bdd59f-4f5b-43cd-8d34-a21ef87bbf30', '933d1603-8c61-4ff2-8489-7f774ac15e97'],\n",
    "    ['b60b4737-c646-4299-85a0-520890e830b7', '757191b0-9db3-4d18-b4ad-97bead5f3221'],\n",
    "    ['75fb0984-2124-444f-881b-30a1a6f8b8f7', '7a9eee5d-95c2-4947-93a8-e31d53a2a09a'],\n",
    "]\n",
    "\n",
    "# Run parameters\n",
    "azure_billing_profile = \"9ee23bed-b46c-4561-9103-d2a723113f7f\"\n",
    "anvil_schema = \"ANV5\"\n",
    "run_data_migration = False\n",
    "skip_ingests = False # Set to True to build datarepo_row_id xwalk and run validation w/o ingesting more records\n",
    "#tables_to_ingest = [\"anvil_biosample\", \"anvil_dataset\", \"anvil_donor\", \"anvil_file\", \"anvil_project\"] # Leave empty for all\n",
    "tables_to_ingest = []\n",
    "run_snapshot_creation = True\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Set up logging\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "logs_stream_file_path = \"processing_details_\" + current_datetime_string + \".log\"\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.FileHandler(logs_stream_file_path), logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for entry in migration_list:\n",
    "    \n",
    "    # Run cross-cloud ingestion, if specified\n",
    "    failure_count = 0\n",
    "    if run_data_migration:\n",
    "        logging.info(f\"\\nMigrating tabular data from TDR dataset {entry[0]} to TDR dataset {entry[1]}.\")\n",
    "        # Build config and submit migration job\n",
    "        config = {\n",
    "            \"source_dataset_id\": entry[0], \n",
    "            \"target_dataset_id\": entry[1],\n",
    "            \"tables_to_ingest\": tables_to_ingest,\n",
    "            \"tdr_host\": \"https://data.terra.bio\",\n",
    "            \"tdr_sa_to_use\": \"datarepo-jade-api@terra-datarepo-production.iam.gserviceaccount.com\",\n",
    "            \"tar_tdr_billing_profile\": azure_billing_profile,\n",
    "            \"chunk_size\": 250000,\n",
    "            \"max_combined_rec_ref_size\": 40000,\n",
    "            \"migration_results\": [],\n",
    "            \"dr_row_id_xwalk\": {},\n",
    "            \"skip_ingests\": skip_ingests\n",
    "        }\n",
    "        failure_count = migrate_tabular_data(config)\n",
    "        status = \"Failure\" if failure_count > 0 else \"Success\"\n",
    "        msg = f\"{failure_count} failures reported. See log for details.\" if failure_count > 0 else \"\"\n",
    "        results.append([entry[0], entry[1], \"Data Ingestion\", status, msg, \"\", \"\"])\n",
    "\n",
    "    # Run snapshotting, if specified and no upstream errors detected\n",
    "    if run_snapshot_creation:\n",
    "        logging.info(f\"Creating a snapshot for TDR dataset {entry[1]}.\")\n",
    "        # Build config and submit snapshot job\n",
    "        config = { \n",
    "            \"target_dataset_id\": entry[1],\n",
    "            \"tdr_host\": \"https://data.terra.bio\",\n",
    "            \"azure_billing_profile\": azure_billing_profile,\n",
    "            \"anvil_schema\": anvil_schema\n",
    "        }\n",
    "        if failure_count > 0:\n",
    "            logging.error(\"Failures noted in upstream data processing. Skipping snapshotting.\")\n",
    "            results.append([entry[0], entry[1], \"Data Snapshotting\", \"Skipped\", \"Failures noted in upstream data processing.\", \"\", \"\"])\n",
    "        else:\n",
    "            status, message, snapshot_id, snapshot_name = recreate_snapshot(config)\n",
    "            results.append([entry[0], entry[1], \"Data Snapshotting\", status, message, snapshot_id, snapshot_name])\n",
    "            \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Source Dataset ID\", \"Target Dataset ID\", \"Processing Step\", \"Status\", \"Message\", \"Snapshot ID\", \"Snapshot Name\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Manual Ingest Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tdr_host = \"https://data.terra.bio\"\n",
    "api_client = refresh_tdr_api_client(tdr_host)\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "float_col_dict = {}\n",
    "float_col_dict[\"sample\"] = ['fold_80_base_penalty', 'fold_enrichment', 'het_snp_sensitivity', 'library_1_mean_insert_size', 'library_1_pct_exc_dupe', 'library_1_percent_duplication', 'mean_bait_coverage', 'mean_insert_size', 'mean_target_coverage', 'on_bait_vs_selected', 'pct_chimeras', 'pct_contamination', 'pct_exc_baseq', 'pct_exc_dupe', 'pct_exc_mapq', 'pct_exc_off_target', 'pct_exc_overlap', 'pct_off_bait', 'pct_pf_reads_aligned', 'pct_reads_aligned_in_pairs', 'pct_selected_bases', 'pct_target_bases_100x', 'pct_target_bases_10x', 'pct_target_bases_20x', 'pct_target_bases_2x', 'pct_target_bases_30x', 'pct_target_bases_50x', 'pct_usable_bases_on_bait', 'pct_usable_bases_on_target', 'pf_hq_error_rate', 'strand_balance', 'zero_cvg_targets_pct', 'library_2_mean_insert_size', 'library_2_pct_exc_dupe', 'library_2_percent_duplication']\n",
    "table = \"sample\"\n",
    "\n",
    "# Pull ingested samples\n",
    "payload = {\n",
    "  \"offset\": 0,\n",
    "  \"limit\": 1000,\n",
    "  \"sort\": \"datarepo_row_id\",\n",
    "  \"direction\": \"asc\",\n",
    "  \"filter\": \"\"\n",
    "}\n",
    "ingested_records = datasets_api.query_dataset_data_by_id(id=\"5f4ece3e-d76e-4d78-99e0-e62a24cd163d\", table=table, query_data_request_model=payload).to_dict()\n",
    "already_processed_samples = [rec[\"sample_id\"] for rec in ingested_records[\"result\"]]\n",
    "\n",
    "# Pull samples to ingest\n",
    "payload = {\n",
    "  \"offset\": 0,\n",
    "  \"limit\": 1000,\n",
    "  \"sort\": \"datarepo_row_id\",\n",
    "  \"direction\": \"asc\",\n",
    "  \"filter\": \"\"\n",
    "}\n",
    "records_orig = datasets_api.query_dataset_data_by_id(id=\"85dbde76-c130-40b2-8a8a-ba815ba499da\", table=table, query_data_request_model=payload).to_dict()\n",
    "records_processed = []\n",
    "for record in records_orig[\"result\"]:\n",
    "    int_record = record.copy()\n",
    "    for fcol in float_col_dict[table]:\n",
    "        if int_record[fcol]:\n",
    "            int_record[fcol] = float(int_record[fcol])\n",
    "    if int_record[\"sample_id\"] not in already_processed_samples:\n",
    "        records_processed.append(int_record)\n",
    "\n",
    "# Build ingest request\n",
    "ingest_request = {\n",
    "    \"table\": table,\n",
    "    \"profile_id\": \"9ee23bed-b46c-4561-9103-d2a723113f7f\",\n",
    "    \"ignore_unknown_values\": True,\n",
    "    \"resolve_existing_files\": True,\n",
    "    \"updateStrategy\": \"append\",\n",
    "    \"format\": \"array\",\n",
    "    \"load_tag\": \"Ingest for 5f4ece3e-d76e-4d78-99e0-e62a24cd163d\",\n",
    "    \"records\": records_processed[0:100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(already_processed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(records_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(ingest_request[\"records\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "json.dumps(ingest_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Manual datarepo_row_id_xwalk modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "tdr_host = \"https://data.terra.bio\"\n",
    "source_dataset_id = '75fb0984-2124-444f-881b-30a1a6f8b8f7'\n",
    "target_dataset_id = '7a9eee5d-95c2-4947-93a8-e31d53a2a09a'\n",
    "table = \"subject\"\n",
    "key_field = \"subject_id\"\n",
    "\n",
    "# Load existing datarepo_row_id_walk\n",
    "logging.info(\"Fetching existing datarepo_row_id crosswalk (if one exists).\")\n",
    "xwalk_json_file_name = f\"{source_dataset_id}_{target_dataset_id}_rowid_xwalk.json\"\n",
    "try:\n",
    "    with open(xwalk_json_file_name,\"r\") as file:\n",
    "        datarepo_row_id_xwalk = json.load(file)\n",
    "except:\n",
    "    datarepo_row_id_xwalk = {}\n",
    "    logging.warning(f\"No datarepo_row_id crosswalk file name '{xwalk_json_file_name}' found.\")\n",
    "\n",
    "# Fetch records from source dataset\n",
    "logging.info(\"Fetching records from source dataset.\")\n",
    "api_client = refresh_tdr_api_client(tdr_host)\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "max_page_size = 1000\n",
    "total_record_count = 1000\n",
    "records_fetched = 0\n",
    "retrieval_error = False\n",
    "source_records = []\n",
    "while records_fetched < total_record_count and not retrieval_error:\n",
    "    row_start = records_fetched\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        payload = {\n",
    "          \"offset\": row_start,\n",
    "          \"limit\": max_page_size,\n",
    "          \"sort\": \"datarepo_row_id\",\n",
    "          \"direction\": \"asc\",\n",
    "          \"filter\": \"\"\n",
    "        }\n",
    "        try:\n",
    "            dataset_results = datasets_api.query_dataset_data_by_id(id=source_dataset_id, table=table, query_data_request_model=payload).to_dict() \n",
    "            total_record_count = dataset_results[\"total_row_count\"]\n",
    "            if len(dataset_results[\"result\"]) == 0:\n",
    "                warn_str = f\"No records found for '{table}' table, which prevents the proper building of the datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream.\"\n",
    "                logging.warning(warn_str)\n",
    "                retrieval_error = True\n",
    "                break  \n",
    "            else:\n",
    "                for record in dataset_results[\"result\"]:\n",
    "                    source_records.append([record[key_field], record[\"datarepo_row_id\"]])\n",
    "                    records_fetched += 1\n",
    "                break\n",
    "        except Exception as e:\n",
    "                if attempt_counter < 0:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    warn_str = f\"Error retrieving records for '{table}' table to build datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream. Error: {str(e)}\"\n",
    "                    logging.warning(warn_str)\n",
    "                    retrieval_error = True\n",
    "                    break\n",
    "    logging.info(f\"Records fetched: {str(records_fetched)}\")\n",
    "\n",
    "# Fetch records from target dataset\n",
    "logging.info(\"Fetching records from target dataset.\")\n",
    "api_client = refresh_tdr_api_client(tdr_host)\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "max_page_size = 1000\n",
    "total_record_count = 1000\n",
    "records_fetched = 0\n",
    "retrieval_error = False\n",
    "target_records = []\n",
    "while records_fetched < total_record_count and not retrieval_error:\n",
    "    row_start = records_fetched\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        payload = {\n",
    "          \"offset\": row_start,\n",
    "          \"limit\": max_page_size,\n",
    "          \"sort\": \"datarepo_row_id\",\n",
    "          \"direction\": \"asc\",\n",
    "          \"filter\": \"\"\n",
    "        }\n",
    "        try:\n",
    "            dataset_results = datasets_api.query_dataset_data_by_id(id=target_dataset_id, table=table, query_data_request_model=payload).to_dict() \n",
    "            total_record_count = dataset_results[\"total_row_count\"]\n",
    "            if len(dataset_results[\"result\"]) == 0:\n",
    "                warn_str = f\"No records found for '{table}' table, which prevents the proper building of the datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream.\"\n",
    "                logging.warning(warn_str)\n",
    "                retrieval_error = True\n",
    "                break  \n",
    "            else:\n",
    "                for record in dataset_results[\"result\"]:\n",
    "                    target_records.append([record[key_field], record[\"datarepo_row_id\"]])\n",
    "                    records_fetched += 1\n",
    "                break\n",
    "        except Exception as e:\n",
    "                if attempt_counter < 0:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    warn_str = f\"Error retrieving records for '{table}' table to build datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream. Error: {str(e)}\"\n",
    "                    logging.warning(warn_str)\n",
    "                    retrieval_error = True\n",
    "                    break\n",
    "    logging.info(f\"Records fetched: {str(records_fetched)}\")\n",
    "\n",
    "# Match records and update datarepo_row_id_xwalk\n",
    "logging.info(\"Building records for datarepo_row_id_xwalk\")\n",
    "temp_dr_xwalk = {}\n",
    "for source_record in source_records:\n",
    "    for target_record in target_records:\n",
    "        if source_record[0] == target_record[0]:\n",
    "            key = table + \":\" + source_record[1]\n",
    "            val = table + \":\" + target_record[1]\n",
    "            temp_dr_xwalk[key] = val\n",
    "            break\n",
    "if len(temp_dr_xwalk) == total_record_count:\n",
    "    datarepo_row_id_xwalk[table] = temp_dr_xwalk\n",
    "    with open(xwalk_json_file_name, 'w') as file:\n",
    "        json.dump(datarepo_row_id_xwalk, file)\n",
    "    logging.info(\"Processing complete.\")\n",
    "else:\n",
    "    logging.error(\"Rows in xwalk doesn't match table record count.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "source_dataset_id = '65793118-3c88-4185-9172-2354850e6056'\n",
    "target_dataset_id = '183ec762-f867-46c5-bb19-8b2b3417f7b2'\n",
    "\n",
    "# Load existing datarepo_row_id_walk\n",
    "logging.info(\"Fetching existing datarepo_row_id crosswalk (if one exists).\")\n",
    "xwalk_json_file_name = f\"{source_dataset_id}_{target_dataset_id}_rowid_xwalk.json\"\n",
    "try:\n",
    "    with open(xwalk_json_file_name,\"r\") as file:\n",
    "        datarepo_row_id_xwalk = json.load(file)\n",
    "except:\n",
    "    datarepo_row_id_xwalk = {}\n",
    "    logging.warning(f\"No datarepo_row_id crosswalk file name '{xwalk_json_file_name}' found.\")\n",
    "\n",
    "# Output crosswalk record counts\n",
    "for key in datarepo_row_id_xwalk.keys():\n",
    "    length = len(datarepo_row_id_xwalk[key])\n",
    "    print(f\"{key}: {length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Pull and Compare Tabular Data between TDR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4,
     109
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/25/2024 11:25:56 PM - INFO: Comparing tabular data record counts between TDR dataset dd2cb8fc-42a6-482f-898e-ef6125feccb8 and TDR dataset 245020b9-7355-4002-95db-12e7234070c5.\n",
      "10/25/2024 11:25:56 PM - INFO: Pulling the superset of tables across the two datasets.\n",
      "10/25/2024 11:25:56 PM - INFO: Comparing record counts for table 'anvil_diagnosis'\n",
      "10/25/2024 11:26:01 PM - INFO: Comparing record counts for table 'sample'\n",
      "10/25/2024 11:26:10 PM - INFO: Comparing record counts for table 'anvil_variantcallingactivity'\n",
      "10/25/2024 11:26:13 PM - INFO: Comparing record counts for table 'anvil_alignmentactivity'\n",
      "10/25/2024 11:26:17 PM - INFO: Comparing record counts for table 'anvil_activity'\n",
      "10/25/2024 11:26:21 PM - INFO: Comparing record counts for table 'file_inventory'\n",
      "10/25/2024 11:26:24 PM - INFO: Comparing record counts for table 'subject'\n",
      "10/25/2024 11:26:27 PM - INFO: Comparing record counts for table 'anvil_dataset'\n",
      "10/25/2024 11:26:30 PM - INFO: Comparing record counts for table 'anvil_file'\n",
      "10/25/2024 11:26:33 PM - INFO: Comparing record counts for table 'anvil_antibody'\n",
      "10/25/2024 11:26:37 PM - INFO: Comparing record counts for table 'anvil_project'\n",
      "10/25/2024 11:26:39 PM - INFO: Comparing record counts for table 'sample_set'\n",
      "10/25/2024 11:26:42 PM - INFO: Comparing record counts for table 'anvil_assayactivity'\n",
      "10/25/2024 11:26:46 PM - INFO: Comparing record counts for table 'anvil_sequencingactivity'\n",
      "10/25/2024 11:26:49 PM - INFO: Comparing record counts for table 'participant'\n",
      "10/25/2024 11:26:52 PM - INFO: Comparing record counts for table 'anvil_biosample'\n",
      "10/25/2024 11:26:55 PM - INFO: Comparing record counts for table 'workspace_attributes'\n",
      "10/25/2024 11:26:58 PM - INFO: Comparing record counts for table 'anvil_donor'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Table</th>\n",
       "      <th>Table in DS1</th>\n",
       "      <th>DS1 Record Count</th>\n",
       "      <th>Table in DS2</th>\n",
       "      <th>DS2 Record Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>sample</td>\n",
       "      <td>True</td>\n",
       "      <td>5031</td>\n",
       "      <td>True</td>\n",
       "      <td>5031</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>True</td>\n",
       "      <td>15095</td>\n",
       "      <td>True</td>\n",
       "      <td>15095</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>file_inventory</td>\n",
       "      <td>True</td>\n",
       "      <td>15101</td>\n",
       "      <td>True</td>\n",
       "      <td>15101</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>subject</td>\n",
       "      <td>True</td>\n",
       "      <td>5031</td>\n",
       "      <td>True</td>\n",
       "      <td>5031</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_file</td>\n",
       "      <td>True</td>\n",
       "      <td>15101</td>\n",
       "      <td>True</td>\n",
       "      <td>15101</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_project</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>sample_set</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>participant</td>\n",
       "      <td>True</td>\n",
       "      <td>5031</td>\n",
       "      <td>True</td>\n",
       "      <td>5031</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>True</td>\n",
       "      <td>5031</td>\n",
       "      <td>True</td>\n",
       "      <td>5031</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>workspace_attributes</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>True</td>\n",
       "      <td>5031</td>\n",
       "      <td>True</td>\n",
       "      <td>5031</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset 1 ID                          Dataset 2 ID                         Table             Table in DS1  DS1 Record Count Table in DS2  DS2 Record Count Status Message\n",
       "0   dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5               anvil_diagnosis     True                0          True                0       Pass         \n",
       "1   dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                        sample     True             5031          True             5031       Pass         \n",
       "2   dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5  anvil_variantcallingactivity     True                0          True                0       Pass         \n",
       "3   dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5       anvil_alignmentactivity     True                0          True                0       Pass         \n",
       "4   dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                anvil_activity     True            15095          True            15095       Pass         \n",
       "5   dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                file_inventory     True            15101          True            15101       Pass         \n",
       "6   dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                       subject     True             5031          True             5031       Pass         \n",
       "7   dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                 anvil_dataset     True                1          True                1       Pass         \n",
       "8   dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                    anvil_file     True            15101          True            15101       Pass         \n",
       "9   dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                anvil_antibody     True                0          True                0       Pass         \n",
       "10  dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                 anvil_project     True                1          True                1       Pass         \n",
       "11  dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                    sample_set     True                1          True                1       Pass         \n",
       "12  dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5           anvil_assayactivity     True                0          True                0       Pass         \n",
       "13  dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5      anvil_sequencingactivity     True                0          True                0       Pass         \n",
       "14  dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                   participant     True             5031          True             5031       Pass         \n",
       "15  dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5               anvil_biosample     True             5031          True             5031       Pass         \n",
       "16  dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5          workspace_attributes     True               27          True               27       Pass         \n",
       "17  dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5                   anvil_donor     True             5031          True             5031       Pass         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/25/2024 11:27:01 PM - INFO: Comparing tabular data record counts between TDR dataset 92382848-f5e9-426c-b7dc-f2841ae97018 and TDR dataset 8a90137a-7aed-4e8c-bd99-1399f1c550fd.\n",
      "10/25/2024 11:27:01 PM - INFO: Pulling the superset of tables across the two datasets.\n",
      "10/25/2024 11:27:02 PM - INFO: Comparing record counts for table 'anvil_diagnosis'\n",
      "10/25/2024 11:27:05 PM - INFO: Comparing record counts for table 'sample'\n",
      "10/25/2024 11:27:09 PM - INFO: Comparing record counts for table 'anvil_alignmentactivity'\n",
      "10/25/2024 11:27:12 PM - INFO: Comparing record counts for table 'anvil_activity'\n",
      "10/25/2024 11:27:15 PM - INFO: Comparing record counts for table 'file_inventory'\n",
      "10/25/2024 11:27:18 PM - INFO: Comparing record counts for table 'workspace_attributes'\n",
      "10/25/2024 11:27:21 PM - INFO: Comparing record counts for table 'anvil_file'\n",
      "10/25/2024 11:27:24 PM - INFO: Comparing record counts for table 'subject'\n",
      "10/25/2024 11:27:27 PM - INFO: Comparing record counts for table 'anvil_dataset'\n",
      "10/25/2024 11:27:30 PM - INFO: Comparing record counts for table 'anvil_project'\n",
      "10/25/2024 11:27:32 PM - INFO: Comparing record counts for table 'sample_set'\n",
      "10/25/2024 11:27:35 PM - INFO: Comparing record counts for table 'anvil_assayactivity'\n",
      "10/25/2024 11:27:38 PM - INFO: Comparing record counts for table 'participant'\n",
      "10/25/2024 11:27:41 PM - INFO: Comparing record counts for table 'anvil_donor'\n",
      "10/25/2024 11:27:44 PM - INFO: Comparing record counts for table 'anvil_sequencingactivity'\n",
      "10/25/2024 11:27:47 PM - INFO: Comparing record counts for table 'anvil_biosample'\n",
      "10/25/2024 11:27:50 PM - INFO: Comparing record counts for table 'anvil_antibody'\n",
      "10/25/2024 11:27:53 PM - INFO: Comparing record counts for table 'anvil_variantcallingactivity'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Table</th>\n",
       "      <th>Table in DS1</th>\n",
       "      <th>DS1 Record Count</th>\n",
       "      <th>Table in DS2</th>\n",
       "      <th>DS2 Record Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>sample</td>\n",
       "      <td>True</td>\n",
       "      <td>1565</td>\n",
       "      <td>True</td>\n",
       "      <td>1565</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>True</td>\n",
       "      <td>4695</td>\n",
       "      <td>True</td>\n",
       "      <td>4695</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>file_inventory</td>\n",
       "      <td>True</td>\n",
       "      <td>4696</td>\n",
       "      <td>True</td>\n",
       "      <td>4696</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>workspace_attributes</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_file</td>\n",
       "      <td>True</td>\n",
       "      <td>4696</td>\n",
       "      <td>True</td>\n",
       "      <td>4696</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>subject</td>\n",
       "      <td>True</td>\n",
       "      <td>1565</td>\n",
       "      <td>True</td>\n",
       "      <td>1565</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_project</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>sample_set</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>participant</td>\n",
       "      <td>True</td>\n",
       "      <td>1565</td>\n",
       "      <td>True</td>\n",
       "      <td>1565</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>True</td>\n",
       "      <td>1565</td>\n",
       "      <td>True</td>\n",
       "      <td>1565</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>True</td>\n",
       "      <td>1565</td>\n",
       "      <td>True</td>\n",
       "      <td>1565</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset 1 ID                          Dataset 2 ID                         Table             Table in DS1  DS1 Record Count Table in DS2  DS2 Record Count Status Message\n",
       "0   92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd               anvil_diagnosis     True               0           True               0        Pass         \n",
       "1   92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                        sample     True            1565           True            1565        Pass         \n",
       "2   92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd       anvil_alignmentactivity     True               0           True               0        Pass         \n",
       "3   92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                anvil_activity     True            4695           True            4695        Pass         \n",
       "4   92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                file_inventory     True            4696           True            4696        Pass         \n",
       "5   92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd          workspace_attributes     True              30           True              30        Pass         \n",
       "6   92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                    anvil_file     True            4696           True            4696        Pass         \n",
       "7   92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                       subject     True            1565           True            1565        Pass         \n",
       "8   92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                 anvil_dataset     True               1           True               1        Pass         \n",
       "9   92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                 anvil_project     True               1           True               1        Pass         \n",
       "10  92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                    sample_set     True               1           True               1        Pass         \n",
       "11  92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd           anvil_assayactivity     True               0           True               0        Pass         \n",
       "12  92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                   participant     True            1565           True            1565        Pass         \n",
       "13  92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                   anvil_donor     True            1565           True            1565        Pass         \n",
       "14  92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd      anvil_sequencingactivity     True               0           True               0        Pass         \n",
       "15  92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd               anvil_biosample     True            1565           True            1565        Pass         \n",
       "16  92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd                anvil_antibody     True               0           True               0        Pass         \n",
       "17  92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd  anvil_variantcallingactivity     True               0           True               0        Pass         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/25/2024 11:27:56 PM - INFO: Comparing tabular data record counts between TDR dataset 1f534eb4-701f-4182-9895-64c5e5b52d82 and TDR dataset 1bf4d70f-db98-4d07-b48f-d177efd25ae4.\n",
      "10/25/2024 11:27:56 PM - INFO: Pulling the superset of tables across the two datasets.\n",
      "10/25/2024 11:27:57 PM - INFO: Comparing record counts for table 'anvil_diagnosis'\n",
      "10/25/2024 11:28:00 PM - INFO: Comparing record counts for table 'sample'\n",
      "10/25/2024 11:28:03 PM - INFO: Comparing record counts for table 'anvil_variantcallingactivity'\n",
      "10/25/2024 11:28:07 PM - INFO: Comparing record counts for table 'anvil_alignmentactivity'\n",
      "10/25/2024 11:28:11 PM - INFO: Comparing record counts for table 'anvil_dataset'\n",
      "10/25/2024 11:28:14 PM - INFO: Comparing record counts for table 'file_inventory'\n",
      "10/25/2024 11:28:17 PM - INFO: Comparing record counts for table 'anvil_antibody'\n",
      "10/25/2024 11:28:20 PM - INFO: Comparing record counts for table 'anvil_file'\n",
      "10/25/2024 11:28:23 PM - INFO: Comparing record counts for table 'anvil_activity'\n",
      "10/25/2024 11:28:26 PM - INFO: Comparing record counts for table 'subject'\n",
      "10/25/2024 11:28:29 PM - INFO: Comparing record counts for table 'sample_set'\n",
      "10/25/2024 11:28:31 PM - INFO: Comparing record counts for table 'anvil_sequencingactivity'\n",
      "10/25/2024 11:28:35 PM - INFO: Comparing record counts for table 'anvil_assayactivity'\n",
      "10/25/2024 11:28:38 PM - INFO: Comparing record counts for table 'participant'\n",
      "10/25/2024 11:28:40 PM - INFO: Comparing record counts for table 'anvil_project'\n",
      "10/25/2024 11:28:43 PM - INFO: Comparing record counts for table 'anvil_biosample'\n",
      "10/25/2024 11:28:46 PM - INFO: Comparing record counts for table 'workspace_attributes'\n",
      "10/25/2024 11:28:49 PM - INFO: Comparing record counts for table 'anvil_donor'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Table</th>\n",
       "      <th>Table in DS1</th>\n",
       "      <th>DS1 Record Count</th>\n",
       "      <th>Table in DS2</th>\n",
       "      <th>DS2 Record Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>sample</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>file_inventory</td>\n",
       "      <td>True</td>\n",
       "      <td>1468</td>\n",
       "      <td>True</td>\n",
       "      <td>1468</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_file</td>\n",
       "      <td>True</td>\n",
       "      <td>1468</td>\n",
       "      <td>True</td>\n",
       "      <td>1468</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>True</td>\n",
       "      <td>1467</td>\n",
       "      <td>True</td>\n",
       "      <td>1467</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>subject</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>sample_set</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>participant</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_project</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>workspace_attributes</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "      <td>True</td>\n",
       "      <td>489</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset 1 ID                          Dataset 2 ID                         Table             Table in DS1  DS1 Record Count Table in DS2  DS2 Record Count Status Message\n",
       "0   1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4               anvil_diagnosis     True               0           True               0        Pass         \n",
       "1   1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                        sample     True             489           True             489        Pass         \n",
       "2   1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4  anvil_variantcallingactivity     True               0           True               0        Pass         \n",
       "3   1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4       anvil_alignmentactivity     True               0           True               0        Pass         \n",
       "4   1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                 anvil_dataset     True               1           True               1        Pass         \n",
       "5   1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                file_inventory     True            1468           True            1468        Pass         \n",
       "6   1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                anvil_antibody     True               0           True               0        Pass         \n",
       "7   1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                    anvil_file     True            1468           True            1468        Pass         \n",
       "8   1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                anvil_activity     True            1467           True            1467        Pass         \n",
       "9   1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                       subject     True             489           True             489        Pass         \n",
       "10  1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                    sample_set     True               1           True               1        Pass         \n",
       "11  1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4      anvil_sequencingactivity     True               0           True               0        Pass         \n",
       "12  1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4           anvil_assayactivity     True               0           True               0        Pass         \n",
       "13  1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                   participant     True             489           True             489        Pass         \n",
       "14  1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                 anvil_project     True               1           True               1        Pass         \n",
       "15  1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4               anvil_biosample     True             489           True             489        Pass         \n",
       "16  1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4          workspace_attributes     True              30           True              30        Pass         \n",
       "17  1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4                   anvil_donor     True             489           True             489        Pass         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/25/2024 11:28:52 PM - INFO: Comparing tabular data record counts between TDR dataset e68d1d39-99df-4cd7-8053-1b298f03eabb and TDR dataset cfcb0f71-1157-4dbb-a76b-926c0cd40ea2.\n",
      "10/25/2024 11:28:52 PM - INFO: Pulling the superset of tables across the two datasets.\n",
      "10/25/2024 11:28:52 PM - INFO: Comparing record counts for table 'anvil_diagnosis'\n",
      "10/25/2024 11:28:56 PM - INFO: Comparing record counts for table 'sample'\n",
      "10/25/2024 11:28:59 PM - INFO: Comparing record counts for table 'anvil_alignmentactivity'\n",
      "10/25/2024 11:29:02 PM - INFO: Comparing record counts for table 'anvil_antibody'\n",
      "10/25/2024 11:29:06 PM - INFO: Comparing record counts for table 'subject'\n",
      "10/25/2024 11:29:08 PM - INFO: Comparing record counts for table 'file_inventory'\n",
      "10/25/2024 11:29:12 PM - INFO: Comparing record counts for table 'anvil_file'\n",
      "10/25/2024 11:29:15 PM - INFO: Comparing record counts for table 'anvil_dataset'\n",
      "10/25/2024 11:29:17 PM - INFO: Comparing record counts for table 'anvil_activity'\n",
      "10/25/2024 11:29:20 PM - INFO: Comparing record counts for table 'anvil_project'\n",
      "10/25/2024 11:29:23 PM - INFO: Comparing record counts for table 'anvil_sequencingactivity'\n",
      "10/25/2024 11:29:26 PM - INFO: Comparing record counts for table 'anvil_assayactivity'\n",
      "10/25/2024 11:29:29 PM - INFO: Comparing record counts for table 'participant'\n",
      "10/25/2024 11:29:32 PM - INFO: Comparing record counts for table 'anvil_donor'\n",
      "10/25/2024 11:29:34 PM - INFO: Comparing record counts for table 'sample_set'\n",
      "10/25/2024 11:29:37 PM - INFO: Comparing record counts for table 'anvil_biosample'\n",
      "10/25/2024 11:29:40 PM - INFO: Comparing record counts for table 'workspace_attributes'\n",
      "10/25/2024 11:29:43 PM - INFO: Comparing record counts for table 'anvil_variantcallingactivity'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Table</th>\n",
       "      <th>Table in DS1</th>\n",
       "      <th>DS1 Record Count</th>\n",
       "      <th>Table in DS2</th>\n",
       "      <th>DS2 Record Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>sample</td>\n",
       "      <td>True</td>\n",
       "      <td>2150</td>\n",
       "      <td>True</td>\n",
       "      <td>2150</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>subject</td>\n",
       "      <td>True</td>\n",
       "      <td>2150</td>\n",
       "      <td>True</td>\n",
       "      <td>2150</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>file_inventory</td>\n",
       "      <td>True</td>\n",
       "      <td>6454</td>\n",
       "      <td>True</td>\n",
       "      <td>6454</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_file</td>\n",
       "      <td>True</td>\n",
       "      <td>6454</td>\n",
       "      <td>True</td>\n",
       "      <td>6454</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>True</td>\n",
       "      <td>6451</td>\n",
       "      <td>True</td>\n",
       "      <td>6451</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_project</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>participant</td>\n",
       "      <td>True</td>\n",
       "      <td>2150</td>\n",
       "      <td>True</td>\n",
       "      <td>2150</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>True</td>\n",
       "      <td>2150</td>\n",
       "      <td>True</td>\n",
       "      <td>2150</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>sample_set</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>True</td>\n",
       "      <td>2150</td>\n",
       "      <td>True</td>\n",
       "      <td>2150</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>workspace_attributes</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset 1 ID                          Dataset 2 ID                         Table             Table in DS1  DS1 Record Count Table in DS2  DS2 Record Count Status Message\n",
       "0   e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2               anvil_diagnosis     True               0           True               0        Pass         \n",
       "1   e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                        sample     True            2150           True            2150        Pass         \n",
       "2   e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2       anvil_alignmentactivity     True               0           True               0        Pass         \n",
       "3   e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                anvil_antibody     True               0           True               0        Pass         \n",
       "4   e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                       subject     True            2150           True            2150        Pass         \n",
       "5   e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                file_inventory     True            6454           True            6454        Pass         \n",
       "6   e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                    anvil_file     True            6454           True            6454        Pass         \n",
       "7   e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                 anvil_dataset     True               1           True               1        Pass         \n",
       "8   e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                anvil_activity     True            6451           True            6451        Pass         \n",
       "9   e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                 anvil_project     True               1           True               1        Pass         \n",
       "10  e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2      anvil_sequencingactivity     True               0           True               0        Pass         \n",
       "11  e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2           anvil_assayactivity     True               0           True               0        Pass         \n",
       "12  e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                   participant     True            2150           True            2150        Pass         \n",
       "13  e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                   anvil_donor     True            2150           True            2150        Pass         \n",
       "14  e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2                    sample_set     True               1           True               1        Pass         \n",
       "15  e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2               anvil_biosample     True            2150           True            2150        Pass         \n",
       "16  e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2          workspace_attributes     True              27           True              27        Pass         \n",
       "17  e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2  anvil_variantcallingactivity     True               0           True               0        Pass         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/25/2024 11:29:46 PM - INFO: Comparing tabular data record counts between TDR dataset 2ef4530a-cc36-4f32-9a1a-63a555346587 and TDR dataset d28cb1d4-2300-4cd7-882b-99ce59305ce0.\n",
      "10/25/2024 11:29:46 PM - INFO: Pulling the superset of tables across the two datasets.\n",
      "10/25/2024 11:29:46 PM - INFO: Comparing record counts for table 'anvil_diagnosis'\n",
      "10/25/2024 11:29:50 PM - INFO: Comparing record counts for table 'sample'\n",
      "10/25/2024 11:29:53 PM - INFO: Comparing record counts for table 'anvil_antibody'\n",
      "10/25/2024 11:29:57 PM - INFO: Comparing record counts for table 'anvil_dataset'\n",
      "10/25/2024 11:29:59 PM - INFO: Comparing record counts for table 'file_inventory'\n",
      "10/25/2024 11:30:03 PM - INFO: Comparing record counts for table 'workspace_attributes'\n",
      "10/25/2024 11:30:06 PM - INFO: Comparing record counts for table 'anvil_file'\n",
      "10/25/2024 11:30:09 PM - INFO: Comparing record counts for table 'subject'\n",
      "10/25/2024 11:30:12 PM - INFO: Comparing record counts for table 'anvil_activity'\n",
      "10/25/2024 11:30:16 PM - INFO: Comparing record counts for table 'anvil_alignmentactivity'\n",
      "10/25/2024 11:30:20 PM - INFO: Comparing record counts for table 'anvil_project'\n",
      "10/25/2024 11:30:22 PM - INFO: Comparing record counts for table 'anvil_sequencingactivity'\n",
      "10/25/2024 11:30:26 PM - INFO: Comparing record counts for table 'anvil_assayactivity'\n",
      "10/25/2024 11:30:29 PM - INFO: Comparing record counts for table 'participant'\n",
      "10/25/2024 11:30:32 PM - INFO: Comparing record counts for table 'anvil_donor'\n",
      "10/25/2024 11:30:35 PM - INFO: Comparing record counts for table 'anvil_biosample'\n",
      "10/25/2024 11:30:38 PM - INFO: Comparing record counts for table 'sample_set'\n",
      "10/25/2024 11:30:41 PM - INFO: Comparing record counts for table 'anvil_variantcallingactivity'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Table</th>\n",
       "      <th>Table in DS1</th>\n",
       "      <th>DS1 Record Count</th>\n",
       "      <th>Table in DS2</th>\n",
       "      <th>DS2 Record Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>sample</td>\n",
       "      <td>True</td>\n",
       "      <td>7483</td>\n",
       "      <td>True</td>\n",
       "      <td>7483</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>file_inventory</td>\n",
       "      <td>True</td>\n",
       "      <td>22450</td>\n",
       "      <td>True</td>\n",
       "      <td>22450</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>workspace_attributes</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_file</td>\n",
       "      <td>True</td>\n",
       "      <td>22450</td>\n",
       "      <td>True</td>\n",
       "      <td>22450</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>subject</td>\n",
       "      <td>True</td>\n",
       "      <td>7483</td>\n",
       "      <td>True</td>\n",
       "      <td>7483</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>True</td>\n",
       "      <td>22449</td>\n",
       "      <td>True</td>\n",
       "      <td>22449</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_project</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>participant</td>\n",
       "      <td>True</td>\n",
       "      <td>7483</td>\n",
       "      <td>True</td>\n",
       "      <td>7483</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>True</td>\n",
       "      <td>7483</td>\n",
       "      <td>True</td>\n",
       "      <td>7483</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>True</td>\n",
       "      <td>7483</td>\n",
       "      <td>True</td>\n",
       "      <td>7483</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>sample_set</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset 1 ID                          Dataset 2 ID                         Table             Table in DS1  DS1 Record Count Table in DS2  DS2 Record Count Status Message\n",
       "0   2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0               anvil_diagnosis     True                0          True                0       Pass         \n",
       "1   2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                        sample     True             7483          True             7483       Pass         \n",
       "2   2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                anvil_antibody     True                0          True                0       Pass         \n",
       "3   2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                 anvil_dataset     True                1          True                1       Pass         \n",
       "4   2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                file_inventory     True            22450          True            22450       Pass         \n",
       "5   2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0          workspace_attributes     True               29          True               29       Pass         \n",
       "6   2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                    anvil_file     True            22450          True            22450       Pass         \n",
       "7   2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                       subject     True             7483          True             7483       Pass         \n",
       "8   2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                anvil_activity     True            22449          True            22449       Pass         \n",
       "9   2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0       anvil_alignmentactivity     True                0          True                0       Pass         \n",
       "10  2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                 anvil_project     True                1          True                1       Pass         \n",
       "11  2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0      anvil_sequencingactivity     True                0          True                0       Pass         \n",
       "12  2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0           anvil_assayactivity     True                0          True                0       Pass         \n",
       "13  2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                   participant     True             7483          True             7483       Pass         \n",
       "14  2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                   anvil_donor     True             7483          True             7483       Pass         \n",
       "15  2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0               anvil_biosample     True             7483          True             7483       Pass         \n",
       "16  2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0                    sample_set     True                1          True                1       Pass         \n",
       "17  2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0  anvil_variantcallingactivity     True                0          True                0       Pass         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/25/2024 11:30:44 PM - INFO: Comparing tabular data record counts between TDR dataset 65793118-3c88-4185-9172-2354850e6056 and TDR dataset 183ec762-f867-46c5-bb19-8b2b3417f7b2.\n",
      "10/25/2024 11:30:44 PM - INFO: Pulling the superset of tables across the two datasets.\n",
      "10/25/2024 11:30:45 PM - INFO: Comparing record counts for table 'anvil_diagnosis'\n",
      "10/25/2024 11:30:48 PM - INFO: Comparing record counts for table 'sample'\n",
      "10/25/2024 11:30:51 PM - INFO: Comparing record counts for table 'anvil_alignmentactivity'\n",
      "10/25/2024 11:30:55 PM - INFO: Comparing record counts for table 'anvil_dataset'\n",
      "10/25/2024 11:30:57 PM - INFO: Comparing record counts for table 'file_inventory'\n",
      "10/25/2024 11:31:01 PM - INFO: Comparing record counts for table 'subject'\n",
      "10/25/2024 11:31:04 PM - INFO: Comparing record counts for table 'anvil_file'\n",
      "10/25/2024 11:31:08 PM - INFO: Comparing record counts for table 'workspace_attributes'\n",
      "10/25/2024 11:31:10 PM - INFO: Comparing record counts for table 'anvil_activity'\n",
      "10/25/2024 11:31:13 PM - INFO: Comparing record counts for table 'anvil_project'\n",
      "10/25/2024 11:31:16 PM - INFO: Comparing record counts for table 'anvil_donor'\n",
      "10/25/2024 11:31:19 PM - INFO: Comparing record counts for table 'anvil_assayactivity'\n",
      "10/25/2024 11:31:22 PM - INFO: Comparing record counts for table 'participant'\n",
      "10/25/2024 11:31:25 PM - INFO: Comparing record counts for table 'anvil_sequencingactivity'\n",
      "10/25/2024 11:31:29 PM - INFO: Comparing record counts for table 'sample_set'\n",
      "10/25/2024 11:31:32 PM - INFO: Comparing record counts for table 'anvil_biosample'\n",
      "10/25/2024 11:31:35 PM - INFO: Comparing record counts for table 'anvil_antibody'\n",
      "10/25/2024 11:31:38 PM - INFO: Comparing record counts for table 'anvil_variantcallingactivity'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Table</th>\n",
       "      <th>Table in DS1</th>\n",
       "      <th>DS1 Record Count</th>\n",
       "      <th>Table in DS2</th>\n",
       "      <th>DS2 Record Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>sample</td>\n",
       "      <td>True</td>\n",
       "      <td>4461</td>\n",
       "      <td>True</td>\n",
       "      <td>4461</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>file_inventory</td>\n",
       "      <td>True</td>\n",
       "      <td>22311</td>\n",
       "      <td>True</td>\n",
       "      <td>22311</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>subject</td>\n",
       "      <td>True</td>\n",
       "      <td>4448</td>\n",
       "      <td>True</td>\n",
       "      <td>4448</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_file</td>\n",
       "      <td>True</td>\n",
       "      <td>22311</td>\n",
       "      <td>True</td>\n",
       "      <td>22311</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>workspace_attributes</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>True</td>\n",
       "      <td>8923</td>\n",
       "      <td>True</td>\n",
       "      <td>8923</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_project</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>True</td>\n",
       "      <td>4461</td>\n",
       "      <td>True</td>\n",
       "      <td>4461</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>participant</td>\n",
       "      <td>True</td>\n",
       "      <td>4461</td>\n",
       "      <td>True</td>\n",
       "      <td>4461</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>sample_set</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>True</td>\n",
       "      <td>4461</td>\n",
       "      <td>True</td>\n",
       "      <td>4461</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset 1 ID                          Dataset 2 ID                         Table             Table in DS1  DS1 Record Count Table in DS2  DS2 Record Count Status Message\n",
       "0   65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2               anvil_diagnosis     True                0          True                0       Pass         \n",
       "1   65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                        sample     True             4461          True             4461       Pass         \n",
       "2   65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2       anvil_alignmentactivity     True                0          True                0       Pass         \n",
       "3   65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                 anvil_dataset     True                1          True                1       Pass         \n",
       "4   65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                file_inventory     True            22311          True            22311       Pass         \n",
       "5   65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                       subject     True             4448          True             4448       Pass         \n",
       "6   65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                    anvil_file     True            22311          True            22311       Pass         \n",
       "7   65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2          workspace_attributes     True               29          True               29       Pass         \n",
       "8   65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                anvil_activity     True             8923          True             8923       Pass         \n",
       "9   65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                 anvil_project     True                1          True                1       Pass         \n",
       "10  65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                   anvil_donor     True             4461          True             4461       Pass         \n",
       "11  65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2           anvil_assayactivity     True                0          True                0       Pass         \n",
       "12  65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                   participant     True             4461          True             4461       Pass         \n",
       "13  65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2      anvil_sequencingactivity     True                0          True                0       Pass         \n",
       "14  65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                    sample_set     True                1          True                1       Pass         \n",
       "15  65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2               anvil_biosample     True             4461          True             4461       Pass         \n",
       "16  65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2                anvil_antibody     True                0          True                0       Pass         \n",
       "17  65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2  anvil_variantcallingactivity     True                0          True                0       Pass         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/25/2024 11:31:42 PM - INFO: Comparing tabular data record counts between TDR dataset 36bdd59f-4f5b-43cd-8d34-a21ef87bbf30 and TDR dataset 933d1603-8c61-4ff2-8489-7f774ac15e97.\n",
      "10/25/2024 11:31:42 PM - INFO: Pulling the superset of tables across the two datasets.\n",
      "10/25/2024 11:31:42 PM - INFO: Comparing record counts for table 'anvil_diagnosis'\n",
      "10/25/2024 11:31:46 PM - INFO: Comparing record counts for table 'sample'\n",
      "10/25/2024 11:31:49 PM - INFO: Comparing record counts for table 'anvil_variantcallingactivity'\n",
      "10/25/2024 11:31:52 PM - INFO: Comparing record counts for table 'anvil_activity'\n",
      "10/25/2024 11:31:55 PM - INFO: Comparing record counts for table 'anvil_dataset'\n",
      "10/25/2024 11:31:58 PM - INFO: Comparing record counts for table 'anvil_alignmentactivity'\n",
      "10/25/2024 11:32:02 PM - INFO: Comparing record counts for table 'workspace_attributes'\n",
      "10/25/2024 11:32:04 PM - INFO: Comparing record counts for table 'file_inventory'\n",
      "10/25/2024 11:32:08 PM - INFO: Comparing record counts for table 'subject'\n",
      "10/25/2024 11:32:10 PM - INFO: Comparing record counts for table 'anvil_file'\n",
      "10/25/2024 11:32:14 PM - INFO: Comparing record counts for table 'anvil_project'\n",
      "10/25/2024 11:32:16 PM - INFO: Comparing record counts for table 'anvil_sequencingactivity'\n",
      "10/25/2024 11:32:20 PM - INFO: Comparing record counts for table 'anvil_assayactivity'\n",
      "10/25/2024 11:32:23 PM - INFO: Comparing record counts for table 'participant'\n",
      "10/25/2024 11:32:26 PM - INFO: Comparing record counts for table 'sample_set'\n",
      "10/25/2024 11:32:28 PM - INFO: Comparing record counts for table 'anvil_biosample'\n",
      "10/25/2024 11:32:31 PM - INFO: Comparing record counts for table 'anvil_antibody'\n",
      "10/25/2024 11:32:34 PM - INFO: Comparing record counts for table 'anvil_donor'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Table</th>\n",
       "      <th>Table in DS1</th>\n",
       "      <th>DS1 Record Count</th>\n",
       "      <th>Table in DS2</th>\n",
       "      <th>DS2 Record Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>sample</td>\n",
       "      <td>True</td>\n",
       "      <td>4546</td>\n",
       "      <td>True</td>\n",
       "      <td>4546</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>True</td>\n",
       "      <td>13639</td>\n",
       "      <td>True</td>\n",
       "      <td>13639</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>workspace_attributes</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>file_inventory</td>\n",
       "      <td>True</td>\n",
       "      <td>13642</td>\n",
       "      <td>True</td>\n",
       "      <td>13642</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>subject</td>\n",
       "      <td>True</td>\n",
       "      <td>4546</td>\n",
       "      <td>True</td>\n",
       "      <td>4546</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_file</td>\n",
       "      <td>True</td>\n",
       "      <td>13642</td>\n",
       "      <td>True</td>\n",
       "      <td>13642</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_project</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>participant</td>\n",
       "      <td>True</td>\n",
       "      <td>4546</td>\n",
       "      <td>True</td>\n",
       "      <td>4546</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>sample_set</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>True</td>\n",
       "      <td>4546</td>\n",
       "      <td>True</td>\n",
       "      <td>4546</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>True</td>\n",
       "      <td>4546</td>\n",
       "      <td>True</td>\n",
       "      <td>4546</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset 1 ID                          Dataset 2 ID                         Table             Table in DS1  DS1 Record Count Table in DS2  DS2 Record Count Status Message\n",
       "0   36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97               anvil_diagnosis     True                0          True                0       Pass         \n",
       "1   36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                        sample     True             4546          True             4546       Pass         \n",
       "2   36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97  anvil_variantcallingactivity     True                0          True                0       Pass         \n",
       "3   36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                anvil_activity     True            13639          True            13639       Pass         \n",
       "4   36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                 anvil_dataset     True                1          True                1       Pass         \n",
       "5   36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97       anvil_alignmentactivity     True                0          True                0       Pass         \n",
       "6   36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97          workspace_attributes     True               30          True               30       Pass         \n",
       "7   36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                file_inventory     True            13642          True            13642       Pass         \n",
       "8   36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                       subject     True             4546          True             4546       Pass         \n",
       "9   36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                    anvil_file     True            13642          True            13642       Pass         \n",
       "10  36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                 anvil_project     True                1          True                1       Pass         \n",
       "11  36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97      anvil_sequencingactivity     True                0          True                0       Pass         \n",
       "12  36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97           anvil_assayactivity     True                0          True                0       Pass         \n",
       "13  36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                   participant     True             4546          True             4546       Pass         \n",
       "14  36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                    sample_set     True                1          True                1       Pass         \n",
       "15  36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97               anvil_biosample     True             4546          True             4546       Pass         \n",
       "16  36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                anvil_antibody     True                0          True                0       Pass         \n",
       "17  36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97                   anvil_donor     True             4546          True             4546       Pass         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/25/2024 11:32:37 PM - INFO: Comparing tabular data record counts between TDR dataset b60b4737-c646-4299-85a0-520890e830b7 and TDR dataset 757191b0-9db3-4d18-b4ad-97bead5f3221.\n",
      "10/25/2024 11:32:37 PM - INFO: Pulling the superset of tables across the two datasets.\n",
      "10/25/2024 11:32:38 PM - INFO: Comparing record counts for table 'anvil_diagnosis'\n",
      "10/25/2024 11:32:41 PM - INFO: Comparing record counts for table 'sample'\n",
      "10/25/2024 11:32:44 PM - INFO: Comparing record counts for table 'anvil_variantcallingactivity'\n",
      "10/25/2024 11:32:48 PM - INFO: Comparing record counts for table 'anvil_activity'\n",
      "10/25/2024 11:32:51 PM - INFO: Comparing record counts for table 'anvil_alignmentactivity'\n",
      "10/25/2024 11:32:54 PM - INFO: Comparing record counts for table 'subject'\n",
      "10/25/2024 11:32:57 PM - INFO: Comparing record counts for table 'file_inventory'\n",
      "10/25/2024 11:33:00 PM - INFO: Comparing record counts for table 'anvil_file'\n",
      "10/25/2024 11:33:04 PM - INFO: Comparing record counts for table 'anvil_antibody'\n",
      "10/25/2024 11:33:07 PM - INFO: Comparing record counts for table 'anvil_dataset'\n",
      "10/25/2024 11:33:10 PM - INFO: Comparing record counts for table 'sample_set'\n",
      "10/25/2024 11:33:12 PM - INFO: Comparing record counts for table 'anvil_project'\n",
      "10/25/2024 11:33:15 PM - INFO: Comparing record counts for table 'anvil_assayactivity'\n",
      "10/25/2024 11:33:18 PM - INFO: Comparing record counts for table 'anvil_sequencingactivity'\n",
      "10/25/2024 11:33:21 PM - INFO: Comparing record counts for table 'participant'\n",
      "10/25/2024 11:33:24 PM - INFO: Comparing record counts for table 'anvil_biosample'\n",
      "10/25/2024 11:33:27 PM - INFO: Comparing record counts for table 'workspace_attributes'\n",
      "10/25/2024 11:33:30 PM - INFO: Comparing record counts for table 'anvil_donor'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Table</th>\n",
       "      <th>Table in DS1</th>\n",
       "      <th>DS1 Record Count</th>\n",
       "      <th>Table in DS2</th>\n",
       "      <th>DS2 Record Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>sample</td>\n",
       "      <td>True</td>\n",
       "      <td>1548</td>\n",
       "      <td>True</td>\n",
       "      <td>1548</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>True</td>\n",
       "      <td>4644</td>\n",
       "      <td>True</td>\n",
       "      <td>4644</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>subject</td>\n",
       "      <td>True</td>\n",
       "      <td>1548</td>\n",
       "      <td>True</td>\n",
       "      <td>1548</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>file_inventory</td>\n",
       "      <td>True</td>\n",
       "      <td>4645</td>\n",
       "      <td>True</td>\n",
       "      <td>4645</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_file</td>\n",
       "      <td>True</td>\n",
       "      <td>4645</td>\n",
       "      <td>True</td>\n",
       "      <td>4645</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>sample_set</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_project</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>participant</td>\n",
       "      <td>True</td>\n",
       "      <td>1548</td>\n",
       "      <td>True</td>\n",
       "      <td>1548</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>True</td>\n",
       "      <td>1548</td>\n",
       "      <td>True</td>\n",
       "      <td>1548</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>workspace_attributes</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>True</td>\n",
       "      <td>1548</td>\n",
       "      <td>True</td>\n",
       "      <td>1548</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset 1 ID                          Dataset 2 ID                         Table             Table in DS1  DS1 Record Count Table in DS2  DS2 Record Count Status Message\n",
       "0   b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221               anvil_diagnosis     True               0           True               0        Pass         \n",
       "1   b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                        sample     True            1548           True            1548        Pass         \n",
       "2   b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221  anvil_variantcallingactivity     True               0           True               0        Pass         \n",
       "3   b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                anvil_activity     True            4644           True            4644        Pass         \n",
       "4   b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221       anvil_alignmentactivity     True               0           True               0        Pass         \n",
       "5   b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                       subject     True            1548           True            1548        Pass         \n",
       "6   b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                file_inventory     True            4645           True            4645        Pass         \n",
       "7   b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                    anvil_file     True            4645           True            4645        Pass         \n",
       "8   b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                anvil_antibody     True               0           True               0        Pass         \n",
       "9   b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                 anvil_dataset     True               1           True               1        Pass         \n",
       "10  b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                    sample_set     True               1           True               1        Pass         \n",
       "11  b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                 anvil_project     True               1           True               1        Pass         \n",
       "12  b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221           anvil_assayactivity     True               0           True               0        Pass         \n",
       "13  b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221      anvil_sequencingactivity     True               0           True               0        Pass         \n",
       "14  b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                   participant     True            1548           True            1548        Pass         \n",
       "15  b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221               anvil_biosample     True            1548           True            1548        Pass         \n",
       "16  b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221          workspace_attributes     True              30           True              30        Pass         \n",
       "17  b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221                   anvil_donor     True            1548           True            1548        Pass         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/25/2024 11:33:32 PM - INFO: Comparing tabular data record counts between TDR dataset 75fb0984-2124-444f-881b-30a1a6f8b8f7 and TDR dataset 7a9eee5d-95c2-4947-93a8-e31d53a2a09a.\n",
      "10/25/2024 11:33:32 PM - INFO: Pulling the superset of tables across the two datasets.\n",
      "10/25/2024 11:33:33 PM - INFO: Comparing record counts for table 'sample'\n",
      "10/25/2024 11:33:38 PM - INFO: Comparing record counts for table 'qc_result_sample'\n",
      "10/25/2024 11:33:41 PM - INFO: Comparing record counts for table 'anvil_activity'\n",
      "10/25/2024 11:33:44 PM - INFO: Comparing record counts for table 'anvil_dataset'\n",
      "10/25/2024 11:33:47 PM - INFO: Comparing record counts for table 'file_inventory'\n",
      "10/25/2024 11:33:50 PM - INFO: Comparing record counts for table 'anvil_alignmentactivity'\n",
      "10/25/2024 11:33:54 PM - INFO: Comparing record counts for table 'anvil_project'\n",
      "10/25/2024 11:33:56 PM - INFO: Comparing record counts for table 'anvil_assayactivity'\n",
      "10/25/2024 11:34:00 PM - INFO: Comparing record counts for table 'workspace_attributes'\n",
      "10/25/2024 11:34:03 PM - INFO: Comparing record counts for table 'anvil_biosample'\n",
      "10/25/2024 11:34:05 PM - INFO: Comparing record counts for table 'anvil_antibody'\n",
      "10/25/2024 11:34:09 PM - INFO: Comparing record counts for table 'sample_set'\n",
      "10/25/2024 11:34:12 PM - INFO: Comparing record counts for table 'anvil_donor'\n",
      "10/25/2024 11:34:14 PM - INFO: Comparing record counts for table 'anvil_diagnosis'\n",
      "10/25/2024 11:34:17 PM - INFO: Comparing record counts for table 'subject'\n",
      "10/25/2024 11:34:20 PM - INFO: Comparing record counts for table 'anvil_file'\n",
      "10/25/2024 11:34:23 PM - INFO: Comparing record counts for table 'anvil_sequencingactivity'\n",
      "10/25/2024 11:34:26 PM - INFO: Comparing record counts for table 'participant'\n",
      "10/25/2024 11:34:29 PM - INFO: Comparing record counts for table 'anvil_variantcallingactivity'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Table</th>\n",
       "      <th>Table in DS1</th>\n",
       "      <th>DS1 Record Count</th>\n",
       "      <th>Table in DS2</th>\n",
       "      <th>DS2 Record Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>sample</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>qc_result_sample</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>True</td>\n",
       "      <td>1489</td>\n",
       "      <td>True</td>\n",
       "      <td>1489</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>file_inventory</td>\n",
       "      <td>True</td>\n",
       "      <td>1492</td>\n",
       "      <td>True</td>\n",
       "      <td>1492</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_project</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>workspace_attributes</td>\n",
       "      <td>True</td>\n",
       "      <td>43</td>\n",
       "      <td>True</td>\n",
       "      <td>43</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>sample_set</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>248</td>\n",
       "      <td>True</td>\n",
       "      <td>248</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>subject</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_file</td>\n",
       "      <td>True</td>\n",
       "      <td>1492</td>\n",
       "      <td>True</td>\n",
       "      <td>1492</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>participant</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset 1 ID                          Dataset 2 ID                         Table             Table in DS1  DS1 Record Count Table in DS2  DS2 Record Count Status Message\n",
       "0   75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                        sample     True             496           True             496        Pass         \n",
       "1   75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a              qc_result_sample     True             496           True             496        Pass         \n",
       "2   75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                anvil_activity     True            1489           True            1489        Pass         \n",
       "3   75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                 anvil_dataset     True               1           True               1        Pass         \n",
       "4   75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                file_inventory     True            1492           True            1492        Pass         \n",
       "5   75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a       anvil_alignmentactivity     True               0           True               0        Pass         \n",
       "6   75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                 anvil_project     True               1           True               1        Pass         \n",
       "7   75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a           anvil_assayactivity     True               0           True               0        Pass         \n",
       "8   75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a          workspace_attributes     True              43           True              43        Pass         \n",
       "9   75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a               anvil_biosample     True             496           True             496        Pass         \n",
       "10  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                anvil_antibody     True               0           True               0        Pass         \n",
       "11  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                    sample_set     True               1           True               1        Pass         \n",
       "12  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                   anvil_donor     True             496           True             496        Pass         \n",
       "13  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a               anvil_diagnosis     True             248           True             248        Pass         \n",
       "14  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                       subject     True             496           True             496        Pass         \n",
       "15  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                    anvil_file     True            1492           True            1492        Pass         \n",
       "16  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a      anvil_sequencingactivity     True               0           True               0        Pass         \n",
       "17  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a                   participant     True             496           True             496        Pass         \n",
       "18  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a  anvil_variantcallingactivity     True               0           True               0        Pass         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Validation Type</th>\n",
       "      <th>Status</th>\n",
       "      <th>Failed Tables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dd2cb8fc-42a6-482f-898e-ef6125feccb8</td>\n",
       "      <td>245020b9-7355-4002-95db-12e7234070c5</td>\n",
       "      <td>Record Count Comparison</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92382848-f5e9-426c-b7dc-f2841ae97018</td>\n",
       "      <td>8a90137a-7aed-4e8c-bd99-1399f1c550fd</td>\n",
       "      <td>Record Count Comparison</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f534eb4-701f-4182-9895-64c5e5b52d82</td>\n",
       "      <td>1bf4d70f-db98-4d07-b48f-d177efd25ae4</td>\n",
       "      <td>Record Count Comparison</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e68d1d39-99df-4cd7-8053-1b298f03eabb</td>\n",
       "      <td>cfcb0f71-1157-4dbb-a76b-926c0cd40ea2</td>\n",
       "      <td>Record Count Comparison</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2ef4530a-cc36-4f32-9a1a-63a555346587</td>\n",
       "      <td>d28cb1d4-2300-4cd7-882b-99ce59305ce0</td>\n",
       "      <td>Record Count Comparison</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>65793118-3c88-4185-9172-2354850e6056</td>\n",
       "      <td>183ec762-f867-46c5-bb19-8b2b3417f7b2</td>\n",
       "      <td>Record Count Comparison</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36bdd59f-4f5b-43cd-8d34-a21ef87bbf30</td>\n",
       "      <td>933d1603-8c61-4ff2-8489-7f774ac15e97</td>\n",
       "      <td>Record Count Comparison</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b60b4737-c646-4299-85a0-520890e830b7</td>\n",
       "      <td>757191b0-9db3-4d18-b4ad-97bead5f3221</td>\n",
       "      <td>Record Count Comparison</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>75fb0984-2124-444f-881b-30a1a6f8b8f7</td>\n",
       "      <td>7a9eee5d-95c2-4947-93a8-e31d53a2a09a</td>\n",
       "      <td>Record Count Comparison</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset 1 ID                          Dataset 2 ID                 Validation Type      Status Failed Tables\n",
       "0  dd2cb8fc-42a6-482f-898e-ef6125feccb8  245020b9-7355-4002-95db-12e7234070c5  Record Count Comparison  Pass               \n",
       "1  92382848-f5e9-426c-b7dc-f2841ae97018  8a90137a-7aed-4e8c-bd99-1399f1c550fd  Record Count Comparison  Pass               \n",
       "2  1f534eb4-701f-4182-9895-64c5e5b52d82  1bf4d70f-db98-4d07-b48f-d177efd25ae4  Record Count Comparison  Pass               \n",
       "3  e68d1d39-99df-4cd7-8053-1b298f03eabb  cfcb0f71-1157-4dbb-a76b-926c0cd40ea2  Record Count Comparison  Pass               \n",
       "4  2ef4530a-cc36-4f32-9a1a-63a555346587  d28cb1d4-2300-4cd7-882b-99ce59305ce0  Record Count Comparison  Pass               \n",
       "5  65793118-3c88-4185-9172-2354850e6056  183ec762-f867-46c5-bb19-8b2b3417f7b2  Record Count Comparison  Pass               \n",
       "6  36bdd59f-4f5b-43cd-8d34-a21ef87bbf30  933d1603-8c61-4ff2-8489-7f774ac15e97  Record Count Comparison  Pass               \n",
       "7  b60b4737-c646-4299-85a0-520890e830b7  757191b0-9db3-4d18-b4ad-97bead5f3221  Record Count Comparison  Pass               \n",
       "8  75fb0984-2124-444f-881b-30a1a6f8b8f7  7a9eee5d-95c2-4947-93a8-e31d53a2a09a  Record Count Comparison  Pass               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def compare_row_counts(dataset_1_id, dataset_2_id):\n",
    "    \n",
    "    # Setup/refresh TDR clients\n",
    "    logging.info(f\"Comparing tabular data record counts between TDR dataset {dataset_1_id} and TDR dataset {dataset_2_id}.\")\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Pull table list across datasets\n",
    "    logging.info(f\"Pulling the superset of tables across the two datasets.\")\n",
    "    try:\n",
    "        dataset_1_details = datasets_api.retrieve_dataset(id=dataset_1_id, include=[\"SCHEMA\"]).to_dict()\n",
    "        dataset_2_details = datasets_api.retrieve_dataset(id=dataset_2_id, include=[\"SCHEMA\"]).to_dict()\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from datasets: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "    table_set = set()\n",
    "    for table_entry in dataset_1_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])\n",
    "    for table_entry in dataset_2_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])   \n",
    "\n",
    "    # For each table in the table list, pull record counts from the two datasets and compare\n",
    "    results = []\n",
    "    payload = {\n",
    "      \"offset\": 0,\n",
    "      \"limit\": 10,\n",
    "      \"sort\": \"datarepo_row_id\",\n",
    "      \"direction\": \"asc\",\n",
    "      \"filter\": \"\"\n",
    "    }\n",
    "    for table in table_set:\n",
    "        logging.info(f\"Comparing record counts for table '{table}'\")\n",
    "        # Pulling record counts for dataset 1\n",
    "        ds1_table_present = \"True\"\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_1_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                ds1_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    ds1_record_count = 0\n",
    "                    ds1_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        ds1_record_count = 0\n",
    "                        ds1_table_present = \"Unknown\"\n",
    "                        break\n",
    "        # Pulling record counts for dataset 2\n",
    "        ds2_table_present = \"True\"\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_2_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                ds2_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    ds2_record_count = 0\n",
    "                    ds2_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        ds2_record_count = 0\n",
    "                        ds2_table_present = \"Unknown\"\n",
    "                        break\n",
    "        # Build table comparison\n",
    "        if ds1_table_present == \"Unknown\" or ds2_table_present == \"Unknown\":\n",
    "            status = \"Fail\"\n",
    "            error_reason = \"Error retrieving table data from dataset(s)\"\n",
    "        elif ds1_table_present == \"False\" or ds2_table_present == \"False\":\n",
    "            status = \"Fail\"\n",
    "            error_reason = \"Table presence mismatch between datasets\"\n",
    "        elif ds1_record_count != ds2_record_count:\n",
    "            status = \"Fail\"\n",
    "            error_reason = \"Difference in record count\"\n",
    "        else:\n",
    "            status = \"Pass\"\n",
    "            error_reason = \"\"\n",
    "        results.append([dataset_1_id, dataset_2_id, table, ds1_table_present, ds1_record_count, ds2_table_present, ds2_record_count, status, error_reason])\n",
    "\n",
    "    # Display detailed results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"Dataset 1 ID\", \"Dataset 2 ID\", \"Table\", \"Table in DS1\", \"DS1 Record Count\", \"Table in DS2\", \"DS2 Record Count\", \"Status\", \"Message\"])\n",
    "    display(results_df)\n",
    "\n",
    "    # Return final aggregated results\n",
    "    status = \"Pass\"\n",
    "    failed_tables = []\n",
    "    for entry in results:\n",
    "        if entry[7] == \"Fail\":\n",
    "            failed_tables.append(entry[2])\n",
    "            status = \"Fail\"\n",
    "    return status, sorted(failed_tables)\n",
    "        \n",
    "def compare_contents_sample(dataset_1_id, dataset_2_id, sample_size, fields_to_ignore):\n",
    "    # Pull schema, record first column in each table (for ordering)\n",
    "    # Setup/refresh TDR clients\n",
    "    logging.info(f\"Comparing tabular data record counts between TDR dataset {dataset_1_id} and TDR dataset {dataset_2_id}.\")\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Pull table list across datasets\n",
    "    logging.info(f\"Pulling the superset of tables across the two datasets.\")\n",
    "    try:\n",
    "        dataset_1_details = datasets_api.retrieve_dataset(id=dataset_1_id, include=[\"SCHEMA\"]).to_dict()\n",
    "        dataset_2_details = datasets_api.retrieve_dataset(id=dataset_2_id, include=[\"SCHEMA\"]).to_dict()\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from datasets: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "    table_set = {}\n",
    "    for table_entry in dataset_1_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])\n",
    "    for table_entry in dataset_2_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])  \n",
    "    \n",
    "    \n",
    "    # Loop through tables, pull xxx records (by sample size), ordering by first column\n",
    "    # Drop fields_to_ignore\n",
    "    # Compare --> How to best do this\n",
    "    pass\n",
    "    \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of dataset pairs to compare\n",
    "dataset_id_pairs_list = [\n",
    "    #[\"gcp_dataset_id\", \"az_dataset_id\"]\n",
    "    ['dd2cb8fc-42a6-482f-898e-ef6125feccb8', '245020b9-7355-4002-95db-12e7234070c5'],\n",
    "    ['92382848-f5e9-426c-b7dc-f2841ae97018', '8a90137a-7aed-4e8c-bd99-1399f1c550fd'],\n",
    "    ['1f534eb4-701f-4182-9895-64c5e5b52d82', '1bf4d70f-db98-4d07-b48f-d177efd25ae4'],\n",
    "    ['e68d1d39-99df-4cd7-8053-1b298f03eabb', 'cfcb0f71-1157-4dbb-a76b-926c0cd40ea2'],\n",
    "    ['2ef4530a-cc36-4f32-9a1a-63a555346587', 'd28cb1d4-2300-4cd7-882b-99ce59305ce0'],\n",
    "    ['65793118-3c88-4185-9172-2354850e6056', '183ec762-f867-46c5-bb19-8b2b3417f7b2'],\n",
    "    ['36bdd59f-4f5b-43cd-8d34-a21ef87bbf30', '933d1603-8c61-4ff2-8489-7f774ac15e97'],\n",
    "    ['b60b4737-c646-4299-85a0-520890e830b7', '757191b0-9db3-4d18-b4ad-97bead5f3221'],\n",
    "    ['75fb0984-2124-444f-881b-30a1a6f8b8f7', '7a9eee5d-95c2-4947-93a8-e31d53a2a09a'],\n",
    "]\n",
    "\n",
    "# Specify whether row comparison checks should run\n",
    "run_row_count_comparison = True\n",
    "\n",
    "# Specify whether table content checks should run, the size of the sample to use (if so), and which fields should be excluded from comparison\n",
    "run_contents_sample_comparison = False\n",
    "contents_sample_comparison_size = 1000\n",
    "fields_to_ignore = [\"datarepo_row_id\", \"orig_datarepo_row_id\", \"orig_file_ref\", \"source_datarepo_row_ids\", \"uri\"]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Run validation\n",
    "results = []\n",
    "for dataset_id_pair in dataset_id_pairs_list:\n",
    "    if run_row_count_comparison:\n",
    "        status, failed_tables = compare_row_counts(dataset_id_pair[0], dataset_id_pair[1])\n",
    "        results.append([dataset_id_pair[0], dataset_id_pair[1], \"Record Count Comparison\", status, ', '.join(failed_tables)])\n",
    "\n",
    "# Display final results\n",
    "print(\"\\nFinal Validation Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Dataset 1 ID\", \"Dataset 2 ID\", \"Validation Type\", \"Status\", \"Failed Tables\"])\n",
    "display(results_df)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_1_id = \"b12fb9be-2ce0-4bfd-8503-732fabba06ab\"\n",
    "dataset_2_id = \"744c85cc-13d2-4f90-9d2e-d3143cb01edb\"\n",
    "contents_sample_comparison_size = 1000\n",
    "fields_to_ignore = [\"datarepo_row_id\", \"orig_datarepo_row_id\", \"orig_file_ref\", \"source_datarepo_row_ids\", \"uri\"]\n",
    "\n",
    "# Setup/refresh TDR clients\n",
    "logging.info(f\"Comparing a sample of tabular data content between TDR dataset {dataset_1_id} and TDR dataset {dataset_2_id}.\")\n",
    "api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# Pull table list across datasets\n",
    "logging.info(f\"Pulling the superset of tables across the two datasets.\")\n",
    "try:\n",
    "    dataset_1_details = datasets_api.retrieve_dataset(id=dataset_1_id, include=[\"SCHEMA\"]).to_dict()\n",
    "    dataset_2_details = datasets_api.retrieve_dataset(id=dataset_2_id, include=[\"SCHEMA\"]).to_dict()\n",
    "except Exception as e:\n",
    "    error_str = f\"Error retrieving details from datasets: {str(e)}\"\n",
    "    logging.error(error_str)\n",
    "table_set = {}\n",
    "for table_entry in dataset_1_details[\"schema\"][\"tables\"]:\n",
    "    table_set[table_entry[\"name\"]] = table_entry[\"columns\"][0][\"name\"]\n",
    "for table_entry in dataset_2_details[\"schema\"][\"tables\"]:\n",
    "    table_set[table_entry[\"name\"]] = table_entry[\"columns\"][0][\"name\"]\n",
    "    \n",
    "# For each table in the table list, pull sample records from the two datasets and compare\n",
    "results = []\n",
    "for table in [\"file_inventory\"]: #table_set.keys():\n",
    "    logging.info(f\"Comparing sample records for table '{table}'\")\n",
    "    # Pulling sample records for dataset 1\n",
    "    ds1_table_present = \"True\"\n",
    "    max_page_size = 1000\n",
    "    total_records_fetched = 0\n",
    "    ds1_final_records = []\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        offset = total_records_fetched\n",
    "        page_size = min(max_page_size, contents_sample_comparison_size - total_records_fetched)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": offset,\n",
    "              \"limit\": page_size,\n",
    "              \"sort\": table_set[table],\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_1_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    record_results = []\n",
    "                    ds1_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        record_results = []\n",
    "                        ds1_table_present = \"Unknown\"\n",
    "                        break\n",
    "        if record_results[\"result\"]:\n",
    "            ds1_final_records.extend(record_results[\"result\"])\n",
    "            total_records_fetched += len(record_results[\"result\"])\n",
    "        else:\n",
    "            break\n",
    "        if total_records_fetched >= contents_sample_comparison_size:\n",
    "            break\n",
    "    # Pulling sample records for dataset 2\n",
    "    ds2_table_present = \"True\"\n",
    "    max_page_size = 1000\n",
    "    total_records_fetched = 0\n",
    "    ds2_final_records = []\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        offset = total_records_fetched\n",
    "        page_size = min(max_page_size, contents_sample_comparison_size - total_records_fetched)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": offset,\n",
    "              \"limit\": page_size,\n",
    "              \"sort\": table_set[table],\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_2_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    record_results = []\n",
    "                    ds2_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        record_results = []\n",
    "                        ds2_table_present = \"Unknown\"\n",
    "                        break\n",
    "        if record_results[\"result\"]:\n",
    "            ds2_final_records.extend(record_results[\"result\"])\n",
    "            total_records_fetched += len(record_results[\"result\"])\n",
    "        else:\n",
    "            break\n",
    "        if total_records_fetched >= contents_sample_comparison_size:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_ds1_records_int = pd.DataFrame.from_dict(ds1_final_records)\n",
    "df_ds2_records_int = pd.DataFrame.from_dict(ds2_final_records)\n",
    "cols = df_ds1_records_int.columns.tolist()\n",
    "for field in fields_to_ignore:\n",
    "    if field in cols:\n",
    "        cols.remove(field)\n",
    "df_ds1_records = df_ds1_records_int[cols]\n",
    "df_ds2_records = df_ds2_records_int[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "diff = df_ds1_records.compare(df_ds2_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if df_ds1_records.equals(df_ds2_records):\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Pull and Compare File Counts and Sizes between TDR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def collect_file_stats(dataset_id_pairs_list):\n",
    "    \n",
    "    results = []\n",
    "    for dataset_id_pair in dataset_id_pairs_list:\n",
    "\n",
    "            # Setup/refresh TDR clients\n",
    "            logging.info(f\"Processing dataset_id_pair: {dataset_id_pair}\")\n",
    "            api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "            # Initialize variables\n",
    "            dataset_id_1 = dataset_id_pair[0]\n",
    "            file_count_1 = 0\n",
    "            total_file_size_1 = 0\n",
    "            max_file_size_1 = 0\n",
    "            status_1 = \"Success\"\n",
    "            message_1 = \"\"\n",
    "            dataset_id_2 = dataset_id_pair[1]\n",
    "            file_count_2 = 0\n",
    "            total_file_size_2 = 0\n",
    "            max_file_size_2 = 0\n",
    "            status_2 = \"Success\"\n",
    "            message_2 = \"\"\n",
    "            validation_status = \"Passed\"\n",
    "            validation_message = \"\"\n",
    "\n",
    "            # For dataset_id_1, loop through dataset files and record information\n",
    "            logging.info(f\"Retrieving files from dataset_id {dataset_id_1}...\")\n",
    "            try:\n",
    "                max_page_size = 1000\n",
    "                total_records_fetched = 0\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        row_start = total_records_fetched\n",
    "                        dataset_file_results = datasets_api.list_files(id=dataset_id_1, offset=row_start, limit=max_page_size)\n",
    "                        if dataset_file_results:\n",
    "                            total_records_fetched += len(dataset_file_results)\n",
    "                            for entry in dataset_file_results:\n",
    "                                file_count_1 += 1\n",
    "                                total_file_size_1 += entry.size\n",
    "                                if entry.size > max_file_size_1:\n",
    "                                    max_file_size_1 = entry.size\n",
    "                            logging.info(f\"{total_records_fetched} records fetched...\")\n",
    "                            attempt_counter = 0\n",
    "                        else:\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        attempt_counter += 1\n",
    "                        if attempt_counter <= 10:\n",
    "                            logging.info(f\"Failure in file retrieval (attempt #{attempt_counter}). Trying again...\")\n",
    "                            continue\n",
    "                        else:\n",
    "                            status_1 = \"Failure\"\n",
    "                            message_1 = str(e)\n",
    "                            logging.error(f\"Failure in file retrieval: {message_1}\")\n",
    "                            break\n",
    "                if status_1 == \"Success\":\n",
    "                    logging.info(f\"File retrieval complete!\")\n",
    "            except Exception as e:\n",
    "                status_1 = \"Failure\"\n",
    "                message_1 = str(e)\n",
    "                logging.error(f\"Failure in file retrieval: {message_1}\")\n",
    "            \n",
    "            # For dataset_id_2, loop through dataset files and record information\n",
    "            logging.info(f\"Retrieving files from dataset_id {dataset_id_2}...\")\n",
    "            try:\n",
    "                max_page_size = 1000\n",
    "                total_records_fetched = 0\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        row_start = total_records_fetched\n",
    "                        dataset_file_results = datasets_api.list_files(id=dataset_id_2, offset=row_start, limit=max_page_size)\n",
    "                        if dataset_file_results:\n",
    "                            total_records_fetched += len(dataset_file_results)\n",
    "                            for entry in dataset_file_results:\n",
    "                                file_count_2 += 1\n",
    "                                total_file_size_2 += entry.size\n",
    "                                if entry.size > max_file_size_2:\n",
    "                                    max_file_size_2 = entry.size\n",
    "                            logging.info(f\"{total_records_fetched} records fetched...\")\n",
    "                            attempt_counter = 0\n",
    "                        else:\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        attempt_counter += 1\n",
    "                        if attempt_counter <= 10:\n",
    "                            logging.info(f\"Failure in file retrieval (attempt #{attempt_counter}). Trying again...\")\n",
    "                            continue\n",
    "                        else:\n",
    "                            status_2 = \"Failure\"\n",
    "                            message_2 = str(e)\n",
    "                            logging.error(f\"Failure in file retrieval: {message_2}\")\n",
    "                            break\n",
    "                if status_2 == \"Success\":\n",
    "                    logging.info(f\"File retrieval complete!\")\n",
    "            except Exception as e:\n",
    "                status_2 = \"Failure\"\n",
    "                message_2 = str(e)\n",
    "                logging.error(f\"Failure in file retrieval: {message_2}\")\n",
    "                \n",
    "            # Record and display interim results\n",
    "            file_count_diff = file_count_1 - file_count_2\n",
    "            total_file_size_diff = total_file_size_1 - total_file_size_2\n",
    "            max_file_size_diff = max_file_size_1 - max_file_size_2\n",
    "            if status_1 == \"Failure\" or status_2 == \"Failure\":\n",
    "                validation_status = \"Failed\"\n",
    "                validation_message = \"Errors pulling counts for one or more datasets.\"\n",
    "            elif file_count_diff > 0 or total_file_size_diff > 0 or max_file_size_diff > 0:\n",
    "                validation_status = \"Failed\"\n",
    "                validation_message = \"Difference in counts between datasets.\"\n",
    "            results.append([dataset_id_1, dataset_id_2, validation_status, validation_message, file_count_diff, total_file_size_diff, max_file_size_diff, file_count_1, total_file_size_1, max_file_size_1, status_1, message_1, file_count_2, total_file_size_2, max_file_size_2, status_2, message_2])\n",
    "            int_results_df = pd.DataFrame([[dataset_id_1, dataset_id_2, validation_status, validation_message, file_count_diff, total_file_size_diff, max_file_size_diff, file_count_1, total_file_size_1, max_file_size_1, status_1, message_1, file_count_2, total_file_size_2, max_file_size_2, status_2, message_2]], columns = [\"Dataset ID 1\", \"Dataset ID 2\", \"Validation Status\", \"Validation Message\", \"File Count Diff\", \"Total File Size (Bytes) Diff\", \"Max File Size (Bytes) Diff\", \"File Count 1\", \"Total File Size (Bytes) 1\", \"Max File Size (Bytes) 1\", \"Status 1 \", \"Message 1\", \"File Count 2\", \"Total File Size (Bytes) 2\", \"Max File Size (Bytes) 2\", \"Status 2 \", \"Message 2\"])\n",
    "            logging.info(\"Results recorded:\")\n",
    "            display(int_results_df)\n",
    "        \n",
    "    # Display final results\n",
    "    logging.info(\"Aggregating results...\")\n",
    "    ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "    destination_dir = \"ingest_pipeline/resources/azure_migration\"\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    output_file = f\"validation_results_{current_datetime_string}.tsv\"\n",
    "    results_df = pd.DataFrame(results, columns = [\"Dataset ID 1\", \"Dataset ID 2\", \"Validation Status\", \"Validation Message\", \"File Count Diff\", \"Total File Size (Bytes) Diff\", \"Max File Size (Bytes) Diff\", \"File Count 1\", \"Total File Size (Bytes) 1\", \"Max File Size (Bytes) 1\", \"Status 1 \", \"Message 1\", \"File Count 2\", \"Total File Size (Bytes) 2\", \"Max File Size (Bytes) 2\", \"Status 2 \", \"Message 2\"])\n",
    "    results_df.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "    !gsutil cp $output_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "    !rm $output_file\n",
    "    print(\"\\nAggregated Validation Results:\")\n",
    "    display(results_df)   \n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of dataset IDs\n",
    "dataset_id_pairs_list = [\n",
    "#    ['bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8', 'c7206e9a-78ad-4c9d-927f-3ca76646227d'],\n",
    "#    ['902596ce-714e-49b3-8271-f3dfece52309', 'e091028e-a6b1-4989-9477-498e7ea206f0'],\n",
    "    ['bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8', 'c7206e9a-78ad-4c9d-927f-3ca76646227d'],\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "collect_file_stats(dataset_id_pairs_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Migrating Workspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Pre-Connector Processing\n",
    "For each GCP Workspace - Azure Workspace pair:\n",
    "1. Build a manifest of files to be copied from the GCP Workspace to the Azure Workspace. \n",
    "2. Write the manifest to BigQuery for consumption by downstream processes.\n",
    "\n",
    "Pre-run steps:\n",
    "1. Use the anvil_ingest_tools notebook to create the Azure workspaces. \n",
    "2. Use the anvil_ingest_tools notebook to add the TDR general SA (datarepo-jade-api@terra-datarepo-production.iam.gserviceaccount.com) as a reader on the source GCP workspaces and a writer on the target Azure workspaces.\n",
    "\n",
    "Post-run steps:\n",
    "1. Use the anvil_ingest_tools notebook to remove the TDR general SA from the GCP and Azure workspaces. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to create file transfer details\n",
    "def output_file_details(source_ws_project, source_ws_name, target_ws_project, target_ws_name, file_bigquery_table, target_bigquery_table, delete_existing_records):\n",
    "    \n",
    "    # Establish credentials and clients\n",
    "    client = bigquery.Client()\n",
    "    creds, project = google.auth.default(scopes=['https://www.googleapis.com/auth/cloud-platform', 'openid', 'email', 'profile'])\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "\n",
    "    # Pull bucket from source workspace\n",
    "    try:\n",
    "        ws_attributes = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{source_ws_project}/{source_ws_name}?fields=workspace.bucketName\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        ws_bucket = ws_attributes[\"workspace\"][\"bucketName\"]\n",
    "    except:\n",
    "        err_str = \"Error retrieving workspace attributes for source workspace.\"\n",
    "        logging.error(err_str)\n",
    "        raise Exception(err_str)\n",
    "\n",
    "    # Pull storage container from target workspace\n",
    "    try:\n",
    "        ws_attributes = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{target_ws_project}/{target_ws_name}?fields=workspace.workspaceId\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        ws_id = ws_attributes[\"workspace\"][\"workspaceId\"] \n",
    "        ws_resources = requests.get(\n",
    "            url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{ws_id}/resources?offset=0&limit=10&resource=AZURE_STORAGE_CONTAINER\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        resource_id = \"\"\n",
    "        for resource_entry in ws_resources[\"resources\"]:\n",
    "            if resource_entry[\"resourceAttributes\"][\"azureStorageContainer\"][\"storageContainerName\"][0:3] == \"sc-\":\n",
    "                resource_id = resource_entry[\"metadata\"][\"resourceId\"]\n",
    "                break\n",
    "        if resource_id:\n",
    "            sas_response = requests.post(\n",
    "                url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{ws_id}/resources/controlled/azure/storageContainer/{resource_id}/getSasToken?sasExpirationDuration=86400\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\", \"accept\": \"application/json\"}\n",
    "            ).json()\n",
    "            base_url = sas_response[\"url\"]\n",
    "            ws_storage_container = re.search(\"^[a-z0-9:\\/=\\-\\.]+\", base_url, re.IGNORECASE).group(0)\n",
    "        else:\n",
    "            err_str = \"Error retrieving resource information for target workspace.\"\n",
    "            logging.error(err_str)\n",
    "            raise Exception(err_str)\n",
    "    except:\n",
    "        err_str = \"Error retrieving workspace attributes for target workspace.\"\n",
    "        logging.error(err_str)\n",
    "        raise Exception(err_str)\n",
    "\n",
    "    # Clear records from target BQ table (if specified)\n",
    "    if delete_existing_records:\n",
    "        logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "        delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE gcp_ws_project = '{source_ws_project}' and gcp_ws_name = '{source_ws_name}'\"\"\"\n",
    "        try:\n",
    "            delete_query_job = client.query(delete_query)\n",
    "            delete_query_job.result()\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Error deleting records for the original dataset from the target BQ table.\") \n",
    "\n",
    "    # Write the query to pull files into a dataframe\n",
    "    logging.info(f\"Building manifest of files to copy from the source '{source_ws_project}.{source_ws_name}' workspace to the target '{target_ws_project}.{target_ws_name}' workspace.\")\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = \"WRITE_APPEND\"\n",
    "    query = f\"\"\"SELECT '{source_ws_project}' AS gcp_ws_project, '{source_ws_name}' AS gcp_ws_name, \n",
    "                '{target_ws_project}' AS az_ws_project, '{target_ws_name}' AS az_ws_name, \n",
    "                 'gs://{ws_bucket}/'||name AS source_path, '{ws_storage_container}/'||name AS target_path, \n",
    "                 size AS size_in_bytes, md5Hash AS md5_hash, '{current_datetime_string}' AS date_added\n",
    "                FROM `{file_bigquery_table}` \n",
    "                WHERE bucket = '{ws_bucket}'\n",
    "                AND name NOT LIKE '%/'\"\"\"\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            df = client.query(query).result().to_dataframe()\n",
    "            job = client.load_table_from_dataframe(df, target_bigquery_table, job_config=job_config)\n",
    "            logging.info(\"Records recorded successfully.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error building and writing file manifest: {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                raise Exception(err_str)\n",
    "\n",
    "            \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# General parameters\n",
    "file_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_inventory.object_metadata_26_02_2024__17_14_55\"\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list_workspaces\"\n",
    "\n",
    "# Specify migration pairs: Source GCP Workspace - Target Azure Workspace\n",
    "migration_list = [\n",
    "    #{\"gcp_ws_project\": \"anvil-datastorage\", \"gcp_ws_name\": \"<name>\", \"az_ws_project\": \"AnVILDataStorage_Azure\", \"az_ws_name\": \"<name>\"}\n",
    "    {'gcp_ws_project': 'anvil-datastorage', 'gcp_ws_name': 'AnVIL_CCDG_WGS_HAIL_Phased-data', 'az_ws_project': 'AnVILDataStorage_Azure', 'az_ws_name': 'AnVIL_CCDG_WGS_HAIL_Phased-data_Azure'},\n",
    "]\n",
    "\n",
    "# Specify whether existing records in the azure_migration_file_list_workspaces table should be deleted before running\n",
    "delete_existing_records = True\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for entry in migration_list:\n",
    "    logging.info(f\"Processing Migration List Entry: {str(entry)}\")\n",
    "    try:\n",
    "        output_file_details(entry[\"gcp_ws_project\"], entry[\"gcp_ws_name\"], entry[\"az_ws_project\"], entry[\"az_ws_name\"], file_bigquery_table, target_bigquery_table, delete_existing_records)\n",
    "        results.append([entry[\"gcp_ws_name\"], entry[\"az_ws_name\"], \"Success\", \"\"])\n",
    "    except Exception as e:\n",
    "        results.append([entry[\"gcp_ws_name\"], entry[\"az_ws_name\"], \"Failure\", str(e)])\n",
    "        \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Source Workspace Name\", \"Target Workspace Name\", \"Status\", \"Message\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Pull and Compare File Counts and Sizes between Workspace Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def collect_file_stats(storage_pairs_list):\n",
    "    \n",
    "    results = []\n",
    "    for storage_pair in storage_pairs_list:\n",
    "\n",
    "            # Initialize variables\n",
    "            logging.info(f\"Processing storage pair: {storage_pair}\")\n",
    "            gcs_storage_location = storage_pair[0]\n",
    "            gcs_file_count = 0\n",
    "            gcs_total_file_size = 0\n",
    "            gcs_max_file_size = 0\n",
    "            gcs_status = \"Success\"\n",
    "            gcs_message = \"\"\n",
    "            az_storage_location = storage_pair[1]\n",
    "            az_file_count = 0\n",
    "            az_total_file_size = 0\n",
    "            az_max_file_size = 0\n",
    "            az_status = \"Success\"\n",
    "            az_message = \"\"\n",
    "            validation_status = \"Passed\"\n",
    "            validation_message = \"\"\n",
    "\n",
    "            # For gcs_storage_location, loop through files and record information\n",
    "            logging.info(\"Pulling and parsing GCP bucket contents to create a list of existing files.\")\n",
    "            existing_gcs_files = []\n",
    "            try:\n",
    "                cmd = f\"gsutil ls -L '{gcs_storage_location}/**'\"\n",
    "                output = subprocess.check_output(cmd, shell=True, universal_newlines=True)\n",
    "                file_name = \"\"\n",
    "                file_size = \"\"\n",
    "                for line in output.split(\"\\n\"):\n",
    "                    if line[0:2] == \"gs\":\n",
    "                        if file_name and file_size and file_name[-1] != \"/\":\n",
    "                            existing_gcs_files.append([file_name, file_size])\n",
    "                        file_name = re.sub(\":$\", \"\", line)\n",
    "                    else:\n",
    "                        if \"Content-Length:\" in line:\n",
    "                            file_size = re.match(\"\\s*Content-Length:\\s*([0-9]+)\", line).group(1)\n",
    "                if file_name and file_size and file_name[-1] != \"/\":\n",
    "                    existing_gcs_files.append([file_name, file_size])\n",
    "                for entry in existing_gcs_files:\n",
    "                    gcs_file_count += 1\n",
    "                    entry_file_size = int(entry[1])\n",
    "                    gcs_total_file_size += entry_file_size\n",
    "                    if entry_file_size > gcs_max_file_size:\n",
    "                        gcs_max_file_size = entry_file_size\n",
    "            except Exception as e:\n",
    "                gcs_status = \"Failure\"\n",
    "                gcs_message = str(e)\n",
    "                logging.error(f\"Failure in file retrieval: {gcs_message}\")\n",
    "                \n",
    "            # For az_storage_location, loop through files and record information\n",
    "            logging.info(\"Pulling and parsing target Azure container contents to create a list of existing files.\")\n",
    "            cmd = f\"azcopy_linux_amd64_10.24.0/azcopy list '{az_storage_location}' --machine-readable\"\n",
    "            output = subprocess.check_output(cmd, shell=True, universal_newlines=True)\n",
    "            existing_az_files = []\n",
    "            try:\n",
    "                for line in output.split(\"\\n\"):\n",
    "                    if line:\n",
    "                        file_name = re.match(r\"^INFO: (.*);\", line).group(1)\n",
    "                        file_size = re.match(r\".*Content Length: ([0-9\\.]+).*\", line).group(1)\n",
    "                        existing_az_files.append([file_name, file_size])\n",
    "                for entry in existing_az_files:\n",
    "                    az_file_count += 1\n",
    "                    entry_file_size = int(entry[1])\n",
    "                    az_total_file_size += entry_file_size\n",
    "                    if entry_file_size > az_max_file_size:\n",
    "                        az_max_file_size = entry_file_size\n",
    "            except Exception as e:\n",
    "                az_status = \"Failure\"\n",
    "                az_message = str(e)\n",
    "                logging.error(f\"Failure in file retrieval: {az_message}\")\n",
    "                \n",
    "            # Record and display interim results\n",
    "            file_count_diff = gcs_file_count - az_file_count\n",
    "            total_file_size_diff = gcs_total_file_size - az_total_file_size\n",
    "            max_file_size_diff = gcs_max_file_size - az_max_file_size\n",
    "            if gcs_status == \"Failure\" or az_status == \"Failure\":\n",
    "                validation_status = \"Failed\"\n",
    "                validation_message = \"Errors pulling counts for one or more storage locations.\"\n",
    "            elif file_count_diff > 0 or total_file_size_diff > 0 or max_file_size_diff > 0:\n",
    "                validation_status = \"Failed\"\n",
    "                validation_message = \"Difference in counts between storage locations.\"\n",
    "            results.append([gcs_storage_location, az_storage_location, validation_status, validation_message, file_count_diff, total_file_size_diff, max_file_size_diff, gcs_file_count, gcs_total_file_size, gcs_max_file_size, gcs_status, gcs_message, az_file_count, az_total_file_size, az_max_file_size, az_status, az_message])\n",
    "            int_results_df = pd.DataFrame([[gcs_storage_location, az_storage_location, validation_status, validation_message, file_count_diff, total_file_size_diff, max_file_size_diff, gcs_file_count, gcs_total_file_size, gcs_max_file_size, gcs_status, gcs_message, az_file_count, az_total_file_size, az_max_file_size, az_status, az_message]], columns = [\"GCS Storage Location\", \"AZ Storage Location\", \"Validation Status\", \"Validation Message\", \"File Count Diff\", \"Total File Size (Bytes) Diff\", \"Max File Size (Bytes) Diff\", \"GCS File Count\", \"GCS Total File Size (Bytes)\", \"GCS Max File Size (Bytes)\", \"GCS Status\", \"GCS Message\", \"AZ File Count\", \"AZ Total File Size (Bytes)\", \"AZ Max File Size (Bytes)\", \"AZ Status\", \"AZ Message\"])\n",
    "            logging.info(\"Results recorded:\")\n",
    "            display(int_results_df)\n",
    "        \n",
    "    # Display final results\n",
    "    logging.info(\"Aggregating results...\")\n",
    "    ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "    destination_dir = \"ingest_pipeline/resources/azure_migration\"\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    output_file = f\"workspace_validation_results_{current_datetime_string}.tsv\"\n",
    "    results_df = pd.DataFrame(results, columns = [\"GCS Storage Location\", \"AZ Storage Location\", \"Validation Status\", \"Validation Message\", \"File Count Diff\", \"Total File Size (Bytes) Diff\", \"Max File Size (Bytes) Diff\", \"GCS File Count\", \"GCS Total File Size (Bytes)\", \"GCS Max File Size (Bytes)\", \"GCS Status\", \"GCS Message\", \"AZ File Count\", \"AZ Total File Size (Bytes)\", \"AZ Max File Size (Bytes)\", \"AZ Status\", \"AZ Message\"])\n",
    "    results_df.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "    !gsutil cp $output_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "    !rm $output_file\n",
    "    print(\"\\nAggregated Validation Results:\")\n",
    "    display(results_df)   \n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of dataset IDs\n",
    "storage_pairs_list = [\n",
    "#     [\"gcs_bucket_path\", \"azure_storage_container_sas_url\"]\n",
    "    ['gs://fc-secure-0932b76c-22e6-4321-94f7-9726ad4aeb76', 'https://lzb34bb58bfb122730765416.blob.core.windows.net/sc-0ef0b0b4-92b6-462e-8b4f-498f1cb7983b?sv=2023-11-03&spr=https&st=2024-04-23T13%3A45%3A11Z&se=2024-04-23T22%3A00%3A11Z&sr=c&sp=racwdlt&sig=7Usayb1DzV4LEcQYheVZrSrvEkiaiot9wEZGmnEO3BM%3D&rscd=2661442731880e5cbc2c9'],\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "collect_file_stats(storage_pairs_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Dataset Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     12,
     23
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to delete a specific TDR Snapshot\n",
    "def delete_snapshot(snapshot_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete snapshot = {}\".format(snapshot_id))\n",
    "    try:\n",
    "        delete_snapshot_result, job_id = wait_for_tdr_job(snapshots_api.delete_snapshot(id=snapshot_id), \"https://data.terra.bio\")\n",
    "        print(\"Result: {}\".format(delete_snapshot_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset\n",
    "def delete_dataset(dataset_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {}\".format(dataset_id))\n",
    "    try:\n",
    "        delete_dataset_result, job_id = wait_for_tdr_job(datasets_api.delete_dataset(id=dataset_id), \"https://data.terra.bio\")\n",
    "        print(\"Result: {}\".format(delete_dataset_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset and all of its Snapshots\n",
    "def delete_dataset_and_all_snapshots(dataset_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {} and all associated snapshots\".format(dataset_id))\n",
    "    dataset_id_list = [dataset_id]\n",
    "    # Delete snapshots\n",
    "    snapshot_list = snapshots_api.enumerate_snapshots(dataset_ids=dataset_id_list)\n",
    "    if snapshot_list.items:\n",
    "        for snapshot in snapshot_list.items:\n",
    "            snapshot_id = str(snapshot.id)\n",
    "            delete_snapshot(snapshot_id)\n",
    "            sleep(10)\n",
    "    # Delete dataset\n",
    "    delete_dataset(dataset_id)\n",
    "\n",
    "# Delete snapshots\n",
    "# snapshot_id_list = [\n",
    "# '1234',\n",
    "# ]\n",
    "# for snapshot_id in snapshot_id_list:\n",
    "#     delete_snapshot(snapshot_id)\n",
    "\n",
    "# Delete datasets and all their associated snapshots\n",
    "dataset_id_list = [\n",
    "'1be5b5e6-019e-419a-9248-6e80d067d697',\n",
    "]\n",
    "for dataset_id in dataset_id_list:\n",
    "    delete_dataset_and_all_snapshots(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Update Migration File List Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# General parameters\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list\"\n",
    "\n",
    "# Update parameters\n",
    "update_list = [\n",
    "    {\"az_dataset_id\": \"6007151f-45bc-4111-8e9a-b667bc722a6a\", \"new_gcp_dataset_id\": \"b22c71b2-2cb2-4b27-a49b-9a2a83d432e8\", \"new_gcp_dataset_name\": \"ANVIL_1000G_PRIMED_data_model_20240301\"},\n",
    "    {\"az_dataset_id\": \"a28e4ab5-a07b-4316-b743-7f5f9cc88211\", \"new_gcp_dataset_id\": \"3a89c170-2939-4c12-9940-f32d96fa9e55\", \"new_gcp_dataset_name\": \"ANVIL_CMH_GAFK_GS_long_read_20240301\"}\n",
    "]\n",
    "\n",
    "# Execute updates\n",
    "client = bigquery.Client()\n",
    "for entry in update_list:\n",
    "    logging.info(f\"Running update for entry: {str(entry)}\")\n",
    "    az_dataset_id = entry[\"az_dataset_id\"]\n",
    "    gcp_dataset_id = entry[\"new_gcp_dataset_id\"]\n",
    "    gcp_dataset_name = entry[\"new_gcp_dataset_name\"]\n",
    "    update_query = f\"\"\"UPDATE `{target_bigquery_table}` \n",
    "                       SET gcp_dataset_id = '{gcp_dataset_id}', gcp_dataset_name = '{gcp_dataset_name}'\n",
    "                       WHERE az_dataset_id = '{az_dataset_id}'\"\"\"\n",
    "    try:\n",
    "        update_query_job = client.query(update_query)\n",
    "        update_query_job.result()\n",
    "        logging.info(\"Update complete.\")\n",
    "    except Exception as e:\n",
    "        logging.info(\"Error running update.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
