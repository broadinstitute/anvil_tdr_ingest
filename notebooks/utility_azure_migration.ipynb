{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     30,
     42
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import import_ipynb\n",
    "import data_repo_client\n",
    "import google.auth\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from time import sleep\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import ingest_pipeline_utilities as utils\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "\n",
    "# Configure logging format\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Function to refresh TDR API client\n",
    "def refresh_tdr_api_client(host):\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    config = data_repo_client.Configuration()\n",
    "    config.host = host\n",
    "    config.access_token = creds.token\n",
    "    api_client = data_repo_client.ApiClient(configuration=config)\n",
    "    api_client.client_side_validation = False\n",
    "    return api_client\n",
    "\n",
    "# Function to wait for TDR job completion\n",
    "def wait_for_tdr_job(job_model, host):\n",
    "    result = job_model\n",
    "    print(\"TDR Job ID: \" + job_model.id)\n",
    "    counter = 0\n",
    "    job_state = \"UNKNOWN\"\n",
    "    while True:\n",
    "        # Re-establish credentials and API clients every 30 minutes\n",
    "        if counter == 0 or counter%180 == 0:\n",
    "            api_client = refresh_tdr_api_client(host)\n",
    "            jobs_api = data_repo_client.JobsApi(api_client=api_client)\n",
    "        # Check for TDR connectivity issues and raise exception if the issue persists\n",
    "        conn_err_counter = 0\n",
    "        while job_state == \"UNKNOWN\":\n",
    "            conn_err_counter += 1\n",
    "            if conn_err_counter >= 10:\n",
    "                raise Exception(\"Error interacting with TDR: {}\".format(result.status_code)) \n",
    "            elif result == None or result.status_code in [\"500\", \"502\", \"503\", \"504\"]:\n",
    "                sleep(10)\n",
    "                counter += 1\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        result = jobs_api.retrieve_job(job_model.id)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 5:\n",
    "                            attempt_counter += 1\n",
    "                            sleep(10)\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "            else:\n",
    "                job_state = \"KNOWN\"\n",
    "        # Check if job is still running, and sleep/re-check if so\n",
    "        if job_state == \"KNOWN\" and result.job_status == \"running\":\n",
    "            sleep(10)\n",
    "            counter += 1\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    result = jobs_api.retrieve_job(job_model.id)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 5:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "        # If job has returned as failed, confirm this is the correct state and retrieve result if so\n",
    "        elif job_state == \"KNOWN\" and result.job_status == \"failed\":\n",
    "            fail_counter = 0\n",
    "            while True:\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        result = jobs_api.retrieve_job(job_model.id)\n",
    "                        if result.job_status == \"failed\":\n",
    "                            fail_counter += 1\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 5:\n",
    "                            sleep(10)\n",
    "                            attempt_counter += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "                if fail_counter >= 3:\n",
    "                    try:\n",
    "                        fail_result = jobs_api.retrieve_job_result(job_model.id)\n",
    "                        raise Exception(\"Job \" + job_model.id + \" failed: \" + fail_result)\n",
    "                    except Exception as e:\n",
    "                        raise Exception(\"Job \" + job_model.id + \" failed: \" + str(e))\n",
    "        # If a job has returned as succeeded, retrieve result\n",
    "        elif job_state == \"KNOWN\" and result.job_status == \"succeeded\":\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return jobs_api.retrieve_job_result(job_model.id), job_model.id\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 3:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        return \"Job succeeded, but error retrieving job result: {}\".format(str(e)), job_model.id\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized job state: {}\".format(result.job_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrating TDR Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pre-Connector Processing\n",
    "For the list of GCP TDR datasets provided:\n",
    "1. Extract the schema\n",
    "2. Create an Azure TDR dataset using the extracted schema\n",
    "3. Build a manifest of files to be copied from the GCP dataset to the Azure dataset and write to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     5,
     14
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/12/2024 08:07:35 PM - INFO: Processing Migration List Entry: ['ec6f49a2-176c-4564-82c5-e751baab46aa', 'fcf41f7a-9de2-4105-af33-48abe616e386', 'Y']\n",
      "04/12/2024 08:07:35 PM - INFO: Retrieving original dataset details from prod environment. UUID:  ec6f49a2-176c-4564-82c5-e751baab46aa\n",
      "04/12/2024 08:07:36 PM - INFO: Retrieving new dataset details from prod environment. UUID:  fcf41f7a-9de2-4105-af33-48abe616e386\n",
      "04/12/2024 08:07:37 PM - INFO: Fetching and recording all rows from table 'file_inventory' in the original dataset (ec6f49a2-176c-4564-82c5-e751baab46aa). BQ Project = 'datarepo-bf821d10' and BQ Dataset = 'datarepo_ANVIL_HPRC_20240401'.\n",
      "04/12/2024 08:07:42 PM - INFO: Records recorded successfully.\n",
      "\n",
      "Final Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Source Dataset ID</th>\n",
       "      <th>Source Dataset Name</th>\n",
       "      <th>Status</th>\n",
       "      <th>New Dataset ID</th>\n",
       "      <th>New Dataset Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ec6f49a2-176c-4564-82c5-e751baab46aa</td>\n",
       "      <td>ANVIL_HPRC_20240401</td>\n",
       "      <td>Success</td>\n",
       "      <td>fcf41f7a-9de2-4105-af33-48abe616e386</td>\n",
       "      <td>ANVIL_HPRC_20240408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Source Dataset ID           Source Dataset Name   Status              New Dataset ID              New Dataset Name  \n",
       "0  ec6f49a2-176c-4564-82c5-e751baab46aa  ANVIL_HPRC_20240401  Success  fcf41f7a-9de2-4105-af33-48abe616e386  ANVIL_HPRC_20240408"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to build default target TDR dataset name\n",
    "def format_dataset_name(input_str):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "    input_str = input_str[:-9]\n",
    "    output_str = \"ANVIL_\" + re.sub(\"^ANVIL[_]?\", \"\", input_str, flags=re.IGNORECASE) + \"_\" + current_date_string\n",
    "    output_str = re.sub(\"[^a-zA-Z0-9_]\", \"_\", output_str)\n",
    "    return output_str\n",
    "\n",
    "# Function to create a new TDR dataset from an existing TDR dataset\n",
    "def create_dataset_from_dataset(src_tdr_object_uuid, tar_tdr_object_uuid, billing_profile):\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Retrieve original dataset details\n",
    "    logging.info(f\"Retrieving original dataset details from prod environment. UUID:  {src_tdr_object_uuid}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=src_tdr_object_uuid, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        orig_object_name = dataset_details[\"name\"]\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset {src_tdr_object_uuid} in TDR prod environment: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # If target dataset specified, retrieve name\n",
    "    if tar_tdr_object_uuid:\n",
    "        new_dataset_id = tar_tdr_object_uuid\n",
    "        logging.info(f\"Retrieving new dataset details from prod environment. UUID:  {tar_tdr_object_uuid}\")\n",
    "        try:\n",
    "            dataset_details = datasets_api.retrieve_dataset(id=tar_tdr_object_uuid, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "            new_object_name = dataset_details[\"name\"]\n",
    "        except Exception as e:\n",
    "            error_str = f\"Error retrieving details from dataset {tar_tdr_object_uuid} in TDR prod environment: {str(e)}\"\n",
    "            logging.error(error_str)\n",
    "            return None, None, None, None, None \n",
    "    else:\n",
    "        # Build new dataset schema\n",
    "        apply_anvil_transforms = True\n",
    "        new_schema_dict = {\"tables\": [], \"relationships\": [], \"assets\": []}\n",
    "        for table_entry in dataset_details[\"schema\"][\"tables\"]:\n",
    "            int_table_dict = table_entry.copy()\n",
    "            int_table_dict[\"primaryKey\"] = int_table_dict.pop(\"primary_key\")\n",
    "            for key in [\"partition_mode\", \"date_partition_options\", \"int_partition_options\", \"row_count\"]:\n",
    "                del int_table_dict[key]\n",
    "            for idx, column_entry in enumerate(table_entry[\"columns\"]):\n",
    "                if column_entry[\"datatype\"] == \"integer\":\n",
    "                    table_entry[\"columns\"][idx][\"datatype\"] = \"int64\"\n",
    "            if apply_anvil_transforms:\n",
    "                if table_entry[\"name\"] == \"file_inventory\":\n",
    "                    int_table_dict[\"columns\"].append({\"name\": \"orig_file_ref\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "                    int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "                elif \"anvil_\" not in table_entry[\"name\"]:\n",
    "                    int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "            new_schema_dict[\"tables\"].append(int_table_dict)\n",
    "        for rel_entry in dataset_details[\"schema\"][\"relationships\"]:\n",
    "            int_rel_dict = rel_entry.copy()\n",
    "            int_rel_dict[\"from\"] = int_rel_dict.pop(\"_from\")\n",
    "            new_schema_dict[\"relationships\"].append(int_rel_dict)\n",
    "        for asset_entry in dataset_details[\"schema\"][\"assets\"]:\n",
    "            int_asset_dict = asset_entry.copy()\n",
    "            int_asset_dict[\"rootTable\"] = int_asset_dict.pop(\"root_table\")\n",
    "            int_asset_dict[\"rootColumn\"] = int_asset_dict.pop(\"root_column\")\n",
    "            new_schema_dict[\"assets\"].append(int_asset_dict)\n",
    "\n",
    "        # Retrieve original dataset policies\n",
    "        try:\n",
    "            dataset_policies = datasets_api.retrieve_dataset_policies(id=src_tdr_object_uuid).to_dict()\n",
    "            for policy in dataset_policies[\"policies\"]:\n",
    "                if policy[\"name\"] == \"steward\":\n",
    "                    stewards_list = policy[\"members\"]\n",
    "                elif policy[\"name\"] == \"custodian\":\n",
    "                    custodians_list = policy[\"members\"]\n",
    "                elif policy[\"name\"] == \"snapshot_creator\":\n",
    "                    snapshot_creators_list = policy[\"members\"]\n",
    "        except:\n",
    "            logging.info(\"Error retrieving original dataset policies. Skipping policy copy.\")\n",
    "            stewards_list = []\n",
    "            custodians_list = []\n",
    "            snapshot_creators_list = []\n",
    "        policies = {\n",
    "            \"stewards\": stewards_list,\n",
    "            \"custodians\": custodians_list,\n",
    "            \"snapshotCreators\": snapshot_creators_list\n",
    "        }\n",
    "\n",
    "        # Determine dataset properties\n",
    "        new_object_name = format_dataset_name(orig_object_name)\n",
    "        new_description = dataset_details[\"description\"] + f\"\\n\\nCopy of dataset {orig_object_name} from TDR prod.\"\n",
    "        self_hosted = False\n",
    "        dedicated_ingest_sa = False\n",
    "        phs_id = dataset_details[\"phs_id\"]\n",
    "        predictable_file_ids = dataset_details[\"predictable_file_ids\"]\n",
    "        secure_monitoring_enabled = dataset_details[\"secure_monitoring_enabled\"]\n",
    "        properties = dataset_details[\"properties\"]\n",
    "        tags = dataset_details[\"tags\"]\n",
    "\n",
    "        # Create new TDR dataset\n",
    "        logging.info(\"Submitting dataset creation request.\")\n",
    "        dataset_request = {\n",
    "            \"name\": new_object_name,\n",
    "            \"description\": new_description,\n",
    "            \"defaultProfileId\": billing_profile,\n",
    "            \"cloudPlatform\": \"azure\",\n",
    "            \"region\": \"southcentralus\",\n",
    "            \"phsId\": phs_id,\n",
    "            \"experimentalSelfHosted\": self_hosted,\n",
    "            \"experimentalPredictableFileIds\": predictable_file_ids,\n",
    "            \"dedicatedIngestServiceAccount\": dedicated_ingest_sa,\n",
    "            \"enableSecureMonitoring\": secure_monitoring_enabled,\n",
    "            \"properties\": properties,\n",
    "            \"tags\": tags,\n",
    "            \"policies\": policies,\n",
    "            \"schema\": new_schema_dict\n",
    "        }\n",
    "        attempt_counter = 1\n",
    "        while True:\n",
    "            try:\n",
    "                create_dataset_result, job_id = wait_for_tdr_job(datasets_api.create_dataset(dataset=dataset_request), \"https://data.terra.bio\")\n",
    "                logging.info(\"Dataset Creation succeeded: {}\".format(create_dataset_result))\n",
    "                new_dataset_id = create_dataset_result[\"id\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = f\"Error on Dataset Creation: {str(e)}\"\n",
    "                logging.error(error_str)\n",
    "                if attempt_counter < 3:\n",
    "                    logging.info(\"Retrying Dataset Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Exiting job.\")\n",
    "                    return None, None, None, None, None\n",
    "        \n",
    "    # Exit function\n",
    "    return orig_object_name, new_dataset_id, new_object_name, bq_project, bq_dataset\n",
    "\n",
    "# Function to create file transfer details\n",
    "def output_file_details(orig_dataset_id, orig_dataset_name, new_dataset_id, new_dataset_name, bq_project, bq_dataset, public_flag, target_bigquery_table, delete_existing_records):\n",
    "    \n",
    "    # Setup/refresh TDR clients (and BQ client)\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Clear records from target BQ table\n",
    "    if delete_existing_records:\n",
    "        logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "        delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE gcp_dataset_id = '{orig_dataset_id}'\"\"\"\n",
    "        try:\n",
    "            delete_query_job = client.query(delete_query)\n",
    "            delete_query_job.result()\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error deleting records for the original dataset from the target BQ table.\")\n",
    "    \n",
    "    # Retrieve table data from the original dataset and write to target BQ table\n",
    "    logging.info(f\"Fetching and recording all rows from table 'file_inventory' in the original dataset ({orig_dataset_id}). BQ Project = '{bq_project}' and BQ Dataset = '{bq_dataset}'.\")\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.destination = target_bigquery_table\n",
    "    job_config.write_disposition = \"WRITE_APPEND\"\n",
    "    query = f\"\"\"WITH drlh_deduped AS\n",
    "                (\n",
    "                  SELECT DISTINCT file_id, target_path, source_name \n",
    "                  FROM \n",
    "                  (\n",
    "                    SELECT *, ROW_NUMBER() OVER (PARTITION BY source_name ORDER BY load_time DESC) AS rn\n",
    "                    --SELECT *, ROW_NUMBER() OVER (PARTITION BY source_name, target_path ORDER BY load_time DESC) AS rn\n",
    "                    FROM `{bq_project}.{bq_dataset}.datarepo_load_history`\n",
    "                    WHERE state = \"succeeded\" \n",
    "                  )\n",
    "                  WHERE rn = 1\n",
    "                ),\n",
    "                file_records AS\n",
    "                (\n",
    "                  SELECT '{orig_dataset_id}' AS gcp_dataset_id, '{orig_dataset_name}' AS gcp_dataset_name, \n",
    "                  '{new_dataset_id}' AS az_dataset_id, '{new_dataset_name}' AS az_dataset_name, \n",
    "                  b.source_name AS source_path, b.target_path, a.size_in_bytes, a.md5_hash, a.file_ref AS orig_tdr_file_id,\n",
    "                  '{current_datetime_string}' AS date_added, '{public_flag}' AS public_flag, ROW_NUMBER() OVER (PARTITION BY a.file_ref ORDER BY b.source_name) AS rn\n",
    "                  FROM `{bq_project}.{bq_dataset}.file_inventory` a\n",
    "                      LEFT JOIN drlh_deduped b\n",
    "                      ON a.uri = b.source_name\n",
    "                      LEFT JOIN `broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list` c\n",
    "                      ON a.file_ref = c.orig_tdr_file_id AND c.az_dataset_id = '{new_dataset_id}'\n",
    "                  WHERE c.source_path IS NULL\n",
    "                )\n",
    "                SELECT * EXCEPT(rn)\n",
    "                FROM file_records\n",
    "                WHERE rn = 1\"\"\"\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            query_job = client.query(query, job_config=job_config)\n",
    "            query_job.result()\n",
    "            logging.info(\"Records recorded successfully.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error recording records for all rows of table 'file_inventory': {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                return\n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# General parameters\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list\"\n",
    "azure_billing_profile = \"9ee23bed-b46c-4561-9103-d2a723113f7f\"\n",
    "\n",
    "# Specify the list of datasets to process, leaving the target Azure dataset ID empty to create a new one\n",
    "migration_list = [\n",
    "    #[\"src_gcp_dataset_id\", \"tar_az_dataset_id\", \"open_access (Y/N)\"]\n",
    "    ['ec6f49a2-176c-4564-82c5-e751baab46aa', 'fcf41f7a-9de2-4105-af33-48abe616e386', 'Y'],\n",
    "]\n",
    "\n",
    "# Specify whether existing records in the azure_migration_file_list table should be deleted before running\n",
    "delete_existing_records = False\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for entry in migration_list:\n",
    "    logging.info(f\"Processing Migration List Entry: {str(entry)}\")\n",
    "    dataset_name, new_dataset_id, new_dataset_name, bq_project, bq_dataset = create_dataset_from_dataset(entry[0], entry[1], azure_billing_profile)\n",
    "    if new_dataset_id:\n",
    "        output_file_details(entry[0], dataset_name, new_dataset_id, new_dataset_name, bq_project, bq_dataset, entry[2], target_bigquery_table, delete_existing_records)\n",
    "        results.append([entry[0], dataset_name, \"Success\", new_dataset_id, new_dataset_name])\n",
    "    else:\n",
    "        results.append([entry[0], dataset_name, \"Failure\", new_dataset_id, new_dataset_name])\n",
    "        \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Source Dataset ID\", \"Source Dataset Name\", \"Status\", \"New Dataset ID\", \"New Dataset Name\"])\n",
    "display(results_df)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 2: Post-Connector Processing\n",
    "For each GCP Dataset - Azure Dataset pair:\n",
    "1. Retrieve the source GCP Dataset for the Snapshot\n",
    "2. Extract, pre-process, and ingest tabular data from the GCP Dataset to the Azure Dataset\n",
    "3. Create a new Azure snapshot based on the GCP snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     117,
     237,
     504
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to fetch data from BigQuery\n",
    "def fetch_source_records_bigquery(config, new_dataset_id, array_col_dict, table, start_row, end_row):\n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source_dataset_id\"]\n",
    "    src_tdr_object_type = \"dataset\"\n",
    "    tdr_host = config[\"tdr_host\"]\n",
    "    files_already_ingested = True\n",
    "    datarepo_row_ids_to_ingest = []\n",
    "    apply_anvil_transforms = True\n",
    "    bq_project = config[\"bigquery_project\"]\n",
    "    bq_dataset = config[\"bigquery_dataset\"]\n",
    "    \n",
    "    # Setup/refresh TDR clients (and BQ client)\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Retrieve table data from the original dataset\n",
    "    logging.info(f\"Fetching rows {str(start_row)}-{str(end_row)} from table '{table}' in the original {src_tdr_object_type} ({src_tdr_object_uuid}).\")\n",
    "    table_recs_str = f\"Table: {table} -- Rows: {str(start_row)}-{str(end_row)}\"\n",
    "    final_records = []\n",
    "    if apply_anvil_transforms and \"anvil_\" not in table:\n",
    "        if table == \"file_inventory\":\n",
    "            if files_already_ingested == False:\n",
    "                file_ref_sql = \"TO_JSON_STRING(STRUCT(source_name AS sourcePath, target_path AS targetPath, 'Ingest of '||source_name AS description, COALESCE(content_type, 'application/octet-stream') AS mimeType))\"\n",
    "            else:\n",
    "                file_ref_sql = \"file_ref\"\n",
    "            rec_fetch_query = f\"\"\"WITH drlh_deduped AS\n",
    "                            (\n",
    "                              SELECT DISTINCT file_id, target_path, source_name\n",
    "                              FROM `{bq_project}.{bq_dataset}.datarepo_load_history`\n",
    "                              WHERE state = \"succeeded\" \n",
    "                            )\n",
    "                            SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT datarepo_row_id, datarepo_row_id AS orig_datarepo_row_id, a.file_id, name, path, target_path AS uri, content_type, full_extension, size_in_bytes, crc32c, md5_hash, ingest_provenance,\n",
    "                              file_ref AS orig_file_ref, {file_ref_sql} AS file_ref,\n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}` a\n",
    "                                  LEFT JOIN drlh_deduped b\n",
    "                                  ON a.file_ref = b.file_id\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "        else:\n",
    "            rec_fetch_query = f\"\"\"SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT *, datarepo_row_id AS orig_datarepo_row_id,\n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}`\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "    else:\n",
    "        rec_fetch_query = f\"\"\"SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT *, \n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}`\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            df = client.query(rec_fetch_query).result().to_dataframe()\n",
    "            df = df.astype(object).where(pd.notnull(df),None)\n",
    "            for column in array_col_dict[table]:\n",
    "                df[column] = df[column].apply(lambda x: list(x))\n",
    "            if apply_anvil_transforms and table == \"file_inventory\" and files_already_ingested == False: \n",
    "                df[\"file_ref\"] = df.apply(lambda x: json.loads(x[\"file_ref\"].replace(\"\\'\", \"\\\"\")), axis=1)\n",
    "            final_records = df.to_dict(orient=\"records\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error retrieving records for rows {str(start_row)}-{str(end_row)} of table {table}: {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "                return {}\n",
    "    \n",
    "    # Filter retrieved data if necessary and return as dict of records\n",
    "    if final_records:\n",
    "        df_temp = pd.DataFrame.from_dict(final_records)\n",
    "        if datarepo_row_ids_to_ingest:\n",
    "            df_orig = df_temp[df_temp[\"datarepo_row_id\"].isin(datarepo_row_ids_to_ingest)].copy()\n",
    "        else:\n",
    "            df_orig = df_temp.copy()\n",
    "        del df_temp\n",
    "        df_orig.drop(columns=[\"datarepo_row_id\"], inplace=True, errors=\"ignore\")\n",
    "        df_orig = df_orig.astype(object).where(pd.notnull(df_orig),None)\n",
    "        records_orig = df_orig.to_dict(orient=\"records\")\n",
    "        if not records_orig:\n",
    "            msg_str = f\"No records found in rows {str(start_row)}-{str(end_row)} of table {table} after filtering based on datarepo_row_ids_to_ingest parameter. Continuing to next record set or table validation.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Skipped\", msg_str])\n",
    "            return records_orig\n",
    "        elif len(final_records) != len(records_orig):\n",
    "            logging.info(f\"Filtering records to ingest based on the datarepo_row_ids_to_ingest parameter. {str(len(records_orig))} of {str(len(final_records))} records to be ingested.\")\n",
    "            return records_orig\n",
    "        else:\n",
    "            return records_orig\n",
    "    else:\n",
    "        msg_str = f\"No records found for rows {str(start_row)}-{str(end_row)} of table {table} in original {src_tdr_object_type}. Continuing to next record set or table validation.\"\n",
    "        logging.info(msg_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Skipped\", msg_str])\n",
    "        return final_records\n",
    "\n",
    "# Function to process ingests for specific table\n",
    "def ingest_table_data(config, new_dataset_id, fileref_col_dict, array_col_dict, table, start_row, end_row):\n",
    "    \n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source_dataset_id\"]\n",
    "    src_tdr_object_type = \"dataset\"\n",
    "    tdr_host = config[\"tdr_host\"]\n",
    "    tar_tdr_billing_profile = config[\"tar_tdr_billing_profile\"]\n",
    "    records_processing_method = \"in_memory\"\n",
    "    write_to_cloud_platform = \"\"\n",
    "    apply_anvil_transforms = True\n",
    "    dr_row_id_xwalk = config[\"dr_row_id_xwalk\"]\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    \n",
    "    # Retrieve table data from the original dataset\n",
    "    table_recs_str = f\"Table: {table} -- Rows: {str(start_row)}-{str(end_row)}\"\n",
    "    records_orig = fetch_source_records_bigquery(config, new_dataset_id, array_col_dict, table, start_row, end_row)\n",
    "    if not records_orig:\n",
    "        return\n",
    "\n",
    "    # Pre-process records before ingest\n",
    "    if \"anvil_\" in table:\n",
    "        try:\n",
    "            # Pre-process records in AnVIL_ records to use new datarepo_row_ids in the source_datarepo_row_ids field\n",
    "            logging.info(\"FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\")\n",
    "            records_processed = []\n",
    "            for record in records_orig:\n",
    "                int_record = record.copy()\n",
    "                new_dr_row_id_list = []\n",
    "                for row_id in int_record[\"source_datarepo_row_ids\"]:\n",
    "                    new_row_id = dr_row_id_xwalk.get(row_id)\n",
    "                    if new_row_id:\n",
    "                        new_dr_row_id_list.append(new_row_id)\n",
    "                    else:\n",
    "                        err_str = f\"Failure in pre-processing: row_id '{row_id}'' not found in datarepo_row_id crosswalk.\"\n",
    "                        logging.error(err_str)\n",
    "                        config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "                        return   \n",
    "                int_record[\"source_datarepo_row_ids\"] = new_dr_row_id_list\n",
    "                records_processed.append(int_record)\n",
    "        except Exception as e:\n",
    "            err_str = f\"Failure in pre-processing: {str(e)}\"\n",
    "            logging.error(err_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "            return\n",
    "    else:\n",
    "        records_processed = records_orig    \n",
    "    \n",
    "    # Write out records to cloud, if specified by user\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        logging.info(f\"Writing records to a control file in the cloud.\")\n",
    "        if write_to_cloud_platform == \"gcp\":\n",
    "            control_file_path = write_records_to_gcp(config, table, records_processed)\n",
    "        else:\n",
    "            control_file_path = write_records_to_azure(config, table, records_processed)\n",
    "\n",
    "    # Build, submit, and monitor ingest request\n",
    "    logging.info(f\"Submitting ingestion request to new dataset ({new_dataset_id}).\")\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        ingest_request = {\n",
    "            \"table\": table,\n",
    "            \"profile_id\": tar_tdr_billing_profile,\n",
    "            \"ignore_unknown_values\": True,\n",
    "            \"resolve_existing_files\": True,\n",
    "            \"updateStrategy\": \"append\",\n",
    "            \"format\": \"json\",\n",
    "            \"load_tag\": \"Ingest for {}\".format(new_dataset_id),\n",
    "            \"path\": control_file_path\n",
    "        }        \n",
    "    else:\n",
    "        ingest_request = {\n",
    "            \"table\": table,\n",
    "            \"profile_id\": tar_tdr_billing_profile,\n",
    "            \"ignore_unknown_values\": True,\n",
    "            \"resolve_existing_files\": True,\n",
    "            \"updateStrategy\": \"append\",\n",
    "            \"format\": \"array\",\n",
    "            \"load_tag\": \"Ingest for {}\".format(new_dataset_id),\n",
    "            \"records\": records_processed\n",
    "        }\n",
    "    attempt_counter = 1\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = refresh_tdr_api_client(tdr_host)\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = wait_for_tdr_job(datasets_api.ingest_dataset(id=new_dataset_id, ingest=ingest_request), tdr_host)\n",
    "            logging.info(\"Ingest succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Success\", str(ingest_request_result)[0:1000]])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on ingest: {}\".format(str(e)[0:2500]))\n",
    "            if attempt_counter < 3:\n",
    "                logging.info(\"Retrying ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Logging error.\")\n",
    "                err_str = f\"Error on ingest: {str(e)[0:2500]}\"\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])  \n",
    "                break\n",
    "\n",
    "    # Remove control file from cloud, if written out\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        logging.info(f\"Removing control file from the cloud.\")\n",
    "        if write_to_cloud_platform == \"gcp\":\n",
    "            client = storage.Client()\n",
    "            target_bucket = control_file_path.split(\"/\")[2]\n",
    "            target_object = \"/\".join(control_file_path.split(\"/\")[3:])\n",
    "            bucket = client.bucket(target_bucket)\n",
    "            blob = bucket.blob(target_object)\n",
    "            blob.delete()\n",
    "        else:\n",
    "            blob = BlobClient.from_blob_url(control_file_path)\n",
    "            blob.delete_blob()\n",
    "\n",
    "# Function to orchestration the migration of tabular data\n",
    "def migrate_tabular_data(config):\n",
    "\n",
    "    # Extract parameters from config\n",
    "    source_dataset_id = config[\"source_dataset_id\"]\n",
    "    target_dataset_id = config[\"target_dataset_id\"] \n",
    "    tables_to_ingest = config[\"tables_to_ingest\"] \n",
    "    tdr_host = config[\"tdr_host\"] \n",
    "    tdr_sa_to_use = config[\"tdr_sa_to_use\"] \n",
    "    chunk_size = config[\"chunk_size\"] \n",
    "    max_combined_rec_ref_size = config[\"max_combined_rec_ref_size\"] \n",
    "    skip_ingests = config[\"skip_ingests\"]\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "\n",
    "    # Add TDR SA to original dataset\n",
    "    logging.info(f\"Adding TDR general SA ({tdr_sa_to_use}) to original dataset: {source_dataset_id}\")\n",
    "    try:\n",
    "        resp = datasets_api.add_dataset_policy_member(id=source_dataset_id, policy_name=\"steward\", policy_member={\"email\": tdr_sa_to_use}) \n",
    "        logging.info(\"TDR SA added successfully.\")\n",
    "    except:\n",
    "        error_str = f\"Error adding TDR SA to dataset {source_dataset_id}: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "        return\n",
    "\n",
    "    # Collect details from original dataset to build inventory of tables to migrate\n",
    "    logging.info(f\"Retrieving dataset details from original dataset: {source_dataset_id}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=source_dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        config[\"bigquery_project\"] = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        config[\"bigquery_dataset\"] = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        fileref_col_dict = {}\n",
    "        array_col_dict = {}\n",
    "        for table_entry in dataset_details[\"schema\"][\"tables\"]:\n",
    "            fileref_list = []\n",
    "            array_list = []\n",
    "            for idx, column_entry in enumerate(table_entry[\"columns\"]):\n",
    "                if column_entry[\"datatype\"] == \"fileref\":\n",
    "                    fileref_list.append(column_entry[\"name\"])\n",
    "                if column_entry[\"array_of\"] == True:\n",
    "                    array_list.append(column_entry[\"name\"])\n",
    "            fileref_col_dict[table_entry[\"name\"]] = fileref_list\n",
    "            array_col_dict[table_entry[\"name\"]] = array_list\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset {source_dataset_id}: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "        return\n",
    "\n",
    "    # Read in existing datarepo_row_id crosswalk, if one exists\n",
    "    logging.info(\"Fetching existing datarepo_row_id crosswalk (if one exists).\")\n",
    "    xwalk_json_file_name = f\"{source_dataset_id}_{target_dataset_id}_rowid_xwalk.json\"\n",
    "    try:\n",
    "        with open(xwalk_json_file_name,\"r\") as file:\n",
    "            datarepo_row_id_xwalk = json.load(file)\n",
    "    except:\n",
    "        datarepo_row_id_xwalk = {}\n",
    "        logging.warning(f\"No datarepo_row_id crosswalk file name '{xwalk_json_file_name}' found.\")\n",
    "\n",
    "    # Order tables for ingestion\n",
    "    logging.info(\"Ordering tables and pulling current record counts for validation.\")\n",
    "    table_rank_dict = {}\n",
    "    for table in fileref_col_dict.keys():\n",
    "        if table == \"file_inventory\":\n",
    "            table_rank_dict[table] = 1\n",
    "        elif \"anvil_\" not in table:\n",
    "            table_rank_dict[table] = 2\n",
    "        else:\n",
    "            table_rank_dict[table] = 3\n",
    "    ordered_table_list = sorted(table_rank_dict, key= lambda key: table_rank_dict[key])\n",
    "\n",
    "    # Fetch total record counts for all tables\n",
    "    populated_table_dict = {}\n",
    "    for table in ordered_table_list:\n",
    "        api_client = refresh_tdr_api_client(tdr_host)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": 0,\n",
    "              \"limit\": 10,\n",
    "              \"sort\": \"datarepo_row_id\",\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=source_dataset_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                total_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 5:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    total_record_count = -1\n",
    "                    break\n",
    "        if total_record_count == -1:\n",
    "            error_str = f\"Error retrieving current record counts for tables in dataset {source_dataset_id}: {str(e)}\"\n",
    "            logging.error(error_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "            return\n",
    "        elif total_record_count > 0:\n",
    "            populated_table_dict[table] = total_record_count\n",
    "\n",
    "    # Loop through and process tables for ingestion\n",
    "    logging.info(\"Processing dataset ingestion requests.\")\n",
    "    pop_fss_table_cnt = 0\n",
    "    for table in ordered_table_list:\n",
    "\n",
    "        # Determine whether table should be processed, and skip if not\n",
    "        logging.info(f\"Processing dataset ingestion for table '{table}'.\")\n",
    "        total_record_count = 0\n",
    "        if tables_to_ingest and table not in tables_to_ingest:\n",
    "            msg_str = f\"Table '{table}' not listed in the tables_to_ingest parameter. Skipping.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "            continue\n",
    "        elif table not in populated_table_dict.keys():\n",
    "            msg_str = f\"No records found for table '{table}' in original dataset. Continuing to next table/record set.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "            continue\n",
    "        elif \"anvil_\" in table:\n",
    "            # Confirm all non-FSS tables are present in datarepo_row_id_xwalk\n",
    "            pop_fss_table_cnt += 1\n",
    "            missing_tab_list = []\n",
    "            for tab in populated_table_dict.keys():\n",
    "                if \"anvil_\" not in tab and tab not in datarepo_row_id_xwalk.keys():\n",
    "                    missing_tab_list.append(tab)\n",
    "            if len(missing_tab_list) > 0:\n",
    "                missing_tab_string = \", \".join(missing_tab_list)\n",
    "                msg_str = f\"Populated non-FSS tables missing from datarepo_row_id crosswalk: {missing_tab_string}. Skipping FSS table '{table}'.\"\n",
    "                logging.info(msg_str)\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "                continue\n",
    "        \n",
    "        # Aggregate datarepo_row_id crosswalk informatino for us in FSS table processing\n",
    "        if pop_fss_table_cnt == 1:\n",
    "            dr_row_id_xwalk = {}\n",
    "            for key in datarepo_row_id_xwalk.keys():\n",
    "                dr_row_id_xwalk.update(datarepo_row_id_xwalk[key])\n",
    "            config[\"dr_row_id_xwalk\"] = dr_row_id_xwalk \n",
    "            \n",
    "        # Chunk table records as necessary, then loop through and process each chunk\n",
    "        total_record_count = populated_table_dict.get(table)\n",
    "        if skip_ingests:\n",
    "            msg_str = f\"Parameter 'skip_ingests' set to true. Skipping ingestion for table '{table}'.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "        else:\n",
    "            if fileref_col_dict[table]:\n",
    "                ref_chunk_size = math.floor(max_combined_rec_ref_size / len(fileref_col_dict[table]))\n",
    "                table_chunk_size = min(chunk_size, ref_chunk_size)\n",
    "                logging.info(f\"Table '{table}' contains fileref columns. Will use a chunk size of {table_chunk_size} rows per ingestion request, to keep the number of file references per chunk below {max_combined_rec_ref_size}.\")\n",
    "            else:\n",
    "                table_chunk_size = chunk_size\n",
    "                logging.info(f\"Table '{table}' does not contain fileref columns. Will use a chunk size of {table_chunk_size} rows per ingestion request.\")\n",
    "            start_row = 1\n",
    "            end_row = min((table_chunk_size), total_record_count)\n",
    "            while start_row <= total_record_count:\n",
    "                if end_row > total_record_count:\n",
    "                    end_row = total_record_count\n",
    "                ingest_table_data(config, target_dataset_id, fileref_col_dict, array_col_dict, table, start_row, end_row)    \n",
    "                start_row += table_chunk_size\n",
    "                end_row += table_chunk_size\n",
    "\n",
    "        # Build datarepo_row_id crosswalk for the table, add to datarepo_row_id_xwalk dict, and write out updated dict to file\n",
    "        if \"anvil_\" not in table: \n",
    "            logging.info(\"Fetching ingested records and building datarepo_row_id lookup for use in AnVIL transforms.\")\n",
    "            temp_dr_xwalk = {}\n",
    "            api_client = refresh_tdr_api_client(tdr_host)\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            max_page_size = 1000\n",
    "            records_fetched = 0\n",
    "            retrieval_error = False\n",
    "            while records_fetched < total_record_count and not retrieval_error:\n",
    "                row_start = records_fetched\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    payload = {\n",
    "                      \"offset\": row_start,\n",
    "                      \"limit\": max_page_size,\n",
    "                      \"sort\": \"datarepo_row_id\",\n",
    "                      \"direction\": \"asc\",\n",
    "                      \"filter\": \"\"\n",
    "                    }\n",
    "                    try:\n",
    "                        dataset_results = datasets_api.query_dataset_data_by_id(id=target_dataset_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                        if len(dataset_results[\"result\"]) == 0:\n",
    "                            warn_str = f\"No records found for '{table}' table, which prevents the proper building of the datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream.\"\n",
    "                            logging.warning(warn_str)\n",
    "                            retrieval_error = True\n",
    "                            break  \n",
    "                        else:\n",
    "                            for record in dataset_results[\"result\"]:\n",
    "                                key = table + \":\" + record[\"orig_datarepo_row_id\"]\n",
    "                                val = table + \":\" + record[\"datarepo_row_id\"]\n",
    "                                temp_dr_xwalk[key] = val\n",
    "                                records_fetched += 1\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 5:\n",
    "                            sleep(10)\n",
    "                            attempt_counter += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            warn_str = f\"Error retrieving records for '{table}' table to build datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream.\"\n",
    "                            logging.warning(warn_str)\n",
    "                            retrieval_error = True\n",
    "                            break\n",
    "            if not retrieval_error:\n",
    "                datarepo_row_id_xwalk[table] = temp_dr_xwalk\n",
    "                with open(xwalk_json_file_name, 'w') as file:\n",
    "                    json.dump(datarepo_row_id_xwalk, file)\n",
    "        \n",
    "        # Fetch total record count for the new table\n",
    "        api_client = refresh_tdr_api_client(tdr_host)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": 0,\n",
    "              \"limit\": 10,\n",
    "              \"sort\": \"datarepo_row_id\",\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=target_dataset_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                new_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 5:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    new_record_count = -1\n",
    "                    break\n",
    "        if new_record_count == -1:\n",
    "            err_str = f\"Error retrieving record count for table '{table}' in new dataset. Skipping validation and continuing to next table.\"\n",
    "            logging.error(err_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Failure\", err_str])\n",
    "            continue \n",
    "\n",
    "        # Validate the new table against the old table, with extra scrutiny given to the file_inventory table for AnVIL migrations\n",
    "        logging.info(f\"Validating table '{table}' in new dataset vs. original dataset.\")\n",
    "        if new_record_count == total_record_count:\n",
    "            config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Success\", f\"{new_record_count} records found in both new and original table.\"])\n",
    "        else:\n",
    "            config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Failure\", f\"{new_record_count} records found in new table doesn't match {total_record_count} records in original table.\"])\n",
    "\n",
    "    # Display results\n",
    "    pipeline_results = pd.DataFrame(config[\"migration_results\"], columns = [\"Task\", \"Step\", \"Status\", \"Message\"])\n",
    "    failures = pipeline_results[pipeline_results[\"Status\"].str.contains(\"Failure\")]\n",
    "    logging.info(\"Migration Pipeline Results:\")\n",
    "    display(pipeline_results)\n",
    "    logging.info(f\"\\nPipeline finished with {len(failures)} failures.\")\n",
    "    return len(failures)\n",
    "\n",
    "# Function for creating a snapshot for the new dataset\n",
    "def recreate_snapshot(config):\n",
    "    \n",
    "    # Extract parameters from config\n",
    "    target_dataset_id = config[\"target_dataset_id\"] \n",
    "    azure_billing_profile = config[\"azure_billing_profile\"] \n",
    "    tdr_host = config[\"tdr_host\"] \n",
    "    anvil_schema = config[\"anvil_schema\"] \n",
    "    \n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    \n",
    "    # Retrieve new dataset details\n",
    "    logging.info(f\"Retrieving dataset details from prod environment. UUID:  {target_dataset_id}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=target_dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        dataset_name = dataset_details[\"name\"]\n",
    "        phs_id = dataset_details[\"phs_id\"]\n",
    "        consent_name = dataset_details[\"properties\"][\"consent_name\"]\n",
    "        auth_domains = dataset_details[\"properties\"][\"auth_domains\"]\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "        snapshot_name = dataset_name + \"_\" + anvil_schema + \"_\" + current_datetime_string\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "    # Build config and submit snapshot job\n",
    "    snapshot_config = {\n",
    "        \"profile_id\": azure_billing_profile,\n",
    "        \"snapshot_readers_list\": [\"azul-anvil-prod@firecloud.org\", \"auth-domain\"],\n",
    "        \"anvil_schema_versin\": anvil_schema,\n",
    "        \"ws_bucket\": os.environ[\"WORKSPACE_BUCKET\"],\n",
    "        \"dataset_id\": entry[1],\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"phs_id\": phs_id,\n",
    "        \"consent_name\": consent_name,\n",
    "        \"auth_domains\": auth_domains,\n",
    "        \"pipeline_results\": [],\n",
    "        \"snapshot_name\": snapshot_name\n",
    "    }\n",
    "    utils.create_and_share_snapshot(snapshot_config)\n",
    "    int_df_results = pd.DataFrame(snapshot_config[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.error(\"Errors reported in snapshotting. See logs for details.\")\n",
    "        status = \"Failure\"\n",
    "        message = f\"{len(errors)} failures reported. See log for details.\"\n",
    "        snapshot_id = \"\"\n",
    "        snapshot_name = \"\"\n",
    "    else:\n",
    "        status = \"Success\"\n",
    "        message = \"\"\n",
    "        snapshot_id = re.search(\"{'id': '([a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12})'\", str(int_df_results[int_df_results[\"Task\"]==\"Create and Share Snapshot\"][\"Message\"]))[1]\n",
    "        snapshot_name = re.search(\"'name': '([a-zA-Z0-9_\\-]+)'\", str(int_df_results[int_df_results[\"Task\"]==\"Create and Share Snapshot\"][\"Message\"]))[1]\n",
    "    return status, message, snapshot_id, snapshot_name\n",
    "        \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify migration pairs: [Source GCP Dataset, Target Azure Dataset]\n",
    "migration_list = [\n",
    "    #[\"gcp_dataset_id\", \"az_dataset_id\"]\n",
    "    [\"09642596-d33a-4261-8bf7-eb1dbb37d572\", \"89d9ed6a-28c6-4b1c-97a1-10b1a26382be\"],\n",
    "    [\"75119ed5-b8aa-4f45-bdef-e3c673bbe44c\", \"cb009d23-9a05-44a9-82ac-82ef0722ab81\"],\n",
    "]\n",
    "\n",
    "# Run parameters\n",
    "azure_billing_profile = \"9ee23bed-b46c-4561-9103-d2a723113f7f\"\n",
    "anvil_schema = \"ANV5\"\n",
    "run_data_migration = True\n",
    "skip_ingests = False # Set to True to build datarepo_row_id xwalk and run validation w/o ingesting more records\n",
    "tables_to_ingest = [] # Leave empty for all\n",
    "run_snapshot_creation = True\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Set up logging\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "logs_stream_file_path = \"processing_details_\" + current_datetime_string + \".log\"\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.FileHandler(logs_stream_file_path), logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for entry in migration_list:\n",
    "    \n",
    "    # Run cross-cloud ingestion, if specified\n",
    "    failure_count = 0\n",
    "    if run_data_migration:\n",
    "        logging.info(f\"\\nMigrating tabular data from TDR dataset {entry[0]} to TDR dataset {entry[1]}.\")\n",
    "        # Build config and submit migration job\n",
    "        config = {\n",
    "            \"source_dataset_id\": entry[0], \n",
    "            \"target_dataset_id\": entry[1],\n",
    "            \"tables_to_ingest\": tables_to_ingest,\n",
    "            \"tdr_host\": \"https://data.terra.bio\",\n",
    "            \"tdr_sa_to_use\": \"datarepo-jade-api@terra-datarepo-production.iam.gserviceaccount.com\",\n",
    "            \"tar_tdr_billing_profile\": azure_billing_profile,\n",
    "            \"chunk_size\": 250000,\n",
    "            \"max_combined_rec_ref_size\": 40000,\n",
    "            \"migration_results\": [],\n",
    "            \"dr_row_id_xwalk\": {},\n",
    "            \"skip_ingests\": skip_ingests\n",
    "        }\n",
    "        failure_count = migrate_tabular_data(config)\n",
    "        status = \"Failure\" if failure_count > 0 else \"Success\"\n",
    "        msg = f\"{failure_count} failures reported. See log for details.\" if failure_count > 0 else \"\"\n",
    "        results.append([entry[0], entry[1], \"Data Ingestion\", status, msg, \"\", \"\"])\n",
    "\n",
    "    # Run snapshotting, if specified and no upstream errors detected\n",
    "    if run_snapshot_creation:\n",
    "        logging.info(f\"Creating a snapshot for TDR dataset {entry[1]}.\")\n",
    "        # Build config and submit snapshot job\n",
    "        config = { \n",
    "            \"target_dataset_id\": entry[1],\n",
    "            \"tdr_host\": \"https://data.terra.bio\",\n",
    "            \"azure_billing_profile\": azure_billing_profile,\n",
    "            \"anvil_schema\": anvil_schema\n",
    "        }\n",
    "        if failure_count > 0:\n",
    "            logging.error(\"Failures noted in upstream data processing. Skipping snapshotting.\")\n",
    "            results.append([entry[0], entry[1], \"Data Snapshotting\", \"Skipped\", \"Failures noted in upstream data processing.\", \"\", \"\"])\n",
    "        else:\n",
    "            status, message, snapshot_id, snapshot_name = recreate_snapshot(config)\n",
    "            results.append([entry[0], entry[1], \"Data Snapshotting\", status, message, snapshot_id, snapshot_name])\n",
    "            \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Source Dataset ID\", \"Target Dataset ID\", \"Processing Step\", \"Status\", \"Message\", \"Snapshot ID\", \"Snapshot Name\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pull and Compare Tabular Data between TDR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     109
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def compare_row_counts(dataset_1_id, dataset_2_id):\n",
    "    \n",
    "    # Setup/refresh TDR clients\n",
    "    logging.info(f\"Comparing tabular data record counts between TDR dataset {dataset_1_id} and TDR dataset {dataset_2_id}.\")\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Pull table list across datasets\n",
    "    logging.info(f\"Pulling the superset of tables across the two datasets.\")\n",
    "    try:\n",
    "        dataset_1_details = datasets_api.retrieve_dataset(id=dataset_1_id, include=[\"SCHEMA\"]).to_dict()\n",
    "        dataset_2_details = datasets_api.retrieve_dataset(id=dataset_2_id, include=[\"SCHEMA\"]).to_dict()\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from datasets: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "    table_set = set()\n",
    "    for table_entry in dataset_1_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])\n",
    "    for table_entry in dataset_2_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])   \n",
    "\n",
    "    # For each table in the table list, pull record counts from the two datasets and compare\n",
    "    results = []\n",
    "    payload = {\n",
    "      \"offset\": 0,\n",
    "      \"limit\": 10,\n",
    "      \"sort\": \"datarepo_row_id\",\n",
    "      \"direction\": \"asc\",\n",
    "      \"filter\": \"\"\n",
    "    }\n",
    "    for table in table_set:\n",
    "        logging.info(f\"Comparing record counts for table '{table}'\")\n",
    "        # Pulling record counts for dataset 1\n",
    "        ds1_table_present = \"True\"\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_1_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                ds1_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    ds1_record_count = 0\n",
    "                    ds1_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        ds1_record_count = 0\n",
    "                        ds1_table_present = \"Unknown\"\n",
    "                        break\n",
    "        # Pulling record counts for dataset 2\n",
    "        ds2_table_present = \"True\"\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_2_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                ds2_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    ds2_record_count = 0\n",
    "                    ds2_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        ds2_record_count = 0\n",
    "                        ds2_table_present = \"Unknown\"\n",
    "                        break\n",
    "        # Build table comparison\n",
    "        if ds1_table_present == \"Unknown\" or ds2_table_present == \"Unknown\":\n",
    "            status = \"Fail\"\n",
    "            error_reason = \"Error retrieving table data from dataset(s)\"\n",
    "        elif ds1_table_present == \"False\" or ds2_table_present == \"False\":\n",
    "            status = \"Fail\"\n",
    "            error_reason = \"Table presence mismatch between datasets\"\n",
    "        elif ds1_record_count != ds2_record_count:\n",
    "            status = \"Fail\"\n",
    "            error_reason = \"Difference in record count\"\n",
    "        else:\n",
    "            status = \"Pass\"\n",
    "            error_reason = \"\"\n",
    "        results.append([dataset_1_id, dataset_2_id, table, ds1_table_present, ds1_record_count, ds2_table_present, ds2_record_count, status, error_reason])\n",
    "\n",
    "    # Display detailed results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"Dataset 1 ID\", \"Dataset 2 ID\", \"Table\", \"Table in DS1\", \"DS1 Record Count\", \"Table in DS2\", \"DS2 Record Count\", \"Status\", \"Message\"])\n",
    "    display(results_df)\n",
    "\n",
    "    # Return final aggregated results\n",
    "    status = \"Pass\"\n",
    "    failed_tables = []\n",
    "    for entry in results:\n",
    "        if entry[7] == \"Fail\":\n",
    "            failed_tables.append(entry[2])\n",
    "            status = \"Fail\"\n",
    "    return status, sorted(failed_tables)\n",
    "        \n",
    "def compare_contents_sample(dataset_1_id, dataset_2_id, sample_size, fields_to_ignore):\n",
    "    # Pull schema, record first column in each table (for ordering)\n",
    "    # Setup/refresh TDR clients\n",
    "    logging.info(f\"Comparing tabular data record counts between TDR dataset {dataset_1_id} and TDR dataset {dataset_2_id}.\")\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Pull table list across datasets\n",
    "    logging.info(f\"Pulling the superset of tables across the two datasets.\")\n",
    "    try:\n",
    "        dataset_1_details = datasets_api.retrieve_dataset(id=dataset_1_id, include=[\"SCHEMA\"]).to_dict()\n",
    "        dataset_2_details = datasets_api.retrieve_dataset(id=dataset_2_id, include=[\"SCHEMA\"]).to_dict()\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from datasets: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "    table_set = {}\n",
    "    for table_entry in dataset_1_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])\n",
    "    for table_entry in dataset_2_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])  \n",
    "    \n",
    "    \n",
    "    # Loop through tables, pull xxx records (by sample size), ordering by first column\n",
    "    # Drop fields_to_ignore\n",
    "    # Compare --> How to best do this\n",
    "    pass\n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of dataset pairs to compare\n",
    "dataset_id_pairs_list = [\n",
    "    #[\"gcp_dataset_id\", \"az_dataset_id\"]\n",
    "    [\"a9212179-efd5-4c53-a57a-93016eff9017\", \"00832d1d-0145-4e04-88dc-2bad4c5d87bf\"],\n",
    "]\n",
    "\n",
    "# Specify whether row comparison checks should run\n",
    "run_row_count_comparison = True\n",
    "\n",
    "# Specify whether table content checks should run, the size of the sample to use (if so), and which fields should be excluded from comparison\n",
    "run_contents_sample_comparison = False\n",
    "contents_sample_comparison_size = 1000\n",
    "fields_to_ignore = [\"datarepo_row_id\", \"orig_datarepo_row_id\", \"orig_file_ref\", \"source_datarepo_row_ids\", \"uri\"]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Run validation\n",
    "results = []\n",
    "for dataset_id_pair in dataset_id_pairs_list:\n",
    "    if run_row_count_comparison:\n",
    "        status, failed_tables = compare_row_counts(dataset_id_pair[0], dataset_id_pair[1])\n",
    "        results.append([dataset_id_pair[0], dataset_id_pair[1], \"Record Count Comparison\", status, ', '.join(failed_tables)])\n",
    "\n",
    "# Display final results\n",
    "print(\"\\nFinal Validation Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Dataset 1 ID\", \"Dataset 2 ID\", \"Validation Type\", \"Status\", \"Failed Tables\"])\n",
    "display(results_df)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_1_id = \"b12fb9be-2ce0-4bfd-8503-732fabba06ab\"\n",
    "dataset_2_id = \"744c85cc-13d2-4f90-9d2e-d3143cb01edb\"\n",
    "contents_sample_comparison_size = 1000\n",
    "fields_to_ignore = [\"datarepo_row_id\", \"orig_datarepo_row_id\", \"orig_file_ref\", \"source_datarepo_row_ids\", \"uri\"]\n",
    "\n",
    "# Setup/refresh TDR clients\n",
    "logging.info(f\"Comparing a sample of tabular data content between TDR dataset {dataset_1_id} and TDR dataset {dataset_2_id}.\")\n",
    "api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# Pull table list across datasets\n",
    "logging.info(f\"Pulling the superset of tables across the two datasets.\")\n",
    "try:\n",
    "    dataset_1_details = datasets_api.retrieve_dataset(id=dataset_1_id, include=[\"SCHEMA\"]).to_dict()\n",
    "    dataset_2_details = datasets_api.retrieve_dataset(id=dataset_2_id, include=[\"SCHEMA\"]).to_dict()\n",
    "except Exception as e:\n",
    "    error_str = f\"Error retrieving details from datasets: {str(e)}\"\n",
    "    logging.error(error_str)\n",
    "table_set = {}\n",
    "for table_entry in dataset_1_details[\"schema\"][\"tables\"]:\n",
    "    table_set[table_entry[\"name\"]] = table_entry[\"columns\"][0][\"name\"]\n",
    "for table_entry in dataset_2_details[\"schema\"][\"tables\"]:\n",
    "    table_set[table_entry[\"name\"]] = table_entry[\"columns\"][0][\"name\"]\n",
    "    \n",
    "# For each table in the table list, pull sample records from the two datasets and compare\n",
    "results = []\n",
    "for table in [\"file_inventory\"]: #table_set.keys():\n",
    "    logging.info(f\"Comparing sample records for table '{table}'\")\n",
    "    # Pulling sample records for dataset 1\n",
    "    ds1_table_present = \"True\"\n",
    "    max_page_size = 1000\n",
    "    total_records_fetched = 0\n",
    "    ds1_final_records = []\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        offset = total_records_fetched\n",
    "        page_size = min(max_page_size, contents_sample_comparison_size - total_records_fetched)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": offset,\n",
    "              \"limit\": page_size,\n",
    "              \"sort\": table_set[table],\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_1_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    record_results = []\n",
    "                    ds1_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        record_results = []\n",
    "                        ds1_table_present = \"Unknown\"\n",
    "                        break\n",
    "        if record_results[\"result\"]:\n",
    "            ds1_final_records.extend(record_results[\"result\"])\n",
    "            total_records_fetched += len(record_results[\"result\"])\n",
    "        else:\n",
    "            break\n",
    "        if total_records_fetched >= contents_sample_comparison_size:\n",
    "            break\n",
    "    # Pulling sample records for dataset 2\n",
    "    ds2_table_present = \"True\"\n",
    "    max_page_size = 1000\n",
    "    total_records_fetched = 0\n",
    "    ds2_final_records = []\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        offset = total_records_fetched\n",
    "        page_size = min(max_page_size, contents_sample_comparison_size - total_records_fetched)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": offset,\n",
    "              \"limit\": page_size,\n",
    "              \"sort\": table_set[table],\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_2_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    record_results = []\n",
    "                    ds2_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        record_results = []\n",
    "                        ds2_table_present = \"Unknown\"\n",
    "                        break\n",
    "        if record_results[\"result\"]:\n",
    "            ds2_final_records.extend(record_results[\"result\"])\n",
    "            total_records_fetched += len(record_results[\"result\"])\n",
    "        else:\n",
    "            break\n",
    "        if total_records_fetched >= contents_sample_comparison_size:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_ds1_records_int = pd.DataFrame.from_dict(ds1_final_records)\n",
    "df_ds2_records_int = pd.DataFrame.from_dict(ds2_final_records)\n",
    "cols = df_ds1_records_int.columns.tolist()\n",
    "for field in fields_to_ignore:\n",
    "    if field in cols:\n",
    "        cols.remove(field)\n",
    "df_ds1_records = df_ds1_records_int[cols]\n",
    "df_ds2_records = df_ds2_records_int[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "diff = df_ds1_records.compare(df_ds2_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if df_ds1_records.equals(df_ds2_records):\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull and Compare File Counts and Sizes between TDR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/12/2024 12:54:10 PM - INFO: Processing dataset_id_pair: ['8de6dae2-55ff-4287-9b75-5b2a950c1f44', 'e1fdd1b9-fe56-42ca-8e86-b4bb32d9bbce']\n",
      "04/12/2024 12:54:10 PM - INFO: Retrieving files from dataset_id 8de6dae2-55ff-4287-9b75-5b2a950c1f44...\n",
      "04/12/2024 12:54:12 PM - INFO: 86 records fetched...\n",
      "04/12/2024 12:54:12 PM - INFO: File retrieval complete!\n",
      "04/12/2024 12:54:12 PM - INFO: Retrieving files from dataset_id e1fdd1b9-fe56-42ca-8e86-b4bb32d9bbce...\n",
      "04/12/2024 12:54:15 PM - INFO: 86 records fetched...\n",
      "04/12/2024 12:54:16 PM - INFO: File retrieval complete!\n",
      "04/12/2024 12:54:17 PM - INFO: Results recorded:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset ID 1</th>\n",
       "      <th>Dataset ID 2</th>\n",
       "      <th>Validation Status</th>\n",
       "      <th>Validation Message</th>\n",
       "      <th>File Count Diff</th>\n",
       "      <th>Total File Size (Bytes) Diff</th>\n",
       "      <th>Max File Size (Bytes) Diff</th>\n",
       "      <th>File Count 1</th>\n",
       "      <th>Total File Size (Bytes) 1</th>\n",
       "      <th>Max File Size (Bytes) 1</th>\n",
       "      <th>Status 1</th>\n",
       "      <th>Message 1</th>\n",
       "      <th>File Count 2</th>\n",
       "      <th>Total File Size (Bytes) 2</th>\n",
       "      <th>Max File Size (Bytes) 2</th>\n",
       "      <th>Status 2</th>\n",
       "      <th>Message 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8de6dae2-55ff-4287-9b75-5b2a950c1f44</td>\n",
       "      <td>e1fdd1b9-fe56-42ca-8e86-b4bb32d9bbce</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>3632191049</td>\n",
       "      <td>349377430</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>86</td>\n",
       "      <td>3632191049</td>\n",
       "      <td>349377430</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset ID 1                          Dataset ID 2             Validation Status Validation Message  File Count Diff  Total File Size (Bytes) Diff  Max File Size (Bytes) Diff  File Count 1  Total File Size (Bytes) 1  Max File Size (Bytes) 1 Status 1  Message 1  File Count 2  Total File Size (Bytes) 2  Max File Size (Bytes) 2 Status 2  Message 2\n",
       "0  8de6dae2-55ff-4287-9b75-5b2a950c1f44  e1fdd1b9-fe56-42ca-8e86-b4bb32d9bbce       Passed                                 0                       0                            0                   86              3632191049                 349377430          Success                 86              3632191049                 349377430          Success          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/12/2024 12:54:17 PM - INFO: Processing dataset_id_pair: ['bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8', 'c7206e9a-78ad-4c9d-927f-3ca76646227d']\n",
      "04/12/2024 12:54:17 PM - INFO: Retrieving files from dataset_id bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8...\n",
      "04/12/2024 12:54:19 PM - INFO: 1000 records fetched...\n",
      "04/12/2024 12:54:21 PM - INFO: 2000 records fetched...\n",
      "04/12/2024 12:54:23 PM - INFO: 3000 records fetched...\n",
      "04/12/2024 12:54:25 PM - INFO: 4000 records fetched...\n",
      "04/12/2024 12:54:28 PM - INFO: 5000 records fetched...\n",
      "04/12/2024 12:54:31 PM - INFO: 6000 records fetched...\n",
      "04/12/2024 12:54:33 PM - INFO: 7000 records fetched...\n",
      "04/12/2024 12:54:35 PM - INFO: 8000 records fetched...\n",
      "04/12/2024 12:54:38 PM - INFO: 9000 records fetched...\n",
      "04/12/2024 12:54:40 PM - INFO: 10000 records fetched...\n",
      "04/12/2024 12:54:42 PM - INFO: 11000 records fetched...\n",
      "04/12/2024 12:54:45 PM - INFO: 12000 records fetched...\n",
      "04/12/2024 12:54:48 PM - INFO: 13000 records fetched...\n",
      "04/12/2024 12:54:50 PM - INFO: 14000 records fetched...\n",
      "04/12/2024 12:54:53 PM - INFO: 15000 records fetched...\n",
      "04/12/2024 12:54:56 PM - INFO: 16000 records fetched...\n",
      "04/12/2024 12:54:59 PM - INFO: 17000 records fetched...\n",
      "04/12/2024 12:55:01 PM - INFO: 18000 records fetched...\n",
      "04/12/2024 12:55:04 PM - INFO: 19000 records fetched...\n",
      "04/12/2024 12:55:08 PM - INFO: 20000 records fetched...\n",
      "04/12/2024 12:55:10 PM - INFO: 21000 records fetched...\n",
      "04/12/2024 12:55:14 PM - INFO: 22000 records fetched...\n",
      "04/12/2024 12:55:17 PM - INFO: 23000 records fetched...\n",
      "04/12/2024 12:55:20 PM - INFO: 24000 records fetched...\n",
      "04/12/2024 12:55:24 PM - INFO: 25000 records fetched...\n",
      "04/12/2024 12:55:27 PM - INFO: 26000 records fetched...\n",
      "04/12/2024 12:55:32 PM - INFO: 27000 records fetched...\n",
      "04/12/2024 12:55:36 PM - INFO: 28000 records fetched...\n",
      "04/12/2024 12:55:39 PM - INFO: 29000 records fetched...\n",
      "04/12/2024 12:55:43 PM - INFO: 30000 records fetched...\n",
      "04/12/2024 12:55:47 PM - INFO: 31000 records fetched...\n",
      "04/12/2024 12:55:51 PM - INFO: 32000 records fetched...\n",
      "04/12/2024 12:55:55 PM - INFO: 33000 records fetched...\n",
      "04/12/2024 12:55:59 PM - INFO: 34000 records fetched...\n",
      "04/12/2024 12:56:03 PM - INFO: 35000 records fetched...\n",
      "04/12/2024 12:56:07 PM - INFO: 36000 records fetched...\n",
      "04/12/2024 12:56:12 PM - INFO: 37000 records fetched...\n",
      "04/12/2024 12:56:17 PM - INFO: 38000 records fetched...\n",
      "04/12/2024 12:56:21 PM - INFO: 39000 records fetched...\n",
      "04/12/2024 12:56:25 PM - INFO: 40000 records fetched...\n",
      "04/12/2024 12:56:30 PM - INFO: 41000 records fetched...\n",
      "04/12/2024 12:56:35 PM - INFO: 42000 records fetched...\n",
      "04/12/2024 12:56:39 PM - INFO: 43000 records fetched...\n",
      "04/12/2024 12:56:44 PM - INFO: 44000 records fetched...\n",
      "04/12/2024 12:56:49 PM - INFO: 45000 records fetched...\n",
      "04/12/2024 12:56:54 PM - INFO: 46000 records fetched...\n",
      "04/12/2024 12:56:58 PM - INFO: 47000 records fetched...\n",
      "04/12/2024 12:57:03 PM - INFO: 48000 records fetched...\n",
      "04/12/2024 12:57:07 PM - INFO: 49000 records fetched...\n",
      "04/12/2024 12:57:12 PM - INFO: 50000 records fetched...\n",
      "04/12/2024 12:57:16 PM - INFO: 50408 records fetched...\n",
      "04/12/2024 12:57:21 PM - INFO: File retrieval complete!\n",
      "04/12/2024 12:57:21 PM - INFO: Retrieving files from dataset_id c7206e9a-78ad-4c9d-927f-3ca76646227d...\n",
      "04/12/2024 12:57:29 PM - INFO: 1000 records fetched...\n",
      "04/12/2024 12:57:35 PM - INFO: 2000 records fetched...\n",
      "04/12/2024 12:57:43 PM - INFO: 3000 records fetched...\n",
      "04/12/2024 12:57:48 PM - INFO: 4000 records fetched...\n",
      "04/12/2024 12:57:52 PM - INFO: 5000 records fetched...\n",
      "04/12/2024 12:57:57 PM - INFO: 6000 records fetched...\n",
      "04/12/2024 12:58:03 PM - INFO: 7000 records fetched...\n",
      "04/12/2024 12:58:09 PM - INFO: 8000 records fetched...\n",
      "04/12/2024 12:58:15 PM - INFO: 9000 records fetched...\n",
      "04/12/2024 12:58:21 PM - INFO: 10000 records fetched...\n",
      "04/12/2024 12:58:29 PM - INFO: 11000 records fetched...\n",
      "04/12/2024 12:58:39 PM - INFO: 12000 records fetched...\n",
      "04/12/2024 12:58:48 PM - INFO: 13000 records fetched...\n",
      "04/12/2024 12:58:56 PM - INFO: 14000 records fetched...\n",
      "04/12/2024 12:59:05 PM - INFO: 15000 records fetched...\n",
      "04/12/2024 12:59:15 PM - INFO: 16000 records fetched...\n",
      "04/12/2024 12:59:23 PM - INFO: 17000 records fetched...\n",
      "04/12/2024 12:59:32 PM - INFO: 18000 records fetched...\n",
      "04/12/2024 12:59:42 PM - INFO: 19000 records fetched...\n",
      "04/12/2024 12:59:52 PM - INFO: 20000 records fetched...\n",
      "04/12/2024 01:00:08 PM - INFO: 21000 records fetched...\n",
      "04/12/2024 01:00:21 PM - INFO: 22000 records fetched...\n",
      "04/12/2024 01:00:35 PM - INFO: 23000 records fetched...\n",
      "04/12/2024 01:00:47 PM - INFO: 24000 records fetched...\n",
      "04/12/2024 01:01:01 PM - INFO: 25000 records fetched...\n",
      "04/12/2024 01:01:13 PM - INFO: 25205 records fetched...\n",
      "04/12/2024 01:01:25 PM - INFO: File retrieval complete!\n",
      "04/12/2024 01:01:25 PM - INFO: Results recorded:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset ID 1</th>\n",
       "      <th>Dataset ID 2</th>\n",
       "      <th>Validation Status</th>\n",
       "      <th>Validation Message</th>\n",
       "      <th>File Count Diff</th>\n",
       "      <th>Total File Size (Bytes) Diff</th>\n",
       "      <th>Max File Size (Bytes) Diff</th>\n",
       "      <th>File Count 1</th>\n",
       "      <th>Total File Size (Bytes) 1</th>\n",
       "      <th>Max File Size (Bytes) 1</th>\n",
       "      <th>Status 1</th>\n",
       "      <th>Message 1</th>\n",
       "      <th>File Count 2</th>\n",
       "      <th>Total File Size (Bytes) 2</th>\n",
       "      <th>Max File Size (Bytes) 2</th>\n",
       "      <th>Status 2</th>\n",
       "      <th>Message 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8</td>\n",
       "      <td>c7206e9a-78ad-4c9d-927f-3ca76646227d</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Difference in counts between datasets.</td>\n",
       "      <td>25203</td>\n",
       "      <td>120403205483569</td>\n",
       "      <td>0</td>\n",
       "      <td>50408</td>\n",
       "      <td>259299049376285</td>\n",
       "      <td>111500944870</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>25205</td>\n",
       "      <td>138895843892716</td>\n",
       "      <td>111500944870</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset ID 1                          Dataset ID 2             Validation Status            Validation Message            File Count Diff  Total File Size (Bytes) Diff  Max File Size (Bytes) Diff  File Count 1  Total File Size (Bytes) 1  Max File Size (Bytes) 1 Status 1  Message 1  File Count 2  Total File Size (Bytes) 2  Max File Size (Bytes) 2 Status 2  Message 2\n",
       "0  bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8  c7206e9a-78ad-4c9d-927f-3ca76646227d       Failed       Difference in counts between datasets.       25203              120403205483569                     0                  50408          259299049376285            111500944870         Success                25205          138895843892716            111500944870         Success          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/12/2024 01:01:25 PM - INFO: Processing dataset_id_pair: ['d306000b-88c1-4220-8d7e-933c0118a983', 'e5dc1a69-cb2c-4626-9799-6bb5fae7b147']\n",
      "04/12/2024 01:01:25 PM - INFO: Retrieving files from dataset_id d306000b-88c1-4220-8d7e-933c0118a983...\n",
      "04/12/2024 01:01:28 PM - INFO: 1000 records fetched...\n",
      "04/12/2024 01:01:31 PM - INFO: 2000 records fetched...\n",
      "04/12/2024 01:01:33 PM - INFO: 3000 records fetched...\n",
      "04/12/2024 01:01:35 PM - INFO: 4000 records fetched...\n",
      "04/12/2024 01:01:38 PM - INFO: 5000 records fetched...\n",
      "04/12/2024 01:01:41 PM - INFO: 6000 records fetched...\n",
      "04/12/2024 01:01:43 PM - INFO: 7000 records fetched...\n",
      "04/12/2024 01:01:46 PM - INFO: 8000 records fetched...\n",
      "04/12/2024 01:01:49 PM - INFO: 9000 records fetched...\n",
      "04/12/2024 01:01:52 PM - INFO: 10000 records fetched...\n",
      "04/12/2024 01:01:55 PM - INFO: 11000 records fetched...\n",
      "04/12/2024 01:01:58 PM - INFO: 12000 records fetched...\n",
      "04/12/2024 01:02:01 PM - INFO: 13000 records fetched...\n",
      "04/12/2024 01:02:04 PM - INFO: 14000 records fetched...\n",
      "04/12/2024 01:02:06 PM - INFO: 14416 records fetched...\n",
      "04/12/2024 01:02:07 PM - INFO: File retrieval complete!\n",
      "04/12/2024 01:02:07 PM - INFO: Retrieving files from dataset_id e5dc1a69-cb2c-4626-9799-6bb5fae7b147...\n",
      "04/12/2024 01:02:10 PM - INFO: 1000 records fetched...\n",
      "04/12/2024 01:02:13 PM - INFO: 2000 records fetched...\n",
      "04/12/2024 01:02:16 PM - INFO: 3000 records fetched...\n",
      "04/12/2024 01:02:20 PM - INFO: 4000 records fetched...\n",
      "04/12/2024 01:02:27 PM - INFO: 5000 records fetched...\n",
      "04/12/2024 01:02:30 PM - INFO: 6000 records fetched...\n",
      "04/12/2024 01:02:35 PM - INFO: 7000 records fetched...\n",
      "04/12/2024 01:02:43 PM - INFO: 8000 records fetched...\n",
      "04/12/2024 01:02:48 PM - INFO: 9000 records fetched...\n",
      "04/12/2024 01:02:55 PM - INFO: 10000 records fetched...\n",
      "04/12/2024 01:03:03 PM - INFO: 11000 records fetched...\n",
      "04/12/2024 01:03:11 PM - INFO: 12000 records fetched...\n",
      "04/12/2024 01:03:20 PM - INFO: 13000 records fetched...\n",
      "04/12/2024 01:03:30 PM - INFO: 14000 records fetched...\n",
      "04/12/2024 01:03:39 PM - INFO: 14416 records fetched...\n",
      "04/12/2024 01:03:46 PM - INFO: File retrieval complete!\n",
      "04/12/2024 01:03:46 PM - INFO: Results recorded:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset ID 1</th>\n",
       "      <th>Dataset ID 2</th>\n",
       "      <th>Validation Status</th>\n",
       "      <th>Validation Message</th>\n",
       "      <th>File Count Diff</th>\n",
       "      <th>Total File Size (Bytes) Diff</th>\n",
       "      <th>Max File Size (Bytes) Diff</th>\n",
       "      <th>File Count 1</th>\n",
       "      <th>Total File Size (Bytes) 1</th>\n",
       "      <th>Max File Size (Bytes) 1</th>\n",
       "      <th>Status 1</th>\n",
       "      <th>Message 1</th>\n",
       "      <th>File Count 2</th>\n",
       "      <th>Total File Size (Bytes) 2</th>\n",
       "      <th>Max File Size (Bytes) 2</th>\n",
       "      <th>Status 2</th>\n",
       "      <th>Message 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d306000b-88c1-4220-8d7e-933c0118a983</td>\n",
       "      <td>e5dc1a69-cb2c-4626-9799-6bb5fae7b147</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14416</td>\n",
       "      <td>493485549019</td>\n",
       "      <td>35643261478</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>14416</td>\n",
       "      <td>493485549019</td>\n",
       "      <td>35643261478</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset ID 1                          Dataset ID 2             Validation Status Validation Message  File Count Diff  Total File Size (Bytes) Diff  Max File Size (Bytes) Diff  File Count 1  Total File Size (Bytes) 1  Max File Size (Bytes) 1 Status 1  Message 1  File Count 2  Total File Size (Bytes) 2  Max File Size (Bytes) 2 Status 2  Message 2\n",
       "0  d306000b-88c1-4220-8d7e-933c0118a983  e5dc1a69-cb2c-4626-9799-6bb5fae7b147       Passed                                 0                       0                            0                  14416           493485549019               35643261478         Success                14416           493485549019               35643261478         Success          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated Validation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset ID 1</th>\n",
       "      <th>Dataset ID 2</th>\n",
       "      <th>Validation Status</th>\n",
       "      <th>Validation Message</th>\n",
       "      <th>File Count Diff</th>\n",
       "      <th>Total File Size (Bytes) Diff</th>\n",
       "      <th>Max File Size (Bytes) Diff</th>\n",
       "      <th>File Count 1</th>\n",
       "      <th>Total File Size (Bytes) 1</th>\n",
       "      <th>Max File Size (Bytes) 1</th>\n",
       "      <th>Status 1</th>\n",
       "      <th>Message 1</th>\n",
       "      <th>File Count 2</th>\n",
       "      <th>Total File Size (Bytes) 2</th>\n",
       "      <th>Max File Size (Bytes) 2</th>\n",
       "      <th>Status 2</th>\n",
       "      <th>Message 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8de6dae2-55ff-4287-9b75-5b2a950c1f44</td>\n",
       "      <td>e1fdd1b9-fe56-42ca-8e86-b4bb32d9bbce</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>3632191049</td>\n",
       "      <td>349377430</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>86</td>\n",
       "      <td>3632191049</td>\n",
       "      <td>349377430</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8</td>\n",
       "      <td>c7206e9a-78ad-4c9d-927f-3ca76646227d</td>\n",
       "      <td>Failed</td>\n",
       "      <td>Difference in counts between datasets.</td>\n",
       "      <td>25203</td>\n",
       "      <td>120403205483569</td>\n",
       "      <td>0</td>\n",
       "      <td>50408</td>\n",
       "      <td>259299049376285</td>\n",
       "      <td>111500944870</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>25205</td>\n",
       "      <td>138895843892716</td>\n",
       "      <td>111500944870</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d306000b-88c1-4220-8d7e-933c0118a983</td>\n",
       "      <td>e5dc1a69-cb2c-4626-9799-6bb5fae7b147</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14416</td>\n",
       "      <td>493485549019</td>\n",
       "      <td>35643261478</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td>14416</td>\n",
       "      <td>493485549019</td>\n",
       "      <td>35643261478</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset ID 1                          Dataset ID 2             Validation Status            Validation Message            File Count Diff  Total File Size (Bytes) Diff  Max File Size (Bytes) Diff  File Count 1  Total File Size (Bytes) 1  Max File Size (Bytes) 1 Status 1  Message 1  File Count 2  Total File Size (Bytes) 2  Max File Size (Bytes) 2 Status 2  Message 2\n",
       "0  8de6dae2-55ff-4287-9b75-5b2a950c1f44  e1fdd1b9-fe56-42ca-8e86-b4bb32d9bbce       Passed                                                        0                            0                     0                     86               3632191049               349377430         Success                   86               3632191049               349377430         Success          \n",
       "1  bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8  c7206e9a-78ad-4c9d-927f-3ca76646227d       Failed       Difference in counts between datasets.       25203              120403205483569                     0                  50408          259299049376285            111500944870         Success                25205          138895843892716            111500944870         Success          \n",
       "2  d306000b-88c1-4220-8d7e-933c0118a983  e5dc1a69-cb2c-4626-9799-6bb5fae7b147       Passed                                                        0                            0                     0                  14416             493485549019             35643261478         Success                14416             493485549019             35643261478         Success          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def collect_file_stats(dataset_id_pairs_list):\n",
    "    \n",
    "    results = []\n",
    "    for dataset_id_pair in dataset_id_pairs_list:\n",
    "\n",
    "            # Setup/refresh TDR clients\n",
    "            logging.info(f\"Processing dataset_id_pair: {dataset_id_pair}\")\n",
    "            api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "            # Initialize variables\n",
    "            dataset_id_1 = dataset_id_pair[0]\n",
    "            file_count_1 = 0\n",
    "            total_file_size_1 = 0\n",
    "            max_file_size_1 = 0\n",
    "            status_1 = \"Success\"\n",
    "            message_1 = \"\"\n",
    "            dataset_id_2 = dataset_id_pair[1]\n",
    "            file_count_2 = 0\n",
    "            total_file_size_2 = 0\n",
    "            max_file_size_2 = 0\n",
    "            status_2 = \"Success\"\n",
    "            message_2 = \"\"\n",
    "            validation_status = \"Passed\"\n",
    "            validation_message = \"\"\n",
    "\n",
    "            # For dataset_id_1, loop through dataset files and record information\n",
    "            logging.info(f\"Retrieving files from dataset_id {dataset_id_1}...\")\n",
    "            try:\n",
    "                max_page_size = 1000\n",
    "                total_records_fetched = 0\n",
    "                while True:\n",
    "                    row_start = total_records_fetched\n",
    "                    dataset_file_results = datasets_api.list_files(id=dataset_id_1, offset=row_start, limit=max_page_size)\n",
    "                    if dataset_file_results:\n",
    "                        total_records_fetched += len(dataset_file_results)\n",
    "                        for entry in dataset_file_results:\n",
    "                            file_count_1 += 1\n",
    "                            total_file_size_1 += entry.size\n",
    "                            if entry.size > max_file_size_1:\n",
    "                                max_file_size_1 = entry.size\n",
    "                        logging.info(f\"{total_records_fetched} records fetched...\")\n",
    "                    else:\n",
    "                        break\n",
    "                logging.info(f\"File retrieval complete!\")\n",
    "            except Exception as e:\n",
    "                status_1 = \"Failure\"\n",
    "                message_1 = str(e)\n",
    "                logging.error(f\"Failure in file retrieval: {message_1}\")\n",
    "            \n",
    "            # For dataset_id_2, loop through dataset files and record information\n",
    "            logging.info(f\"Retrieving files from dataset_id {dataset_id_2}...\")\n",
    "            try:\n",
    "                max_page_size = 1000\n",
    "                total_records_fetched = 0\n",
    "                while True:\n",
    "                    row_start = total_records_fetched\n",
    "                    dataset_file_results = datasets_api.list_files(id=dataset_id_2, offset=row_start, limit=max_page_size)\n",
    "                    if dataset_file_results:\n",
    "                        total_records_fetched += len(dataset_file_results)\n",
    "                        for entry in dataset_file_results:\n",
    "                            file_count_2 += 1\n",
    "                            total_file_size_2 += entry.size\n",
    "                            if entry.size > max_file_size_2:\n",
    "                                max_file_size_2 = entry.size\n",
    "                        logging.info(f\"{total_records_fetched} records fetched...\")\n",
    "                    else:\n",
    "                        break\n",
    "                logging.info(f\"File retrieval complete!\")\n",
    "            except Exception as e:\n",
    "                status_2 = \"Failure\"\n",
    "                message_2 = str(e)\n",
    "                logging.error(f\"Failure in file retrieval: {message_2}\")\n",
    "                \n",
    "            # Record and display interim results\n",
    "            file_count_diff = file_count_1 - file_count_2\n",
    "            total_file_size_diff = total_file_size_1 - total_file_size_2\n",
    "            max_file_size_diff = max_file_size_1 - max_file_size_2\n",
    "            if status_1 == \"Failure\" or status_2 == \"Failure\":\n",
    "                validation_status = \"Failed\"\n",
    "                validation_message = \"Errors pulling counts for one or more datasets.\"\n",
    "            elif file_count_diff > 0 or total_file_size_diff > 0 or max_file_size_diff > 0:\n",
    "                validation_status = \"Failed\"\n",
    "                validation_message = \"Difference in counts between datasets.\"\n",
    "            results.append([dataset_id_1, dataset_id_2, validation_status, validation_message, file_count_diff, total_file_size_diff, max_file_size_diff, file_count_1, total_file_size_1, max_file_size_1, status_1, message_1, file_count_2, total_file_size_2, max_file_size_2, status_2, message_2])\n",
    "            int_results_df = pd.DataFrame([[dataset_id_1, dataset_id_2, validation_status, validation_message, file_count_diff, total_file_size_diff, max_file_size_diff, file_count_1, total_file_size_1, max_file_size_1, status_1, message_1, file_count_2, total_file_size_2, max_file_size_2, status_2, message_2]], columns = [\"Dataset ID 1\", \"Dataset ID 2\", \"Validation Status\", \"Validation Message\", \"File Count Diff\", \"Total File Size (Bytes) Diff\", \"Max File Size (Bytes) Diff\", \"File Count 1\", \"Total File Size (Bytes) 1\", \"Max File Size (Bytes) 1\", \"Status 1 \", \"Message 1\", \"File Count 2\", \"Total File Size (Bytes) 2\", \"Max File Size (Bytes) 2\", \"Status 2 \", \"Message 2\"])\n",
    "            logging.info(\"Results recorded:\")\n",
    "            display(int_results_df)\n",
    "        \n",
    "    # Display final results\n",
    "    destination_dir = \"ingest_pipeline/resources/azure_migration\"\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    output_file = f\"validation_results_{current_datetime_string}.tsv\"\n",
    "    results_df = pd.DataFrame(results, columns = [\"Dataset ID 1\", \"Dataset ID 2\", \"Validation Status\", \"Validation Message\", \"File Count Diff\", \"Total File Size (Bytes) Diff\", \"Max File Size (Bytes) Diff\", \"File Count 1\", \"Total File Size (Bytes) 1\", \"Max File Size (Bytes) 1\", \"Status 1 \", \"Message 1\", \"File Count 2\", \"Total File Size (Bytes) 2\", \"Max File Size (Bytes) 2\", \"Status 2 \", \"Message 2\"])\n",
    "    results_df.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "    !gsutil cp $output_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "    !rm $output_file\n",
    "    print(\"\\nAggregated Validation Results:\")\n",
    "    display(results_df)   \n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of dataset IDs\n",
    "dataset_id_pairs_list = [\n",
    "    ['8de6dae2-55ff-4287-9b75-5b2a950c1f44', 'e1fdd1b9-fe56-42ca-8e86-b4bb32d9bbce'],\n",
    "    ['bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8', 'c7206e9a-78ad-4c9d-927f-3ca76646227d'],\n",
    "    ['d306000b-88c1-4220-8d7e-933c0118a983', 'e5dc1a69-cb2c-4626-9799-6bb5fae7b147'],\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "collect_file_stats(dataset_id_pairs_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrating Workspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pre-Connector Processing\n",
    "For each GCP Workspace - Azure Workspace pair:\n",
    "1. Build a manifest of files to be copied from the GCP Workspace to the Azure Workspace. \n",
    "2. Write the manifest to BigQuery for consumption by downstream processes.\n",
    "3. Add the appropriate SAs to the source and destination workspaces to facilitate the copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to create file transfer details\n",
    "def output_file_details(source_ws_project, source_ws_name, target_ws_project, target_ws_name, file_bigquery_table, target_bigquery_table, delete_existing_records):\n",
    "    \n",
    "    # Establish credentials and clients\n",
    "    client = bigquery.Client()\n",
    "    creds, project = google.auth.default(scopes=['https://www.googleapis.com/auth/cloud-platform', 'openid', 'email', 'profile'])\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "\n",
    "    # Pull bucket from source workspace\n",
    "    try:\n",
    "        ws_attributes = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{source_ws_project}/{source_ws_name}?fields=workspace.bucketName\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        ws_bucket = ws_attributes[\"workspace\"][\"bucketName\"]\n",
    "    except:\n",
    "        err_str = \"Error retrieving workspace attributes for source workspace.\"\n",
    "        logging.error(err_str)\n",
    "        raise Exception(err_str)\n",
    "\n",
    "    # Pull storage container from target workspace\n",
    "    try:\n",
    "        ws_attributes = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{target_ws_project}/{target_ws_name}?fields=workspace.workspaceId\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        ws_id = ws_attributes[\"workspace\"][\"workspaceId\"] \n",
    "        ws_resources = requests.get(\n",
    "            url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{ws_id}/resources?offset=0&limit=10&resource=AZURE_STORAGE_CONTAINER\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        resource_id = \"\"\n",
    "        for resource_entry in ws_resources[\"resources\"]:\n",
    "            if resource_entry[\"resourceAttributes\"][\"azureStorageContainer\"][\"storageContainerName\"][0:3] == \"sc-\":\n",
    "                resource_id = resource_entry[\"metadata\"][\"resourceId\"]\n",
    "                break\n",
    "        if resource_id:\n",
    "            sas_response = requests.post(\n",
    "                url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{ws_id}/resources/controlled/azure/storageContainer/{resource_id}/getSasToken?sasExpirationDuration=86400\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\", \"accept\": \"application/json\"}\n",
    "            ).json()\n",
    "            base_url = sas_response[\"url\"]\n",
    "            ws_storage_container = re.search(\"^[a-z0-9:\\/=\\-\\.]+\", base_url, re.IGNORECASE).group(0)\n",
    "        else:\n",
    "            err_str = \"Error retrieving resource information for target workspace.\"\n",
    "            logging.error(err_str)\n",
    "            raise Exception(err_str)\n",
    "    except:\n",
    "        err_str = \"Error retrieving workspace attributes for target workspace.\"\n",
    "        logging.error(err_str)\n",
    "        raise Exception(err_str)\n",
    "\n",
    "    # Clear records from target BQ table (if specified)\n",
    "    if delete_existing_records:\n",
    "        logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "        delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE gcp_ws_project = '{source_ws_project}' and gcp_ws_name = '{source_ws_name}'\"\"\"\n",
    "        try:\n",
    "            delete_query_job = client.query(delete_query)\n",
    "            delete_query_job.result()\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Error deleting records for the original dataset from the target BQ table.\") \n",
    "\n",
    "    # Write the query to pull files into a dataframe\n",
    "    logging.info(f\"Building manifest of files to copy from the source '{source_ws_project}.{source_ws_name}' workspace to the target '{target_ws_project}.{target_ws_name}' workspace.\")\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = \"WRITE_APPEND\"\n",
    "    query = f\"\"\"SELECT '{source_ws_project}' AS gcp_ws_project, '{source_ws_name}' AS gcp_ws_name, \n",
    "                '{target_ws_project}' AS az_ws_project, '{target_ws_name}' AS az_ws_name, \n",
    "                 'gs://{ws_bucket}/'||name AS source_path, '{ws_storage_container}/'||name AS target_path, \n",
    "                 size AS size_in_bytes, md5Hash AS md5_hash, '{current_datetime_string}' AS date_added\n",
    "                FROM `{file_bigquery_table}` \n",
    "                WHERE bucket = '{ws_bucket}'\n",
    "                AND name NOT LIKE '%/'\"\"\"\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            df = client.query(query).result().to_dataframe()\n",
    "            job = client.load_table_from_dataframe(df, target_bigquery_table, job_config=job_config)\n",
    "            logging.info(\"Records recorded successfully.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error building and writing file manifest: {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                raise Exception(err_str)\n",
    "    \n",
    "    # Add SAs where needed\n",
    "    pass\n",
    "\n",
    "            \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# General parameters\n",
    "file_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_inventory.object_metadata_26_02_2024__17_14_55\"\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list_workspaces\"\n",
    "\n",
    "# Specify migration pairs: Source GCP Workspace - Target Azure Workspace\n",
    "migration_list = [\n",
    "    #{\"gcp_ws_project\": \"anvil-datastorage\", \"gcp_ws_name\": \"<name>\", \"az_ws_project\": \"AnVILDataStorage_Azure\", \"az_ws_name\": \"<name>\"}\n",
    "    {\"gcp_ws_project\": \"anvil-datastorage\", \"gcp_ws_name\": \"AnVIL_GTEx_Deposit\", \"az_ws_project\": \"AnVILDataStorage_Azure\", \"az_ws_name\": \"AnVIL_GTEx_Deposit_Azure\"}\n",
    "]\n",
    "\n",
    "# Specify whether existing records in the azure_migration_file_list_workspaces table should be deleted before running\n",
    "delete_existing_records = True\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for entry in migration_list:\n",
    "    logging.info(f\"Processing Migration List Entry: {str(entry)}\")\n",
    "    try:\n",
    "        output_file_details(entry[\"gcp_ws_project\"], entry[\"gcp_ws_name\"], entry[\"az_ws_project\"], entry[\"az_ws_name\"], file_bigquery_table, target_bigquery_table, delete_existing_records)\n",
    "        results.append([entry[\"gcp_ws_name\"], entry[\"az_ws_name\"], \"Success\", \"\"])\n",
    "    except Exception as e:\n",
    "        results.append([entry[\"gcp_ws_name\"], entry[\"az_ws_name\"], \"Failure\", str(e)])\n",
    "        \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Source Workspace Name\", \"Target Workspace Name\", \"Status\", \"Message\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_ws_project = \"anvil-datastorage\"\n",
    "source_ws_name = \"AnVIL_GTEx_Deposit\"\n",
    "target_ws_project = \"AnVILDataStorage_Azure\"\n",
    "target_ws_name = \"AnVIL_GTEx_Deposit_Azure\"\n",
    "\n",
    "# ADD SA to resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Connector Processing\n",
    "1. Remove access for any SAs added in the pre-connector step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting Workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Dataset Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     12,
     23
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to delete a specific TDR Snapshot\n",
    "def delete_snapshot(snapshot_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete snapshot = {}\".format(snapshot_id))\n",
    "    try:\n",
    "        delete_snapshot_result, job_id = wait_for_tdr_job(snapshots_api.delete_snapshot(id=snapshot_id), \"https://data.terra.bio\")\n",
    "        print(\"Result: {}\".format(delete_snapshot_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset\n",
    "def delete_dataset(dataset_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {}\".format(dataset_id))\n",
    "    try:\n",
    "        delete_dataset_result, job_id = wait_for_tdr_job(datasets_api.delete_dataset(id=dataset_id), \"https://data.terra.bio\")\n",
    "        print(\"Result: {}\".format(delete_dataset_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset and all of its Snapshots\n",
    "def delete_dataset_and_all_snapshots(dataset_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {} and all associated snapshots\".format(dataset_id))\n",
    "    dataset_id_list = [dataset_id]\n",
    "    # Delete snapshots\n",
    "    snapshot_list = snapshots_api.enumerate_snapshots(dataset_ids=dataset_id_list)\n",
    "    if snapshot_list.items:\n",
    "        for snapshot in snapshot_list.items:\n",
    "            snapshot_id = str(snapshot.id)\n",
    "            delete_snapshot(snapshot_id)\n",
    "            sleep(10)\n",
    "    # Delete dataset\n",
    "    delete_dataset(dataset_id)\n",
    "\n",
    "# Delete snapshots\n",
    "# snapshot_id_list = [\n",
    "# '1234',\n",
    "# ]\n",
    "# for snapshot_id in snapshot_id_list:\n",
    "#     delete_snapshot(snapshot_id)\n",
    "\n",
    "# Delete datasets and all their associated snapshots\n",
    "dataset_id_list = [\n",
    "'1be5b5e6-019e-419a-9248-6e80d067d697',\n",
    "]\n",
    "for dataset_id in dataset_id_list:\n",
    "    delete_dataset_and_all_snapshots(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Update Migration File List Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# General parameters\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list\"\n",
    "\n",
    "# Update parameters\n",
    "update_list = [\n",
    "    {\"az_dataset_id\": \"6007151f-45bc-4111-8e9a-b667bc722a6a\", \"new_gcp_dataset_id\": \"b22c71b2-2cb2-4b27-a49b-9a2a83d432e8\", \"new_gcp_dataset_name\": \"ANVIL_1000G_PRIMED_data_model_20240301\"},\n",
    "    {\"az_dataset_id\": \"a28e4ab5-a07b-4316-b743-7f5f9cc88211\", \"new_gcp_dataset_id\": \"3a89c170-2939-4c12-9940-f32d96fa9e55\", \"new_gcp_dataset_name\": \"ANVIL_CMH_GAFK_GS_long_read_20240301\"}\n",
    "]\n",
    "\n",
    "# Execute updates\n",
    "client = bigquery.Client()\n",
    "for entry in update_list:\n",
    "    logging.info(f\"Running update for entry: {str(entry)}\")\n",
    "    az_dataset_id = entry[\"az_dataset_id\"]\n",
    "    gcp_dataset_id = entry[\"new_gcp_dataset_id\"]\n",
    "    gcp_dataset_name = entry[\"new_gcp_dataset_name\"]\n",
    "    update_query = f\"\"\"UPDATE `{target_bigquery_table}` \n",
    "                       SET gcp_dataset_id = '{gcp_dataset_id}', gcp_dataset_name = '{gcp_dataset_name}'\n",
    "                       WHERE az_dataset_id = '{az_dataset_id}'\"\"\"\n",
    "    try:\n",
    "        update_query_job = client.query(update_query)\n",
    "        update_query_job.result()\n",
    "        logging.info(\"Update complete.\")\n",
    "    except Exception as e:\n",
    "        logging.info(\"Error running update.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
