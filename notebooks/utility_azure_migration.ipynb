{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade data_repo_client\n",
    "# !wget https://aka.ms/downloadazcopy-v10-linux\n",
    "# !tar -xvf downloadazcopy-v10-linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import import_ipynb\n",
    "import data_repo_client\n",
    "import google.auth\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from time import sleep\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import ingest_pipeline_utilities as utils\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import subprocess\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import base64\n",
    "\n",
    "# Configure logging format\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)])\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Function to refresh TDR API client\n",
    "def refresh_tdr_api_client(host):\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    config = data_repo_client.Configuration()\n",
    "    config.host = host\n",
    "    config.access_token = creds.token\n",
    "    api_client = data_repo_client.ApiClient(configuration=config)\n",
    "    api_client.client_side_validation = False\n",
    "    return api_client\n",
    "\n",
    "# Function to wait for TDR job completion\n",
    "def wait_for_tdr_job(job_model, host):\n",
    "    result = job_model\n",
    "    print(\"TDR Job ID: \" + job_model.id)\n",
    "    counter = 0\n",
    "    job_state = \"UNKNOWN\"\n",
    "    while True:\n",
    "        # Re-establish credentials and API clients every 30 minutes\n",
    "        if counter == 0 or counter%180 == 0:\n",
    "            api_client = refresh_tdr_api_client(host)\n",
    "            jobs_api = data_repo_client.JobsApi(api_client=api_client)\n",
    "        # Check for TDR connectivity issues and raise exception if the issue persists\n",
    "        conn_err_counter = 0\n",
    "        while job_state == \"UNKNOWN\":\n",
    "            conn_err_counter += 1\n",
    "            if conn_err_counter >= 10:\n",
    "                raise Exception(\"Error interacting with TDR: {}\".format(result.status_code)) \n",
    "            elif result == None or result.status_code in [\"500\", \"502\", \"503\", \"504\"]:\n",
    "                sleep(10)\n",
    "                counter += 1\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        result = jobs_api.retrieve_job(job_model.id)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 5:\n",
    "                            attempt_counter += 1\n",
    "                            sleep(10)\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "            else:\n",
    "                job_state = \"KNOWN\"\n",
    "        # Check if job is still running, and sleep/re-check if so\n",
    "        if job_state == \"KNOWN\" and result.job_status == \"running\":\n",
    "            sleep(10)\n",
    "            counter += 1\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    result = jobs_api.retrieve_job(job_model.id)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 5:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "        # If job has returned as failed, confirm this is the correct state and retrieve result if so\n",
    "        elif job_state == \"KNOWN\" and result.job_status == \"failed\":\n",
    "            fail_counter = 0\n",
    "            while True:\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        result = jobs_api.retrieve_job(job_model.id)\n",
    "                        if result.job_status == \"failed\":\n",
    "                            fail_counter += 1\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 5:\n",
    "                            sleep(10)\n",
    "                            attempt_counter += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise Exception(\"Error retrieving job status from TDR: {}\".format(str(e)))\n",
    "                if fail_counter >= 3:\n",
    "                    try:\n",
    "                        fail_result = jobs_api.retrieve_job_result(job_model.id)\n",
    "                        raise Exception(\"Job \" + job_model.id + \" failed: \" + fail_result)\n",
    "                    except Exception as e:\n",
    "                        raise Exception(\"Job \" + job_model.id + \" failed: \" + str(e))\n",
    "        # If a job has returned as succeeded, retrieve result\n",
    "        elif job_state == \"KNOWN\" and result.job_status == \"succeeded\":\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return jobs_api.retrieve_job_result(job_model.id), job_model.id\n",
    "                except Exception as e:\n",
    "                    if attempt_counter < 3:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        return \"Job succeeded, but error retrieving job result: {}\".format(str(e)), job_model.id\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized job state: {}\".format(result.job_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrating TDR Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 1: Pre-Connector Processing\n",
    "For the list of GCP TDR datasets provided:\n",
    "1. Extract the schema\n",
    "2. Create an Azure TDR dataset using the extracted schema\n",
    "3. Build a manifest of files to be copied from the GCP dataset to the Azure dataset and write to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     14,
     145
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to build default target TDR dataset name\n",
    "def format_dataset_name(input_str):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "    input_str = input_str[:-9]\n",
    "    output_str = \"ANVIL_\" + re.sub(\"^ANVIL[_]?\", \"\", input_str, flags=re.IGNORECASE) + \"_\" + current_date_string\n",
    "    output_str = re.sub(\"[^a-zA-Z0-9_]\", \"_\", output_str)\n",
    "    return output_str\n",
    "\n",
    "# Function to create a new TDR dataset from an existing TDR dataset\n",
    "def create_dataset_from_dataset(src_tdr_object_uuid, tar_tdr_object_uuid, billing_profile):\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Retrieve original dataset details\n",
    "    logging.info(f\"Retrieving original dataset details from prod environment. UUID:  {src_tdr_object_uuid}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=src_tdr_object_uuid, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        orig_object_name = dataset_details[\"name\"]\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset {src_tdr_object_uuid} in TDR prod environment: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # If target dataset specified, retrieve name\n",
    "    if tar_tdr_object_uuid:\n",
    "        new_dataset_id = tar_tdr_object_uuid\n",
    "        logging.info(f\"Retrieving new dataset details from prod environment. UUID:  {tar_tdr_object_uuid}\")\n",
    "        try:\n",
    "            dataset_details = datasets_api.retrieve_dataset(id=tar_tdr_object_uuid, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "            new_object_name = dataset_details[\"name\"]\n",
    "        except Exception as e:\n",
    "            error_str = f\"Error retrieving details from dataset {tar_tdr_object_uuid} in TDR prod environment: {str(e)}\"\n",
    "            logging.error(error_str)\n",
    "            return None, None, None, None, None \n",
    "    else:\n",
    "        # Build new dataset schema\n",
    "        apply_anvil_transforms = True\n",
    "        new_schema_dict = {\"tables\": [], \"relationships\": [], \"assets\": []}\n",
    "        for table_entry in dataset_details[\"schema\"][\"tables\"]:\n",
    "            int_table_dict = table_entry.copy()\n",
    "            int_table_dict[\"primaryKey\"] = int_table_dict.pop(\"primary_key\")\n",
    "            for key in [\"partition_mode\", \"date_partition_options\", \"int_partition_options\", \"row_count\"]:\n",
    "                del int_table_dict[key]\n",
    "            for idx, column_entry in enumerate(table_entry[\"columns\"]):\n",
    "                if column_entry[\"datatype\"] == \"integer\":\n",
    "                    table_entry[\"columns\"][idx][\"datatype\"] = \"int64\"\n",
    "            if apply_anvil_transforms:\n",
    "                if table_entry[\"name\"] == \"file_inventory\":\n",
    "                    int_table_dict[\"columns\"].append({\"name\": \"orig_file_ref\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "                    int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "                elif \"anvil_\" not in table_entry[\"name\"]:\n",
    "                    int_table_dict[\"columns\"].append({\"name\": \"orig_datarepo_row_id\", \"datatype\": \"string\", \"array_of\": False, \"required\": False})\n",
    "            new_schema_dict[\"tables\"].append(int_table_dict)\n",
    "        for rel_entry in dataset_details[\"schema\"][\"relationships\"]:\n",
    "            int_rel_dict = rel_entry.copy()\n",
    "            int_rel_dict[\"from\"] = int_rel_dict.pop(\"_from\")\n",
    "            new_schema_dict[\"relationships\"].append(int_rel_dict)\n",
    "        for asset_entry in dataset_details[\"schema\"][\"assets\"]:\n",
    "            int_asset_dict = asset_entry.copy()\n",
    "            int_asset_dict[\"rootTable\"] = int_asset_dict.pop(\"root_table\")\n",
    "            int_asset_dict[\"rootColumn\"] = int_asset_dict.pop(\"root_column\")\n",
    "            new_schema_dict[\"assets\"].append(int_asset_dict)\n",
    "\n",
    "        # Retrieve original dataset policies\n",
    "        try:\n",
    "            dataset_policies = datasets_api.retrieve_dataset_policies(id=src_tdr_object_uuid).to_dict()\n",
    "            for policy in dataset_policies[\"policies\"]:\n",
    "                if policy[\"name\"] == \"steward\":\n",
    "                    stewards_list = policy[\"members\"]\n",
    "                elif policy[\"name\"] == \"custodian\":\n",
    "                    custodians_list = policy[\"members\"]\n",
    "                elif policy[\"name\"] == \"snapshot_creator\":\n",
    "                    snapshot_creators_list = policy[\"members\"]\n",
    "        except:\n",
    "            logging.info(\"Error retrieving original dataset policies. Skipping policy copy.\")\n",
    "            stewards_list = []\n",
    "            custodians_list = []\n",
    "            snapshot_creators_list = []\n",
    "        policies = {\n",
    "            \"stewards\": stewards_list,\n",
    "            \"custodians\": custodians_list,\n",
    "            \"snapshotCreators\": snapshot_creators_list\n",
    "        }\n",
    "\n",
    "        # Determine dataset properties\n",
    "        new_object_name = format_dataset_name(orig_object_name)\n",
    "        new_description = dataset_details[\"description\"] + f\"\\n\\nCopy of dataset {orig_object_name} from TDR prod.\"\n",
    "        self_hosted = False\n",
    "        dedicated_ingest_sa = False\n",
    "        phs_id = dataset_details[\"phs_id\"]\n",
    "        predictable_file_ids = dataset_details[\"predictable_file_ids\"]\n",
    "        secure_monitoring_enabled = dataset_details[\"secure_monitoring_enabled\"]\n",
    "        properties = dataset_details[\"properties\"]\n",
    "        tags = dataset_details[\"tags\"]\n",
    "\n",
    "        # Create new TDR dataset\n",
    "        logging.info(\"Submitting dataset creation request.\")\n",
    "        dataset_request = {\n",
    "            \"name\": new_object_name,\n",
    "            \"description\": new_description,\n",
    "            \"defaultProfileId\": billing_profile,\n",
    "            \"cloudPlatform\": \"azure\",\n",
    "            \"region\": \"southcentralus\",\n",
    "            \"phsId\": phs_id,\n",
    "            \"experimentalSelfHosted\": self_hosted,\n",
    "            \"experimentalPredictableFileIds\": predictable_file_ids,\n",
    "            \"dedicatedIngestServiceAccount\": dedicated_ingest_sa,\n",
    "            \"enableSecureMonitoring\": secure_monitoring_enabled,\n",
    "            \"properties\": properties,\n",
    "            \"tags\": tags,\n",
    "            \"policies\": policies,\n",
    "            \"schema\": new_schema_dict\n",
    "        }\n",
    "        attempt_counter = 1\n",
    "        while True:\n",
    "            try:\n",
    "                create_dataset_result, job_id = wait_for_tdr_job(datasets_api.create_dataset(dataset=dataset_request), \"https://data.terra.bio\")\n",
    "                logging.info(\"Dataset Creation succeeded: {}\".format(create_dataset_result))\n",
    "                new_dataset_id = create_dataset_result[\"id\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_str = f\"Error on Dataset Creation: {str(e)}\"\n",
    "                logging.error(error_str)\n",
    "                if attempt_counter < 3:\n",
    "                    logging.info(\"Retrying Dataset Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Exiting job.\")\n",
    "                    return None, None, None, None, None\n",
    "        \n",
    "    # Exit function\n",
    "    return orig_object_name, new_dataset_id, new_object_name, bq_project, bq_dataset\n",
    "\n",
    "# Function to create file transfer details\n",
    "def output_file_details(orig_dataset_id, orig_dataset_name, new_dataset_id, new_dataset_name, bq_project, bq_dataset, public_flag, target_bigquery_table, delete_existing_records):\n",
    "    \n",
    "    # Setup/refresh TDR clients (and BQ client)\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Clear records from target BQ table\n",
    "    if delete_existing_records:\n",
    "        logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "        delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE gcp_dataset_id = '{orig_dataset_id}'\"\"\"\n",
    "        try:\n",
    "            delete_query_job = client.query(delete_query)\n",
    "            delete_query_job.result()\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error deleting records for the original dataset from the target BQ table.\")\n",
    "    \n",
    "    # Retrieve table data from the original dataset and write to target BQ table\n",
    "    logging.info(f\"Fetching and recording all rows from table 'file_inventory' in the original dataset ({orig_dataset_id}). BQ Project = '{bq_project}' and BQ Dataset = '{bq_dataset}'.\")\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.destination = target_bigquery_table\n",
    "    job_config.write_disposition = \"WRITE_APPEND\"\n",
    "    query = f\"\"\"WITH drlh_deduped AS\n",
    "                (\n",
    "                  SELECT DISTINCT file_id, target_path, source_name \n",
    "                  FROM \n",
    "                  (\n",
    "                    SELECT *, ROW_NUMBER() OVER (PARTITION BY source_name ORDER BY load_time DESC) AS rn\n",
    "                    --SELECT *, ROW_NUMBER() OVER (PARTITION BY source_name, target_path ORDER BY load_time DESC) AS rn\n",
    "                    FROM `{bq_project}.{bq_dataset}.datarepo_load_history`\n",
    "                    WHERE state = \"succeeded\" \n",
    "                  )\n",
    "                  WHERE rn = 1\n",
    "                ),\n",
    "                file_records AS\n",
    "                (\n",
    "                  SELECT '{orig_dataset_id}' AS gcp_dataset_id, '{orig_dataset_name}' AS gcp_dataset_name, \n",
    "                  '{new_dataset_id}' AS az_dataset_id, '{new_dataset_name}' AS az_dataset_name, \n",
    "                  b.source_name AS source_path, b.target_path, a.size_in_bytes, a.md5_hash, a.file_ref AS orig_tdr_file_id,\n",
    "                  '{current_datetime_string}' AS date_added, '{public_flag}' AS public_flag, ROW_NUMBER() OVER (PARTITION BY a.file_ref ORDER BY b.source_name) AS rn\n",
    "                  FROM `{bq_project}.{bq_dataset}.file_inventory` a\n",
    "                      LEFT JOIN drlh_deduped b\n",
    "                      ON a.uri = b.source_name\n",
    "                      LEFT JOIN `broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list` c\n",
    "                      ON a.file_ref = c.orig_tdr_file_id AND c.az_dataset_id = '{new_dataset_id}'\n",
    "                  WHERE c.source_path IS NULL\n",
    "                )\n",
    "                SELECT * EXCEPT(rn)\n",
    "                FROM file_records\n",
    "                WHERE rn = 1\"\"\"\n",
    "    #print(query)\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            query_job = client.query(query, job_config=job_config)\n",
    "            query_job.result()\n",
    "            logging.info(\"Records recorded successfully.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error recording records for all rows of table 'file_inventory': {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                return\n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# General parameters\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list\"\n",
    "azure_billing_profile = \"9ee23bed-b46c-4561-9103-d2a723113f7f\"\n",
    "\n",
    "# Specify the list of datasets to process, leaving the target Azure dataset ID empty to create a new one\n",
    "migration_list = [\n",
    "    #[\"src_gcp_dataset_id\", \"tar_az_dataset_id\", \"open_access (Y/N)\"]\n",
    "    ['902596ce-714e-49b3-8271-f3dfece52309', 'e091028e-a6b1-4989-9477-498e7ea206f0', 'N'],\n",
    "]\n",
    "\n",
    "# Specify whether existing records in the azure_migration_file_list table should be deleted before running\n",
    "delete_existing_records = False\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for entry in migration_list:\n",
    "    logging.info(f\"Processing Migration List Entry: {str(entry)}\")\n",
    "    dataset_name, new_dataset_id, new_dataset_name, bq_project, bq_dataset = create_dataset_from_dataset(entry[0], entry[1], azure_billing_profile)\n",
    "    if new_dataset_id:\n",
    "        output_file_details(entry[0], dataset_name, new_dataset_id, new_dataset_name, bq_project, bq_dataset, entry[2], target_bigquery_table, delete_existing_records)\n",
    "        results.append([entry[0], dataset_name, \"Success\", new_dataset_id, new_dataset_name])\n",
    "    else:\n",
    "        results.append([entry[0], dataset_name, \"Failure\", new_dataset_id, new_dataset_name])\n",
    "        \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Source Dataset ID\", \"Source Dataset Name\", \"Status\", \"New Dataset ID\", \"New Dataset Name\"])\n",
    "display(results_df)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Post-Connector Processing\n",
    "For each GCP Dataset - Azure Dataset pair:\n",
    "1. Retrieve the source GCP Dataset for the Snapshot\n",
    "2. Extract, pre-process, and ingest tabular data from the GCP Dataset to the Azure Dataset\n",
    "3. Create a new Azure snapshot based on the GCP snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     5,
     117,
     252,
     532
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/16/2025 06:28:29 PM - INFO: \n",
      "Migrating tabular data from TDR dataset c6f3bd64-ea67-488f-904f-f0bdf6320b5c to TDR dataset fbc7f442-585f-4885-9e2e-bdb38425867d.\n",
      "06/16/2025 06:28:29 PM - INFO: Adding TDR general SA (datarepo-jade-api@terra-datarepo-production.iam.gserviceaccount.com) to original dataset: c6f3bd64-ea67-488f-904f-f0bdf6320b5c\n",
      "06/16/2025 06:28:30 PM - INFO: TDR SA added successfully.\n",
      "06/16/2025 06:28:30 PM - INFO: Retrieving dataset details from original dataset: c6f3bd64-ea67-488f-904f-f0bdf6320b5c\n",
      "06/16/2025 06:28:30 PM - INFO: Fetching existing datarepo_row_id crosswalk (if one exists).\n",
      "06/16/2025 06:28:30 PM - WARNING: No datarepo_row_id crosswalk file name 'c6f3bd64-ea67-488f-904f-f0bdf6320b5c_fbc7f442-585f-4885-9e2e-bdb38425867d_rowid_xwalk.json' found.\n",
      "06/16/2025 06:28:30 PM - INFO: Ordering tables and pulling current record counts for validation.\n",
      "06/16/2025 06:28:45 PM - INFO: Processing dataset ingestion requests.\n",
      "06/16/2025 06:28:45 PM - INFO: Processing dataset ingestion for table 'file_inventory'.\n",
      "06/16/2025 06:28:45 PM - INFO: Table 'file_inventory' contains fileref columns. Will use a chunk size of 40000 rows per ingestion request, to keep the number of file references per chunk below 40000.\n",
      "06/16/2025 06:28:45 PM - INFO: Fetching rows 1-8619 from table 'file_inventory' in the original dataset (c6f3bd64-ea67-488f-904f-f0bdf6320b5c).\n",
      "06/16/2025 06:28:50 PM - INFO: Submitting ingestion request to new dataset (fbc7f442-585f-4885-9e2e-bdb38425867d).\n",
      "TDR Job ID: G-QIADJeRp-MRj7o9f-nKQ\n",
      "06/16/2025 06:30:14 PM - INFO: Ingest succeeded: {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'file_inventory', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 8619, 'bad_row_count': 0, 'load_result': None}\n",
      "06/16/2025 06:30:14 PM - INFO: Fetching ingested records and building datarepo_row_id lookup for use in AnVIL transforms.\n",
      "06/16/2025 06:30:48 PM - INFO: Validating table 'file_inventory' in new dataset vs. original dataset.\n",
      "06/16/2025 06:30:48 PM - INFO: Processing dataset ingestion for table 'workspace_attributes'.\n",
      "06/16/2025 06:30:48 PM - INFO: Table 'workspace_attributes' does not contain fileref columns. Will use a chunk size of 250000 rows per ingestion request.\n",
      "06/16/2025 06:30:48 PM - INFO: Fetching rows 1-37 from table 'workspace_attributes' in the original dataset (c6f3bd64-ea67-488f-904f-f0bdf6320b5c).\n",
      "06/16/2025 06:30:51 PM - INFO: Submitting ingestion request to new dataset (fbc7f442-585f-4885-9e2e-bdb38425867d).\n",
      "TDR Job ID: XtTCtxl6S9K7C4j53YyGng\n",
      "06/16/2025 06:31:34 PM - INFO: Ingest succeeded: {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'workspace_attributes', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 37, 'bad_row_count': 0, 'load_result': None}\n",
      "06/16/2025 06:31:34 PM - INFO: Fetching ingested records and building datarepo_row_id lookup for use in AnVIL transforms.\n",
      "06/16/2025 06:31:39 PM - INFO: Validating table 'workspace_attributes' in new dataset vs. original dataset.\n",
      "06/16/2025 06:31:39 PM - INFO: Processing dataset ingestion for table 'anvil_file'.\n",
      "06/16/2025 06:31:39 PM - INFO: Table 'anvil_file' contains fileref columns. Will use a chunk size of 40000 rows per ingestion request, to keep the number of file references per chunk below 40000.\n",
      "06/16/2025 06:31:39 PM - INFO: Fetching rows 1-8619 from table 'anvil_file' in the original dataset (c6f3bd64-ea67-488f-904f-f0bdf6320b5c).\n",
      "06/16/2025 06:33:46 PM - INFO: FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\n",
      "06/16/2025 06:33:46 PM - INFO: Submitting ingestion request to new dataset (fbc7f442-585f-4885-9e2e-bdb38425867d).\n",
      "TDR Job ID: fOUZ23c2SBOCmYOfT1b1cQ\n",
      "06/16/2025 06:35:00 PM - INFO: Ingest succeeded: {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_file', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 8619, 'bad_row_count': 0, 'load_result': None}\n",
      "06/16/2025 06:35:04 PM - INFO: Validating table 'anvil_file' in new dataset vs. original dataset.\n",
      "06/16/2025 06:35:04 PM - INFO: Processing dataset ingestion for table 'anvil_biosample'.\n",
      "06/16/2025 06:35:04 PM - INFO: No records found for table 'anvil_biosample' in original dataset. Continuing to next table/record set.\n",
      "06/16/2025 06:35:04 PM - INFO: Processing dataset ingestion for table 'anvil_project'.\n",
      "06/16/2025 06:35:04 PM - INFO: Table 'anvil_project' does not contain fileref columns. Will use a chunk size of 250000 rows per ingestion request.\n",
      "06/16/2025 06:35:04 PM - INFO: Fetching rows 1-1 from table 'anvil_project' in the original dataset (c6f3bd64-ea67-488f-904f-f0bdf6320b5c).\n",
      "06/16/2025 06:35:06 PM - INFO: FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\n",
      "06/16/2025 06:35:06 PM - INFO: Submitting ingestion request to new dataset (fbc7f442-585f-4885-9e2e-bdb38425867d).\n",
      "TDR Job ID: Fk-csYdHRAWNfpzy9WWe1A\n",
      "06/16/2025 06:35:48 PM - INFO: Ingest succeeded: {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_project', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "06/16/2025 06:35:51 PM - INFO: Validating table 'anvil_project' in new dataset vs. original dataset.\n",
      "06/16/2025 06:35:51 PM - INFO: Processing dataset ingestion for table 'anvil_dataset'.\n",
      "06/16/2025 06:35:51 PM - INFO: Table 'anvil_dataset' does not contain fileref columns. Will use a chunk size of 250000 rows per ingestion request.\n",
      "06/16/2025 06:35:51 PM - INFO: Fetching rows 1-1 from table 'anvil_dataset' in the original dataset (c6f3bd64-ea67-488f-904f-f0bdf6320b5c).\n",
      "06/16/2025 06:35:53 PM - INFO: FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\n",
      "06/16/2025 06:35:53 PM - INFO: Submitting ingestion request to new dataset (fbc7f442-585f-4885-9e2e-bdb38425867d).\n",
      "TDR Job ID: be4YsI4dQp2zl2cvC5pLaw\n",
      "06/16/2025 06:36:35 PM - INFO: Ingest succeeded: {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_dataset', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "06/16/2025 06:36:37 PM - INFO: Validating table 'anvil_dataset' in new dataset vs. original dataset.\n",
      "06/16/2025 06:36:37 PM - INFO: Processing dataset ingestion for table 'anvil_alignmentactivity'.\n",
      "06/16/2025 06:36:37 PM - INFO: No records found for table 'anvil_alignmentactivity' in original dataset. Continuing to next table/record set.\n",
      "06/16/2025 06:36:37 PM - INFO: Processing dataset ingestion for table 'anvil_assayactivity'.\n",
      "06/16/2025 06:36:37 PM - INFO: No records found for table 'anvil_assayactivity' in original dataset. Continuing to next table/record set.\n",
      "06/16/2025 06:36:37 PM - INFO: Processing dataset ingestion for table 'anvil_activity'.\n",
      "06/16/2025 06:36:37 PM - INFO: Table 'anvil_activity' does not contain fileref columns. Will use a chunk size of 250000 rows per ingestion request.\n",
      "06/16/2025 06:36:37 PM - INFO: Fetching rows 1-1210 from table 'anvil_activity' in the original dataset (c6f3bd64-ea67-488f-904f-f0bdf6320b5c).\n",
      "06/16/2025 06:36:39 PM - INFO: FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\n",
      "06/16/2025 06:36:39 PM - INFO: Submitting ingestion request to new dataset (fbc7f442-585f-4885-9e2e-bdb38425867d).\n",
      "TDR Job ID: aFQt3XmpQ2e450DAjDnFQw\n",
      "06/16/2025 06:37:21 PM - INFO: Ingest succeeded: {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_activity', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 1210, 'bad_row_count': 0, 'load_result': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/16/2025 06:37:25 PM - INFO: Validating table 'anvil_activity' in new dataset vs. original dataset.\n",
      "06/16/2025 06:37:25 PM - INFO: Processing dataset ingestion for table 'anvil_antibody'.\n",
      "06/16/2025 06:37:25 PM - INFO: No records found for table 'anvil_antibody' in original dataset. Continuing to next table/record set.\n",
      "06/16/2025 06:37:25 PM - INFO: Processing dataset ingestion for table 'anvil_diagnosis'.\n",
      "06/16/2025 06:37:25 PM - INFO: No records found for table 'anvil_diagnosis' in original dataset. Continuing to next table/record set.\n",
      "06/16/2025 06:37:25 PM - INFO: Processing dataset ingestion for table 'anvil_sequencingactivity'.\n",
      "06/16/2025 06:37:25 PM - INFO: No records found for table 'anvil_sequencingactivity' in original dataset. Continuing to next table/record set.\n",
      "06/16/2025 06:37:25 PM - INFO: Processing dataset ingestion for table 'anvil_variantcallingactivity'.\n",
      "06/16/2025 06:37:25 PM - INFO: No records found for table 'anvil_variantcallingactivity' in original dataset. Continuing to next table/record set.\n",
      "06/16/2025 06:37:25 PM - INFO: Processing dataset ingestion for table 'anvil_donor'.\n",
      "06/16/2025 06:37:25 PM - INFO: No records found for table 'anvil_donor' in original dataset. Continuing to next table/record set.\n",
      "06/16/2025 06:37:25 PM - INFO: Migration Pipeline Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: file_inventory -- Rows: 1-8619</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'file_inventory', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 8619, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: file_inventory</td>\n",
       "      <td>Success</td>\n",
       "      <td>8619 records found in both new and original table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: workspace_attributes -- Rows: 1-37</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'workspace_attributes', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 37, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: workspace_attributes</td>\n",
       "      <td>Success</td>\n",
       "      <td>37 records found in both new and original table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_file -- Rows: 1-8619</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_file', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 8619, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: anvil_file</td>\n",
       "      <td>Success</td>\n",
       "      <td>8619 records found in both new and original table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_biosample</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table 'anvil_biosample' in original dataset. Continuing to next table/record set.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_project -- Rows: 1-1</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_project', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: anvil_project</td>\n",
       "      <td>Success</td>\n",
       "      <td>1 records found in both new and original table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_dataset -- Rows: 1-1</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_dataset', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: anvil_dataset</td>\n",
       "      <td>Success</td>\n",
       "      <td>1 records found in both new and original table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_alignmentactivity</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table 'anvil_alignmentactivity' in original dataset. Continuing to next table/record set.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_assayactivity</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table 'anvil_assayactivity' in original dataset. Continuing to next table/record set.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_activity -- Rows: 1-1210</td>\n",
       "      <td>Success</td>\n",
       "      <td>{'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_activity', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 1210, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Dataset Validation</td>\n",
       "      <td>Table: anvil_activity</td>\n",
       "      <td>Success</td>\n",
       "      <td>1210 records found in both new and original table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_antibody</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table 'anvil_antibody' in original dataset. Continuing to next table/record set.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_diagnosis</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table 'anvil_diagnosis' in original dataset. Continuing to next table/record set.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_sequencingactivity</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table 'anvil_sequencingactivity' in original dataset. Continuing to next table/record set.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_variantcallingactivity</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table 'anvil_variantcallingactivity' in original dataset. Continuing to next table/record set.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Dataset Ingestion</td>\n",
       "      <td>Table: anvil_donor</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>No records found for table 'anvil_donor' in original dataset. Continuing to next table/record set.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Task                           Step                     Status                                                                                                                                   Message                                                                                                                                 \n",
       "0    Dataset Ingestion      Table: file_inventory -- Rows: 1-8619  Success      {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'file_inventory', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 8619, 'bad_row_count': 0, 'load_result': None}\n",
       "1   Dataset Validation                      Table: file_inventory  Success                                                                                                                                                                                                                        8619 records found in both new and original table.\n",
       "2    Dataset Ingestion  Table: workspace_attributes -- Rows: 1-37  Success  {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'workspace_attributes', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 37, 'bad_row_count': 0, 'load_result': None}\n",
       "3   Dataset Validation                Table: workspace_attributes  Success                                                                                                                                                                                                                          37 records found in both new and original table.\n",
       "4    Dataset Ingestion          Table: anvil_file -- Rows: 1-8619  Success          {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_file', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 8619, 'bad_row_count': 0, 'load_result': None}\n",
       "5   Dataset Validation                          Table: anvil_file  Success                                                                                                                                                                                                                        8619 records found in both new and original table.\n",
       "6    Dataset Ingestion                     Table: anvil_biosample  Skipped                                                                                                                                                                    No records found for table 'anvil_biosample' in original dataset. Continuing to next table/record set.\n",
       "7    Dataset Ingestion          Table: anvil_project -- Rows: 1-1  Success          {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_project', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
       "8   Dataset Validation                       Table: anvil_project  Success                                                                                                                                                                                                                           1 records found in both new and original table.\n",
       "9    Dataset Ingestion          Table: anvil_dataset -- Rows: 1-1  Success          {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_dataset', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
       "10  Dataset Validation                       Table: anvil_dataset  Success                                                                                                                                                                                                                           1 records found in both new and original table.\n",
       "11   Dataset Ingestion             Table: anvil_alignmentactivity  Skipped                                                                                                                                                            No records found for table 'anvil_alignmentactivity' in original dataset. Continuing to next table/record set.\n",
       "12   Dataset Ingestion                 Table: anvil_assayactivity  Skipped                                                                                                                                                                No records found for table 'anvil_assayactivity' in original dataset. Continuing to next table/record set.\n",
       "13   Dataset Ingestion      Table: anvil_activity -- Rows: 1-1210  Success      {'dataset_id': 'fbc7f442-585f-4885-9e2e-bdb38425867d', 'dataset': 'ANVIL_GTEx_V9_hg38_20240221', 'table': 'anvil_activity', 'path': None, 'load_tag': 'Ingest for fbc7f442-585f-4885-9e2e-bdb38425867d', 'row_count': 1210, 'bad_row_count': 0, 'load_result': None}\n",
       "14  Dataset Validation                      Table: anvil_activity  Success                                                                                                                                                                                                                        1210 records found in both new and original table.\n",
       "15   Dataset Ingestion                      Table: anvil_antibody  Skipped                                                                                                                                                                     No records found for table 'anvil_antibody' in original dataset. Continuing to next table/record set.\n",
       "16   Dataset Ingestion                     Table: anvil_diagnosis  Skipped                                                                                                                                                                    No records found for table 'anvil_diagnosis' in original dataset. Continuing to next table/record set.\n",
       "17   Dataset Ingestion            Table: anvil_sequencingactivity  Skipped                                                                                                                                                           No records found for table 'anvil_sequencingactivity' in original dataset. Continuing to next table/record set.\n",
       "18   Dataset Ingestion        Table: anvil_variantcallingactivity  Skipped                                                                                                                                                       No records found for table 'anvil_variantcallingactivity' in original dataset. Continuing to next table/record set.\n",
       "19   Dataset Ingestion                         Table: anvil_donor  Skipped                                                                                                                                                                        No records found for table 'anvil_donor' in original dataset. Continuing to next table/record set."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/16/2025 06:37:25 PM - INFO: \n",
      "Pipeline finished with 0 failures.\n",
      "\n",
      "Final Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Source Dataset ID</th>\n",
       "      <th>Target Dataset ID</th>\n",
       "      <th>Processing Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "      <th>Snapshot ID</th>\n",
       "      <th>Snapshot Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>Data Ingestion</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Source Dataset ID                     Target Dataset ID           Processing Step  Status  Message Snapshot ID Snapshot Name\n",
       "0  c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d  Data Ingestion  Success                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to fetch data from BigQuery\n",
    "def fetch_source_records_bigquery(config, new_dataset_id, array_col_dict, table, start_row, end_row):\n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source_dataset_id\"]\n",
    "    src_tdr_object_type = \"dataset\"\n",
    "    tdr_host = config[\"tdr_host\"]\n",
    "    files_already_ingested = True\n",
    "    datarepo_row_ids_to_ingest = []\n",
    "    apply_anvil_transforms = True\n",
    "    bq_project = config[\"bigquery_project\"]\n",
    "    bq_dataset = config[\"bigquery_dataset\"]\n",
    "    \n",
    "    # Setup/refresh TDR clients (and BQ client)\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Retrieve table data from the original dataset\n",
    "    logging.info(f\"Fetching rows {str(start_row)}-{str(end_row)} from table '{table}' in the original {src_tdr_object_type} ({src_tdr_object_uuid}).\")\n",
    "    table_recs_str = f\"Table: {table} -- Rows: {str(start_row)}-{str(end_row)}\"\n",
    "    final_records = []\n",
    "    if apply_anvil_transforms and \"anvil_\" not in table:\n",
    "        if table == \"file_inventory\":\n",
    "            if files_already_ingested == False:\n",
    "                file_ref_sql = \"TO_JSON_STRING(STRUCT(source_name AS sourcePath, target_path AS targetPath, 'Ingest of '||source_name AS description, COALESCE(content_type, 'application/octet-stream') AS mimeType))\"\n",
    "            else:\n",
    "                file_ref_sql = \"file_ref\"\n",
    "            rec_fetch_query = f\"\"\"WITH drlh_deduped AS\n",
    "                            (\n",
    "                              SELECT DISTINCT file_id, target_path, source_name\n",
    "                              FROM `{bq_project}.{bq_dataset}.datarepo_load_history`\n",
    "                              WHERE state = \"succeeded\" \n",
    "                            )\n",
    "                            SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT datarepo_row_id, datarepo_row_id AS orig_datarepo_row_id, a.file_id, name, path, target_path AS uri, content_type, full_extension, size_in_bytes, crc32c, md5_hash, ingest_provenance,\n",
    "                              file_ref AS orig_file_ref, {file_ref_sql} AS file_ref,\n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}` a\n",
    "                                  LEFT JOIN drlh_deduped b\n",
    "                                  ON a.file_ref = b.file_id\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "        else:\n",
    "            rec_fetch_query = f\"\"\"SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT *, datarepo_row_id AS orig_datarepo_row_id,\n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}`\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "    else:\n",
    "        rec_fetch_query = f\"\"\"SELECT * EXCEPT(rownum)\n",
    "                            FROM\n",
    "                            (\n",
    "                              SELECT *, \n",
    "                              ROW_NUMBER() OVER (ORDER BY datarepo_row_id) AS rownum\n",
    "                              FROM `{bq_project}.{bq_dataset}.{table}`\n",
    "                            )\n",
    "                            WHERE rownum BETWEEN {start_row} AND {end_row}\"\"\"\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            df = client.query(rec_fetch_query).result().to_dataframe()\n",
    "            df = df.astype(object).where(pd.notnull(df),None)\n",
    "            for column in array_col_dict[table]:\n",
    "                df[column] = df[column].apply(lambda x: list(x))\n",
    "            if apply_anvil_transforms and table == \"file_inventory\" and files_already_ingested == False: \n",
    "                df[\"file_ref\"] = df.apply(lambda x: json.loads(x[\"file_ref\"].replace(\"\\'\", \"\\\"\")), axis=1)\n",
    "            final_records = df.to_dict(orient=\"records\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error retrieving records for rows {str(start_row)}-{str(end_row)} of table {table}: {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "                return {}\n",
    "    \n",
    "    # Filter retrieved data if necessary and return as dict of records\n",
    "    if final_records:\n",
    "        df_temp = pd.DataFrame.from_dict(final_records)\n",
    "        if datarepo_row_ids_to_ingest:\n",
    "            df_orig = df_temp[df_temp[\"datarepo_row_id\"].isin(datarepo_row_ids_to_ingest)].copy()\n",
    "        else:\n",
    "            df_orig = df_temp.copy()\n",
    "        del df_temp\n",
    "        df_orig.drop(columns=[\"datarepo_row_id\"], inplace=True, errors=\"ignore\")\n",
    "        df_orig = df_orig.astype(object).where(pd.notnull(df_orig),None)\n",
    "        records_orig = df_orig.to_dict(orient=\"records\")\n",
    "        if not records_orig:\n",
    "            msg_str = f\"No records found in rows {str(start_row)}-{str(end_row)} of table {table} after filtering based on datarepo_row_ids_to_ingest parameter. Continuing to next record set or table validation.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Skipped\", msg_str])\n",
    "            return records_orig\n",
    "        elif len(final_records) != len(records_orig):\n",
    "            logging.info(f\"Filtering records to ingest based on the datarepo_row_ids_to_ingest parameter. {str(len(records_orig))} of {str(len(final_records))} records to be ingested.\")\n",
    "            return records_orig\n",
    "        else:\n",
    "            return records_orig\n",
    "    else:\n",
    "        msg_str = f\"No records found for rows {str(start_row)}-{str(end_row)} of table {table} in original {src_tdr_object_type}. Continuing to next record set or table validation.\"\n",
    "        logging.info(msg_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Skipped\", msg_str])\n",
    "        return final_records\n",
    "\n",
    "# Function to process ingests for specific table\n",
    "def ingest_table_data(config, new_dataset_id, array_col_dict, data_type_col_dict, table, start_row, end_row):\n",
    "    \n",
    "    # Extract parameters from config\n",
    "    src_tdr_object_uuid = config[\"source_dataset_id\"]\n",
    "    src_tdr_object_type = \"dataset\"\n",
    "    tdr_host = config[\"tdr_host\"]\n",
    "    tar_tdr_billing_profile = config[\"tar_tdr_billing_profile\"]\n",
    "    records_processing_method = \"in_memory\"\n",
    "    write_to_cloud_platform = \"\"\n",
    "    apply_anvil_transforms = True\n",
    "    dr_row_id_xwalk = config[\"dr_row_id_xwalk\"]\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    \n",
    "    # Retrieve table data from the original dataset\n",
    "    table_recs_str = f\"Table: {table} -- Rows: {str(start_row)}-{str(end_row)}\"\n",
    "    records_orig = fetch_source_records_bigquery(config, new_dataset_id, array_col_dict, table, start_row, end_row)\n",
    "    if not records_orig:\n",
    "        return\n",
    "\n",
    "    # Pre-process records before ingest\n",
    "    if \"anvil_\" in table:\n",
    "        try:\n",
    "            # Pre-process records in AnVIL_ records to use new datarepo_row_ids in the source_datarepo_row_ids field\n",
    "            logging.info(\"FSS (anvil_%) table with ingest.apply_anvil_transforms parameter set to 'True'. Pre-processing records before submitting ingestion request.\")\n",
    "            records_processed = []\n",
    "            for record in records_orig:\n",
    "                int_record = record.copy()\n",
    "                new_dr_row_id_list = []\n",
    "                for row_id in int_record[\"source_datarepo_row_ids\"]:\n",
    "                    new_row_id = dr_row_id_xwalk.get(row_id)\n",
    "                    if new_row_id:\n",
    "                        new_dr_row_id_list.append(new_row_id)\n",
    "                    else:\n",
    "                        err_str = f\"Failure in pre-processing: row_id '{row_id}'' not found in datarepo_row_id crosswalk.\"\n",
    "                        logging.error(err_str)\n",
    "                        config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "                        return   \n",
    "                int_record[\"source_datarepo_row_ids\"] = new_dr_row_id_list\n",
    "                for fcol in data_type_col_dict[\"float\"][table]:\n",
    "                    if int_record[fcol]:\n",
    "                        int_record[fcol] = float(int_record[fcol])\n",
    "                for icol in data_type_col_dict[\"int\"][table]:\n",
    "                    if int_record[icol]:\n",
    "                        int_record[icol] = int(int_record[icol])\n",
    "                records_processed.append(int_record)\n",
    "        except Exception as e:\n",
    "            err_str = f\"Failure in pre-processing: {str(e)}\"\n",
    "            logging.error(err_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])\n",
    "            return\n",
    "    else:\n",
    "        records_processed = []\n",
    "        for record in records_orig:\n",
    "            int_record = record.copy()\n",
    "            for fcol in data_type_col_dict[\"float\"][table]:\n",
    "                if int_record[fcol]:\n",
    "                    int_record[fcol] = float(int_record[fcol])\n",
    "            for icol in data_type_col_dict[\"int\"][table]:\n",
    "                if int_record[icol]:\n",
    "                    int_record[icol] = int(int_record[icol])\n",
    "            records_processed.append(int_record)\n",
    "    \n",
    "    # Write out records to cloud, if specified by user\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        logging.info(f\"Writing records to a control file in the cloud.\")\n",
    "        if write_to_cloud_platform == \"gcp\":\n",
    "            control_file_path = write_records_to_gcp(config, table, records_processed)\n",
    "        else:\n",
    "            control_file_path = write_records_to_azure(config, table, records_processed)\n",
    "\n",
    "    # Build, submit, and monitor ingest request\n",
    "    logging.info(f\"Submitting ingestion request to new dataset ({new_dataset_id}).\")\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        ingest_request = {\n",
    "            \"table\": table,\n",
    "            \"profile_id\": tar_tdr_billing_profile,\n",
    "            \"ignore_unknown_values\": True,\n",
    "            \"resolve_existing_files\": True,\n",
    "            \"updateStrategy\": \"append\",\n",
    "            \"format\": \"json\",\n",
    "            \"load_tag\": \"Ingest for {}\".format(new_dataset_id),\n",
    "            \"path\": control_file_path\n",
    "        }        \n",
    "    else:\n",
    "        ingest_request = {\n",
    "            \"table\": table,\n",
    "            \"profile_id\": tar_tdr_billing_profile,\n",
    "            \"ignore_unknown_values\": True,\n",
    "            \"resolve_existing_files\": True,\n",
    "            \"updateStrategy\": \"append\",\n",
    "            \"format\": \"array\",\n",
    "            \"load_tag\": \"Ingest for {}\".format(new_dataset_id),\n",
    "            \"records\": records_processed\n",
    "        }\n",
    "    attempt_counter = 1\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = refresh_tdr_api_client(tdr_host)\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = wait_for_tdr_job(datasets_api.ingest_dataset(id=new_dataset_id, ingest=ingest_request), tdr_host)\n",
    "            logging.info(\"Ingest succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Success\", str(ingest_request_result)[0:1000]])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on ingest: {}\".format(str(e)[0:2500]))\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Logging error.\")\n",
    "                err_str = f\"Error on ingest: {str(e)[0:2500]}\"\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", table_recs_str, \"Failure\", err_str])  \n",
    "                break\n",
    "\n",
    "    # Remove control file from cloud, if written out\n",
    "    if records_processing_method == \"write_to_cloud\":\n",
    "        logging.info(f\"Removing control file from the cloud.\")\n",
    "        if write_to_cloud_platform == \"gcp\":\n",
    "            client = storage.Client()\n",
    "            target_bucket = control_file_path.split(\"/\")[2]\n",
    "            target_object = \"/\".join(control_file_path.split(\"/\")[3:])\n",
    "            bucket = client.bucket(target_bucket)\n",
    "            blob = bucket.blob(target_object)\n",
    "            blob.delete()\n",
    "        else:\n",
    "            blob = BlobClient.from_blob_url(control_file_path)\n",
    "            blob.delete_blob()\n",
    "\n",
    "# Function to orchestration the migration of tabular data\n",
    "def migrate_tabular_data(config):\n",
    "\n",
    "    # Extract parameters from config\n",
    "    source_dataset_id = config[\"source_dataset_id\"]\n",
    "    target_dataset_id = config[\"target_dataset_id\"] \n",
    "    tables_to_ingest = config[\"tables_to_ingest\"] \n",
    "    tdr_host = config[\"tdr_host\"] \n",
    "    tdr_sa_to_use = config[\"tdr_sa_to_use\"] \n",
    "    chunk_size = config[\"chunk_size\"] \n",
    "    max_combined_rec_ref_size = config[\"max_combined_rec_ref_size\"] \n",
    "    skip_ingests = config[\"skip_ingests\"]\n",
    "\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "\n",
    "    # Add TDR SA to original dataset\n",
    "    logging.info(f\"Adding TDR general SA ({tdr_sa_to_use}) to original dataset: {source_dataset_id}\")\n",
    "    try:\n",
    "        resp = datasets_api.add_dataset_policy_member(id=source_dataset_id, policy_name=\"steward\", policy_member={\"email\": tdr_sa_to_use}) \n",
    "        logging.info(\"TDR SA added successfully.\")\n",
    "    except:\n",
    "        error_str = f\"Error adding TDR SA to dataset {source_dataset_id}: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "        return\n",
    "\n",
    "    # Collect details from original dataset to build inventory of tables to migrate\n",
    "    logging.info(f\"Retrieving dataset details from original dataset: {source_dataset_id}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=source_dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        config[\"bigquery_project\"] = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        config[\"bigquery_dataset\"] = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        fileref_col_dict = {}\n",
    "        array_col_dict = {}\n",
    "        data_type_col_dict = {}\n",
    "        float_col_dict = {}\n",
    "        int_col_dict = {}\n",
    "        for table_entry in dataset_details[\"schema\"][\"tables\"]:\n",
    "            fileref_list = []\n",
    "            array_list = []\n",
    "            float_list = []\n",
    "            int_list = []\n",
    "            for idx, column_entry in enumerate(table_entry[\"columns\"]):\n",
    "                if column_entry[\"datatype\"] == \"fileref\":\n",
    "                    fileref_list.append(column_entry[\"name\"])\n",
    "                elif column_entry[\"datatype\"] == \"float\":\n",
    "                    float_list.append(column_entry[\"name\"])\n",
    "                elif column_entry[\"datatype\"] == \"integer\":\n",
    "                    int_list.append(column_entry[\"name\"])\n",
    "                if column_entry[\"array_of\"] == True:\n",
    "                    array_list.append(column_entry[\"name\"])\n",
    "            fileref_col_dict[table_entry[\"name\"]] = fileref_list\n",
    "            array_col_dict[table_entry[\"name\"]] = array_list\n",
    "            float_col_dict[table_entry[\"name\"]] = float_list\n",
    "            data_type_col_dict[\"float\"] = float_col_dict\n",
    "            int_col_dict[table_entry[\"name\"]] = int_list\n",
    "            data_type_col_dict[\"int\"] = int_col_dict\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset {source_dataset_id}: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "        config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "        return\n",
    "\n",
    "    # Read in existing datarepo_row_id crosswalk, if one exists\n",
    "    logging.info(\"Fetching existing datarepo_row_id crosswalk (if one exists).\")\n",
    "    xwalk_json_file_name = f\"{source_dataset_id}_{target_dataset_id}_rowid_xwalk.json\"\n",
    "    try:\n",
    "        with open(xwalk_json_file_name,\"r\") as file:\n",
    "            datarepo_row_id_xwalk = json.load(file)\n",
    "    except:\n",
    "        datarepo_row_id_xwalk = {}\n",
    "        logging.warning(f\"No datarepo_row_id crosswalk file name '{xwalk_json_file_name}' found.\")\n",
    "\n",
    "    # Order tables for ingestion\n",
    "    logging.info(\"Ordering tables and pulling current record counts for validation.\")\n",
    "    table_rank_dict = {}\n",
    "    for table in fileref_col_dict.keys():\n",
    "        if table == \"file_inventory\":\n",
    "            table_rank_dict[table] = 1\n",
    "        elif \"anvil_\" not in table:\n",
    "            table_rank_dict[table] = 2\n",
    "        else:\n",
    "            table_rank_dict[table] = 3\n",
    "    ordered_table_list = sorted(table_rank_dict, key= lambda key: table_rank_dict[key])\n",
    "\n",
    "    # Fetch total record counts for all tables\n",
    "    populated_table_dict = {}\n",
    "    for table in ordered_table_list:\n",
    "        api_client = refresh_tdr_api_client(tdr_host)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": 0,\n",
    "              \"limit\": 10,\n",
    "              \"sort\": \"datarepo_row_id\",\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=source_dataset_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                total_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 2:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    total_record_count = -1\n",
    "                    break\n",
    "        if total_record_count == -1:\n",
    "            error_str = f\"Error retrieving current record counts for tables in dataset {source_dataset_id}: {str(e)}\"\n",
    "            logging.error(error_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", \"All Tables\", \"Failure\", error_str])\n",
    "            return\n",
    "        elif total_record_count > 0:\n",
    "            populated_table_dict[table] = total_record_count\n",
    "\n",
    "    # Loop through and process tables for ingestion\n",
    "    logging.info(\"Processing dataset ingestion requests.\")\n",
    "    pop_fss_table_cnt = 0\n",
    "    for table in ordered_table_list:\n",
    "\n",
    "        # Determine whether table should be processed, and skip if not\n",
    "        logging.info(f\"Processing dataset ingestion for table '{table}'.\")\n",
    "        total_record_count = 0\n",
    "        if tables_to_ingest and table not in tables_to_ingest:\n",
    "            msg_str = f\"Table '{table}' not listed in the tables_to_ingest parameter. Skipping.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "            continue\n",
    "        elif table not in populated_table_dict.keys():\n",
    "            msg_str = f\"No records found for table '{table}' in original dataset. Continuing to next table/record set.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "            continue\n",
    "        elif \"anvil_\" in table:\n",
    "            # Confirm all non-FSS tables are present in datarepo_row_id_xwalk\n",
    "            pop_fss_table_cnt += 1\n",
    "            missing_tab_list = []\n",
    "            for tab in populated_table_dict.keys():\n",
    "                if \"anvil_\" not in tab and tab not in datarepo_row_id_xwalk.keys():\n",
    "                    missing_tab_list.append(tab)\n",
    "            if len(missing_tab_list) > 0:\n",
    "                missing_tab_string = \", \".join(missing_tab_list)\n",
    "                msg_str = f\"Populated non-FSS tables missing from datarepo_row_id crosswalk: {missing_tab_string}. Skipping FSS table '{table}'.\"\n",
    "                logging.info(msg_str)\n",
    "                config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "                continue\n",
    "        \n",
    "        # Aggregate datarepo_row_id crosswalk information for use in FSS table processing\n",
    "        if pop_fss_table_cnt == 1:\n",
    "            dr_row_id_xwalk = {}\n",
    "            for key in datarepo_row_id_xwalk.keys():\n",
    "                dr_row_id_xwalk.update(datarepo_row_id_xwalk[key])\n",
    "            config[\"dr_row_id_xwalk\"] = dr_row_id_xwalk \n",
    "            \n",
    "        # Chunk table records as necessary, then loop through and process each chunk\n",
    "        total_record_count = populated_table_dict.get(table)\n",
    "        if skip_ingests:\n",
    "            msg_str = f\"Parameter 'skip_ingests' set to true. Skipping ingestion for table '{table}'.\"\n",
    "            logging.info(msg_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Ingestion\", f\"Table: {table}\", \"Skipped\", msg_str])\n",
    "        else:\n",
    "            if fileref_col_dict[table]:\n",
    "                ref_chunk_size = math.floor(max_combined_rec_ref_size / len(fileref_col_dict[table]))\n",
    "                table_chunk_size = min(chunk_size, ref_chunk_size)\n",
    "                logging.info(f\"Table '{table}' contains fileref columns. Will use a chunk size of {table_chunk_size} rows per ingestion request, to keep the number of file references per chunk below {max_combined_rec_ref_size}.\")\n",
    "            else:\n",
    "                table_chunk_size = chunk_size\n",
    "                logging.info(f\"Table '{table}' does not contain fileref columns. Will use a chunk size of {table_chunk_size} rows per ingestion request.\")\n",
    "            start_row = 1\n",
    "            end_row = min((table_chunk_size), total_record_count)\n",
    "            while start_row <= total_record_count:\n",
    "                if end_row > total_record_count:\n",
    "                    end_row = total_record_count\n",
    "                ingest_table_data(config, target_dataset_id, array_col_dict, data_type_col_dict, table, start_row, end_row)    \n",
    "                start_row += table_chunk_size\n",
    "                end_row += table_chunk_size\n",
    "\n",
    "        # Build datarepo_row_id crosswalk for the table, add to datarepo_row_id_xwalk dict, and write out updated dict to file\n",
    "        if \"anvil_\" not in table: \n",
    "            logging.info(\"Fetching ingested records and building datarepo_row_id lookup for use in AnVIL transforms.\")\n",
    "            temp_dr_xwalk = {}\n",
    "            api_client = refresh_tdr_api_client(tdr_host)\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            max_page_size = 1000\n",
    "            records_fetched = 0\n",
    "            retrieval_error = False\n",
    "            while records_fetched < total_record_count and not retrieval_error:\n",
    "                row_start = records_fetched\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    payload = {\n",
    "                      \"offset\": row_start,\n",
    "                      \"limit\": max_page_size,\n",
    "                      \"sort\": \"datarepo_row_id\",\n",
    "                      \"direction\": \"asc\",\n",
    "                      \"filter\": \"\"\n",
    "                    }\n",
    "                    try:\n",
    "                        dataset_results = datasets_api.query_dataset_data_by_id(id=target_dataset_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                        if len(dataset_results[\"result\"]) == 0:\n",
    "                            warn_str = f\"No records found for '{table}' table, which prevents the proper building of the datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream.\"\n",
    "                            logging.warning(warn_str)\n",
    "                            retrieval_error = True\n",
    "                            break  \n",
    "                        else:\n",
    "                            for record in dataset_results[\"result\"]:\n",
    "                                key = table + \":\" + record[\"orig_datarepo_row_id\"]\n",
    "                                val = table + \":\" + record[\"datarepo_row_id\"]\n",
    "                                temp_dr_xwalk[key] = val\n",
    "                                records_fetched += 1\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        if attempt_counter < 2:\n",
    "                            sleep(10)\n",
    "                            attempt_counter += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            warn_str = f\"Error retrieving records for '{table}' table to build datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream. Error: {str(e)}\"\n",
    "                            logging.warning(warn_str)\n",
    "                            retrieval_error = True\n",
    "                            break\n",
    "            if not retrieval_error:\n",
    "                datarepo_row_id_xwalk[table] = temp_dr_xwalk\n",
    "                with open(xwalk_json_file_name, 'w') as file:\n",
    "                    json.dump(datarepo_row_id_xwalk, file)\n",
    "        \n",
    "        # Fetch total record count for the new table\n",
    "        api_client = refresh_tdr_api_client(tdr_host)\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": 0,\n",
    "              \"limit\": 10,\n",
    "              \"sort\": \"datarepo_row_id\",\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=target_dataset_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                new_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 2:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    new_record_count = -1\n",
    "                    break\n",
    "        if new_record_count == -1:\n",
    "            err_str = f\"Error retrieving record count for table '{table}' in new dataset. Skipping validation and continuing to next table.\"\n",
    "            logging.error(err_str)\n",
    "            config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Failure\", err_str])\n",
    "            continue \n",
    "\n",
    "        # Validate the new table against the old table, with extra scrutiny given to the file_inventory table for AnVIL migrations\n",
    "        logging.info(f\"Validating table '{table}' in new dataset vs. original dataset.\")\n",
    "        if new_record_count == total_record_count:\n",
    "            config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Success\", f\"{new_record_count} records found in both new and original table.\"])\n",
    "        else:\n",
    "            config[\"migration_results\"].append([\"Dataset Validation\", f\"Table: {table}\", \"Failure\", f\"{new_record_count} records found in new table doesn't match {total_record_count} records in original table.\"])\n",
    "\n",
    "    # Display results\n",
    "    pipeline_results = pd.DataFrame(config[\"migration_results\"], columns = [\"Task\", \"Step\", \"Status\", \"Message\"])\n",
    "    failures = pipeline_results[pipeline_results[\"Status\"].str.contains(\"Failure\")]\n",
    "    logging.info(\"Migration Pipeline Results:\")\n",
    "    display(pipeline_results)\n",
    "    logging.info(f\"\\nPipeline finished with {len(failures)} failures.\")\n",
    "    return len(failures)\n",
    "\n",
    "# Function for creating a snapshot for the new dataset\n",
    "def recreate_snapshot(config):\n",
    "    \n",
    "    # Extract parameters from config\n",
    "    target_dataset_id = config[\"target_dataset_id\"] \n",
    "    azure_billing_profile = config[\"azure_billing_profile\"] \n",
    "    tdr_host = config[\"tdr_host\"] \n",
    "    anvil_schema = config[\"anvil_schema\"] \n",
    "    \n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client(tdr_host)\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    \n",
    "    # Retrieve new dataset details\n",
    "    logging.info(f\"Retrieving dataset details from prod environment. UUID:  {target_dataset_id}\")\n",
    "    try:\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=target_dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\", \"DATA_PROJECT\", \"STORAGE\"]).to_dict()\n",
    "        dataset_name = dataset_details[\"name\"]\n",
    "        phs_id = dataset_details[\"phs_id\"]\n",
    "        consent_name = dataset_details[\"properties\"][\"consent_name\"]\n",
    "        auth_domains = dataset_details[\"properties\"][\"auth_domains\"]\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "        snapshot_name = dataset_name + \"_\" + anvil_schema + \"_\" + current_datetime_string\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from dataset: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "    # Build config and submit snapshot job\n",
    "    snapshot_config = {\n",
    "        \"profile_id\": azure_billing_profile,\n",
    "        \"snapshot_readers_list\": [\"azul-anvil-prod@firecloud.org\", \"auth-domain\"],\n",
    "        \"anvil_schema_versin\": anvil_schema,\n",
    "        \"ws_bucket\": os.environ[\"WORKSPACE_BUCKET\"],\n",
    "        \"dataset_id\": entry[1],\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"phs_id\": phs_id,\n",
    "        \"consent_name\": consent_name,\n",
    "        \"auth_domains\": auth_domains,\n",
    "        \"pipeline_results\": [],\n",
    "        \"snapshot_name\": snapshot_name\n",
    "    }\n",
    "    utils.create_and_share_snapshot(snapshot_config)\n",
    "    int_df_results = pd.DataFrame(snapshot_config[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.error(\"Errors reported in snapshotting. See logs for details.\")\n",
    "        status = \"Failure\"\n",
    "        message = f\"{len(errors)} failures reported. See log for details.\"\n",
    "        snapshot_id = \"\"\n",
    "        snapshot_name = \"\"\n",
    "    else:\n",
    "        status = \"Success\"\n",
    "        message = \"\"\n",
    "        snapshot_id = re.search(\"{'id': '([a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12})'\", str(int_df_results[int_df_results[\"Task\"]==\"Create and Share Snapshot\"][\"Message\"]))[1]\n",
    "        snapshot_name = re.search(\"'name': '([a-zA-Z0-9_\\-]+)'\", str(int_df_results[int_df_results[\"Task\"]==\"Create and Share Snapshot\"][\"Message\"]))[1]\n",
    "    return status, message, snapshot_id, snapshot_name\n",
    "        \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify migration pairs: [Source GCP Dataset, Target Azure Dataset]\n",
    "migration_list = [\n",
    "    #[\"gcp_dataset_id\", \"az_dataset_id\"]\n",
    "    ['c6f3bd64-ea67-488f-904f-f0bdf6320b5c', 'fbc7f442-585f-4885-9e2e-bdb38425867d'],\n",
    "]\n",
    "\n",
    "# Run parameters\n",
    "azure_billing_profile = \"9ee23bed-b46c-4561-9103-d2a723113f7f\"\n",
    "anvil_schema = \"ANV5\"\n",
    "run_data_migration = Fa\n",
    "skip_ingests = False # Set to True to build datarepo_row_id xwalk and run validation w/o ingesting more records\n",
    "#tables_to_ingest = [\"anvil_biosample\", \"anvil_dataset\", \"anvil_donor\", \"anvil_file\", \"anvil_project\"] # Leave empty for all\n",
    "tables_to_ingest = []\n",
    "run_snapshot_creation = False\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Set up logging\n",
    "current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "logs_stream_file_path = \"processing_details_\" + current_datetime_string + \".log\"\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.FileHandler(logs_stream_file_path), logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for entry in migration_list:\n",
    "    \n",
    "    # Run cross-cloud ingestion, if specified\n",
    "    failure_count = 0\n",
    "    if run_data_migration:\n",
    "        logging.info(f\"\\nMigrating tabular data from TDR dataset {entry[0]} to TDR dataset {entry[1]}.\")\n",
    "        # Build config and submit migration job\n",
    "        config = {\n",
    "            \"source_dataset_id\": entry[0], \n",
    "            \"target_dataset_id\": entry[1],\n",
    "            \"tables_to_ingest\": tables_to_ingest,\n",
    "            \"tdr_host\": \"https://data.terra.bio\",\n",
    "            \"tdr_sa_to_use\": \"datarepo-jade-api@terra-datarepo-production.iam.gserviceaccount.com\",\n",
    "            \"tar_tdr_billing_profile\": azure_billing_profile,\n",
    "            \"chunk_size\": 250000,\n",
    "            \"max_combined_rec_ref_size\": 40000,\n",
    "            \"migration_results\": [],\n",
    "            \"dr_row_id_xwalk\": {},\n",
    "            \"skip_ingests\": skip_ingests\n",
    "        }\n",
    "        failure_count = migrate_tabular_data(config)\n",
    "        status = \"Failure\" if failure_count > 0 else \"Success\"\n",
    "        msg = f\"{failure_count} failures reported. See log for details.\" if failure_count > 0 else \"\"\n",
    "        results.append([entry[0], entry[1], \"Data Ingestion\", status, msg, \"\", \"\"])\n",
    "\n",
    "    # Run snapshotting, if specified and no upstream errors detected\n",
    "    if run_snapshot_creation:\n",
    "        logging.info(f\"Creating a snapshot for TDR dataset {entry[1]}.\")\n",
    "        # Build config and submit snapshot job\n",
    "        config = { \n",
    "            \"target_dataset_id\": entry[1],\n",
    "            \"tdr_host\": \"https://data.terra.bio\",\n",
    "            \"azure_billing_profile\": azure_billing_profile,\n",
    "            \"anvil_schema\": anvil_schema\n",
    "        }\n",
    "        if failure_count > 0:\n",
    "            logging.error(\"Failures noted in upstream data processing. Skipping snapshotting.\")\n",
    "            results.append([entry[0], entry[1], \"Data Snapshotting\", \"Skipped\", \"Failures noted in upstream data processing.\", \"\", \"\"])\n",
    "        else:\n",
    "            status, message, snapshot_id, snapshot_name = recreate_snapshot(config)\n",
    "            results.append([entry[0], entry[1], \"Data Snapshotting\", status, message, snapshot_id, snapshot_name])\n",
    "            \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Source Dataset ID\", \"Target Dataset ID\", \"Processing Step\", \"Status\", \"Message\", \"Snapshot ID\", \"Snapshot Name\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Manual Ingest Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tdr_host = \"https://data.terra.bio\"\n",
    "api_client = refresh_tdr_api_client(tdr_host)\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "float_col_dict = {}\n",
    "float_col_dict[\"sample\"] = ['fold_80_base_penalty', 'fold_enrichment', 'het_snp_sensitivity', 'library_1_mean_insert_size', 'library_1_pct_exc_dupe', 'library_1_percent_duplication', 'mean_bait_coverage', 'mean_insert_size', 'mean_target_coverage', 'on_bait_vs_selected', 'pct_chimeras', 'pct_contamination', 'pct_exc_baseq', 'pct_exc_dupe', 'pct_exc_mapq', 'pct_exc_off_target', 'pct_exc_overlap', 'pct_off_bait', 'pct_pf_reads_aligned', 'pct_reads_aligned_in_pairs', 'pct_selected_bases', 'pct_target_bases_100x', 'pct_target_bases_10x', 'pct_target_bases_20x', 'pct_target_bases_2x', 'pct_target_bases_30x', 'pct_target_bases_50x', 'pct_usable_bases_on_bait', 'pct_usable_bases_on_target', 'pf_hq_error_rate', 'strand_balance', 'zero_cvg_targets_pct', 'library_2_mean_insert_size', 'library_2_pct_exc_dupe', 'library_2_percent_duplication']\n",
    "table = \"sample\"\n",
    "\n",
    "# Pull ingested samples\n",
    "payload = {\n",
    "  \"offset\": 0,\n",
    "  \"limit\": 1000,\n",
    "  \"sort\": \"datarepo_row_id\",\n",
    "  \"direction\": \"asc\",\n",
    "  \"filter\": \"\"\n",
    "}\n",
    "ingested_records = datasets_api.query_dataset_data_by_id(id=\"5f4ece3e-d76e-4d78-99e0-e62a24cd163d\", table=table, query_data_request_model=payload).to_dict()\n",
    "already_processed_samples = [rec[\"sample_id\"] for rec in ingested_records[\"result\"]]\n",
    "\n",
    "# Pull samples to ingest\n",
    "payload = {\n",
    "  \"offset\": 0,\n",
    "  \"limit\": 1000,\n",
    "  \"sort\": \"datarepo_row_id\",\n",
    "  \"direction\": \"asc\",\n",
    "  \"filter\": \"\"\n",
    "}\n",
    "records_orig = datasets_api.query_dataset_data_by_id(id=\"85dbde76-c130-40b2-8a8a-ba815ba499da\", table=table, query_data_request_model=payload).to_dict()\n",
    "records_processed = []\n",
    "for record in records_orig[\"result\"]:\n",
    "    int_record = record.copy()\n",
    "    for fcol in float_col_dict[table]:\n",
    "        if int_record[fcol]:\n",
    "            int_record[fcol] = float(int_record[fcol])\n",
    "    if int_record[\"sample_id\"] not in already_processed_samples:\n",
    "        records_processed.append(int_record)\n",
    "\n",
    "# Build ingest request\n",
    "ingest_request = {\n",
    "    \"table\": table,\n",
    "    \"profile_id\": \"9ee23bed-b46c-4561-9103-d2a723113f7f\",\n",
    "    \"ignore_unknown_values\": True,\n",
    "    \"resolve_existing_files\": True,\n",
    "    \"updateStrategy\": \"append\",\n",
    "    \"format\": \"array\",\n",
    "    \"load_tag\": \"Ingest for 5f4ece3e-d76e-4d78-99e0-e62a24cd163d\",\n",
    "    \"records\": records_processed[0:100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(already_processed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(records_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(ingest_request[\"records\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "json.dumps(ingest_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Manual datarepo_row_id_xwalk modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "tdr_host = \"https://data.terra.bio\"\n",
    "source_dataset_id = '75fb0984-2124-444f-881b-30a1a6f8b8f7'\n",
    "target_dataset_id = '7a9eee5d-95c2-4947-93a8-e31d53a2a09a'\n",
    "table = \"subject\"\n",
    "key_field = \"subject_id\"\n",
    "\n",
    "# Load existing datarepo_row_id_walk\n",
    "logging.info(\"Fetching existing datarepo_row_id crosswalk (if one exists).\")\n",
    "xwalk_json_file_name = f\"{source_dataset_id}_{target_dataset_id}_rowid_xwalk.json\"\n",
    "try:\n",
    "    with open(xwalk_json_file_name,\"r\") as file:\n",
    "        datarepo_row_id_xwalk = json.load(file)\n",
    "except:\n",
    "    datarepo_row_id_xwalk = {}\n",
    "    logging.warning(f\"No datarepo_row_id crosswalk file name '{xwalk_json_file_name}' found.\")\n",
    "\n",
    "# Fetch records from source dataset\n",
    "logging.info(\"Fetching records from source dataset.\")\n",
    "api_client = refresh_tdr_api_client(tdr_host)\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "max_page_size = 1000\n",
    "total_record_count = 1000\n",
    "records_fetched = 0\n",
    "retrieval_error = False\n",
    "source_records = []\n",
    "while records_fetched < total_record_count and not retrieval_error:\n",
    "    row_start = records_fetched\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        payload = {\n",
    "          \"offset\": row_start,\n",
    "          \"limit\": max_page_size,\n",
    "          \"sort\": \"datarepo_row_id\",\n",
    "          \"direction\": \"asc\",\n",
    "          \"filter\": \"\"\n",
    "        }\n",
    "        try:\n",
    "            dataset_results = datasets_api.query_dataset_data_by_id(id=source_dataset_id, table=table, query_data_request_model=payload).to_dict() \n",
    "            total_record_count = dataset_results[\"total_row_count\"]\n",
    "            if len(dataset_results[\"result\"]) == 0:\n",
    "                warn_str = f\"No records found for '{table}' table, which prevents the proper building of the datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream.\"\n",
    "                logging.warning(warn_str)\n",
    "                retrieval_error = True\n",
    "                break  \n",
    "            else:\n",
    "                for record in dataset_results[\"result\"]:\n",
    "                    source_records.append([record[key_field], record[\"datarepo_row_id\"]])\n",
    "                    records_fetched += 1\n",
    "                break\n",
    "        except Exception as e:\n",
    "                if attempt_counter < 0:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    warn_str = f\"Error retrieving records for '{table}' table to build datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream. Error: {str(e)}\"\n",
    "                    logging.warning(warn_str)\n",
    "                    retrieval_error = True\n",
    "                    break\n",
    "    logging.info(f\"Records fetched: {str(records_fetched)}\")\n",
    "\n",
    "# Fetch records from target dataset\n",
    "logging.info(\"Fetching records from target dataset.\")\n",
    "api_client = refresh_tdr_api_client(tdr_host)\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "max_page_size = 1000\n",
    "total_record_count = 1000\n",
    "records_fetched = 0\n",
    "retrieval_error = False\n",
    "target_records = []\n",
    "while records_fetched < total_record_count and not retrieval_error:\n",
    "    row_start = records_fetched\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        payload = {\n",
    "          \"offset\": row_start,\n",
    "          \"limit\": max_page_size,\n",
    "          \"sort\": \"datarepo_row_id\",\n",
    "          \"direction\": \"asc\",\n",
    "          \"filter\": \"\"\n",
    "        }\n",
    "        try:\n",
    "            dataset_results = datasets_api.query_dataset_data_by_id(id=target_dataset_id, table=table, query_data_request_model=payload).to_dict() \n",
    "            total_record_count = dataset_results[\"total_row_count\"]\n",
    "            if len(dataset_results[\"result\"]) == 0:\n",
    "                warn_str = f\"No records found for '{table}' table, which prevents the proper building of the datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream.\"\n",
    "                logging.warning(warn_str)\n",
    "                retrieval_error = True\n",
    "                break  \n",
    "            else:\n",
    "                for record in dataset_results[\"result\"]:\n",
    "                    target_records.append([record[key_field], record[\"datarepo_row_id\"]])\n",
    "                    records_fetched += 1\n",
    "                break\n",
    "        except Exception as e:\n",
    "                if attempt_counter < 0:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    warn_str = f\"Error retrieving records for '{table}' table to build datarepo_row_id_xwalk. Note that this may cause failures in FSS table ingestion requests downstream. Error: {str(e)}\"\n",
    "                    logging.warning(warn_str)\n",
    "                    retrieval_error = True\n",
    "                    break\n",
    "    logging.info(f\"Records fetched: {str(records_fetched)}\")\n",
    "\n",
    "# Match records and update datarepo_row_id_xwalk\n",
    "logging.info(\"Building records for datarepo_row_id_xwalk\")\n",
    "temp_dr_xwalk = {}\n",
    "for source_record in source_records:\n",
    "    for target_record in target_records:\n",
    "        if source_record[0] == target_record[0]:\n",
    "            key = table + \":\" + source_record[1]\n",
    "            val = table + \":\" + target_record[1]\n",
    "            temp_dr_xwalk[key] = val\n",
    "            break\n",
    "if len(temp_dr_xwalk) == total_record_count:\n",
    "    datarepo_row_id_xwalk[table] = temp_dr_xwalk\n",
    "    with open(xwalk_json_file_name, 'w') as file:\n",
    "        json.dump(datarepo_row_id_xwalk, file)\n",
    "    logging.info(\"Processing complete.\")\n",
    "else:\n",
    "    logging.error(\"Rows in xwalk doesn't match table record count.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "source_dataset_id = '65793118-3c88-4185-9172-2354850e6056'\n",
    "target_dataset_id = '183ec762-f867-46c5-bb19-8b2b3417f7b2'\n",
    "\n",
    "# Load existing datarepo_row_id_walk\n",
    "logging.info(\"Fetching existing datarepo_row_id crosswalk (if one exists).\")\n",
    "xwalk_json_file_name = f\"{source_dataset_id}_{target_dataset_id}_rowid_xwalk.json\"\n",
    "try:\n",
    "    with open(xwalk_json_file_name,\"r\") as file:\n",
    "        datarepo_row_id_xwalk = json.load(file)\n",
    "except:\n",
    "    datarepo_row_id_xwalk = {}\n",
    "    logging.warning(f\"No datarepo_row_id crosswalk file name '{xwalk_json_file_name}' found.\")\n",
    "\n",
    "# Output crosswalk record counts\n",
    "for key in datarepo_row_id_xwalk.keys():\n",
    "    length = len(datarepo_row_id_xwalk[key])\n",
    "    print(f\"{key}: {length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull and Compare Tabular Data between TDR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     4,
     109
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/16/2025 07:33:27 PM - INFO: Comparing tabular data record counts between TDR dataset c6f3bd64-ea67-488f-904f-f0bdf6320b5c and TDR dataset fbc7f442-585f-4885-9e2e-bdb38425867d.\n",
      "06/16/2025 07:33:27 PM - INFO: Pulling the superset of tables across the two datasets.\n",
      "06/16/2025 07:33:28 PM - INFO: Comparing record counts for table 'anvil_donor'\n",
      "06/16/2025 07:33:33 PM - INFO: Comparing record counts for table 'anvil_project'\n",
      "06/16/2025 07:33:40 PM - INFO: Comparing record counts for table 'anvil_sequencingactivity'\n",
      "06/16/2025 07:33:45 PM - INFO: Comparing record counts for table 'anvil_biosample'\n",
      "06/16/2025 07:33:49 PM - INFO: Comparing record counts for table 'anvil_diagnosis'\n",
      "06/16/2025 07:33:53 PM - INFO: Comparing record counts for table 'anvil_assayactivity'\n",
      "06/16/2025 07:33:56 PM - INFO: Comparing record counts for table 'anvil_alignmentactivity'\n",
      "06/16/2025 07:34:00 PM - INFO: Comparing record counts for table 'anvil_dataset'\n",
      "06/16/2025 07:34:04 PM - INFO: Comparing record counts for table 'anvil_activity'\n",
      "06/16/2025 07:34:07 PM - INFO: Comparing record counts for table 'anvil_antibody'\n",
      "06/16/2025 07:34:10 PM - INFO: Comparing record counts for table 'workspace_attributes'\n",
      "06/16/2025 07:34:13 PM - INFO: Comparing record counts for table 'anvil_variantcallingactivity'\n",
      "06/16/2025 07:34:16 PM - INFO: Comparing record counts for table 'anvil_file'\n",
      "06/16/2025 07:34:19 PM - INFO: Comparing record counts for table 'file_inventory'\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Table</th>\n",
       "      <th>Table in DS1</th>\n",
       "      <th>DS1 Record Count</th>\n",
       "      <th>Table in DS2</th>\n",
       "      <th>DS2 Record Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_project</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>True</td>\n",
       "      <td>1210</td>\n",
       "      <td>True</td>\n",
       "      <td>1210</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>workspace_attributes</td>\n",
       "      <td>True</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>37</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>anvil_file</td>\n",
       "      <td>True</td>\n",
       "      <td>8619</td>\n",
       "      <td>True</td>\n",
       "      <td>8619</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>file_inventory</td>\n",
       "      <td>True</td>\n",
       "      <td>8619</td>\n",
       "      <td>True</td>\n",
       "      <td>8619</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset 1 ID                          Dataset 2 ID                         Table             Table in DS1  DS1 Record Count Table in DS2  DS2 Record Count Status Message\n",
       "0   c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d                   anvil_donor     True               0           True               0        Pass         \n",
       "1   c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d                 anvil_project     True               1           True               1        Pass         \n",
       "2   c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d      anvil_sequencingactivity     True               0           True               0        Pass         \n",
       "3   c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d               anvil_biosample     True               0           True               0        Pass         \n",
       "4   c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d               anvil_diagnosis     True               0           True               0        Pass         \n",
       "5   c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d           anvil_assayactivity     True               0           True               0        Pass         \n",
       "6   c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d       anvil_alignmentactivity     True               0           True               0        Pass         \n",
       "7   c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d                 anvil_dataset     True               1           True               1        Pass         \n",
       "8   c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d                anvil_activity     True            1210           True            1210        Pass         \n",
       "9   c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d                anvil_antibody     True               0           True               0        Pass         \n",
       "10  c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d          workspace_attributes     True              37           True              37        Pass         \n",
       "11  c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d  anvil_variantcallingactivity     True               0           True               0        Pass         \n",
       "12  c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d                    anvil_file     True            8619           True            8619        Pass         \n",
       "13  c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d                file_inventory     True            8619           True            8619        Pass         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset 1 ID</th>\n",
       "      <th>Dataset 2 ID</th>\n",
       "      <th>Validation Type</th>\n",
       "      <th>Status</th>\n",
       "      <th>Failed Tables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c6f3bd64-ea67-488f-904f-f0bdf6320b5c</td>\n",
       "      <td>fbc7f442-585f-4885-9e2e-bdb38425867d</td>\n",
       "      <td>Record Count Comparison</td>\n",
       "      <td>Pass</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset 1 ID                          Dataset 2 ID                 Validation Type      Status Failed Tables\n",
       "0  c6f3bd64-ea67-488f-904f-f0bdf6320b5c  fbc7f442-585f-4885-9e2e-bdb38425867d  Record Count Comparison  Pass               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def compare_row_counts(dataset_1_id, dataset_2_id):\n",
    "    \n",
    "    # Setup/refresh TDR clients\n",
    "    logging.info(f\"Comparing tabular data record counts between TDR dataset {dataset_1_id} and TDR dataset {dataset_2_id}.\")\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Pull table list across datasets\n",
    "    logging.info(f\"Pulling the superset of tables across the two datasets.\")\n",
    "    try:\n",
    "        dataset_1_details = datasets_api.retrieve_dataset(id=dataset_1_id, include=[\"SCHEMA\"]).to_dict()\n",
    "        dataset_2_details = datasets_api.retrieve_dataset(id=dataset_2_id, include=[\"SCHEMA\"]).to_dict()\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from datasets: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "    table_set = set()\n",
    "    for table_entry in dataset_1_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])\n",
    "    for table_entry in dataset_2_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])   \n",
    "\n",
    "    # For each table in the table list, pull record counts from the two datasets and compare\n",
    "    results = []\n",
    "    payload = {\n",
    "      \"offset\": 0,\n",
    "      \"limit\": 10,\n",
    "      \"sort\": \"datarepo_row_id\",\n",
    "      \"direction\": \"asc\",\n",
    "      \"filter\": \"\"\n",
    "    }\n",
    "    for table in table_set:\n",
    "        logging.info(f\"Comparing record counts for table '{table}'\")\n",
    "        # Pulling record counts for dataset 1\n",
    "        ds1_table_present = \"True\"\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_1_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                ds1_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    ds1_record_count = 0\n",
    "                    ds1_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        ds1_record_count = 0\n",
    "                        ds1_table_present = \"Unknown\"\n",
    "                        break\n",
    "        # Pulling record counts for dataset 2\n",
    "        ds2_table_present = \"True\"\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_2_id, table=table, query_data_request_model=payload).to_dict()\n",
    "                ds2_record_count = record_results[\"total_row_count\"]\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    ds2_record_count = 0\n",
    "                    ds2_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        ds2_record_count = 0\n",
    "                        ds2_table_present = \"Unknown\"\n",
    "                        break\n",
    "        # Build table comparison\n",
    "        if ds1_table_present == \"Unknown\" or ds2_table_present == \"Unknown\":\n",
    "            status = \"Fail\"\n",
    "            error_reason = \"Error retrieving table data from dataset(s)\"\n",
    "        elif ds1_table_present == \"False\" or ds2_table_present == \"False\":\n",
    "            status = \"Fail\"\n",
    "            error_reason = \"Table presence mismatch between datasets\"\n",
    "        elif ds1_record_count != ds2_record_count:\n",
    "            status = \"Fail\"\n",
    "            error_reason = \"Difference in record count\"\n",
    "        else:\n",
    "            status = \"Pass\"\n",
    "            error_reason = \"\"\n",
    "        results.append([dataset_1_id, dataset_2_id, table, ds1_table_present, ds1_record_count, ds2_table_present, ds2_record_count, status, error_reason])\n",
    "\n",
    "    # Display detailed results\n",
    "    print(\"\\nResults:\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"Dataset 1 ID\", \"Dataset 2 ID\", \"Table\", \"Table in DS1\", \"DS1 Record Count\", \"Table in DS2\", \"DS2 Record Count\", \"Status\", \"Message\"])\n",
    "    display(results_df)\n",
    "\n",
    "    # Return final aggregated results\n",
    "    status = \"Pass\"\n",
    "    failed_tables = []\n",
    "    for entry in results:\n",
    "        if entry[7] == \"Fail\":\n",
    "            failed_tables.append(entry[2])\n",
    "            status = \"Fail\"\n",
    "    return status, sorted(failed_tables)\n",
    "        \n",
    "def compare_contents_sample(dataset_1_id, dataset_2_id, sample_size, fields_to_ignore):\n",
    "    # Pull schema, record first column in each table (for ordering)\n",
    "    # Setup/refresh TDR clients\n",
    "    logging.info(f\"Comparing tabular data record counts between TDR dataset {dataset_1_id} and TDR dataset {dataset_2_id}.\")\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "    # Pull table list across datasets\n",
    "    logging.info(f\"Pulling the superset of tables across the two datasets.\")\n",
    "    try:\n",
    "        dataset_1_details = datasets_api.retrieve_dataset(id=dataset_1_id, include=[\"SCHEMA\"]).to_dict()\n",
    "        dataset_2_details = datasets_api.retrieve_dataset(id=dataset_2_id, include=[\"SCHEMA\"]).to_dict()\n",
    "    except Exception as e:\n",
    "        error_str = f\"Error retrieving details from datasets: {str(e)}\"\n",
    "        logging.error(error_str)\n",
    "    table_set = {}\n",
    "    for table_entry in dataset_1_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])\n",
    "    for table_entry in dataset_2_details[\"schema\"][\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])  \n",
    "    \n",
    "    \n",
    "    # Loop through tables, pull xxx records (by sample size), ordering by first column\n",
    "    # Drop fields_to_ignore\n",
    "    # Compare --> How to best do this\n",
    "    pass\n",
    "    \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of dataset pairs to compare\n",
    "dataset_id_pairs_list = [\n",
    "    #[\"gcp_dataset_id\", \"az_dataset_id\"]\n",
    "    ['c6f3bd64-ea67-488f-904f-f0bdf6320b5c', 'fbc7f442-585f-4885-9e2e-bdb38425867d'],\n",
    "]\n",
    "\n",
    "# Specify whether row comparison checks should run\n",
    "run_row_count_comparison = True\n",
    "\n",
    "# Specify whether table content checks should run, the size of the sample to use (if so), and which fields should be excluded from comparison\n",
    "run_contents_sample_comparison = False\n",
    "contents_sample_comparison_size = 1000\n",
    "fields_to_ignore = [\"datarepo_row_id\", \"orig_datarepo_row_id\", \"orig_file_ref\", \"source_datarepo_row_ids\", \"uri\"]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Run validation\n",
    "results = []\n",
    "for dataset_id_pair in dataset_id_pairs_list:\n",
    "    if run_row_count_comparison:\n",
    "        status, failed_tables = compare_row_counts(dataset_id_pair[0], dataset_id_pair[1])\n",
    "        results.append([dataset_id_pair[0], dataset_id_pair[1], \"Record Count Comparison\", status, ', '.join(failed_tables)])\n",
    "\n",
    "# Display final results\n",
    "print(\"\\nFinal Validation Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Dataset 1 ID\", \"Dataset 2 ID\", \"Validation Type\", \"Status\", \"Failed Tables\"])\n",
    "display(results_df)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dataset_1_id = \"b12fb9be-2ce0-4bfd-8503-732fabba06ab\"\n",
    "dataset_2_id = \"744c85cc-13d2-4f90-9d2e-d3143cb01edb\"\n",
    "contents_sample_comparison_size = 1000\n",
    "fields_to_ignore = [\"datarepo_row_id\", \"orig_datarepo_row_id\", \"orig_file_ref\", \"source_datarepo_row_ids\", \"uri\"]\n",
    "\n",
    "# Setup/refresh TDR clients\n",
    "logging.info(f\"Comparing a sample of tabular data content between TDR dataset {dataset_1_id} and TDR dataset {dataset_2_id}.\")\n",
    "api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# Pull table list across datasets\n",
    "logging.info(f\"Pulling the superset of tables across the two datasets.\")\n",
    "try:\n",
    "    dataset_1_details = datasets_api.retrieve_dataset(id=dataset_1_id, include=[\"SCHEMA\"]).to_dict()\n",
    "    dataset_2_details = datasets_api.retrieve_dataset(id=dataset_2_id, include=[\"SCHEMA\"]).to_dict()\n",
    "except Exception as e:\n",
    "    error_str = f\"Error retrieving details from datasets: {str(e)}\"\n",
    "    logging.error(error_str)\n",
    "table_set = {}\n",
    "for table_entry in dataset_1_details[\"schema\"][\"tables\"]:\n",
    "    table_set[table_entry[\"name\"]] = table_entry[\"columns\"][0][\"name\"]\n",
    "for table_entry in dataset_2_details[\"schema\"][\"tables\"]:\n",
    "    table_set[table_entry[\"name\"]] = table_entry[\"columns\"][0][\"name\"]\n",
    "    \n",
    "# For each table in the table list, pull sample records from the two datasets and compare\n",
    "results = []\n",
    "for table in [\"file_inventory\"]: #table_set.keys():\n",
    "    logging.info(f\"Comparing sample records for table '{table}'\")\n",
    "    # Pulling sample records for dataset 1\n",
    "    ds1_table_present = \"True\"\n",
    "    max_page_size = 1000\n",
    "    total_records_fetched = 0\n",
    "    ds1_final_records = []\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        offset = total_records_fetched\n",
    "        page_size = min(max_page_size, contents_sample_comparison_size - total_records_fetched)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": offset,\n",
    "              \"limit\": page_size,\n",
    "              \"sort\": table_set[table],\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_1_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    record_results = []\n",
    "                    ds1_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        record_results = []\n",
    "                        ds1_table_present = \"Unknown\"\n",
    "                        break\n",
    "        if record_results[\"result\"]:\n",
    "            ds1_final_records.extend(record_results[\"result\"])\n",
    "            total_records_fetched += len(record_results[\"result\"])\n",
    "        else:\n",
    "            break\n",
    "        if total_records_fetched >= contents_sample_comparison_size:\n",
    "            break\n",
    "    # Pulling sample records for dataset 2\n",
    "    ds2_table_present = \"True\"\n",
    "    max_page_size = 1000\n",
    "    total_records_fetched = 0\n",
    "    ds2_final_records = []\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        offset = total_records_fetched\n",
    "        page_size = min(max_page_size, contents_sample_comparison_size - total_records_fetched)\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            payload = {\n",
    "              \"offset\": offset,\n",
    "              \"limit\": page_size,\n",
    "              \"sort\": table_set[table],\n",
    "              \"direction\": \"asc\",\n",
    "              \"filter\": \"\"\n",
    "            }\n",
    "            try:\n",
    "                record_results = datasets_api.query_dataset_data_by_id(id=dataset_2_id, table=table, query_data_request_model=payload).to_dict() \n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"No dataset table exists\" in str(e):\n",
    "                    record_results = []\n",
    "                    ds2_table_present = \"False\"\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        record_results = []\n",
    "                        ds2_table_present = \"Unknown\"\n",
    "                        break\n",
    "        if record_results[\"result\"]:\n",
    "            ds2_final_records.extend(record_results[\"result\"])\n",
    "            total_records_fetched += len(record_results[\"result\"])\n",
    "        else:\n",
    "            break\n",
    "        if total_records_fetched >= contents_sample_comparison_size:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds1_records_int = pd.DataFrame.from_dict(ds1_final_records)\n",
    "df_ds2_records_int = pd.DataFrame.from_dict(ds2_final_records)\n",
    "cols = df_ds1_records_int.columns.tolist()\n",
    "for field in fields_to_ignore:\n",
    "    if field in cols:\n",
    "        cols.remove(field)\n",
    "df_ds1_records = df_ds1_records_int[cols]\n",
    "df_ds2_records = df_ds2_records_int[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df_ds1_records.compare(df_ds2_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_ds1_records.equals(df_ds2_records):\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pull and Compare File Counts and Sizes between TDR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def collect_file_stats(dataset_id_pairs_list):\n",
    "    \n",
    "    results = []\n",
    "    for dataset_id_pair in dataset_id_pairs_list:\n",
    "\n",
    "            # Setup/refresh TDR clients\n",
    "            logging.info(f\"Processing dataset_id_pair: {dataset_id_pair}\")\n",
    "            api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "            # Initialize variables\n",
    "            dataset_id_1 = dataset_id_pair[0]\n",
    "            file_count_1 = 0\n",
    "            total_file_size_1 = 0\n",
    "            max_file_size_1 = 0\n",
    "            status_1 = \"Success\"\n",
    "            message_1 = \"\"\n",
    "            dataset_id_2 = dataset_id_pair[1]\n",
    "            file_count_2 = 0\n",
    "            total_file_size_2 = 0\n",
    "            max_file_size_2 = 0\n",
    "            status_2 = \"Success\"\n",
    "            message_2 = \"\"\n",
    "            validation_status = \"Passed\"\n",
    "            validation_message = \"\"\n",
    "\n",
    "            # For dataset_id_1, loop through dataset files and record information\n",
    "            logging.info(f\"Retrieving files from dataset_id {dataset_id_1}...\")\n",
    "            try:\n",
    "                max_page_size = 1000\n",
    "                total_records_fetched = 0\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        row_start = total_records_fetched\n",
    "                        dataset_file_results = datasets_api.list_files(id=dataset_id_1, offset=row_start, limit=max_page_size)\n",
    "                        if dataset_file_results:\n",
    "                            total_records_fetched += len(dataset_file_results)\n",
    "                            for entry in dataset_file_results:\n",
    "                                file_count_1 += 1\n",
    "                                total_file_size_1 += entry.size\n",
    "                                if entry.size > max_file_size_1:\n",
    "                                    max_file_size_1 = entry.size\n",
    "                            logging.info(f\"{total_records_fetched} records fetched...\")\n",
    "                            attempt_counter = 0\n",
    "                        else:\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        attempt_counter += 1\n",
    "                        if attempt_counter <= 10:\n",
    "                            logging.info(f\"Failure in file retrieval (attempt #{attempt_counter}). Trying again...\")\n",
    "                            continue\n",
    "                        else:\n",
    "                            status_1 = \"Failure\"\n",
    "                            message_1 = str(e)\n",
    "                            logging.error(f\"Failure in file retrieval: {message_1}\")\n",
    "                            break\n",
    "                if status_1 == \"Success\":\n",
    "                    logging.info(f\"File retrieval complete!\")\n",
    "            except Exception as e:\n",
    "                status_1 = \"Failure\"\n",
    "                message_1 = str(e)\n",
    "                logging.error(f\"Failure in file retrieval: {message_1}\")\n",
    "            \n",
    "            # For dataset_id_2, loop through dataset files and record information\n",
    "            logging.info(f\"Retrieving files from dataset_id {dataset_id_2}...\")\n",
    "            try:\n",
    "                max_page_size = 1000\n",
    "                total_records_fetched = 0\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        row_start = total_records_fetched\n",
    "                        dataset_file_results = datasets_api.list_files(id=dataset_id_2, offset=row_start, limit=max_page_size)\n",
    "                        if dataset_file_results:\n",
    "                            total_records_fetched += len(dataset_file_results)\n",
    "                            for entry in dataset_file_results:\n",
    "                                file_count_2 += 1\n",
    "                                total_file_size_2 += entry.size\n",
    "                                if entry.size > max_file_size_2:\n",
    "                                    max_file_size_2 = entry.size\n",
    "                            logging.info(f\"{total_records_fetched} records fetched...\")\n",
    "                            attempt_counter = 0\n",
    "                        else:\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        attempt_counter += 1\n",
    "                        if attempt_counter <= 10:\n",
    "                            logging.info(f\"Failure in file retrieval (attempt #{attempt_counter}). Trying again...\")\n",
    "                            continue\n",
    "                        else:\n",
    "                            status_2 = \"Failure\"\n",
    "                            message_2 = str(e)\n",
    "                            logging.error(f\"Failure in file retrieval: {message_2}\")\n",
    "                            break\n",
    "                if status_2 == \"Success\":\n",
    "                    logging.info(f\"File retrieval complete!\")\n",
    "            except Exception as e:\n",
    "                status_2 = \"Failure\"\n",
    "                message_2 = str(e)\n",
    "                logging.error(f\"Failure in file retrieval: {message_2}\")\n",
    "                \n",
    "            # Record and display interim results\n",
    "            file_count_diff = file_count_1 - file_count_2\n",
    "            total_file_size_diff = total_file_size_1 - total_file_size_2\n",
    "            max_file_size_diff = max_file_size_1 - max_file_size_2\n",
    "            if status_1 == \"Failure\" or status_2 == \"Failure\":\n",
    "                validation_status = \"Failed\"\n",
    "                validation_message = \"Errors pulling counts for one or more datasets.\"\n",
    "            elif file_count_diff > 0 or total_file_size_diff > 0 or max_file_size_diff > 0:\n",
    "                validation_status = \"Failed\"\n",
    "                validation_message = \"Difference in counts between datasets.\"\n",
    "            results.append([dataset_id_1, dataset_id_2, validation_status, validation_message, file_count_diff, total_file_size_diff, max_file_size_diff, file_count_1, total_file_size_1, max_file_size_1, status_1, message_1, file_count_2, total_file_size_2, max_file_size_2, status_2, message_2])\n",
    "            int_results_df = pd.DataFrame([[dataset_id_1, dataset_id_2, validation_status, validation_message, file_count_diff, total_file_size_diff, max_file_size_diff, file_count_1, total_file_size_1, max_file_size_1, status_1, message_1, file_count_2, total_file_size_2, max_file_size_2, status_2, message_2]], columns = [\"Dataset ID 1\", \"Dataset ID 2\", \"Validation Status\", \"Validation Message\", \"File Count Diff\", \"Total File Size (Bytes) Diff\", \"Max File Size (Bytes) Diff\", \"File Count 1\", \"Total File Size (Bytes) 1\", \"Max File Size (Bytes) 1\", \"Status 1 \", \"Message 1\", \"File Count 2\", \"Total File Size (Bytes) 2\", \"Max File Size (Bytes) 2\", \"Status 2 \", \"Message 2\"])\n",
    "            logging.info(\"Results recorded:\")\n",
    "            display(int_results_df)\n",
    "        \n",
    "    # Display final results\n",
    "    logging.info(\"Aggregating results...\")\n",
    "    ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "    destination_dir = \"ingest_pipeline/resources/azure_migration\"\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    output_file = f\"validation_results_{current_datetime_string}.tsv\"\n",
    "    results_df = pd.DataFrame(results, columns = [\"Dataset ID 1\", \"Dataset ID 2\", \"Validation Status\", \"Validation Message\", \"File Count Diff\", \"Total File Size (Bytes) Diff\", \"Max File Size (Bytes) Diff\", \"File Count 1\", \"Total File Size (Bytes) 1\", \"Max File Size (Bytes) 1\", \"Status 1 \", \"Message 1\", \"File Count 2\", \"Total File Size (Bytes) 2\", \"Max File Size (Bytes) 2\", \"Status 2 \", \"Message 2\"])\n",
    "    results_df.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "    !gsutil cp $output_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "    !rm $output_file\n",
    "    print(\"\\nAggregated Validation Results:\")\n",
    "    display(results_df)   \n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of dataset IDs\n",
    "dataset_id_pairs_list = [\n",
    "#    ['bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8', 'c7206e9a-78ad-4c9d-927f-3ca76646227d'],\n",
    "#    ['902596ce-714e-49b3-8271-f3dfece52309', 'e091028e-a6b1-4989-9477-498e7ea206f0'],\n",
    "    ['bef62e8a-5f5c-4e81-a8f8-ddeaf657b4e8', 'c7206e9a-78ad-4c9d-927f-3ca76646227d'],\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "collect_file_stats(dataset_id_pairs_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Migrating Workspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Pre-Connector Processing\n",
    "For each GCP Workspace - Azure Workspace pair:\n",
    "1. Build a manifest of files to be copied from the GCP Workspace to the Azure Workspace. \n",
    "2. Write the manifest to BigQuery for consumption by downstream processes.\n",
    "\n",
    "Pre-run steps:\n",
    "1. Use the anvil_ingest_tools notebook to create the Azure workspaces. \n",
    "2. Use the anvil_ingest_tools notebook to add the TDR general SA (datarepo-jade-api@terra-datarepo-production.iam.gserviceaccount.com) as a reader on the source GCP workspaces and a writer on the target Azure workspaces.\n",
    "\n",
    "Post-run steps:\n",
    "1. Use the anvil_ingest_tools notebook to remove the TDR general SA from the GCP and Azure workspaces. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to create file transfer details\n",
    "def output_file_details(source_ws_project, source_ws_name, target_ws_project, target_ws_name, file_bigquery_table, target_bigquery_table, delete_existing_records):\n",
    "    \n",
    "    # Establish credentials and clients\n",
    "    client = bigquery.Client()\n",
    "    creds, project = google.auth.default(scopes=['https://www.googleapis.com/auth/cloud-platform', 'openid', 'email', 'profile'])\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "\n",
    "    # Pull bucket from source workspace\n",
    "    try:\n",
    "        ws_attributes = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{source_ws_project}/{source_ws_name}?fields=workspace.bucketName\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        ws_bucket = ws_attributes[\"workspace\"][\"bucketName\"]\n",
    "    except:\n",
    "        err_str = \"Error retrieving workspace attributes for source workspace.\"\n",
    "        logging.error(err_str)\n",
    "        raise Exception(err_str)\n",
    "\n",
    "    # Pull storage container from target workspace\n",
    "    try:\n",
    "        ws_attributes = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{target_ws_project}/{target_ws_name}?fields=workspace.workspaceId\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        ws_id = ws_attributes[\"workspace\"][\"workspaceId\"] \n",
    "        ws_resources = requests.get(\n",
    "            url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{ws_id}/resources?offset=0&limit=10&resource=AZURE_STORAGE_CONTAINER\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        resource_id = \"\"\n",
    "        for resource_entry in ws_resources[\"resources\"]:\n",
    "            if resource_entry[\"resourceAttributes\"][\"azureStorageContainer\"][\"storageContainerName\"][0:3] == \"sc-\":\n",
    "                resource_id = resource_entry[\"metadata\"][\"resourceId\"]\n",
    "                break\n",
    "        if resource_id:\n",
    "            sas_response = requests.post(\n",
    "                url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{ws_id}/resources/controlled/azure/storageContainer/{resource_id}/getSasToken?sasExpirationDuration=86400\",\n",
    "                headers={\"Authorization\": f\"Bearer {creds.token}\", \"accept\": \"application/json\"}\n",
    "            ).json()\n",
    "            base_url = sas_response[\"url\"]\n",
    "            ws_storage_container = re.search(\"^[a-z0-9:\\/=\\-\\.]+\", base_url, re.IGNORECASE).group(0)\n",
    "        else:\n",
    "            err_str = \"Error retrieving resource information for target workspace.\"\n",
    "            logging.error(err_str)\n",
    "            raise Exception(err_str)\n",
    "    except:\n",
    "        err_str = \"Error retrieving workspace attributes for target workspace.\"\n",
    "        logging.error(err_str)\n",
    "        raise Exception(err_str)\n",
    "\n",
    "    # Clear records from target BQ table (if specified)\n",
    "    if delete_existing_records:\n",
    "        logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "        delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE gcp_ws_project = '{source_ws_project}' and gcp_ws_name = '{source_ws_name}'\"\"\"\n",
    "        try:\n",
    "            delete_query_job = client.query(delete_query)\n",
    "            delete_query_job.result()\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Error deleting records for the original dataset from the target BQ table.\") \n",
    "\n",
    "    # Write the query to pull files into a dataframe\n",
    "    logging.info(f\"Building manifest of files to copy from the source '{source_ws_project}.{source_ws_name}' workspace to the target '{target_ws_project}.{target_ws_name}' workspace.\")\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = \"WRITE_APPEND\"\n",
    "    query = f\"\"\"SELECT '{source_ws_project}' AS gcp_ws_project, '{source_ws_name}' AS gcp_ws_name, \n",
    "                '{target_ws_project}' AS az_ws_project, '{target_ws_name}' AS az_ws_name, \n",
    "                 'gs://{ws_bucket}/'||name AS source_path, '{ws_storage_container}/'||name AS target_path, \n",
    "                 size AS size_in_bytes, md5Hash AS md5_hash, '{current_datetime_string}' AS date_added\n",
    "                FROM `{file_bigquery_table}` \n",
    "                WHERE bucket = '{ws_bucket}'\n",
    "                AND name NOT LIKE '%/'\"\"\"\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            df = client.query(query).result().to_dataframe()\n",
    "            job = client.load_table_from_dataframe(df, target_bigquery_table, job_config=job_config)\n",
    "            logging.info(\"Records recorded successfully.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt_counter < 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_str = f\"Error building and writing file manifest: {str(e)}.\"\n",
    "                logging.error(err_str)\n",
    "                raise Exception(err_str)\n",
    "\n",
    "            \n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# General parameters\n",
    "file_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_inventory.object_metadata_26_02_2024__17_14_55\"\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list_workspaces\"\n",
    "\n",
    "# Specify migration pairs: Source GCP Workspace - Target Azure Workspace\n",
    "migration_list = [\n",
    "    #{\"gcp_ws_project\": \"anvil-datastorage\", \"gcp_ws_name\": \"<name>\", \"az_ws_project\": \"AnVILDataStorage_Azure\", \"az_ws_name\": \"<name>\"}\n",
    "    {'gcp_ws_project': 'anvil-datastorage', 'gcp_ws_name': 'AnVIL_CCDG_WGS_HAIL_Phased-data', 'az_ws_project': 'AnVILDataStorage_Azure', 'az_ws_name': 'AnVIL_CCDG_WGS_HAIL_Phased-data_Azure'},\n",
    "]\n",
    "\n",
    "# Specify whether existing records in the azure_migration_file_list_workspaces table should be deleted before running\n",
    "delete_existing_records = True\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for entry in migration_list:\n",
    "    logging.info(f\"Processing Migration List Entry: {str(entry)}\")\n",
    "    try:\n",
    "        output_file_details(entry[\"gcp_ws_project\"], entry[\"gcp_ws_name\"], entry[\"az_ws_project\"], entry[\"az_ws_name\"], file_bigquery_table, target_bigquery_table, delete_existing_records)\n",
    "        results.append([entry[\"gcp_ws_name\"], entry[\"az_ws_name\"], \"Success\", \"\"])\n",
    "    except Exception as e:\n",
    "        results.append([entry[\"gcp_ws_name\"], entry[\"az_ws_name\"], \"Failure\", str(e)])\n",
    "        \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Source Workspace Name\", \"Target Workspace Name\", \"Status\", \"Message\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Pull and Compare File Counts and Sizes between Workspace Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "def collect_file_stats(storage_pairs_list):\n",
    "    \n",
    "    results = []\n",
    "    for storage_pair in storage_pairs_list:\n",
    "\n",
    "            # Initialize variables\n",
    "            logging.info(f\"Processing storage pair: {storage_pair}\")\n",
    "            gcs_storage_location = storage_pair[0]\n",
    "            gcs_file_count = 0\n",
    "            gcs_total_file_size = 0\n",
    "            gcs_max_file_size = 0\n",
    "            gcs_status = \"Success\"\n",
    "            gcs_message = \"\"\n",
    "            az_storage_location = storage_pair[1]\n",
    "            az_file_count = 0\n",
    "            az_total_file_size = 0\n",
    "            az_max_file_size = 0\n",
    "            az_status = \"Success\"\n",
    "            az_message = \"\"\n",
    "            validation_status = \"Passed\"\n",
    "            validation_message = \"\"\n",
    "\n",
    "            # For gcs_storage_location, loop through files and record information\n",
    "            logging.info(\"Pulling and parsing GCP bucket contents to create a list of existing files.\")\n",
    "            existing_gcs_files = []\n",
    "            try:\n",
    "                cmd = f\"gsutil ls -L '{gcs_storage_location}/**'\"\n",
    "                output = subprocess.check_output(cmd, shell=True, universal_newlines=True)\n",
    "                file_name = \"\"\n",
    "                file_size = \"\"\n",
    "                for line in output.split(\"\\n\"):\n",
    "                    if line[0:2] == \"gs\":\n",
    "                        if file_name and file_size and file_name[-1] != \"/\":\n",
    "                            existing_gcs_files.append([file_name, file_size])\n",
    "                        file_name = re.sub(\":$\", \"\", line)\n",
    "                    else:\n",
    "                        if \"Content-Length:\" in line:\n",
    "                            file_size = re.match(\"\\s*Content-Length:\\s*([0-9]+)\", line).group(1)\n",
    "                if file_name and file_size and file_name[-1] != \"/\":\n",
    "                    existing_gcs_files.append([file_name, file_size])\n",
    "                for entry in existing_gcs_files:\n",
    "                    gcs_file_count += 1\n",
    "                    entry_file_size = int(entry[1])\n",
    "                    gcs_total_file_size += entry_file_size\n",
    "                    if entry_file_size > gcs_max_file_size:\n",
    "                        gcs_max_file_size = entry_file_size\n",
    "            except Exception as e:\n",
    "                gcs_status = \"Failure\"\n",
    "                gcs_message = str(e)\n",
    "                logging.error(f\"Failure in file retrieval: {gcs_message}\")\n",
    "                \n",
    "            # For az_storage_location, loop through files and record information\n",
    "            logging.info(\"Pulling and parsing target Azure container contents to create a list of existing files.\")\n",
    "            cmd = f\"azcopy_linux_amd64_10.24.0/azcopy list '{az_storage_location}' --machine-readable\"\n",
    "            output = subprocess.check_output(cmd, shell=True, universal_newlines=True)\n",
    "            existing_az_files = []\n",
    "            try:\n",
    "                for line in output.split(\"\\n\"):\n",
    "                    if line:\n",
    "                        file_name = re.match(r\"^INFO: (.*);\", line).group(1)\n",
    "                        file_size = re.match(r\".*Content Length: ([0-9\\.]+).*\", line).group(1)\n",
    "                        existing_az_files.append([file_name, file_size])\n",
    "                for entry in existing_az_files:\n",
    "                    az_file_count += 1\n",
    "                    entry_file_size = int(entry[1])\n",
    "                    az_total_file_size += entry_file_size\n",
    "                    if entry_file_size > az_max_file_size:\n",
    "                        az_max_file_size = entry_file_size\n",
    "            except Exception as e:\n",
    "                az_status = \"Failure\"\n",
    "                az_message = str(e)\n",
    "                logging.error(f\"Failure in file retrieval: {az_message}\")\n",
    "                \n",
    "            # Record and display interim results\n",
    "            file_count_diff = gcs_file_count - az_file_count\n",
    "            total_file_size_diff = gcs_total_file_size - az_total_file_size\n",
    "            max_file_size_diff = gcs_max_file_size - az_max_file_size\n",
    "            if gcs_status == \"Failure\" or az_status == \"Failure\":\n",
    "                validation_status = \"Failed\"\n",
    "                validation_message = \"Errors pulling counts for one or more storage locations.\"\n",
    "            elif file_count_diff > 0 or total_file_size_diff > 0 or max_file_size_diff > 0:\n",
    "                validation_status = \"Failed\"\n",
    "                validation_message = \"Difference in counts between storage locations.\"\n",
    "            results.append([gcs_storage_location, az_storage_location, validation_status, validation_message, file_count_diff, total_file_size_diff, max_file_size_diff, gcs_file_count, gcs_total_file_size, gcs_max_file_size, gcs_status, gcs_message, az_file_count, az_total_file_size, az_max_file_size, az_status, az_message])\n",
    "            int_results_df = pd.DataFrame([[gcs_storage_location, az_storage_location, validation_status, validation_message, file_count_diff, total_file_size_diff, max_file_size_diff, gcs_file_count, gcs_total_file_size, gcs_max_file_size, gcs_status, gcs_message, az_file_count, az_total_file_size, az_max_file_size, az_status, az_message]], columns = [\"GCS Storage Location\", \"AZ Storage Location\", \"Validation Status\", \"Validation Message\", \"File Count Diff\", \"Total File Size (Bytes) Diff\", \"Max File Size (Bytes) Diff\", \"GCS File Count\", \"GCS Total File Size (Bytes)\", \"GCS Max File Size (Bytes)\", \"GCS Status\", \"GCS Message\", \"AZ File Count\", \"AZ Total File Size (Bytes)\", \"AZ Max File Size (Bytes)\", \"AZ Status\", \"AZ Message\"])\n",
    "            logging.info(\"Results recorded:\")\n",
    "            display(int_results_df)\n",
    "        \n",
    "    # Display final results\n",
    "    logging.info(\"Aggregating results...\")\n",
    "    ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "    destination_dir = \"ingest_pipeline/resources/azure_migration\"\n",
    "    current_datetime_string = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    output_file = f\"workspace_validation_results_{current_datetime_string}.tsv\"\n",
    "    results_df = pd.DataFrame(results, columns = [\"GCS Storage Location\", \"AZ Storage Location\", \"Validation Status\", \"Validation Message\", \"File Count Diff\", \"Total File Size (Bytes) Diff\", \"Max File Size (Bytes) Diff\", \"GCS File Count\", \"GCS Total File Size (Bytes)\", \"GCS Max File Size (Bytes)\", \"GCS Status\", \"GCS Message\", \"AZ File Count\", \"AZ Total File Size (Bytes)\", \"AZ Max File Size (Bytes)\", \"AZ Status\", \"AZ Message\"])\n",
    "    results_df.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "    !gsutil cp $output_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "    !rm $output_file\n",
    "    print(\"\\nAggregated Validation Results:\")\n",
    "    display(results_df)   \n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# Specify the list of dataset IDs\n",
    "storage_pairs_list = [\n",
    "#     [\"gcs_bucket_path\", \"azure_storage_container_sas_url\"]\n",
    "    ['gs://fc-secure-0932b76c-22e6-4321-94f7-9726ad4aeb76', 'https://lzb34bb58bfb122730765416.blob.core.windows.net/sc-0ef0b0b4-92b6-462e-8b4f-498f1cb7983b?sv=2023-11-03&spr=https&st=2024-04-23T13%3A45%3A11Z&se=2024-04-23T22%3A00%3A11Z&sr=c&sp=racwdlt&sig=7Usayb1DzV4LEcQYheVZrSrvEkiaiot9wEZGmnEO3BM%3D&rscd=2661442731880e5cbc2c9'],\n",
    "]\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "collect_file_stats(storage_pairs_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Migrating Data Back!?!?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Collecting Files in TDR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to record file information from TDR datasets\n",
    "def output_file_details(dataset_id, target_bigquery_table):\n",
    "\n",
    "    # Delete records for dataset if already in BQ table\n",
    "    client = bigquery.Client()\n",
    "    logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "    delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE object_id = '{dataset_id}'\"\"\"\n",
    "    try:\n",
    "        delete_query_job = client.query(delete_query)\n",
    "        delete_query_job.result()\n",
    "    except Exception as e:\n",
    "        err_msg = f\"Error deleting records for the original dataset from the target BQ table: {str(e)}\"\n",
    "        logging.info(err_msg)\n",
    "        return \"Failure\", err_msg\n",
    "    \n",
    "    # Page through and pull files from dataset\n",
    "    logging.info(f\"Fetching and recording all files found in the original dataset ({dataset_id}).\") \n",
    "    file_list = []\n",
    "    total_files_fetched = 0\n",
    "    page_number = 1\n",
    "    max_page_size = 1000\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        logging.info(f\"Pulling files from page {str(page_number)}...\")\n",
    "        try:\n",
    "            api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            file_results = datasets_api.list_files(id=dataset_id, offset=total_files_fetched, limit=max_page_size)\n",
    "        except Exception as e:\n",
    "            if \"Invalid UUID string\" in str(e) or \"Dataset not found\" in str(e):\n",
    "                err_msg = f\"Error fetching files: {str(e)}\"\n",
    "                logging.info(err_msg)\n",
    "                return \"Failure\", err_msg\n",
    "            elif attempt_counter <= 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_msg = f\"Error fetching files: {str(e)}\"\n",
    "                logging.info(err_msg)\n",
    "                return \"Failure\", err_msg\n",
    "        if file_results:\n",
    "            total_files_fetched += len(file_results)\n",
    "            page_number += 1\n",
    "            for file in file_results:\n",
    "                file_id = file.file_id\n",
    "                file_path = file.path\n",
    "                file_size = file.size\n",
    "                file_md5 = \"\"\n",
    "                for cs in file.checksums:\n",
    "                    if cs.type == \"md5\":\n",
    "                        file_md5 = cs.checksum\n",
    "                original_url = file.description.replace(\"Ingest of \", \"\")\n",
    "                access_url = file.file_detail.access_url\n",
    "                file_list.append([\"tdr\", dataset_id, file_id, file_path, file_size, file_md5, original_url, access_url])\n",
    "            if len(file_results) < max_page_size:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Convert to dataframe and write to BigQuery\n",
    "    client = bigquery.Client()\n",
    "    logging.info(f\"{total_files_fetched} files found. Writing results to BigQuery.\")\n",
    "    df_file_list = pipeline_results = pd.DataFrame(file_list, columns = [\"object_type\", \"object_id\", \"file_id\", \"file_path\", \"file_size\", \"file_md5\", \"file_original_url\", \"file_access_url\"])\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = \"WRITE_APPEND\"\n",
    "    job = client.load_table_from_dataframe(df_file_list, target_bigquery_table, job_config=job_config)\n",
    "    job.result()\n",
    "    return \"Success\", \"\"\n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# General parameters\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.current_azure_inventory\"\n",
    "dataset_id_list = [\n",
    "    '9fe49126-f4ac-4e46-a231-99820fb0d4c2',\n",
    "    '20a58869-8e3c-4959-8b04-fb12044ac0d3',\n",
    "]\n",
    "#dataset_id_list = ['2c7b4971-a67b-4786-ac86-de56f968cc84']\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    logging.info(f\"Processing Dataset ID: {dataset_id}\")\n",
    "    status, message = output_file_details(dataset_id, target_bigquery_table)\n",
    "    results.append([dataset_id, status, message])\n",
    "        \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Dataset ID\", \"Status\", \"Message\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Collecting Files in Azure Workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to record file information from Azure datasets\n",
    "def output_file_details(workspace, target_bigquery_table):\n",
    "    \n",
    "    # Setup/refresh credentials and BQ client\n",
    "    creds, project = google.auth.default(scopes=['https://www.googleapis.com/auth/cloud-platform', 'openid', 'email', 'profile'])\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Delete records for dataset if already in BQ table\n",
    "#     logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "#     delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE object_id = '{workspace}'\"\"\"\n",
    "#     try:\n",
    "#         delete_query_job = client.query(delete_query)\n",
    "#         delete_query_job.result()\n",
    "#     except Exception as e:\n",
    "#         err_msg = f\"Error deleting records for the original workspace from the target BQ table: {str(e)}\"\n",
    "#         logging.info(err_msg)\n",
    "#         return \"Failure\", err_msg\n",
    "\n",
    "    # Get Workspace ID\n",
    "    logging.info(f\"Fetching and recording all files found in the original workspace ({workspace}).\") \n",
    "    try:\n",
    "        workspace_id = \"\"\n",
    "        workspace_response = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/AnVILDataStorage_Azure/{workspace}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "        workspace_id = workspace_response[\"workspace\"][\"workspaceId\"]\n",
    "\n",
    "        # Get Workspace Resources\n",
    "        resource_id = \"\"\n",
    "        storage_container = \"\"\n",
    "        workspace_response = requests.get(\n",
    "            url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{workspace_id}/resources?offset=0&limit=10&resource=AZURE_STORAGE_CONTAINER\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json() \n",
    "        for resource_entry in workspace_response[\"resources\"]:\n",
    "            storage_container_name = resource_entry[\"resourceAttributes\"][\"azureStorageContainer\"][\"storageContainerName\"]\n",
    "            if storage_container_name.startswith(\"sc-\"):\n",
    "                resource_id = resource_entry[\"metadata\"][\"resourceId\"]\n",
    "                storage_container = storage_container_name\n",
    "\n",
    "        # Generate SAS URL\n",
    "        sas_response = requests.post(\n",
    "            url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{workspace_id}/resources/controlled/azure/storageContainer/{resource_id}/getSasToken?sasExpirationDuration=86400\",\n",
    "            headers={\"Authorization\": \"Bearer \" + creds.token, \"accept\": \"application/json\"}\n",
    "        )\n",
    "        sas_response_json = json.loads(sas_response.text)\n",
    "        sas_token = sas_response_json.get(\"token\")\n",
    "        sas_url = sas_response_json.get(\"url\")\n",
    "        print(sas_url)\n",
    "        return \"Temp\", \"Temp\"\n",
    "        base_url = sas_url.replace(\"/\" + storage_container + \"?\" + sas_token, \"\")\n",
    "\n",
    "        # Establish Azure Clients and pull files\n",
    "        logger = logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\")\n",
    "        logger.setLevel(logging.WARNING)   \n",
    "        file_list = []\n",
    "        blob_service_client = BlobServiceClient(account_url=base_url, credential=sas_token)\n",
    "        container_client = blob_service_client.get_container_client(storage_container)\n",
    "        blob_list = container_client.list_blobs(results_per_page=1000)\n",
    "        paged_list = blob_list.by_page()\n",
    "        page_number = 0\n",
    "        for blob_page in paged_list:\n",
    "            page_number += 1\n",
    "            logging.info(f\"Pulling files from page {str(page_number)}...\")\n",
    "            for blob in blob_page:\n",
    "                blob_client = container_client.get_blob_client(blob)\n",
    "                props = blob_client.get_blob_properties()\n",
    "                if not blob.name.endswith('/') and not blob.deleted:\n",
    "                    md5_hash = base64.b64encode(props.content_settings.content_md5).decode(\"utf-8\") if props.content_settings.content_md5 else \"\"\n",
    "                    full_path = blob_client.url.replace(f\"?{sas_token}\", \"\")\n",
    "                    file_id = \"\"\n",
    "                    file_path = \"/\" + blob.name\n",
    "                    file_size = props.size\n",
    "                    file_list.append([\"workspace\", workspace, file_id, file_path, file_size, md5_hash, \"\", full_path]) \n",
    "        total_files_fetched = len(file_list)\n",
    "    except Exception as e:\n",
    "        err_msg = f\"Error fetching files: {str(e)}\"\n",
    "        logging.info(err_msg)\n",
    "        return \"Failure\", err_msg\n",
    "                \n",
    "    # Convert to dataframe and write to BigQuery\n",
    "    logging.info(f\"{total_files_fetched} files found. Writing results to BigQuery.\")\n",
    "    df_file_list = pipeline_results = pd.DataFrame(file_list, columns = [\"object_type\", \"object_id\", \"file_id\", \"file_path\", \"file_size\", \"file_md5\", \"file_original_url\", \"file_access_url\"])\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = \"WRITE_APPEND\"\n",
    "    job = client.load_table_from_dataframe(df_file_list, target_bigquery_table, job_config=job_config)\n",
    "    job.result()\n",
    "    return \"Success\", \"\"\n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# General parameters\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.current_azure_inventory\"\n",
    "workspace_list = [\n",
    "    'ANVIL_dbGap_data_conversion_Azure',\n",
    "]\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for workspace in workspace_list:\n",
    "    logging.info(f\"Processing Workspace: {workspace}\")\n",
    "    status, message = output_file_details(workspace, target_bigquery_table)\n",
    "    results.append([workspace, status, message])\n",
    "        \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Workspace\", \"Status\", \"Message\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "workspace = 'AnVIL_NIA_CARD_LR_WGS_Deposit_Azure'\n",
    "\n",
    "# Setup/refresh credentials and BQ client\n",
    "creds, project = google.auth.default(scopes=['https://www.googleapis.com/auth/cloud-platform', 'openid', 'email', 'profile'])\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "client = bigquery.Client()\n",
    "\n",
    "# # Delete records for dataset if already in BQ table\n",
    "# logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "# delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE object_id = '{workspace}'\"\"\"\n",
    "# try:\n",
    "#     delete_query_job = client.query(delete_query)\n",
    "#     delete_query_job.result()\n",
    "# except Exception as e:\n",
    "#     err_msg = f\"Error deleting records for the original workspace from the target BQ table: {str(e)}\"\n",
    "#     logging.info(err_msg)\n",
    "#     return \"Failure\", err_msg\n",
    "\n",
    "# Get Workspace ID\n",
    "logging.info(f\"Fetching and recording all files found in the original workspace ({workspace}).\") \n",
    "try:\n",
    "    workspace_id = \"\"\n",
    "    workspace_response = requests.get(\n",
    "        url=f\"https://api.firecloud.org/api/workspaces/AnVILDataStorage_Azure/{workspace}\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "    ).json()\n",
    "    workspace_id = workspace_response[\"workspace\"][\"workspaceId\"]\n",
    "\n",
    "    # Get Workspace Resources\n",
    "    resource_id = \"\"\n",
    "    storage_container = \"\"\n",
    "    workspace_response = requests.get(\n",
    "        url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{workspace_id}/resources?offset=0&limit=10&resource=AZURE_STORAGE_CONTAINER\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "    ).json() \n",
    "    for resource_entry in workspace_response[\"resources\"]:\n",
    "        storage_container_name = resource_entry[\"resourceAttributes\"][\"azureStorageContainer\"][\"storageContainerName\"]\n",
    "        if storage_container_name.startswith(\"sc-\"):\n",
    "            resource_id = resource_entry[\"metadata\"][\"resourceId\"]\n",
    "            storage_container = storage_container_name\n",
    "\n",
    "    # Generate SAS URL\n",
    "    sas_response = requests.post(\n",
    "        url=f\"https://workspace.dsde-prod.broadinstitute.org/api/workspaces/v1/{workspace_id}/resources/controlled/azure/storageContainer/{resource_id}/getSasToken?sasExpirationDuration=86400\",\n",
    "        headers={\"Authorization\": \"Bearer \" + creds.token, \"accept\": \"application/json\"}\n",
    "    )\n",
    "    sas_response_json = json.loads(sas_response.text)\n",
    "    sas_token = sas_response_json[\"token\"]\n",
    "    sas_url = sas_response_json[\"url\"]\n",
    "    base_url = sas_url.replace(\"/\" + storage_container + \"?\" +sas_token, \"\")\n",
    "\n",
    "    # Establish Azure Clients and pull files\n",
    "    logger = logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\")\n",
    "    logger.setLevel(logging.WARNING)   \n",
    "    file_list = []\n",
    "    blob_service_client = BlobServiceClient(account_url=base_url, credential=sas_token)\n",
    "    container_client = blob_service_client.get_container_client(storage_container)\n",
    "    blob_list = container_client.list_blobs(results_per_page=1000, name_starts_with='CARD_cell_line_first_data_release/HG002', include='deleted')\n",
    "    paged_list = blob_list.by_page()\n",
    "    page_number = 0\n",
    "    for blob_page in paged_list:\n",
    "        page_number += 1\n",
    "        logging.info(f\"Pulling files from page {str(page_number)}...\")\n",
    "        for blob in blob_page:\n",
    "            print(blob.name)\n",
    "            print(blob.deleted)\n",
    "            blob_client = container_client.get_blob_client(blob)\n",
    "            props = blob_client.get_blob_properties()\n",
    "            if blob.deleted:\n",
    "                print(blob.name)\n",
    "                print(blob.deleted)\n",
    "                print(type(blob.deleted))\n",
    "                break\n",
    "            if not blob.name.endswith('/') and not blob.deleted:\n",
    "                md5_hash = base64.b64encode(props.content_settings.content_md5).decode(\"utf-8\") if props.content_settings.content_md5 else \"\"\n",
    "                full_path = blob_client.url.replace(f\"?{sas_token}\", \"\")\n",
    "                file_id = \"\"\n",
    "                file_path = \"/\" + blob.name\n",
    "                file_size = props.size\n",
    "                file_list.append([\"workspace\", workspace, file_id, file_path, file_size, md5_hash, \"\", full_path]) \n",
    "    total_files_fetched = len(file_list)\n",
    "except Exception as e:\n",
    "    err_msg = f\"Error fetching files: {str(e)}\"\n",
    "    logging.info(err_msg)\n",
    "    #return \"Failure\", err_msg\n",
    "\n",
    "# # Convert to dataframe and write to BigQuery\n",
    "# logging.info(f\"{total_files_fetched} files found. Writing results to BigQuery.\")\n",
    "# df_file_list = pipeline_results = pd.DataFrame(file_list, columns = [\"object_type\", \"object_id\", \"file_id\", \"file_path\", \"file_size\", \"file_md5\", \"file_original_url\", \"file_access_url\"])\n",
    "# job_config = bigquery.LoadJobConfig()\n",
    "# job_config.write_disposition = \"WRITE_APPEND\"\n",
    "# job = client.load_table_from_dataframe(df_file_list, target_bigquery_table, job_config=job_config)\n",
    "# job.result()\n",
    "# return \"Success\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for page in paged_list:\n",
    "    for blob in blob_page:\n",
    "            print(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "blob_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Collecting Files in TDR Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     88
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Functions\n",
    "#############################################\n",
    "\n",
    "# Function to record file information from TDR snapshots\n",
    "def output_file_details(snapshot_id, target_bigquery_table):\n",
    "\n",
    "    # Delete records for dataset if already in BQ table\n",
    "    client = bigquery.Client()\n",
    "    logging.info(f\"Preparing target BQ table ({target_bigquery_table}).\")\n",
    "    delete_query = f\"\"\"DELETE FROM `{target_bigquery_table}` WHERE snapshot_id = '{snapshot_id}'\"\"\"\n",
    "    try:\n",
    "        delete_query_job = client.query(delete_query)\n",
    "        delete_query_job.result()\n",
    "    except Exception as e:\n",
    "        err_msg = f\"Error deleting records for the original snapshot from the target BQ table: {str(e)}\"\n",
    "        logging.info(err_msg)\n",
    "        return \"Failure\", err_msg\n",
    "    \n",
    "    # Pull snapshot details\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    snapshot_detail = snapshots_api.retrieve_snapshot(id=snapshot_id, include=[\"SOURCES\"])\n",
    "    phs_id = snapshot_detail.source[0].dataset.phs_id\n",
    "    consent_code = snapshot_detail.consent_code\n",
    "    \n",
    "    # Page through and pull files from snapshot\n",
    "    logging.info(f\"Fetching and recording all files found in the original snapshot ({snapshot_id}).\") \n",
    "    file_list = []\n",
    "    total_files_fetched = 0\n",
    "    page_number = 1\n",
    "    max_page_size = 1000\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        logging.info(f\"Pulling files from page {str(page_number)}...\")\n",
    "        try:\n",
    "            api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "            snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "            file_results = snapshots_api.list_files(id=snapshot_id, offset=total_files_fetched, limit=max_page_size)\n",
    "        except Exception as e:\n",
    "            if \"Invalid UUID string\" in str(e) or \"Snapshot not found\" in str(e):\n",
    "                err_msg = f\"Error fetching files: {str(e)}\"\n",
    "                logging.info(err_msg)\n",
    "                return \"Failure\", err_msg\n",
    "            elif attempt_counter <= 5:\n",
    "                sleep(10)\n",
    "                attempt_counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                err_msg = f\"Error fetching files: {str(e)}\"\n",
    "                logging.info(err_msg)\n",
    "                return \"Failure\", err_msg\n",
    "        if file_results:\n",
    "            total_files_fetched += len(file_results)\n",
    "            page_number += 1\n",
    "            for file in file_results:\n",
    "                file_id = file.file_id\n",
    "                file_path = file.path\n",
    "                file_size = file.size\n",
    "                file_md5 = \"\"\n",
    "                for cs in file.checksums:\n",
    "                    if cs.type == \"md5\":\n",
    "                        file_md5 = cs.checksum\n",
    "                access_url = file.file_detail.access_url\n",
    "                drs_uri = \"drs://drs.anv0:v2_\" + file_id\n",
    "                file_list.append([snapshot_id, phs_id, consent_code, drs_uri, file_id, file_path, file_size, file_md5, access_url])\n",
    "            if len(file_results) < max_page_size:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Convert to dataframe and write to BigQuery\n",
    "    client = bigquery.Client()\n",
    "    logging.info(f\"{total_files_fetched} files found. Writing results to BigQuery.\")\n",
    "    df_file_list = pipeline_results = pd.DataFrame(file_list, columns = [\"snapshot_id\", \"phs_id\", \"consent_code\", \"drs_uri\", \"file_id\", \"file_path\", \"file_size\", \"file_md5\", \"file_access_url\"])\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = \"WRITE_APPEND\"\n",
    "    job = client.load_table_from_dataframe(df_file_list, target_bigquery_table, job_config=job_config)\n",
    "    job.result()\n",
    "    return \"Success\", \"\"\n",
    "    \n",
    "\n",
    "#############################################\n",
    "## Input Parameters\n",
    "#############################################\n",
    "\n",
    "# General parameters\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_inventory.snapshot_drs_export\"\n",
    "snapshot_id_list = [\n",
    "    '01cf2450-604b-43e5-9f4e-9ec4e0bf0a61',\n",
    "    '85b0b351-cd0a-4efe-95a4-e39273c42831',\n",
    "    '2fdba9a4-6593-439a-a7fc-c3a5825c26cd',\n",
    "    'ad4ed62b-bf63-4dff-ab94-70a6432c161c',\n",
    "    'b300b5ae-6ca3-4350-bc46-345173f6faba',\n",
    "    '7bc891a2-a634-4cf2-b41e-0b1e98fce599',\n",
    "    '9dbac1be-a33c-419c-be92-d1a5452c1292',\n",
    "    '40d6feec-e6f7-42f1-8e74-a3404e1f9208',\n",
    "    '95b4c57b-8e88-45f5-9dbb-e2575f4b2a68',\n",
    "    '6f1d6a31-1997-4b59-a311-f84631ebdcbf',\n",
    "    '79502d0c-bc1c-4d51-a6de-eb0334b3b660',\n",
    "    '0af0d35e-1f9a-464d-80fe-474b5dbbd914',\n",
    "    '79c20af6-5788-47ce-9651-f6a6ae084cbc',\n",
    "    'f90f565e-0ade-4750-a308-5c8e1677b43d',\n",
    "    '194c4b14-cb6a-469c-83db-d37f7ec65f29',\n",
    "    '88b16321-7f0a-44b1-8131-d4b2188d9839',\n",
    "    '9345adce-2f83-4c02-8859-72ddccb22069',\n",
    "    '75f5452d-ceca-402e-bfc4-759c8352f4da',\n",
    "    'cdcfc6ac-6c9f-4d99-a8c3-4d1e5d171261',\n",
    "    '6441b9e0-ca7b-4ab4-b7e7-9c7c7041ebaa',\n",
    "    'ae0c27d6-c8e3-4dd5-abf5-06e5f39fc4a0',\n",
    "    'cb5a6268-c0c8-433d-b62c-7beeeb0a6a92',\n",
    "    'ad18153b-870c-491a-9d4e-df30d902a03f',\n",
    "    '461c1b26-7306-4feb-b141-f83c209baf27',\n",
    "    '6a477149-a7f0-4758-8570-b288a8314fbd',\n",
    "    '07b0243c-48fc-4eee-a338-c7571cc2df1a',\n",
    "    '94f79040-68f5-4801-bf41-6f29bc0be8c6',\n",
    "    '53fd76c8-6745-414e-adbe-62ff72011fc5',\n",
    "    'f6db6471-03c5-44b6-a463-4976d8fc6350',\n",
    "    '1985e363-b6da-47ec-8c92-dabcd587e6b6',\n",
    "    'e8dacaea-d37d-48e5-b0a9-c88777753423',\n",
    "    '0d3a9994-49e4-45ee-add9-1af8909ed298',\n",
    "    '50a46bb1-1fd3-4745-999c-0201edd5dcd2',\n",
    "    'a8febca7-6f5e-4284-b6c6-fd345a01997f',\n",
    "    'a14a4e7e-5bb9-41e9-a000-728ed7a24418',\n",
    "    'a29a3406-c176-4129-b778-adf27cdb4ced',\n",
    "    '694937e1-4919-47f1-aa35-4db860e70763',\n",
    "    'a00fcb63-6b32-46e6-b31e-39424da76a15',\n",
    "    'd858e821-cfb9-435e-b1c0-39e95898b6b8',\n",
    "    '01d7ebf5-b429-403d-aaae-831bba6bc08b',\n",
    "    'b905ceb2-d9b3-4b0c-8c2c-4d3552aa0a65',\n",
    "    '14e83711-6650-4649-9921-4f8dc93f20e3',\n",
    "    '66d53a48-3fe3-4069-a664-9955e5a61f4b',\n",
    "    'fab48331-5eb9-4546-b24b-4153912fdce3',\n",
    "    '4cb9a680-c7f6-4b78-915e-6761af08489d',\n",
    "    '5d2df2e4-e93e-41de-910f-547c29751891',\n",
    "    'fc8a4c6d-02d8-4bf4-ae4e-db326056c383',\n",
    "    '35533657-7416-4fcb-b8e6-edc0ced2d845',\n",
    "    'c24cd039-b55d-486c-88f4-4ad36c732998',\n",
    "    '7211c075-4d65-43f6-91b0-20afbbf52ae8',\n",
    "    '53426657-9ee3-4a28-848a-6371e42590a5',\n",
    "    '27c5d19b-311a-4fa3-aa85-fe577117e835',\n",
    "    '5e81dcbb-86c1-43d8-9b54-8ec248abfe3b',\n",
    "    '0fe1fbab-7d97-42d3-bea4-74d8d25d41a3',\n",
    "    '79e9bf95-531b-4351-a1c0-670af028be26',\n",
    "    'fcd069d1-c657-4631-a4a5-334b2ab32535',\n",
    "    '8a57de06-0bb0-4e9f-9fde-3c24582078d8',\n",
    "    'ee46ffd0-0ccc-401e-b352-13b18cdc5d44',\n",
    "    '53a55f8d-949d-4f63-be0d-d02f466be469',\n",
    "    '8f3aaeb7-997e-4378-9ae4-73de81999edb',\n",
    "    '4f1c52eb-0aff-4d4d-a45b-076998ebb092',\n",
    "    '6c105846-e057-4378-8670-c9efa53402a6',\n",
    "    'f41d7fc3-5756-482f-92c3-453e68e211a6',\n",
    "    '7ea38205-4437-4dab-b9d5-3e05e33a9650',\n",
    "    'f12683d4-f970-4d19-98b8-51f17bc7cae1',\n",
    "    '9f317e49-d880-4b34-b192-b22880869efd',\n",
    "    '09b6df1a-2fe6-4b9a-8134-936bae416497',\n",
    "    'ee25a947-21d0-4272-bb2b-6f3007ccce9a',\n",
    "    '4d509d4b-14d5-4d78-a09f-4de74a9cd39a',\n",
    "    '3eb6bb6f-f7e6-43d4-9f1f-7864d22485f6',\n",
    "    '7efd80c0-1f52-4866-a0b9-094b6409de83',\n",
    "    '6ffb37a4-28ce-47d8-be08-3b3c82035a41',\n",
    "    '47279ceb-fd83-4ad0-bca8-70d54da9422b',\n",
    "    '4b4299e1-4792-4de8-8fcb-568f46cf8412',\n",
    "    'd93b025f-5cec-42e1-b3f7-e02f695a14e8',\n",
    "    'fcb345ff-3371-4cf4-ab0a-174c61162150',\n",
    "    'f52a7bc0-0caa-478e-87ca-30915002434c',\n",
    "    '832e5fb4-d584-4c55-8e2d-af3762344194',\n",
    "    '2feb61bb-2b60-4042-a90a-94687a9954d3',\n",
    "    '97721323-affa-4d87-9e6d-05002df97338',\n",
    "    '23189b7e-aed8-4311-92ba-587d1116d749',\n",
    "    '3dfd6eb9-9a7b-4745-8cba-f8f52565bd4d',\n",
    "    '5dd57133-db66-41d4-b922-0db2b5645632',\n",
    "    'e437f3b5-afc9-4c98-a828-9bb8b683ff26',\n",
    "    '2104eae5-a3e6-4bc2-b558-aa60db07338e',\n",
    "    '71ff1f40-bb8c-4173-a203-ba23bcd0ee24',\n",
    "    'b58c7949-510e-48d7-80c3-b5fda1669ecb',\n",
    "    'acbe4c99-c129-409d-a60b-5033c6053f1f',\n",
    "    'a9129165-0a5a-4817-a26e-9d68a75870da',\n",
    "    '0eb173b4-afac-463c-a7dd-3d296fa104da',\n",
    "    'b97322c6-c769-4c42-8fa2-6724cddf7575',\n",
    "    '07864c11-e488-43b6-ad56-bf400adbb289',\n",
    "    '535705ef-bd8f-437d-bc50-31353965d232',\n",
    "    'b7ef6b4d-efa5-46e4-9f11-5a3f009c9d94',\n",
    "    'a3f4ad5c-7e26-4fbc-8106-1c0aba10744d',\n",
    "    'b5ca118d-ef68-4861-bc29-b1245154a57e',\n",
    "    'f457da62-894b-44be-a3a7-f908be937ab2',\n",
    "    '18b009a8-6537-4d07-aa3f-93820789bdfc',\n",
    "    'd7dc181f-ccbf-4715-9bc4-79ee63f395af',\n",
    "    'e7d96fd3-4e6c-41df-9d01-ad88e084dc6f',\n",
    "    'dce7657c-14b3-4c9f-b067-eb32163b28dc',\n",
    "    'e2ae5f40-13f9-4f65-8bb1-32ac544a0fec',\n",
    "    '3c0c307f-b507-4463-a0bd-6852e2657e53',\n",
    "    'e7c5a9bf-108f-4e63-85eb-0f12d98342e9',\n",
    "    'e6eddaa4-d5b4-4cec-bbf1-40242ea3a6e1',\n",
    "    'bf348a75-eb6d-464a-a705-16047d93b824',\n",
    "    '76e02df6-28d5-474d-b3df-d9427ab4b5e4',\n",
    "    'c6b976b2-5473-41ad-96ac-e04e4821cd2d',\n",
    "    'c6de2268-5d7a-4b0d-a055-8e558dbc60cc',\n",
    "    '634d44c2-f101-44dd-9568-0ea2e080855d',\n",
    "    'eb81e0ad-5ba9-46e9-971d-e291b407b441',\n",
    "    'b26a2be2-beab-4d72-a2ce-de45f6aebfeb',\n",
    "    '7bda878d-cfdf-42a8-b9ef-f82f39b0c651',\n",
    "    'f7242460-e413-4d3e-92c9-21e69709f9ae',\n",
    "    'b4c4452a-d588-457d-877d-bc652a5e0ddc',\n",
    "    '19826317-e7de-4cf3-9e36-1a34b556e310',\n",
    "    'c873e8c0-d38f-4b5c-8bc4-582f57bb7811',\n",
    "    '247b3216-ab1b-4d36-8fab-f6695c6481f9',\n",
    "    '3c9aea01-f190-4721-99c7-30e84dee1464',\n",
    "    '6fb0cf8b-09b9-49f7-9ea4-0009f968ddb3',\n",
    "    'b37faff3-0851-4866-986c-171c663dba66',\n",
    "    '9da2472f-65b5-4dc4-8dbc-b6ad298bdbed',\n",
    "    'a744898d-be25-483c-bf08-d3dd4f99346d',\n",
    "    '5b2d5e9d-07f6-409c-b759-e9e12561df45',\n",
    "    '0ff4e69c-fd1c-4c6e-83c7-02334364cc2a',\n",
    "    'de61f82c-f51f-476c-a5cd-14757ceb571d',\n",
    "    'f291a61b-8d1c-47fc-b4de-09048a6db2ac',\n",
    "    'f5ad2a09-5885-4164-b160-36bb8d33cd77',\n",
    "    '82f10584-d7e8-40ea-9347-fdf6437b3db1',\n",
    "    '3a573fa9-08c4-43a1-b248-1ca97b501718',\n",
    "    '2c87eb60-e958-4fc3-aa8d-fa393647da11',\n",
    "    '7f735e63-bbc6-495c-86b4-aa4f8eb3596e',\n",
    "    'd2a81a67-f615-4dd1-a86a-b877d6eaaa2b',\n",
    "    '4d1e2bdd-7763-4c33-8e96-2702d81cab59',\n",
    "    '7baf715d-930d-45b6-afbd-15a9ff360eb2',\n",
    "    '1a243684-5924-4b26-a1e5-0d62de99b974',\n",
    "    'c0369447-9bd4-4b92-a426-ec69e432630d',\n",
    "    'e3a0f135-28ff-4882-9c66-4ddee0561b7f',\n",
    "    'e038aa3d-3a46-458c-a5af-77cccef31e11',\n",
    "    'b7cc8070-e616-43e5-8dde-12c7962278dc',\n",
    "    '0d8d1286-433a-4b1f-8b99-09e3f8066998',\n",
    "    'ee9f92a0-eee3-4af0-816a-98ab698be2d4',\n",
    "    '4508cd6a-527c-45b1-b36e-a8253c037c01',\n",
    "    'b8df672a-23f6-4d4e-b6bd-cdec514b7e1f',\n",
    "    '66eceadc-7f5d-4951-aa7b-417c477e4e25',\n",
    "    '7f0397d1-ae43-4201-a36a-f597c64598e3',\n",
    "    '8df70ead-fa9d-4ec4-b5d4-9f6d9a9afe03',\n",
    "    '9499831f-712b-4d38-8c5a-491eb5229844',\n",
    "    '1ee44373-1e44-4a5c-a435-eaa702c946f5',\n",
    "    '4f3220c9-8cbc-4245-9394-f752fd6063eb',\n",
    "    'aa955b24-23b8-4310-9c20-38daa20e9881',\n",
    "    '96375f1f-59a4-4fe5-b58c-bb459b70cc17',\n",
    "    '8451b97b-fad4-44e1-b9a7-6bd78702a546',\n",
    "    '2b82d3da-39ed-4814-b497-d7e7c1c8c431',\n",
    "    'e876fd42-5765-4946-8686-0147084d3b47',\n",
    "    'f5f576e6-715e-4e42-a53b-5eb2cf59b1c6',\n",
    "    '2a758bcb-ba02-4be7-ac8e-8b5750a3bfa4',\n",
    "    'a31e12de-826d-4fc5-8191-e48906179fdc',\n",
    "    '9ecaeb8e-8e12-4437-8c98-9c57fc151d4a',\n",
    "    '0cd6e0a3-616a-4993-885e-088f9abee915',\n",
    "    'fceeb22c-feb3-439a-a457-6b34097a4532',\n",
    "    '97f2da30-bb21-4e69-a84c-443fcf7ea6bc',\n",
    "    '05058de3-8f8a-4098-bab8-844427923e6c',\n",
    "    '8bb724f7-e407-4e9c-8b62-e6ce60a7e4f8',\n",
    "    'd1d6bcc3-e7d0-4bdb-9f48-a87a222869f4',\n",
    "    'c2561327-2ba0-4c79-ad74-b393f3b3a933',\n",
    "    '719337c5-7c4d-4dd8-bb31-462a182781b8',\n",
    "    '0d98abdd-01ba-4af8-b56a-ed90f89e5bd1',\n",
    "    '340d35d2-df52-4786-b0c7-dd1b8ee343cf',\n",
    "    'b9e66305-c7c1-4b84-859b-d716fbb928f6',\n",
    "    'ce8a56ce-a016-4eda-8cdf-4e5ff3ce5e9f',\n",
    "    'a90ec7cf-dce8-4fb6-9fb8-8adb130ae518',\n",
    "    '163d6dc2-2770-4c30-a293-7c8576737f5c',\n",
    "    '4729a86a-1e08-4392-b5a6-d687ff48cfb2',\n",
    "    '1e2d99b0-c98f-471e-a177-2e57a83634bc',\n",
    "    '77ff80cd-bcf9-49b1-9823-ea38665cd8c2',\n",
    "    '37488483-1ae1-495e-a85b-0aeba08fad39',\n",
    "    'ce8a8649-d311-4363-a220-eceafaa7615e',\n",
    "    'fec9dec9-7ce4-4bb2-bd52-243a06c0ecce',\n",
    "    '0e4c60e0-74c7-478e-9ffe-400697b5217a',\n",
    "    '43df6fd8-b590-4bf6-afe0-13030d683d8e',\n",
    "    '81caad3d-22f5-4c75-8a38-96e7b7e740c1',\n",
    "    '43115bac-0dea-4dcd-b881-414182a37b35',\n",
    "    '60c90a71-0928-41aa-b7b3-30cc043383be',\n",
    "    'ccaa2c0f-214d-4ed9-94ec-b8df01351dd8',\n",
    "    '4616ebc2-2864-41e0-90f2-f05abc3193d8',\n",
    "    '4b67c08e-85e0-49d5-82ea-76dd3787db5d',\n",
    "    '16c06f69-2241-4cf1-8a33-a9b0a0c41b7d',\n",
    "    '51a36b75-41ac-4b3e-8d25-6b3bc4f1ced3',\n",
    "    '9034e587-f306-4a7a-931d-cb161ce9ddf3',\n",
    "    'd70ef6b0-de45-46cc-9587-7ebf0c806db8',\n",
    "    'f1b9f860-46ce-4503-8314-b0d2d33f3f1e',\n",
    "    '4032ce09-e024-4d4b-8149-54ac5477c8fa',\n",
    "    'cd6e3a69-dfc1-4166-b128-682996a48798',\n",
    "    '0ac01a18-0bf7-4e53-acbe-3971234b8899',\n",
    "    'bf7a5681-ca5d-4e92-8726-071ac409a85c',\n",
    "    '8433547b-8182-4cab-a3f4-44a1deedbb0f',\n",
    "    'd5cb020a-b3cc-4569-8873-58924ef9dad7',\n",
    "    'dd9f78e4-757b-455d-83ac-ba9dfeeed6e1',\n",
    "    'f7307ad5-7250-483c-a8e2-aeb1c1226b13',\n",
    "    '9e210e43-61b5-4a85-8d1a-0abc111f60db',\n",
    "    'c720c477-dfe2-45e0-805f-94ed11d006b4',\n",
    "    'e4bc2365-282a-4c51-afe3-971899f61f74',\n",
    "    '2df015fc-06f8-46aa-a9ac-e12bc608b2f3',\n",
    "    'c68d1d19-aac9-4042-ab62-5f40837fe10d',\n",
    "    '55d26ed5-f046-414f-8c8b-bdc941fa58aa',\n",
    "    '5ee23a8c-e9d7-49b9-b23e-ecf1c1cf51a1',\n",
    "    '39fd1d5b-2e6f-4035-beb3-05322174dd4b',\n",
    "    'dc9c9587-1ec3-42d5-bcd5-ce425f79e7d0',\n",
    "    'c10c8878-2b5a-4468-9931-c35a6ee034ae',\n",
    "    '71a24ce8-9e13-4b16-9812-367d929d6367',\n",
    "    'a65a95b8-d6ae-4349-9478-f4df9f43801e',\n",
    "    'e70ce083-4e8e-403a-a222-6bf235521930',\n",
    "    'f6d1d60f-d5b7-4944-94be-a9dbb17d9057',\n",
    "    '450cc5ad-47d6-434a-81a6-783fe277a58c',\n",
    "    'cbfe8c00-7683-4e34-a7ee-23cfb0a1145d',\n",
    "    'e8afa16f-16de-4bc7-ab91-0a1667f088a3',\n",
    "    '03ce0d8a-13a8-4b6f-8a80-7c016c583ee2',\n",
    "    'e4245e63-a141-4547-aa20-bbb5ad7c6f4c',\n",
    "    'a0be830f-6d75-405c-a14b-95c56ee88f06',\n",
    "    'a8b8b258-2b61-40a8-95b2-68247cf29eb3',\n",
    "    '1c6223bf-9665-4b54-bbc3-40847ebaf92c',\n",
    "    'c0d588bc-5a77-490f-86b0-fcbe3f06654c',\n",
    "    '4277cb35-8d37-49c7-8bb1-eeb57a68b739',\n",
    "    'ba20eb5d-3553-4f0c-92eb-5793697da74d',\n",
    "    '2054866e-b906-42cd-9e1a-d3eaf1b6057e',\n",
    "    '568ba9a0-95ba-486b-bab1-efc3798e5f41',\n",
    "    'd5f3d7ce-5a55-4508-b662-e624a83b304d',\n",
    "    'ce99c021-32fc-4278-bad0-03f9dadeeed8',\n",
    "    '00c1a1f1-6bfc-478b-8b32-5d5911081638',\n",
    "    '2283bdf2-82c2-45c8-9e2c-f5855bd6e103',\n",
    "    '07c0ce25-4c13-443c-b2c0-721620747ec3',\n",
    "    '29ec75f0-53ac-405f-a973-b034126ae457',\n",
    "    '9393d37f-8c9d-43fa-a42a-52536a24236d',\n",
    "    '79bdf4d2-10e0-4736-83fc-9a4d207659d9',\n",
    "    'ed88c711-d973-4363-8fee-c88439a8dd57',\n",
    "    '2cff71df-202e-43d1-9f62-997f01bf23ac',\n",
    "    '97a2190a-ca36-4423-bcef-f9f8be800187',\n",
    "    '753d9871-495a-4cab-944a-e925ec269282',\n",
    "    '7397dca5-45a0-4744-9222-50dbcfbc1500',\n",
    "    '4e5d4393-1693-44bb-ab65-1d7a51c13dff',\n",
    "    'fd8eab82-8317-4d9c-97aa-9752a0b9340e',\n",
    "    '7daf63d7-56af-4ddc-9601-4ae7605f0420',\n",
    "    '583b36e5-8843-4be2-b40f-9fde7151497f',\n",
    "    '3532b277-89ff-46b3-a6fd-7f002e524bb4',\n",
    "    '161c78cb-899e-41d8-af6b-a400ec7322f0',\n",
    "    '82d35b38-8b69-4eaf-b298-ff35fae9c092',\n",
    "    '9406166c-28ce-454c-9db6-fc2cdafa4913',\n",
    "    'fde2ddfd-3edf-415f-888b-cc5dba868669',\n",
    "    'ff8f595d-9b15-419a-9df1-fc3bd94184f0',\n",
    "    'fde093c5-a4b4-4080-ba36-7663e0de7047',\n",
    "    'c1009cce-77ff-47cd-b349-109a1bd283d5',\n",
    "    '03ad9d34-f5f7-428a-8411-44b97815a95c',\n",
    "    '26edc6c1-f79c-4bb6-84b0-d1492fd64738',\n",
    "    'b48a3a5c-9d3f-4228-8a24-19f315890f7b',\n",
    "    '4312ca34-388a-4d28-baa3-b64ec6eaf7af',\n",
    "    'f71051be-17e3-42bf-b534-5af0e3740937',\n",
    "    '98a13ebb-0a91-427e-bdd5-8f5833036d81',\n",
    "    '4a815323-5dc1-4a0c-93c0-1c2333345c4d',\n",
    "    'db0c9cff-f3f9-4315-addb-6b6c9088b124',\n",
    "    '3c279159-bd0e-4df9-bcea-9640e49a694b',\n",
    "    '1cd2be27-fc8b-4779-9398-d06979ce43c3',\n",
    "    '08568995-f347-4c5a-bd55-d9cc44441e07',\n",
    "    '4998e043-4d6a-4003-b6d4-0170b0ca31b5',\n",
    "    '82d7d1b6-15b7-4c1f-b272-b5fa3f1eedbd',\n",
    "    '3d8ec3f3-cd6e-42c7-bcdb-51baa5113160',\n",
    "    '16ac2579-72d8-4615-bcc3-c8d0438ded30',\n",
    "    '8ac300b2-0d66-49ad-b859-b3634aa82a1a',\n",
    "    'aacb9f15-d921-47ae-bbe0-e5f7ae160fa8',\n",
    "    '8afa7677-ce77-4ff4-9968-04f8794f26bf',\n",
    "    '7dc0b534-a3be-40d7-becd-653bc1b0cb96',\n",
    "    '3a334096-5835-4a7c-b32f-42d5b913f3a2',\n",
    "    'a8554428-ea43-4f67-9b65-1dbb7555f2cd',\n",
    "    '8b57c879-6550-48da-85c9-96bfa0011b80',\n",
    "    '69d0762d-8acd-4962-86eb-b924630858d0',\n",
    "    '48417de5-c3b9-4a1b-807b-f7cb5ba05fea',\n",
    "    '5a82adb8-0fd6-4875-b6ab-cb32aea747d6',\n",
    "    'a1053a9f-f328-4af5-8268-32e1b5b364a0',\n",
    "    '2a85b62d-863e-4e74-a272-899c21bb5be7',\n",
    "    '10e413c2-729c-4802-a387-6763a7798d8f',\n",
    "    'cd9e1b65-cda3-441a-a75c-1c63512c9819',\n",
    "    '8af6ff06-4720-4e6c-9a44-323c2b3d73a8',\n",
    "    'f8bd54c4-8d6b-446a-8060-8c5a29720933',\n",
    "    'a1f2aa06-9139-4b37-8478-5157de27002a',\n",
    "]\n",
    "\n",
    "\n",
    "#############################################\n",
    "## Execution\n",
    "#############################################\n",
    "\n",
    "# Loop through migration list and process entries\n",
    "results = []\n",
    "for snapshot_id in snapshot_id_list:\n",
    "    logging.info(f\"Processing Snapshot ID: {snapshot_id}\")\n",
    "    status, message = output_file_details(snapshot_id, target_bigquery_table)\n",
    "    results.append([snapshot_id, status, message])\n",
    "        \n",
    "# Display final results\n",
    "print(\"\\nFinal Results:\")\n",
    "results_df = pd.DataFrame(results, columns = [\"Snapshot ID\", \"Status\", \"Message\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Dataset Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     12,
     23
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to delete a specific TDR Snapshot\n",
    "def delete_snapshot(snapshot_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete snapshot = {}\".format(snapshot_id))\n",
    "    try:\n",
    "        delete_snapshot_result, job_id = wait_for_tdr_job(snapshots_api.delete_snapshot(id=snapshot_id), \"https://data.terra.bio\")\n",
    "        print(\"Result: {}\".format(delete_snapshot_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset\n",
    "def delete_dataset(dataset_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {}\".format(dataset_id))\n",
    "    try:\n",
    "        delete_dataset_result, job_id = wait_for_tdr_job(datasets_api.delete_dataset(id=dataset_id), \"https://data.terra.bio\")\n",
    "        print(\"Result: {}\".format(delete_dataset_result))\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "\n",
    "# Function to delete a specific TDR Dataset and all of its Snapshots\n",
    "def delete_dataset_and_all_snapshots(dataset_id):\n",
    "    api_client = refresh_tdr_api_client(\"https://data.terra.bio\")\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    print(\"Attempting to delete dataset = {} and all associated snapshots\".format(dataset_id))\n",
    "    dataset_id_list = [dataset_id]\n",
    "    # Delete snapshots\n",
    "    snapshot_list = snapshots_api.enumerate_snapshots(dataset_ids=dataset_id_list)\n",
    "    if snapshot_list.items:\n",
    "        for snapshot in snapshot_list.items:\n",
    "            snapshot_id = str(snapshot.id)\n",
    "            delete_snapshot(snapshot_id)\n",
    "            sleep(10)\n",
    "    # Delete dataset\n",
    "    delete_dataset(dataset_id)\n",
    "\n",
    "# Delete snapshots\n",
    "# snapshot_id_list = [\n",
    "# '1234',\n",
    "# ]\n",
    "# for snapshot_id in snapshot_id_list:\n",
    "#     delete_snapshot(snapshot_id)\n",
    "\n",
    "# Delete datasets and all their associated snapshots\n",
    "dataset_id_list = [\n",
    "'1be5b5e6-019e-419a-9248-6e80d067d697',\n",
    "]\n",
    "for dataset_id in dataset_id_list:\n",
    "    delete_dataset_and_all_snapshots(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Update Migration File List Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# General parameters\n",
    "target_bigquery_table = \"broad-dsde-prod-analytics-dev.anvil_azure_migration.azure_migration_file_list\"\n",
    "\n",
    "# Update parameters\n",
    "update_list = [\n",
    "    {\"az_dataset_id\": \"6007151f-45bc-4111-8e9a-b667bc722a6a\", \"new_gcp_dataset_id\": \"b22c71b2-2cb2-4b27-a49b-9a2a83d432e8\", \"new_gcp_dataset_name\": \"ANVIL_1000G_PRIMED_data_model_20240301\"},\n",
    "    {\"az_dataset_id\": \"a28e4ab5-a07b-4316-b743-7f5f9cc88211\", \"new_gcp_dataset_id\": \"3a89c170-2939-4c12-9940-f32d96fa9e55\", \"new_gcp_dataset_name\": \"ANVIL_CMH_GAFK_GS_long_read_20240301\"}\n",
    "]\n",
    "\n",
    "# Execute updates\n",
    "client = bigquery.Client()\n",
    "for entry in update_list:\n",
    "    logging.info(f\"Running update for entry: {str(entry)}\")\n",
    "    az_dataset_id = entry[\"az_dataset_id\"]\n",
    "    gcp_dataset_id = entry[\"new_gcp_dataset_id\"]\n",
    "    gcp_dataset_name = entry[\"new_gcp_dataset_name\"]\n",
    "    update_query = f\"\"\"UPDATE `{target_bigquery_table}` \n",
    "                       SET gcp_dataset_id = '{gcp_dataset_id}', gcp_dataset_name = '{gcp_dataset_name}'\n",
    "                       WHERE az_dataset_id = '{az_dataset_id}'\"\"\"\n",
    "    try:\n",
    "        update_query_job = client.query(update_query)\n",
    "        update_query_job.result()\n",
    "        logging.info(\"Update complete.\")\n",
    "    except Exception as e:\n",
    "        logging.info(\"Error running update.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
