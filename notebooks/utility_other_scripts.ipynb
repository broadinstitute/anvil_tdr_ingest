{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import import_ipynb\n",
    "import ingest_pipeline_utilities as utils\n",
    "import data_repo_client\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# AnVIL Resource Access Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Bulk Add Users to Workspaces (and associated Auth Domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Grant my ncalvane account workspace access/auth_domain member access for workspaces\n",
    "user_role_list = [\n",
    "    #[\"user_email\", \"role - READER, WRITER, OWNER, NO ACCESS\"]\n",
    "     [\"ncalvane@broadinstitute.org\", \"WRITER\"]\n",
    "]\n",
    "workspace_list = [\n",
    "'AnVIL_CCDG_Broad_MI_BRAVE_GRU_WES',\n",
    "'AnVIL_HPRC',\n",
    "]\n",
    "\n",
    "for user_role in user_role_list:\n",
    "    user = user_role[0]\n",
    "    role = user_role[1]\n",
    "    print(f\"Processing ACL updates for user: {user}\")\n",
    "    results = []\n",
    "    for workspace in workspace_list:\n",
    "\n",
    "        # Initialize\n",
    "        print(f\"\\tProcessing ACL updates for {workspace}.\")\n",
    "        error_list = []\n",
    "\n",
    "        # Establish credentials\n",
    "        creds, project = google.auth.default()\n",
    "        auth_req = google.auth.transport.requests.Request()\n",
    "        creds.refresh(auth_req)\n",
    "\n",
    "        # Add user as writer on workspace\n",
    "        payload = [{\n",
    "            \"email\": user,\n",
    "            \"accessLevel\": role,\n",
    "            \"canShare\": True,\n",
    "            \"canCompute\": True\n",
    "        }]\n",
    "        response = requests.patch(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/anvil-datastorage/{workspace}/acl\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "            json=payload\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            error_list.append(\"Error adding to workspace ACL\")\n",
    "\n",
    "        # Pull workspace attributes\n",
    "        ws_attributes = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/anvil-datastorage/{workspace}?fields=workspace.attributes,workspace.authorizationDomain,workspace.googleProject,workspace.bucketName\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        ).json()\n",
    "\n",
    "        # Add user to auth domains\n",
    "        try:\n",
    "            for ad in ws_attributes[\"workspace\"][\"authorizationDomain\"]:\n",
    "                auth_domain = ad[\"membersGroupName\"]\n",
    "                response = requests.put(\n",
    "                    url=f\"https://api.firecloud.org/api/groups/{auth_domain}/member/{user}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "                )\n",
    "                if response.status_code != 204:\n",
    "                    error_list.append(f\"Error adding to auth domain ({auth_domain})\")\n",
    "        except:\n",
    "            error_list.append(f\"Error accessing workspace.\")\n",
    "\n",
    "        # Record status\n",
    "        status = \"Success\" if not error_list else \"Failure\"\n",
    "        error_str = \"; \".join(error_list)\n",
    "        results.append([workspace, status, error_str])\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nResults for user: {user}\")\n",
    "    results_df = pd.DataFrame(results, columns = [\"workspace\", \"status\", \"errors\"])\n",
    "    display(results_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TDR Reader Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Remove Undesired Readers from TDR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to remove erroneous readers from snapshot\n",
    "def clean_up_ad_readers(snapshot_id, readers):\n",
    "    print(\"Cleaning up readers for {}...\".format(snapshot_id))\n",
    "    reader_list = readers\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    # Retrieve snapshot, grab auth_domain\n",
    "    if '$AUTH_DOMAIN' in reader_list:\n",
    "        snapshot_response = snapshots_api.retrieve_snapshot(id=snapshot_id)\n",
    "        snapshot_name = snapshot_response.name\n",
    "        print(\"Snapshot name: {}\".format(snapshot_name))\n",
    "        try:\n",
    "            auth_domain_list = snapshot_response.source[0].dataset_properties[\"auth_domains\"]\n",
    "        except:\n",
    "            auth_domain_list = []\n",
    "        for ad in auth_domain_list:\n",
    "            reader_list.append(ad + \"@firecloud.org\")\n",
    "\n",
    "    # Retrieve snapshot policies and delete readers that aren't in reader list\n",
    "    snapshot_policy_response = snapshots_api.retrieve_snapshot_policies(id=snapshot_id)\n",
    "    delete_count = 0\n",
    "    for policy in snapshot_policy_response.policies:\n",
    "        if policy.name == \"reader\":\n",
    "            for policymember in policy.members:\n",
    "                if policymember not in reader_list:\n",
    "                    api_client = utils.refresh_tdr_api_client()\n",
    "                    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "                    retry_count = 0\n",
    "                    while retry_count < 1:\n",
    "                        try:\n",
    "                            delete_response = snapshots_api.delete_snapshot_policy_member(id=snapshot_id, policy_name=\"reader\", member_email=policymember)\n",
    "                            delete_count += 1\n",
    "                            break\n",
    "                        except:\n",
    "                            retry_count += 1\n",
    "                        \n",
    "    # Print results\n",
    "    snapshot_policy_response = snapshots_api.retrieve_snapshot_policies(id=snapshot_id)\n",
    "    print(f\"\\t{delete_count} erroneous readers deleted.\")\n",
    "    \n",
    "    for role in snapshot_policy_response.policies:\n",
    "        if role.name == \"reader\":\n",
    "            rem_readers = \", \".join(role.members)\n",
    "            print(f\"\\tRemaining readers: {rem_readers}\")\n",
    "    return \n",
    "\n",
    "# Clean-up snapshots\n",
    "reader_list = [\"azul-anvil-prod@firecloud.org\"]#, '$AUTH_DOMAIN']\n",
    "snapshot_id_list = [\n",
    "'b0fc6253-d274-4e53-9977-85d943116f7c',\n",
    "]\n",
    "for snapshot_id in snapshot_id_list:\n",
    "    clean_up_ad_readers(snapshot_id, reader_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Add Auth Domain Users to TDR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to remove add readers to snapshot --> TO BE DONE WHEN ADs NEED TO BE ADDED BACK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect AnVIL Snapshots and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Anvil datasets and snapshots\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "datasets_list = datasets_api.enumerate_datasets(filter=\"anvil\", limit=2000)\n",
    "records_list = []\n",
    "for dataset_entry in datasets_list.items:\n",
    "    if re.match(\"^ANVIL_[a-zA-Z0-9-_]+_[0-9]{8}\", dataset_entry.name.upper()):\n",
    "        dataset_detail = datasets_api.retrieve_dataset(id=dataset_entry.id)\n",
    "        snapshots_list = snapshots_api.enumerate_snapshots(dataset_ids=[dataset_entry.id], limit=1000)\n",
    "        if len(snapshots_list.items) == 0:\n",
    "            record = [None, None, None, None, None, dataset_entry.id, dataset_entry.name, dataset_detail.ingest_service_account, dataset_entry.created_date[0:10]]\n",
    "            records_list.append(record)\n",
    "        else:\n",
    "            for snapshot_entry in snapshots_list.items:\n",
    "                record = [snapshot_entry.id, snapshot_entry.name, snapshot_entry.data_project, snapshot_entry.created_date[0:10], snapshot_entry.created_date, dataset_entry.id, dataset_entry.name, dataset_detail.ingest_service_account, dataset_entry.created_date[0:10]]\n",
    "                records_list.append(record)\n",
    "df = pd.DataFrame(records_list, columns =[\"Snapshot ID\", \"Snapshot Name\", \"Snapshot Google Project\", \"Snapshot Created Date\", \"Snapshot Created Datetime\", \"Source Dataset ID\", \"Source Dataset Name\", \"Source Dataset SA\", \"Source Dataset Created Date\"])\n",
    "df_sorted = df.sort_values([\"Source Dataset Name\", \"Snapshot Name\"], ascending=[True, True], ignore_index=True)\n",
    "display(df_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Snapshot Row Count Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def return_row_counts(snapshot_id, results_list):\n",
    "    # Grab access information from schema\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = snapshots_api.retrieve_snapshot(id=snapshot_id, include=[\"TABLES\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        tdr_schema_dict = {}\n",
    "        tdr_schema_dict[\"tables\"] = response[\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except:\n",
    "        results_list.append([snapshot_id, 0])\n",
    "        return results_list\n",
    "    \n",
    "    # Build row count query\n",
    "    table_set = set()\n",
    "    table_count = 0\n",
    "    row_count_subquery = \"\"\n",
    "    for table_entry in tdr_schema_dict[\"tables\"]:\n",
    "        table_set.add(table_entry[\"name\"])\n",
    "    for table_entry in table_set:\n",
    "        table_count += 1\n",
    "        if table_count == 1:\n",
    "            row_count_subquery += \"SELECT datarepo_row_id FROM `{project}.{dataset}.{table}` \".format(project=bq_project, dataset=bq_dataset, table=table_entry)\n",
    "        else:\n",
    "            row_count_subquery += \"UNION ALL SELECT datarepo_row_id FROM `{project}.{dataset}.{table}` \".format(project=bq_project, dataset=bq_dataset, table=table_entry)\n",
    "    row_count_query = \"SELECT COUNT(*) AS row_count FROM ({subquery})\".format(subquery=row_count_subquery)\n",
    "    \n",
    "    # Execute query and write results to results dict\n",
    "    try:\n",
    "        client = bigquery.Client()\n",
    "        df_results = client.query(row_count_query).result().to_dataframe()\n",
    "        row_count = df_results[\"row_count\"].values[0]\n",
    "        results_list.append([snapshot_id, row_count])\n",
    "    except:\n",
    "        results_list.append([snapshot_id, 0])\n",
    "    return results_list\n",
    "    \n",
    "# Loop through snapshots and collect row counts\n",
    "results_list = []\n",
    "snapshot_id_list = [\n",
    "'bb7eaad8-b02c-455c-964d-c9242019d9e5',\n",
    "]\n",
    "for snapshot_id in snapshot_id_list:\n",
    "    results_list = return_row_counts(snapshot_id, results_list)\n",
    "    \n",
    "# Convert results to dataframe and display\n",
    "results_df = pd.DataFrame(results_list, columns = [\"snapshot_id\", \"row_count\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pulling Dataset Sizes Across AnVIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Pulling file counts and sizes from TDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "billing_profile = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "\n",
    "# Establish API client\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "config = data_repo_client.Configuration()\n",
    "config.host = \"https://data.terra.bio\"\n",
    "config.access_token = creds.token\n",
    "api_client = data_repo_client.ApiClient(configuration=config)\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# Loop through enumerated datasets and create records for those related to AnVIL\n",
    "print(f\"Start time: {datetime.datetime.now()}\")\n",
    "records_list = []\n",
    "datasets_list = datasets_api.enumerate_datasets(limit=2000)\n",
    "for dataset_entry in datasets_list.items:\n",
    "    if dataset_entry.default_profile_id == billing_profile:\n",
    "        # Retrieve dataset details and pull source workspace(s)\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=dataset_entry.id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        try:\n",
    "            source_workspace = \",\".join(dataset_details[\"properties\"][\"source_workspaces\"])\n",
    "        except:\n",
    "            source_workspace = \"\"\n",
    "        \n",
    "        # Pull data file size sum from BigQuery\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        client = bigquery.Client()\n",
    "        file_size_query = \"\"\"SELECT COUNT(*) AS file_count, COALESCE(SUM(size_in_bytes),0) AS file_size FROM `{project}.{schema}.file_inventory`\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "        try:\n",
    "            df_output = client.query(file_size_query).result().to_dataframe()\n",
    "            file_count = df_output[\"file_count\"].values[0]\n",
    "            byte_size = df_output[\"file_size\"].values[0]\n",
    "            status = \"Success\"\n",
    "        except:\n",
    "            file_count = 0\n",
    "            byte_size = 0\n",
    "            status = \"Error\"\n",
    "    \n",
    "        # Build record for dataset\n",
    "        record = [dataset_entry.id, dataset_entry.name, source_workspace, file_count, byte_size, status]\n",
    "        records_list.append(record)\n",
    "        \n",
    "# Read records into a dataframe\n",
    "df = pd.DataFrame(records_list, columns =[\"Dataset UUID\", \"Dataset Name\", \"Source Workspaces\", \"File Count\", \"File Size (Bytes)\", \"Retrieval Status\"])\n",
    "df[\"File Size (Bytes)\"] = df[\"File Size (Bytes)\"].astype(int).astype(str)\n",
    "df_sorted = df.sort_values([\"Source Workspaces\"], ascending=[True], ignore_index=True)\n",
    "print(f\"End time: {datetime.datetime.now()}\")\n",
    "display(df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pulling file counts and sizes from WS Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# List of buckets:\n",
    "bucket_list = [\n",
    "'fc-secure-e202b041-84ec-4ca9-80de-60ba7dda6178',\n",
    "'fc-secure-21247341-3748-47a4-9dbe-42b8839446f9',\n",
    "'fc-secure-8b2e7dfb-cf02-411b-8b17-9b15937794e7',\n",
    "'fc-46bf051e-aec3-4adb-8178-3c51bc5e64ae',\n",
    "'fc-89ed8c3d-3e5e-455b-952b-e4cdf21a08f9',\n",
    "'fc-secure-b47581eb-772e-4bd3-9c30-614ad11ac955',\n",
    "'fc-secure-a3a7e66a-6268-48a6-a001-fc3a6d2290d2',\n",
    "'fc-97c4f159-5d6b-44ce-904c-8ccd16b28388',\n",
    "'fc-secure-89d09af9-ce39-42b6-8b41-735cfe64561c',\n",
    "'fc-secure-e9b536b4-a871-4a9d-8822-85658c7f8fc3',\n",
    "'fc-secure-494df267-5bd4-4518-8ea0-6358420d9ecb',\n",
    "'fc-secure-827d3809-7703-47db-b6fb-08a463569419',\n",
    "'fc-secure-59794551-d924-4ad7-905b-8727646d9aad',\n",
    "'fc-secure-3fe46cfd-c69a-4d77-aed5-2932e91abc8d',\n",
    "'fc-secure-449c0e31-cb6a-478b-a0a0-e617d822f9fb',\n",
    "'fc-secure-b66a9e2f-896e-4786-a601-4714fe1f78d7',\n",
    "'fc-secure-86a7da7d-f63c-44ae-b25f-5dbec63209b4',\n",
    "'fc-secure-e211f4ca-b14a-40d2-8408-8faa8cb1e81a',\n",
    "'fc-secure-6df86ae7-d972-4019-961b-a4214fe7437a',\n",
    "'fc-secure-9e3357c0-389c-41d7-94ee-56673db6b75f',\n",
    "'fc-secure-22309b02-2059-4d9f-be94-9bdbed77ff24',\n",
    "'fc-secure-408943c5-2694-4538-b729-73a61ac948f1',\n",
    "'fc-secure-4a0f9fdc-6316-4e82-86a8-88598bdf6882',\n",
    "'fc-secure-4ab74011-4864-4f81-83d8-825e378bb67b',\n",
    "'fc-secure-dd8b31a3-b890-4b85-9edb-477daed4fa65',\n",
    "'fc-secure-afd398cc-e3c9-4d65-94b3-28427b3a02d7',\n",
    "'fc-secure-3bbde5a9-b898-41d6-8ce3-2c81ca53e34f',\n",
    "'fc-secure-a563ccd1-2a2f-4ecf-8b87-83ad66b24ed1',\n",
    "'fc-secure-e028b530-4f98-4e35-a698-b3a47ab4fc0e',\n",
    "'fc-secure-862a13b2-7696-442e-aa14-0e2241f43c87',\n",
    "'fc-secure-fde12f71-78e4-4512-9ee4-a98cdac6ea63',\n",
    "'fc-secure-65e0696d-6c1c-46f5-b68c-7189acbb12e6',\n",
    "'fc-secure-a183fc0d-9291-4fa4-86e4-10f66954dea6',\n",
    "'fc-secure-3c0c744a-f0dd-4ffa-a4a8-43d2f9abe890',\n",
    "'fc-secure-fa2670e5-f99c-4a21-8915-dfb3b2964ac8',\n",
    "'fc-secure-2195f7f0-69eb-4d8f-b9c4-60ff45201f87',\n",
    "'fc-secure-b8d20102-3579-464c-9b5b-d086c8076d34',\n",
    "'fc-secure-b8e21bed-89ae-4aca-a4a1-a07fc2ea6f4e',\n",
    "'fc-secure-f27a9d2e-f2a0-4eb3-8276-fe8a570ba591',\n",
    "'fc-secure-e5065947-acd3-4407-9b96-6a5443b7f7f5',\n",
    "'fc-secure-0914b286-b97e-43c6-a0c5-d9742258ec9b',\n",
    "'fc-secure-fd35dec1-45c3-41ca-9156-fa16f004413e',\n",
    "'fc-secure-ceb4e1fe-841d-4d06-8213-eec1397b0af7',\n",
    "'fc-secure-2c8de1e3-2ea8-4ba9-a8bc-4e0b907305ec',\n",
    "'fc-secure-7d249549-33e9-4b96-9448-31bb6b944c71',\n",
    "'fc-secure-389d967f-63f5-4729-9cbe-52c14f071ac1',\n",
    "'fc-secure-00819d85-1657-472b-95f7-0ccb934d310b',\n",
    "'fc-secure-29a7c62b-bf87-4ef9-b1c7-b28fb178d400',\n",
    "'fc-secure-e6b3315b-eefa-444a-87d5-fd86163a0d40',\n",
    "'fc-secure-e0503432-75b9-4674-8e6d-2597dc529c4c',\n",
    "'fc-secure-e442bbfd-4364-47f7-8fce-1c249347bf1a',\n",
    "'fc-secure-f7f89b29-6590-4119-a259-08745d6b564a',\n",
    "'fc-secure-b1713b51-6a84-419c-b979-5c9454e05561',\n",
    "'fc-secure-a343ca65-169f-4a71-883c-b10aef1180f3',\n",
    "'fc-secure-e3e31dd4-66e6-462a-a8ea-aefb73828777',\n",
    "'fc-secure-180323ab-f749-4063-ae83-3bb93c739046',\n",
    "'fc-secure-863474e6-0c25-4b97-a471-a6070227c7ab',\n",
    "'fc-secure-d4bead53-0db1-4e25-87da-c02be5819368',\n",
    "'fc-secure-89bba08d-ef3b-47bb-9c9b-a937d7550a97',\n",
    "'fc-secure-870d27c3-a758-4535-b8dd-5fc0514c5215',\n",
    "'fc-secure-a5618092-a9f4-447a-850f-21739f6c0c83',\n",
    "'fc-secure-51788316-df65-4bc8-b6b3-e4a50f3de6cd',\n",
    "'fc-secure-c3d677a2-778e-4459-ab03-ad08dd00f69e',\n",
    "'fc-secure-c31dbfd1-8654-4c71-93ca-567439c75193',\n",
    "'fc-secure-4d553a3c-b4fa-4ddf-9dfa-d945b8bf16b7',\n",
    "'fc-secure-c794198e-e001-4695-8c56-130d202d4eec',\n",
    "'fc-secure-deeb8626-424b-4a7e-b500-e15bcd245f2c',\n",
    "'fc-secure-70409918-97e6-498c-a564-3d816da25184',\n",
    "'fc-secure-be72b12d-7097-45ae-b624-20d1fbcb7d37',\n",
    "'fc-secure-62493204-9a02-4282-8b22-fd69a25aa5e8',\n",
    "'fc-secure-dd54449b-efc7-4a81-9e83-25e5efdbe4c5',\n",
    "'fc-secure-8ccb6590-51a3-4d87-8503-3c5ca94d3443',\n",
    "'fc-secure-5845816b-03bd-4ea7-9054-633abe95ba6b',\n",
    "'fc-secure-c824996b-683d-430c-919b-a617ca092140',\n",
    "'fc-secure-1ffeb3d9-85e3-4fbc-bf80-3e4785f1ef43',\n",
    "'fc-secure-aecef290-3094-4070-9db2-91b23e3a2284',\n",
    "'fc-secure-fbfd37de-ac4b-46e3-9cb0-b76f7789ca4a',\n",
    "'fc-secure-ef198352-7a8f-40df-b351-d9c17f8b8bf2',\n",
    "'fc-secure-9dd49799-299d-4686-a21c-9df0abfaddc5',\n",
    "'fc-secure-f04b0ded-dcf2-4872-a447-b58f43d760a1',\n",
    "'fc-secure-6058ce81-98fd-43fd-bb65-32d59e960370',\n",
    "'fc-secure-1cb49bd3-c89b-4ebc-9d5c-7e2c941981a2',\n",
    "'fc-secure-69af8f70-c0b3-4ffd-bdbe-cd18aecb1b3d',\n",
    "'fc-secure-32713de1-b20c-4986-a916-618fd0dfed20',\n",
    "'fc-02e8222a-9286-425f-a2c3-92d3b9786807',\n",
    "'fc-db8d28e8-e27d-4c0c-8559-2ac15d4f82c9',\n",
    "'fc-secure-cf7b22ec-6585-47e0-a96d-8037a4d3a670',\n",
    "'fc-fe57063d-7dfd-4a77-a1a9-ce8007f796e7',\n",
    "'fc-secure-11637185-0c16-4b4c-aec3-4926f280292c',\n",
    "'fc-secure-5fa642ee-c48f-4b5e-8a0e-46d625d0b4ca',\n",
    "'fc-secure-f19f92e2-92c0-4ae6-ac9e-26e09733dcd2',\n",
    "'fc-secure-bae1021e-0789-41b7-94af-8461afe58856',\n",
    "'fc-secure-e195dd68-4f18-405c-8955-efb6aa4a55b6',\n",
    "'fc-secure-db8c2ae9-ef80-4516-8de2-61af9675b3e9',\n",
    "'fc-secure-ded35a4b-965f-42be-a471-4bd1ea830170',\n",
    "'fc-secure-3a8dd10b-8615-40dd-819b-66721cbeb3bc',\n",
    "'fc-secure-aa1fee47-c5d1-4468-ac5f-eb2f11c97e9c',\n",
    "]\n",
    "\n",
    "# Loop through buckets and record size and file count\n",
    "print(f\"Start time: {datetime.datetime.now()}\")\n",
    "results = []\n",
    "for bucket in bucket_list:\n",
    "    start = time.time()\n",
    "    obj_list = []\n",
    "    file_count = 0\n",
    "    size = 0\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        storage_bucket = storage_client.bucket(bucket, user_project=\"anvil-datastorage\")\n",
    "        objects = list(storage_client.list_blobs(storage_bucket))\n",
    "        file_count = len(objects)\n",
    "        for i in range(0,file_count): size += objects[i].size\n",
    "        status = \"Success\"\n",
    "        fail_message = \"\"\n",
    "    except Exception as e:\n",
    "        status = \"Failure\"\n",
    "        fail_message = f\"; Fail Message: {str(e)}\"\n",
    "    end = time.time()\n",
    "    duration = round(end-start,2)\n",
    "    message = f\"Duration: {duration}s{fail_message}\"\n",
    "    results.append([bucket, size, file_count, status, message])\n",
    "    df_temp = pd.DataFrame([[bucket, size, file_count, status, message]], columns =[\"Bucket\", \"Size in Bytes\", \"Object Count\", \"Run Status\", \"Message\"])\n",
    "    display(df_temp)\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "print(\"---------------------------------------------------------------------------------\")  \n",
    "df = pd.DataFrame(results, columns =[\"Bucket\", \"Size in Bytes\", \"Object Count\", \"Run Status\", \"Message\"])\n",
    "print(f\"End time: {datetime.datetime.now()}\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pulling MD5 Population Across AnVIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pulling High Level MD5 Population Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "billing_profile = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "\n",
    "# Establish API client\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "config = data_repo_client.Configuration()\n",
    "config.host = \"https://data.terra.bio\"\n",
    "config.access_token = creds.token\n",
    "api_client = data_repo_client.ApiClient(configuration=config)\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# Loop through enumerated datasets and create records for those related to AnVIL\n",
    "print(f\"Start time: {datetime.datetime.now()}\")\n",
    "records_list = []\n",
    "datasets_list = datasets_api.enumerate_datasets(limit=2000)\n",
    "for dataset_entry in datasets_list.items:\n",
    "    if dataset_entry.default_profile_id == billing_profile:\n",
    "        # Retrieve dataset details and pull source workspace(s)\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=dataset_entry.id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        try:\n",
    "            source_workspace = \",\".join(dataset_details[\"properties\"][\"source_workspaces\"])\n",
    "        except:\n",
    "            source_workspace = \"\"\n",
    "        \n",
    "        # Pull data file size sum from BigQuery\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        client = bigquery.Client()\n",
    "        file_size_query = \"\"\"SELECT COUNT(*) AS file_count, COUNT(md5_hash) AS file_w_md5 FROM `{project}.{schema}.file_inventory`\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "        try:\n",
    "            df_output = client.query(file_size_query).result().to_dataframe()\n",
    "            file_count = df_output[\"file_count\"].values[0]\n",
    "            file_w_md5 = df_output[\"file_w_md5\"].values[0]\n",
    "            if file_count != file_w_md5:\n",
    "                missing_md5 = True\n",
    "            else:\n",
    "                missing_md5 = False\n",
    "            status = \"Success\"\n",
    "        except:\n",
    "            file_count = 0\n",
    "            file_w_md5 = 0\n",
    "            missing_md5 = False\n",
    "            status = \"Error\"\n",
    "    \n",
    "        # Build record for dataset\n",
    "        record = [dataset_entry.id, dataset_entry.name, source_workspace, file_count, file_w_md5, missing_md5, status]\n",
    "        records_list.append(record)\n",
    "        \n",
    "# Read records into a dataframe\n",
    "df = pd.DataFrame(records_list, columns =[\"Dataset UUID\", \"Dataset Name\", \"Source Workspaces\", \"File Count\", \"MD5 Populated Count\", \"Missing MD5s\", \"Retrieval Status\"])\n",
    "df_sorted = df.sort_values([\"Source Workspaces\"], ascending=[True], ignore_index=True)\n",
    "print(f\"End time: {datetime.datetime.now()}\")\n",
    "display(df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pulling Specific Problematic Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "billing_profile = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "\n",
    "# Establish API client\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "config = data_repo_client.Configuration()\n",
    "config.host = \"https://data.terra.bio\"\n",
    "config.access_token = creds.token\n",
    "api_client = data_repo_client.ApiClient(configuration=config)\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# Loop through enumerated datasets and create records for those related to AnVIL\n",
    "print(f\"Start time: {datetime.datetime.now()}\")\n",
    "\n",
    "df_results = pd.DataFrame(columns = [\"Dataset UUID\", \"Dataset Name\", \"Source Workspaces\", \"File Path\", \"Byte Size\", \"Retrieval Status\"])\n",
    "records_list = []\n",
    "datasets_list = datasets_api.enumerate_datasets(limit=2000)\n",
    "for dataset_entry in datasets_list.items:\n",
    "    if dataset_entry.default_profile_id == billing_profile:\n",
    "        # Retrieve dataset details and pull source workspace(s)\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=dataset_entry.id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        try:\n",
    "            source_workspace = \",\".join(dataset_details[\"properties\"][\"source_workspaces\"])\n",
    "        except:\n",
    "            source_workspace = \"\"\n",
    "        \n",
    "        # Pull data file size sum from BigQuery\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        client = bigquery.Client()\n",
    "        file_size_query = \"\"\"SELECT uri AS file_name, size_in_bytes AS file_size FROM `{project}.{schema}.file_inventory` WHERE md5_hash IS NULL\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "        try:\n",
    "            output = client.query(file_size_query).result()\n",
    "            if output.total_rows > 0:\n",
    "                df_output = output.to_dataframe()\n",
    "                df_output.rename(columns = {\"file_name\":\"File Path\", \"file_size\":\"Byte Size\"}, inplace = True)\n",
    "                df_output[\"Dataset UUID\"] = dataset_entry.id\n",
    "                df_output[\"Dataset Name\"] = dataset_entry.name\n",
    "                df_output[\"Source Workspaces\"] = source_workspace\n",
    "                df_output[\"Retrieval Status\"] = \"Success - Files Found\"\n",
    "                df_results = df_results.append(df_output)\n",
    "            else:\n",
    "                output = [[dataset_entry.id, dataset_entry.name, source_workspace, None, 0, \"Success - No Files Found\"]]\n",
    "                df_output = pd.DataFrame(output, columns = [\"Dataset UUID\", \"Dataset Name\", \"Source Workspaces\", \"File Path\", \"Byte Size\", \"Retrieval Status\"])\n",
    "                df_results = df_results.append(df_output)\n",
    "        except:\n",
    "            output = [[dataset_entry.id, dataset_entry.name, source_workspace, None, 0, \"Error\"]]\n",
    "            df_output = pd.DataFrame(output, columns = [\"Dataset UUID\", \"Dataset Name\", \"Source Workspaces\", \"File Path\", \"Byte Size\", \"Retrieval Status\"])\n",
    "            df_results = df_results.append(df_output)\n",
    "        \n",
    "# Sort dataframe records and write out to file\n",
    "df_sorted = df_results.sort_values([\"Source Workspaces\", \"File Path\"], ascending=[True, True], ignore_index=True)\n",
    "output_file_path = \"null_md5_files.tsv\"\n",
    "df_sorted.to_csv(output_file_path, index=False, sep=\"\\t\")\n",
    "!gsutil cp $output_file_path $ws_bucket/ingest_pipeline/resources/ 2> stdout\n",
    "!rm $output_file_path\n",
    "print(f\"End time: {datetime.datetime.now()}\")\n",
    "print(f\"Results copied to: {ws_bucket}/ingest_pipeline/resources/{output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pulling Specific Files Across AnVIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "billing_profile = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "search_string = \"SubsetHailJointCall\"\n",
    "\n",
    "# Establish API client\n",
    "creds, project = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "creds.refresh(auth_req)\n",
    "config = data_repo_client.Configuration()\n",
    "config.host = \"https://data.terra.bio\"\n",
    "config.access_token = creds.token\n",
    "api_client = data_repo_client.ApiClient(configuration=config)\n",
    "api_client.client_side_validation = False\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# Loop through enumerated datasets and create records for those related to AnVIL\n",
    "print(f\"Start time: {datetime.datetime.now()}\")\n",
    "\n",
    "df_results = pd.DataFrame(columns = [\"Dataset UUID\", \"Dataset Name\", \"Source Workspaces\", \"File Count\", \"Match File Count\", \"Distinct File Count\", \"Retrieval Status\"])\n",
    "records_list = []\n",
    "datasets_list = datasets_api.enumerate_datasets(limit=2000)\n",
    "for dataset_entry in datasets_list.items:\n",
    "    if dataset_entry.default_profile_id == billing_profile:\n",
    "        # Retrieve dataset details and pull source workspace(s)\n",
    "        dataset_details = datasets_api.retrieve_dataset(id=dataset_entry.id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        try:\n",
    "            source_workspace = \",\".join(dataset_details[\"properties\"][\"source_workspaces\"])\n",
    "        except:\n",
    "            source_workspace = \"\"\n",
    "        \n",
    "        # Pull data file size sum from BigQuery\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "        client = bigquery.Client()\n",
    "        file_size_query = \"\"\"SELECT COUNT(*) AS file_count,  \n",
    "                            COUNT(CASE WHEN uri like '%{search_string}%' THEN 1 END) AS match_file_count,\n",
    "                            COUNT(DISTINCT uri) AS distinct_file_count \n",
    "                            FROM `{project}.{schema}.file_inventory`\"\"\".format(project = bq_project, schema = bq_schema, search_string = search_string)\n",
    "        try:\n",
    "            output = client.query(file_size_query).result()\n",
    "            if output.total_rows > 0:\n",
    "                df_output = output.to_dataframe()\n",
    "                df_output.rename(columns = {\"file_count\":\"File Count\", \"match_file_count\":\"Match File Count\", \"distinct_file_count\":\"Distinct File Count\"}, inplace = True)\n",
    "                df_output[\"Dataset UUID\"] = dataset_entry.id\n",
    "                df_output[\"Dataset Name\"] = dataset_entry.name\n",
    "                df_output[\"Source Workspaces\"] = source_workspace\n",
    "                df_output[\"Retrieval Status\"] = \"Success - Files Found\"\n",
    "                df_results = df_results.append(df_output)\n",
    "            else:\n",
    "                output = [[dataset_entry.id, dataset_entry.name, source_workspace, 0, 0, 0, \"Success - No Files Found\"]]\n",
    "                df_output = pd.DataFrame(output, columns = [\"Dataset UUID\", \"Dataset Name\", \"Source Workspaces\", \"File Path\", \"Byte Size\", \"Retrieval Status\"])\n",
    "                df_results = df_results.append(df_output)\n",
    "        except:\n",
    "            output = [[dataset_entry.id, dataset_entry.name, source_workspace, 0, 0, 0, \"Error\"]]\n",
    "            df_output = pd.DataFrame(output, columns = [\"Dataset UUID\", \"Dataset Name\", \"Source Workspaces\", \"File Count\", \"Match File Count\", \"Distinct File Count\", \"Retrieval Status\"])\n",
    "            df_results = df_results.append(df_output)\n",
    "        \n",
    "# Sort dataframe records and display results\n",
    "df_sorted = df_results.sort_values([\"Source Workspaces\"], ascending=[True], ignore_index=True)\n",
    "print(f\"End time: {datetime.datetime.now()}\")\n",
    "print(\"Results:\")\n",
    "display(df_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
