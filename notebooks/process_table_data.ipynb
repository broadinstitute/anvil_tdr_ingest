{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "# print('Version: 1.0.0: 9/8/2022 8:43pm - Nate Calvanese - First version')\n",
    "print('Version: 1.0.1: 9/16/2022 10:57am - Nate Calvanese - Fixed bug in file_inventory table creation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and Environment Variables\n",
    "\n",
    "# Imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "import re\n",
    "import hashlib\n",
    "import logging\n",
    "import import_ipynb\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_file_inventory as bfi\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# Workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Main table data processing function\n",
    "def process_table_data(params):\n",
    "    \n",
    "    # Collect parameters\n",
    "    log_status = \"Success\"\n",
    "    log_dict = {}\n",
    "    google_project = params[\"google_project\"]\n",
    "    input_dir = params[\"input_dir\"]\n",
    "    el_output_dir = params[\"el_output_dir\"]\n",
    "    el_schema_file = params[\"el_schema_file\"]\n",
    "    data_file_ref_mode = params[\"data_file_ref_mode\"]\n",
    "    data_file_ref_table_name = params[\"data_file_ref_table_name\"]\n",
    "    data_file_refs = params[\"data_file_refs\"]\n",
    "    file_inventory = params[\"file_inventory\"]\n",
    "    file_inventory_dir = params[\"file_inventory_dir\"]\n",
    "    \n",
    "    # Attempt to read in file_inventory file if it's empty in the params dictionary\n",
    "    if len(file_inventory) == 0:\n",
    "        logging.info(\"File inventory not populated. Attempting to populate from latest file (if one exists).\")\n",
    "        try: \n",
    "            inventory_file_path = \"gs://\" + ws_bucket_name + \"/\" + file_inventory_dir + \"/file_inventory.tsv\"\n",
    "            df_inv = pd.read_csv(inventory_file_path, delimiter = \"\\t\")\n",
    "            file_manifest = df_inv.to_dict(orient='records')\n",
    "            logging.info(\"File inventory populated successfully.\")\n",
    "            log_dict[\"file_inventory_population\"] = \"File inventory populated\"\n",
    "        except Exception as e:\n",
    "            error_message = \"File inventory not populated. Error attempting to populate from file: {}\".format(e)\n",
    "            logging.error(\"File inventory not populated. Error attempting to populate from file: {}\".format(e))\n",
    "            log_status = \"Error\"\n",
    "            log_dict[\"file_inventory_population\"] = \"File inventory not populated. Error attempting to populate from file: {}\".format(e)\n",
    "    else:\n",
    "        log_dict[\"file_inventory_population\"] = \"File inventory populated\"\n",
    "    \n",
    "    # Empty destination directory\n",
    "    !gsutil -m rm -r $ws_bucket/$el_output_dir/* 2> /dev/null || true\n",
    "    \n",
    "    # Get list of table data files to process\n",
    "    obj_list = bfi.get_objects_list(ws_bucket_name, google_project, dirs_to_include=[input_dir])\n",
    "    target_table_dict = {}\n",
    "    for item in obj_list:\n",
    "        path_split = item.split(\"/\")\n",
    "        tar_table_idx = len(path_split) - 2\n",
    "        if re.match(\".+(\\.(tsv|csv))$\", item):\n",
    "            source_file = item\n",
    "            target_table = path_split[tar_table_idx]\n",
    "            if target_table_dict.get(target_table) == None:\n",
    "                target_table_dict[target_table] = [source_file]\n",
    "            else:\n",
    "                source_file_list = target_table_dict[target_table]\n",
    "                source_file_list.append(source_file)\n",
    "                target_table_dict[target_table] = source_file_list\n",
    "    if data_file_ref_mode == \"fileref_table\":\n",
    "        target_table_dict[data_file_ref_table_name] = [file_inventory_dir + \"/file_inventory.tsv\"]\n",
    "    logging.info(\"Target tables and files to be processed: \" + json.dumps(target_table_dict))\n",
    "\n",
    "    # Loop through target tables and process files associated with them (limited to tsv/csv for the moment)\n",
    "    table_list = []\n",
    "    relationship_list = [] \n",
    "    for key in target_table_dict:\n",
    "        logging.info(\"Processing files for target table: {}.\".format(key))\n",
    "        try:\n",
    "            tablename = key\n",
    "            table_dict = {}\n",
    "            table_dict[\"name\"] = tablename\n",
    "            column_list = []\n",
    "\n",
    "            # Build data frame from source files\n",
    "            file_iterator = 0\n",
    "            filename_list = []\n",
    "            for file_entry in target_table_dict[key]:\n",
    "                filename = os.path.split(file_entry)[1]\n",
    "                filename_list.append(filename)\n",
    "                file_iterator += 1\n",
    "                full_file_path = \"gs://\" + ws_bucket_name + \"/\" + file_entry\n",
    "                if re.match(\".+(\\.tsv)$\", file_entry):\n",
    "                    delim = \"\\t\"\n",
    "                else:\n",
    "                    delim = \",\"\n",
    "                if file_iterator == 1:\n",
    "                    df = pd.read_csv(full_file_path, delimiter = delim)\n",
    "                else:\n",
    "                    df_int = pd.read_csv(full_file_path, delimiter = delim) # We\"ll want the code to be able to handle the case where a target table has multiple source files. Haven\"t tested/developed yet.\n",
    "                    df = pd.concat([df, df_int])\n",
    "            \n",
    "            # Perform some initial dataframe clean up (clean up names, attempt to convert data types, replace NaN, etc.)\n",
    "            df.rename(columns=lambda x: utils.encode_name(x), inplace=True)\n",
    "            df = df.where((pd.notnull(df)), None)\n",
    "            df = df.convert_dtypes()\n",
    "            \n",
    "            # Set file reference variables\n",
    "            fileref_tablename = data_file_ref_table_name\n",
    "            fileref_columnname = \"file_ref\"\n",
    "            fileref_id_columnname = \"file_id\"\n",
    "            \n",
    "            # Loop through columns and build TDR schema table entry from dataframe\n",
    "            for column in df.columns:\n",
    "\n",
    "                # Build column dictionary\n",
    "                column_dict = {}\n",
    "                column_dict[\"name\"] = column\n",
    "                column_dict[\"array_of\"] = False   #CSV/TSV can\"t support arrays, so this is safe for now.\n",
    "                base_type = str(df[column].dtype)\n",
    "                mapped_type = utils.map_datatype(base_type)\n",
    "\n",
    "                # Force convert objects to strings (or fileref for file inventory file_ref field)\n",
    "                if data_file_ref_mode == \"fileref_table\" and tablename == fileref_tablename and column == fileref_columnname:\n",
    "                    mapped_type = \"fileref\"\n",
    "                    df[column] = df.apply(lambda x: json.loads(x[column].replace(\"\\'\", \"\\\"\")), axis=1)\n",
    "                elif mapped_type == \"other\":\n",
    "                    mapped_type = \"string\"\n",
    "                    df[column] = df[column].astype(\"string\")\n",
    "\n",
    "                # Set column datatype and append to column list\n",
    "                column_dict[\"datatype\"] = mapped_type\n",
    "                column_list.append(column_dict)\n",
    "\n",
    "            # Check for and build file references as necessary\n",
    "            for filename in filename_list:\n",
    "                if filename in data_file_refs:\n",
    "                    for column_entry in data_file_refs[filename]:\n",
    "                        encoded_col_name = utils.encode_name(column_entry[\"column\"])\n",
    "\n",
    "                        # If column exists in the data frame, determine whether or not a new field will be created, and the method that will be used for building the file reference\n",
    "                        # Once determined, execute the appropriate file reference building function and then update the final schema to match the inputted parameters\n",
    "                        if encoded_col_name in df.columns and column_entry[\"method\"] in [\"file_path_match\", \"tdr_file_id\"]:\n",
    "\n",
    "                            # Derive common variables\n",
    "                            if data_file_ref_mode == \"fileref_table\":\n",
    "                                return_field = \"file_id\"\n",
    "                            else:\n",
    "                                return_field = \"file_ref\"\n",
    "                            if column_entry[\"match_multiple_files\"] == True:\n",
    "                                array_of = True\n",
    "                            else:\n",
    "                                array_of = False\n",
    "\n",
    "                            # If not creating a new field, update existing field\n",
    "                            if column_entry[\"create_new_field\"] == False:\n",
    "                                # If method is file_path_match, replace column values with file IDs or references, otherwise leave columns alone\n",
    "                                if column_entry[\"method\"] in [\"file_path_match\"]:\n",
    "                                    df[encoded_col_name] = df.apply(lambda x: utils.find_file_in_inventory(x[encoded_col_name], file_inventory, return_field, column_entry[\"match_multiple_files\"], column_entry[\"match_regex\"]), axis=1)\n",
    "                                # Update the column_list entry for the schema as appropriate\n",
    "                                for idx, val in enumerate(column_list):\n",
    "                                    if val[\"name\"] == encoded_col_name:\n",
    "                                        col_list_idx = idx\n",
    "                                column_list[col_list_idx][\"array_of\"] = array_of\n",
    "                                if data_file_ref_mode == \"fileref_in_line\" or column_entry[\"method\"] == \"tdr_file_id\":\n",
    "                                    column_list[col_list_idx][\"datatype\"] = \"fileref\"\n",
    "                                else:\n",
    "                                    column_list[col_list_idx][\"datatype\"] = \"string\"\n",
    "                                    # If file reference mode is \"fileref_table\" and method is not \"tdr_file_id\", add the appropriate relationship to link the file reference to the new file table\n",
    "                                    rel_dict = utils.construct_relationship(tablename, encoded_col_name, fileref_tablename, fileref_id_columnname)\n",
    "                                    relationship_list.append(rel_dict)\n",
    "\n",
    "                            # Otherwise, create new field\n",
    "                            else:\n",
    "                                # Record or derive new field name\n",
    "                                if column_entry[\"new_field_name\"] != None:\n",
    "                                    new_col_name = utils.encode_name(column_entry[\"new_field_name\"])\n",
    "                                else:\n",
    "                                    new_col_name = encoded_col_name + \"_fileref\"\n",
    "                                # If method is file_path_match, replace column values with file IDs or references, otherwise leave columns alone\n",
    "                                if column_entry[\"method\"] in [\"file_path_match\"]:\n",
    "                                    df[new_col_name] = df.apply(lambda x: utils.find_file_in_inventory(x[encoded_col_name], file_inventory, return_field, column_entry[\"match_multiple_files\"], column_entry[\"match_regex\"]), axis=1)\n",
    "                                # Create new column list entry for schema\n",
    "                                column_dict = {}\n",
    "                                column_dict[\"name\"] = new_col_name\n",
    "                                column_dict[\"array_of\"] = array_of \n",
    "                                if data_file_ref_mode == \"fileref_in_line\" or column_entry[\"method\"] == \"tdr_file_id\":\n",
    "                                    column_dict[\"datatype\"] = \"fileref\"\n",
    "                                else:\n",
    "                                    column_dict[\"datatype\"] = \"string\"\n",
    "                                    # If file reference mode is \"fileref_table\" and method is not \"tdr_file_id\", add the appropriate relationship to link the file reference to the new file table\n",
    "                                    rel_dict = utils.construct_relationship(tablename, new_col_name, fileref_tablename, fileref_id_columnname)\n",
    "                                    relationship_list.append(rel_dict)\n",
    "                                column_list.append(column_dict)\n",
    "            \n",
    "            # Add column list to table dict and table_dict to table_list\n",
    "            table_dict[\"columns\"] = column_list\n",
    "            table_dict[\"primaryKey\"] = []\n",
    "            table_list.append(table_dict)\n",
    "\n",
    "            # Write out file\n",
    "            records_json = df.to_json(orient=\"records\")\n",
    "            records_list = json.loads(records_json)\n",
    "            records_cnt = len(records_list)\n",
    "            destination_file = tablename + \".json\"\n",
    "            with open(destination_file, \"w\") as outfile:\n",
    "                for idx, val in enumerate(records_list):\n",
    "                    json.dump(val, outfile)\n",
    "                    if idx < (records_cnt - 1):\n",
    "                        outfile.write(\"\\n\")\n",
    "\n",
    "            # Copy file to workspace bucket and delete from notebook environment\n",
    "            !gsutil cp $destination_file $ws_bucket/$el_output_dir/ 2> stdout \n",
    "            log_dict[key] = \"No errors raised\"\n",
    "        except Exception as e:\n",
    "            error_message = \"Error processing files for target table: {}. Error message: {}\".format(key, e)\n",
    "            logging.info(error_message)\n",
    "            log_dict[key] = error_message\n",
    "            log_status = \"Error\"\n",
    "\n",
    "    # Finish building TDR schema object\n",
    "    logging.info(\"Creating schema object and copying to cloud storage.\")\n",
    "    schema_dict = {}\n",
    "    schema_dict[\"tables\"] = table_list\n",
    "    schema_dict[\"relationships\"] = relationship_list\n",
    "\n",
    "    # Write out schema file, copy to workspace bucket, and delete from notebook environment\n",
    "    destination_file = \"tdr_schema_object.json\"\n",
    "    with open(destination_file, \"w\") as outfile:\n",
    "        json.dump(schema_dict, outfile)\n",
    "    !gsutil cp $destination_file $ws_bucket/$el_schema_file 2> stdout\n",
    "    \n",
    "    # Remove .json files\n",
    "    !rm *.json\n",
    "    \n",
    "    # Build and return table list for use in ingest\n",
    "    table_list = list(target_table_dict.keys())\n",
    "    log_string = json.dumps(log_dict)\n",
    "    logging.info(\"File processing complete. Status: {}. Details: {}. Tables to ingest: {}\".format(log_status, log_string, \", \".join(table_list)))\n",
    "    return table_list, log_status, log_string\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/16/2022 02:50:07 PM - INFO: Target tables and files to be processed: {\"ws_family\": [\"ingest_pipeline/input/table_data/ws_family/ws_family.tsv\"], \"ws_sample\": [\"ingest_pipeline/input/table_data/ws_sample/ws_sample.tsv\"], \"ws_sequencing\": [\"ingest_pipeline/input/table_data/ws_sequencing/ws_sequencing.tsv\"], \"ws_subject\": [\"ingest_pipeline/input/table_data/ws_subject/ws_subject.tsv\"], \"ws_workspace_attributes\": [\"ingest_pipeline/input/table_data/ws_workspace_attributes/ws_workspace_attributes.tsv\"], \"ws_file_inventory\": [\"ingest_pipeline/input/data_files/file_inventory/file_inventory.tsv\"]}\n",
      "09/16/2022 02:50:07 PM - INFO: Processing files for target table: ws_family.\n",
      "09/16/2022 02:50:10 PM - INFO: Processing files for target table: ws_sample.\n",
      "09/16/2022 02:50:12 PM - INFO: Processing files for target table: ws_sequencing.\n",
      "09/16/2022 02:50:15 PM - INFO: Processing files for target table: ws_subject.\n",
      "09/16/2022 02:50:18 PM - INFO: Processing files for target table: ws_workspace_attributes.\n",
      "09/16/2022 02:50:21 PM - INFO: Processing files for target table: ws_file_inventory.\n",
      "09/16/2022 02:50:23 PM - INFO: Creating schema object and copying to cloud storage.\n",
      "09/16/2022 02:50:27 PM - INFO: File processing complete. Status: Success. Details: {\"file_inventory_population\": \"File inventory populated\", \"ws_family\": \"No errors raised\", \"ws_sample\": \"No errors raised\", \"ws_sequencing\": \"No errors raised\", \"ws_subject\": \"No errors raised\", \"ws_workspace_attributes\": \"No errors raised\", \"ws_file_inventory\": \"No errors raised\"}. Tables to ingest: ws_family, ws_sample, ws_sequencing, ws_subject, ws_workspace_attributes, ws_file_inventory\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "# params = {}\n",
    "# ws_attributes = utils.get_workspace_attributes(ws_project, ws_name)\n",
    "# params[\"google_project\"] = ws_attributes[\"googleProject\"]\n",
    "# params[\"input_dir\"] = \"ingest_pipeline/input/table_data\"\n",
    "# params[\"file_inventory_dir\"] = \"ingest_pipeline/input/data_files/file_inventory\"\n",
    "# params[\"el_output_dir\"] = \"ingest_pipeline/output/source/table_data\"\n",
    "# params[\"el_schema_file\"] = \"ingest_pipeline/output/source/schema/tdr_schema_object.json\"\n",
    "# params[\"data_file_ref_mode\"] = \"fileref_table\"\n",
    "# params[\"data_file_ref_table_name\"] = \"ws_file_inventory\"\n",
    "# params[\"data_file_refs\"] = {\n",
    "#     \"ws_sequencing.tsv\": [{\n",
    "#         \"column\": \"sequencing_id\",\n",
    "#         \"method\": \"file_path_match\",\n",
    "#         \"match_multiple_files\": True, \n",
    "#         \"match_regex\": None,\n",
    "#         \"create_new_field\": True,\n",
    "#         \"new_field_name\": \"sequencing_id_file_id\"\n",
    "#     }, {\n",
    "#         \"column\": \"seq_filename\",\n",
    "#         \"method\": \"file_path_match\",\n",
    "#         \"match_multiple_files\": True, \n",
    "#         \"match_regex\": None,\n",
    "#         \"create_new_field\": True,\n",
    "#         \"new_field_name\": \"seq_filename_file_id\"\n",
    "#     }, {\n",
    "#         \"column\": \"capture_region_bed_file\",\n",
    "#         \"method\": \"file_path_match\",\n",
    "#         \"match_multiple_files\": True, \n",
    "#         \"match_regex\": None,\n",
    "#         \"create_new_field\": True,\n",
    "#         \"new_field_name\": \"capture_region_bed_file_file_id\"\n",
    "#     }, {\n",
    "#         \"column\": \"file_id\",\n",
    "#         \"method\": \"file_path_match\",\n",
    "#         \"match_multiple_files\": True, \n",
    "#         \"match_regex\": None,\n",
    "#         \"create_new_field\": True,\n",
    "#         \"new_field_name\": \"file_id_file_id\"\n",
    "#     }, {\n",
    "#         \"column\": \"cram\",\n",
    "#         \"method\": \"file_path_match\",\n",
    "#         \"match_multiple_files\": True, \n",
    "#         \"match_regex\": None,\n",
    "#         \"create_new_field\": True,\n",
    "#         \"new_field_name\": \"cram_file_id\"\n",
    "#     }, {\n",
    "#         \"column\": \"cram\",\n",
    "#         \"method\": \"file_path_match\",\n",
    "#         \"match_multiple_files\": True, \n",
    "#         \"match_regex\": None,\n",
    "#         \"create_new_field\": True,\n",
    "#         \"new_field_name\": \"cram_file_id\"\n",
    "#     }]\n",
    "# }\n",
    "# params[\"data_files_src_bucket\"] = ws_attributes[\"attributes\"][\"data_files_src_bucket\"]\n",
    "# params[\"data_files_src_dirs\"] = []  # Leave empty to include all\n",
    "# params[\"data_files_src_dirs_exclude\"] = [] \n",
    "# file_inventory = bfi.build_inventory(params)\n",
    "# params[\"file_inventory\"] = file_inventory\n",
    "# #params[\"file_inventory\"] = []\n",
    "# target_tables, log_status, log_string = process_table_data(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
