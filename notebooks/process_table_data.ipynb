{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print('Version: 1.0.0: 9/8/2022 8:43pm - Nate Calvanese - First version')\n",
    "#print('Version: 1.0.1: 9/16/2022 10:57am - Nate Calvanese - Fixed bug in file_inventory table creation')\n",
    "#print('Version: 1.0.2: 10/4/2022 4:00pm - Nate Calvanese - Added support for arrays in TSVs')\n",
    "#print('Version: 1.0.3: 10/5/2022 11:34am - Nate Calvanese - Chunked pre-processed table data into multiple files')\n",
    "#print('Version: 1.0.4: 10/12/2022 12:31pm - Nate Calvanese - Added support for column-level file reference configuration')\n",
    "#print('Version: 1.0.5: 10/13/2022 12:40pm - Nate Calvanese - Fixed bug building file refs when reading in file inventory')\n",
    "#print('Version: 1.0.6: 10/26/2022 3:05pm - Nate Calvanese - Added logic to deal with duplicate column names in source files')\n",
    "#print('Version: 1.0.7: 1/30/2023 3:00pm - Nate Calvanese - Added logic to deal with workspaces that do not contain files')\n",
    "#print('Version: 1.0.8: 1/31/2023 10:52am - Nate Calvanese - Added logic to deal with workspaces that are completely empty')\n",
    "#print('Version: 1.0.9: 3/8/2023 12:09pm - Nate Calvanese - Performance improvements for file ref lookups')\n",
    "print('Version: 1.0.10: 1/12/2024 11:25am - Nate Calvanese - Made max_combined_rec_ref_size configurable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional modules (one time effort per cloud environment)\n",
    "#!pip install --upgrade pip import_ipynb data_repo_client urllib3 xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Imports and Environment Variables\n",
    "\n",
    "# Imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "import re\n",
    "import hashlib\n",
    "import logging\n",
    "import import_ipynb\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_file_inventory as bfi\n",
    "import math\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# Workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Main table data processing function\n",
    "def process_table_data(params):\n",
    "    \n",
    "    # Collect parameters\n",
    "    log_status = \"Success\"\n",
    "    log_dict = {}\n",
    "    google_project = params[\"google_project\"]\n",
    "    input_dir = params[\"input_dir\"]\n",
    "    el_output_dir = params[\"el_output_dir\"]\n",
    "    el_schema_file = params[\"el_schema_file\"]\n",
    "    create_file_table = params[\"create_file_table\"]\n",
    "    file_table_name = params[\"file_table_name\"]\n",
    "    data_file_refs = params[\"data_file_refs\"]\n",
    "    file_inventory = params[\"file_inventory\"]\n",
    "    file_inventory_dir = params[\"file_inventory_dir\"]\n",
    "    max_combined_rec_ref_size = params[\"max_combined_rec_ref_size\"]\n",
    "    \n",
    "    # Attempt to read in file_inventory file if it's empty in the params dictionary\n",
    "    if len(file_inventory) == 0:\n",
    "        logging.info(\"File inventory not populated. Attempting to populate from latest file (if one exists).\")\n",
    "        try: \n",
    "            inventory_file_path = \"gs://\" + ws_bucket_name + \"/\" + file_inventory_dir + \"/file_inventory.tsv\"\n",
    "            df_inv = pd.read_csv(inventory_file_path, delimiter = \"\\t\")\n",
    "            df_inv[\"file_ref\"] = df_inv.apply(lambda x: json.loads(x[\"file_ref\"].replace(\"\\'\", \"\\\"\")), axis=1)\n",
    "            file_inventory = df_inv.to_dict(orient='records')\n",
    "            logging.info(\"File inventory populated successfully.\")\n",
    "            log_dict[\"file_inventory_population\"] = \"File inventory populated\"\n",
    "        except Exception as e:\n",
    "            error_message = \"File inventory not populated. Unable to populate from file: {}\".format(e)\n",
    "            logging.warning(\"File inventory not populated. Unable to populate from file: {}\".format(e))\n",
    "            log_status = \"Warning\"\n",
    "            log_dict[\"file_inventory_population\"] = \"File inventory not populated. Unable to populate from file: {}\".format(e)\n",
    "    else:\n",
    "        log_dict[\"file_inventory_population\"] = \"File inventory populated\"\n",
    "    \n",
    "    # Format file_inventory to support file ref lookup\n",
    "    file_lookup = {}\n",
    "    for file in file_inventory:\n",
    "        file_lookup[file[\"uri\"]] = {\n",
    "            \"file_id\": file[\"file_id\"],\n",
    "            \"file_ref\": file[\"file_ref\"] \n",
    "        } \n",
    "    \n",
    "    # Empty destination directory\n",
    "    !gsutil -m rm -r $ws_bucket/$el_output_dir/* 2> /dev/null || true\n",
    "    \n",
    "    # Get list of table data files to process\n",
    "    obj_list = bfi.get_objects_list(ws_bucket_name, google_project, dirs_to_include=[input_dir])\n",
    "    target_table_dict = {}\n",
    "    for item in obj_list:\n",
    "        path_split = item.split(\"/\")\n",
    "        tar_table_idx = len(path_split) - 2\n",
    "        if re.match(\".+(\\.(tsv|csv))$\", item):\n",
    "            source_file = item\n",
    "            target_table = path_split[tar_table_idx]\n",
    "            if target_table_dict.get(target_table) == None:\n",
    "                target_table_dict[target_table] = [source_file]\n",
    "            else:\n",
    "                source_file_list = target_table_dict[target_table]\n",
    "                source_file_list.append(source_file)\n",
    "                target_table_dict[target_table] = source_file_list\n",
    "    if create_file_table == True and len(file_inventory) > 0:\n",
    "        target_table_dict[file_table_name] = [file_inventory_dir + \"/file_inventory.tsv\"]\n",
    "    logging.info(\"Target tables and files to be processed: \" + json.dumps(target_table_dict))\n",
    "\n",
    "    # If no table data or data files, error out pipeline\n",
    "    if (len(target_table_dict) == 0) or (len(target_table_dict) == 1 and target_table_dict.get(\"workspace_attributes\") != None):\n",
    "        if len(target_table_dict) == 0:\n",
    "            error_message = \"No data files or tabular data found. Unable to process table data.\"\n",
    "        else:\n",
    "            error_message = \"No data file or tabular data found (workspace attributes only). Unable to process table data.\"\n",
    "        logging.error(error_message)\n",
    "        log_dict[\"table_data_processing\"] = error_message\n",
    "        log_status = \"Error\"\n",
    "        table_list = []\n",
    "        log_string = json.dumps(log_dict)\n",
    "        return table_list, log_status, log_string\n",
    "    \n",
    "    # Loop through target tables and process files associated with them (limited to tsv/csv for the moment)\n",
    "    table_list = []\n",
    "    relationship_list = [] \n",
    "    for key in target_table_dict:\n",
    "        logging.info(\"Processing files for target table: {}.\".format(key))\n",
    "        try:\n",
    "            tablename = key\n",
    "            table_dict = {}\n",
    "            table_dict[\"name\"] = tablename\n",
    "            column_list = []\n",
    "\n",
    "            # Build data frame from source files\n",
    "            file_iterator = 0\n",
    "            for file_entry in target_table_dict[key]:\n",
    "                filename = os.path.split(file_entry)[1]\n",
    "                file_iterator += 1\n",
    "                full_file_path = \"gs://\" + ws_bucket_name + \"/\" + file_entry\n",
    "                if re.match(\".+(\\.tsv)$\", file_entry):\n",
    "                    delim = \"\\t\"\n",
    "                else:\n",
    "                    delim = \",\"\n",
    "                if file_iterator == 1:\n",
    "                    df = pd.read_csv(full_file_path, delimiter = delim)\n",
    "                    df[\"ingest_provenance\"] = filename\n",
    "                else:\n",
    "                    df_int = pd.read_csv(full_file_path, delimiter = delim)\n",
    "                    df_int[\"ingest_provenance\"] = filename\n",
    "                    df = pd.concat([df, df_int], ignore_index=True)\n",
    "            \n",
    "            # Perform initial dataframe clean up (encode names, deal with duplicate names, attempt to convert data types, replace NaN, etc.)\n",
    "            original_cols = list(df.columns)\n",
    "            df.rename(columns=lambda x: utils.encode_name(x), inplace=True)\n",
    "            processed_cols = []\n",
    "            duplicate_cols = []\n",
    "            for idx, column in enumerate(list(df.columns)):\n",
    "                if column not in processed_cols:\n",
    "                    processed_cols.append(column)\n",
    "                else:\n",
    "                    duplicate_cols.append(column)\n",
    "                    col_suffix = \"_\" + str(duplicate_cols.count(column) + 1)\n",
    "                    new_column_name = column + col_suffix\n",
    "                    processed_cols.append(new_column_name)\n",
    "            df.columns = processed_cols\n",
    "            column_lookup = {}\n",
    "            for idx, orig_col in enumerate(original_cols):\n",
    "                column_lookup[orig_col] = processed_cols[idx]\n",
    "            df = df.where((pd.notnull(df)), None)\n",
    "            df = df.convert_dtypes()\n",
    "            \n",
    "            # Set file reference variables\n",
    "            fileref_tablename = file_table_name\n",
    "            fileref_columnname = \"file_ref\"\n",
    "            fileref_id_columnname = \"file_id\"\n",
    "            \n",
    "            # Loop through columns and build TDR schema table entry from dataframe\n",
    "            array_cols = []\n",
    "            for column in df.columns:\n",
    "                \n",
    "                # Scan column to determine if it contains arrays\n",
    "                scan_rows = min(df[column].size, 100)\n",
    "                contains_lists = False\n",
    "                for i in range(0, scan_rows):\n",
    "                    col_val = df.at[i, column]\n",
    "                    try:\n",
    "                        col_val = json.loads(col_val)\n",
    "                        if isinstance(col_val, list):\n",
    "                            contains_lists = True\n",
    "                            array_cols.append(column)\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                # If list column, convert values to lists and build column dictionary\n",
    "                if contains_lists:\n",
    "                    df[column] = df.apply(lambda x: utils.convert_to_list(x[column]) if(pd.notnull(x[column])) else [], axis=1)\n",
    "                    column_dict = {}\n",
    "                    column_dict[\"name\"] = column\n",
    "                    column_dict[\"array_of\"] = True \n",
    "                    column_dict[\"datatype\"] = \"string\"\n",
    "                    column_list.append(column_dict)\n",
    "                else:\n",
    "                    # Build column dictionary\n",
    "                    column_dict = {}\n",
    "                    column_dict[\"name\"] = column\n",
    "                    column_dict[\"array_of\"] = False \n",
    "                    base_type = str(df[column].dtype)\n",
    "                    mapped_type = utils.map_datatype(base_type)\n",
    "                    \n",
    "                    # Force convert unknown type objects to strings (or fileref for file inventory file_ref fields)\n",
    "                    if create_file_table == True and tablename == fileref_tablename and column == fileref_columnname:\n",
    "                        mapped_type = \"fileref\"\n",
    "                        df[column] = df.apply(lambda x: json.loads(x[column].replace(\"\\'\", \"\\\"\")), axis=1)\n",
    "                    elif mapped_type == \"other\":\n",
    "                        mapped_type = \"string\"\n",
    "                        df[column] = df[column].astype(\"string\")\n",
    "                    \n",
    "                    # Set column datatype and append to column list\n",
    "                    column_dict[\"datatype\"] = mapped_type\n",
    "                    column_list.append(column_dict)\n",
    "\n",
    "            # Check for and build file references as necessary\n",
    "            if tablename in data_file_refs:\n",
    "                for column_entry in data_file_refs[tablename]:\n",
    "                    if column_lookup.get(column_entry[\"column\"]):\n",
    "                        encoded_col_name = column_lookup[column_entry[\"column\"]]\n",
    "                    else:\n",
    "                        encoded_col_name = utils.encode_name(column_entry[\"column\"])\n",
    "                    data_file_ref_mode = column_entry[\"mode\"]\n",
    "\n",
    "                    # If column exists in the data frame, determine whether or not a new field will be created, and the method that will be used for building the file reference\n",
    "                    # Once determined, execute the appropriate file reference building function and then update the final schema to match the inputted parameters\n",
    "                    if encoded_col_name in df.columns and column_entry[\"method\"] in [\"file_path_match\", \"tdr_file_id\"]:\n",
    "                        \n",
    "                        # Derive common variables\n",
    "                        if data_file_ref_mode == \"fileref_table_ref\" and create_file_table == True:\n",
    "                            return_field = \"file_id\"\n",
    "                        else:\n",
    "                            return_field = \"file_ref\"\n",
    "                        if encoded_col_name in array_cols or column_entry[\"match_multiple_files\"] == True:\n",
    "                            array_of = True\n",
    "                        else:\n",
    "                            array_of = False\n",
    "                        \n",
    "                        # If not creating a new field, update existing field\n",
    "                        if column_entry[\"create_new_field\"] == False:\n",
    "                            # If method is file_path_match, replace column values with file IDs or references, otherwise leave columns alone\n",
    "                            if column_entry[\"method\"] in [\"file_path_match\"]:\n",
    "                                df[encoded_col_name] = df.apply(lambda x: utils.find_file_in_inventory(x[encoded_col_name], file_lookup, return_field, column_entry[\"match_multiple_files\"], column_entry[\"match_regex\"], column_entry[\"match_type\"]), axis=1)\n",
    "                            # Update the column_list entry for the schema as appropriate\n",
    "                            for idx, val in enumerate(column_list):\n",
    "                                if val[\"name\"] == encoded_col_name:\n",
    "                                    col_list_idx = idx\n",
    "                            column_list[col_list_idx][\"array_of\"] = array_of\n",
    "                            if data_file_ref_mode == \"fileref_in_line\" or column_entry[\"method\"] == \"tdr_file_id\":\n",
    "                                column_list[col_list_idx][\"datatype\"] = \"fileref\"\n",
    "                            else:\n",
    "                                column_list[col_list_idx][\"datatype\"] = \"string\"\n",
    "                                # If file reference mode is \"fileref_table\" and method is not \"tdr_file_id\", add the appropriate relationship to link the file reference to the new file table\n",
    "                                rel_dict = utils.construct_relationship(tablename, encoded_col_name, fileref_tablename, fileref_id_columnname)\n",
    "                                relationship_list.append(rel_dict)\n",
    "                        \n",
    "                        # Otherwise, create new field\n",
    "                        else:\n",
    "                            # Record or derive new field name\n",
    "                            if column_entry[\"new_field_name\"] != None:\n",
    "                                new_col_name = utils.encode_name(column_entry[\"new_field_name\"])\n",
    "                            else:\n",
    "                                new_col_name = encoded_col_name + \"_fileref\"\n",
    "                            # If method is file_path_match, replace column values with file IDs or references, otherwise leave columns alone\n",
    "                            if column_entry[\"method\"] in [\"file_path_match\"]:\n",
    "                                df[new_col_name] = df.apply(lambda x: utils.find_file_in_inventory(x[encoded_col_name], file_lookup, return_field, column_entry[\"match_multiple_files\"], column_entry[\"match_regex\"], column_entry[\"match_type\"]), axis=1)\n",
    "                            # Create new column list entry for schema\n",
    "                            column_dict = {}\n",
    "                            column_dict[\"name\"] = new_col_name\n",
    "                            column_dict[\"array_of\"] = array_of \n",
    "                            if data_file_ref_mode == \"fileref_in_line\" or column_entry[\"method\"] == \"tdr_file_id\":\n",
    "                                column_dict[\"datatype\"] = \"fileref\"\n",
    "                            else:\n",
    "                                column_dict[\"datatype\"] = \"string\"\n",
    "                                # If file reference mode is \"fileref_table\" and method is not \"tdr_file_id\", add the appropriate relationship to link the file reference to the new file table\n",
    "                                rel_dict = utils.construct_relationship(tablename, new_col_name, fileref_tablename, fileref_id_columnname)\n",
    "                                relationship_list.append(rel_dict)\n",
    "                            column_list.append(column_dict)\n",
    "\n",
    "            # Add column list to table dict and table_dict to table_list\n",
    "            table_dict[\"columns\"] = column_list\n",
    "            table_dict[\"primaryKey\"] = []\n",
    "            table_list.append(table_dict)\n",
    "\n",
    "            # Write out file in chunks (based on rows*fileref columns)\n",
    "            records_json = df.to_json(orient=\"records\")\n",
    "            records_list = json.loads(records_json)\n",
    "            records_cnt = len(records_list)\n",
    "            fileref_cnt = 0\n",
    "            for col_entry in column_list:\n",
    "                if col_entry[\"datatype\"] == \"fileref\":\n",
    "                    fileref_cnt += 1\n",
    "            if fileref_cnt > 0:\n",
    "                combined_rec_ref_size = records_cnt*fileref_cnt\n",
    "                chunk_cnt = math.ceil(combined_rec_ref_size/max_combined_rec_ref_size)\n",
    "                chunk_size = math.ceil(records_cnt/chunk_cnt)\n",
    "            else:\n",
    "                chunk_cnt = 1\n",
    "                chunk_size = records_cnt\n",
    "            for i in range(0, chunk_cnt):\n",
    "                if i == 0:\n",
    "                    start_row = 0\n",
    "                    end_row = chunk_size\n",
    "                else:\n",
    "                    start_row = (i*chunk_size) + 1\n",
    "                    end_row = min((i+1)*chunk_size, records_cnt)\n",
    "                destination_file = tablename + \"_\" + str(i) + \".json\"\n",
    "                with open(destination_file, \"w\") as outfile:\n",
    "                    for idx, val in enumerate(records_list):\n",
    "                        if idx >= start_row and idx <= end_row:\n",
    "                            json.dump(val, outfile)\n",
    "                            if idx < end_row:\n",
    "                                outfile.write(\"\\n\")\n",
    "                !gsutil cp $destination_file $ws_bucket/$el_output_dir/$tablename/ 2> stdout \n",
    "            log_dict[key] = \"No errors raised\"\n",
    "        except Exception as e:\n",
    "            error_message = \"Error processing files for target table: {}. Error message: {}\".format(key, e)\n",
    "            logging.info(error_message)\n",
    "            log_dict[key] = error_message\n",
    "            log_status = \"Error\"\n",
    "\n",
    "    # Finish building TDR schema object\n",
    "    logging.info(\"Creating schema object and copying to cloud storage.\")\n",
    "    schema_dict = {}\n",
    "    schema_dict[\"tables\"] = table_list\n",
    "    schema_dict[\"relationships\"] = relationship_list\n",
    "\n",
    "    # Write out schema file, copy to workspace bucket, and delete from notebook environment\n",
    "    destination_file = \"tdr_schema_object.json\"\n",
    "    with open(destination_file, \"w\") as outfile:\n",
    "        json.dump(schema_dict, outfile)\n",
    "    !gsutil cp $destination_file $ws_bucket/$el_schema_file 2> stdout\n",
    "    \n",
    "    # Remove .json files\n",
    "    !rm *.json\n",
    "    \n",
    "    # Build and return table list for use in ingest\n",
    "    table_list = list(target_table_dict.keys())\n",
    "    log_string = json.dumps(log_dict)\n",
    "    logging.info(\"File processing complete. Status: {}. Details: {}. Tables to ingest: {}\".format(log_status, log_string, \", \".join(table_list)))\n",
    "    return table_list, log_status, log_string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Test - General\n",
    "# params = {}\n",
    "# ws_project = \"anvil-datastorage\"\n",
    "# ws_name = \"AnVIL_NIA_CARD_LR_WGS_HBCC_Staging\"\n",
    "# ws_attributes = utils.get_workspace_attributes(ws_project, ws_name, \"https://api.firecloud.org\")\n",
    "# params[\"workspace_bucket\"] = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\" \n",
    "# params[\"data_files_src_buckets\"] = {}\n",
    "# params[\"data_files_src_buckets\"][params[\"workspace_bucket\"]] = {\n",
    "#             \"include_dirs\": [],\n",
    "#             \"exclude_dirs\": []\n",
    "# }\n",
    "# params[\"google_project\"] = \"terra-92e58ed4\"\n",
    "# params[\"max_combined_rec_ref_size\"] = 40000\n",
    "# #params[\"input_dir\"] = \"ingest_pipeline/input/{}/table_data\".format(\"test\")\n",
    "# #params[\"file_inventory_dir\"] = \"ingest_pipeline/input/{}/data_files/file_inventory\".format(\"test\")\n",
    "# params[\"input_dir\"] = \"ingest_pipeline/input/{}/table_data\".format(ws_name)\n",
    "# params[\"file_inventory_dir\"] = \"ingest_pipeline/input/{}/data_files/file_inventory\".format(ws_name)\n",
    "# params[\"el_output_dir\"] = \"ingest_pipeline/output/source/test/table_data\"\n",
    "# params[\"el_schema_file\"] = \"ingest_pipeline/output/source/test/schema/tdr_schema_object.json\"\n",
    "# params[\"create_file_table\"] = True\n",
    "# params[\"file_table_name\"] = \"file_inventory\"\n",
    "# data_file_refs_dict = {   \n",
    "# }\n",
    "# file_ref_list, params[\"data_file_refs\"], params[\"data_files_src_buckets\"], remote_list = utils.find_and_add_fileref_fields(ws_project, ws_name, params[\"workspace_bucket\"], data_file_refs_dict, params[\"data_files_src_buckets\"], base_url=\"https://api.firecloud.org\")\n",
    "# params[\"run_env\"] = \"prod\"\n",
    "# params[\"global_file_exclusions\"] = [\"SubsetHailJointCall\", \".vds/\", \"ingest_ignore\"]\n",
    "# file_inventory, retry_count = bfi.build_inventory(params)\n",
    "# params[\"file_inventory\"] = file_inventory\n",
    "# #params[\"file_inventory\"] = []\n",
    "# target_tables, log_status, log_string = process_table_data(params)\n",
    "# # file_lookup = {}\n",
    "# # for file in file_inventory:\n",
    "# #     file_lookup[file[\"uri\"]] = {\n",
    "# #         \"file_id\": file[\"file_id\"],\n",
    "# #         \"file_ref\": file[\"file_ref\"] \n",
    "# #     } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test - File reference building\n",
    "# ws_name = \"test\"\n",
    "# file_inventory_dir = \"ingest_pipeline/input/{}/data_files/file_inventory\".format(ws_name)\n",
    "# inventory_file_path = \"gs://\" + ws_bucket_name + \"/\" + file_inventory_dir + \"/file_inventory.tsv\"\n",
    "# df_inv = pd.read_csv(inventory_file_path, delimiter = \"\\t\")\n",
    "# file_inventory = df_inv.to_dict(orient='records')\n",
    "\n",
    "# # Format file_inventory to support file ref lookup\n",
    "# file_lookup = {}\n",
    "# for file in file_inventory:\n",
    "#     file_lookup[file[\"uri\"]] = {\n",
    "#         \"file_id\": file[\"file_id\"],\n",
    "#         \"file_ref\": file[\"file_ref\"] \n",
    "#     } \n",
    "\n",
    "# value = \"gs://fc-secure-6513d7e1-2dbb-41a2-baea-3f7fdbcbb620/AnVIL_CCDG_Broad_CVD_AF_BioVU_HMB_GSO_WES/RP-2098/Exome/AF100017/v1/AF100017.cram\"\n",
    "# return_field = \"file_ref\"\n",
    "# match_multiple_files = True\n",
    "# match_regex = None\n",
    "# match_type = \"exact\"\n",
    "# return_val = utils.find_file_in_inventory(value, file_lookup, return_field, match_multiple_files, match_regex, match_type)\n",
    "# print(return_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
