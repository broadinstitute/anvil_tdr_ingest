{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version\n",
    "#print('Version 1.0.0: 10/19/2022 11:59am - Nate Calvanese - Initial version')\n",
    "print('Version 1.0.1: 10/24/2022 3:18pm - Nate Calvanese - Added pass through of params dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ingest_pipeline_utilities.ipynb\n",
      "Version 1.0.13: 10/24/2022 1:29pm - Nate Calvanese - Added function to derive project entity name\n",
      "importing Jupyter notebook from source_files_creation.ipynb\n",
      "Version 1.0.8: 10/12/2022 3:50pm - Nate Calvanese - Reverted source table prefixing\n",
      "importing Jupyter notebook from build_file_inventory.ipynb\n",
      "Version 1.0.4: 10/12/2022 10:26am - Nate Calvanese - Added support for specifying additional buckets/dirs\n",
      "importing Jupyter notebook from process_table_data.ipynb\n",
      "Version: 1.0.5: 10/13/2022 12:40pm - Nate Calvanese - Fixed bug building file refs when reading in file inventory\n",
      "importing Jupyter notebook from build_mapping_query.ipynb\n",
      "Version 1.0.7: 10/15/2022 3:08pm - Nate Calvanese - Added support for column aggregation logic\n",
      "importing Jupyter notebook from output_data_validation.ipynb\n",
      "Version 2.0.4: 10/19/2022 9:49am -- Updated schema processing to support schemas not sourced from API\n",
      "importing Jupyter notebook from resolve_dangling_foreign_keys.ipynb\n",
      "Version 1.0.0: 10/19/2022 11:59am - Nate Calvanese - Initial version\n"
     ]
    }
   ],
   "source": [
    "## imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import data_repo_client\n",
    "from google.cloud import bigquery\n",
    "import ingest_pipeline_utilities as utils\n",
    "import output_data_validation as odv\n",
    "import logging\n",
    "from time import sleep\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ingest_records(params, dataset_id, table, records_dict):\n",
    "    logging.info(\"Missing key values found for {}: {} new records to ingest\".format(table, str(len(records_dict))))\n",
    "    logging.info(\"Submitting dangling foreign key ingest for {}.\".format(table))\n",
    "    \n",
    "    # Build, submit, and monitor ingest request\n",
    "    ingest_request = {\n",
    "        \"table\": table,\n",
    "        \"profile_id\": params[\"profile_id\"],\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"array\",\n",
    "        \"load_tag\": \"Dangling foreign key ingest for {} in {}\".format(table, dataset_id),\n",
    "        \"records\": records_dict\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"Dangling foreign key ingest for {} succeeded: {}\".format(table, str(ingest_request_result)[0:1000]))\n",
    "            return_str = \"Ingest Succeeded\"\n",
    "            status = \"Success\"\n",
    "            return return_str, status\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on dangling foreign key ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying dangling foreign key ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Logging error.\")\n",
    "                return_str = \"Ingest Failed ({})\".format(str(e))\n",
    "                status = \"Error\"\n",
    "                return return_str, status\n",
    "\n",
    "def identify_dangling_fks(params, table, client, bq_project, bq_schema, field_list, array_field_set):\n",
    "    for column_entry in field_list:\n",
    "        if column_entry[\"table\"] == table and column_entry[\"is_primary_key\"] == True and (len(column_entry[\"joins_from\"]) > 0):\n",
    "            logging.info(\"Identifying dangling foreign keys for {}...\".format(table))\n",
    "            \n",
    "            # Construct a CTE that includes all foreign keys that reference the primary key\n",
    "            table_name = column_entry[\"table\"]\n",
    "            col_name = column_entry[\"column\"]\n",
    "            counter = 0\n",
    "            cte_query = \"WITH temp_fks AS (\"\n",
    "            source_col_list = []\n",
    "            for entry in column_entry[\"joins_from\"]:\n",
    "                cte_query_segment = \"\"\n",
    "                counter += 1\n",
    "                source_table = entry[\"table\"]\n",
    "                source_column = entry[\"column\"]\n",
    "                source_table_col = entry[\"table\"] + \".\" + entry[\"column\"]\n",
    "                if counter > 1:\n",
    "                    cte_query_segment = \"UNION ALL \"\n",
    "                if source_table_col in array_field_set:\n",
    "                    cte_query_segment += \"SELECT DISTINCT {tar_col} FROM `{project}.{schema}.{table}` CROSS JOIN UNNEST({src_col}) AS {tar_col}\".format(project = bq_project, schema = bq_schema, table = source_table, src_col = source_column, tar_col = col_name)\n",
    "                else:\n",
    "                    cte_query_segment += \"SELECT DISTINCT {src_col} as {tar_col}  FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = source_table, src_col = source_column, tar_col = col_name)\n",
    "                cte_query = cte_query + cte_query_segment + \" \"\n",
    "            cte_query = cte_query + \")\"\n",
    "            \n",
    "            # Construct the query to identify foreign keys not present in the primary key field\n",
    "            dangling_fk_query = \"\"\"{cte} SELECT DISTINCT src.{col} AS {col},\n",
    "                                   FROM temp_fks src LEFT JOIN `{project}.{schema}.{table}` tar ON src.{col} = tar.{col}\n",
    "                                   WHERE src.{col} IS NOT NULL AND tar.{col} IS NULL\"\"\".format(cte = cte_query, project = bq_project, schema = bq_schema, table = table_name, col = col_name)\n",
    "            \n",
    "            # Execute the query and convert results to a dict\n",
    "            try:\n",
    "                df = client.query(dangling_fk_query).result().to_dataframe()\n",
    "                record_dict = df.to_dict(orient=\"records\")\n",
    "                return record_dict, \"Missing {} values\".format(str(len(record_dict))), \"Success\"\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "                return [], \"Error retrieving missing values ({})\".format(str(e)), \"Error\"\n",
    "    return [], \"No foreign keys identified\", \"Success\"  \n",
    "\n",
    "def resolve_dangling_fks(params, dataset_id, target_schema):\n",
    "    # Build return log items\n",
    "    return_log = \"\"\n",
    "    fail_count = 0\n",
    "    return_status = \"Success\"\n",
    "    \n",
    "    # Establish TDR API client and retrieve the schema for the specified dataset\n",
    "    logging.info(\"Attempting to identify the TDR object, and collect and parse its schema...\")\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    full_tdr_schema, bq_project, bq_schema, skip_bq_queries = odv.retrieve_tdr_schema(dataset_id, \"dataset\", api_client)\n",
    "    if skip_bq_queries:\n",
    "        return \"Error retrieving BQ project and schema\", \"Error\"\n",
    "    table_set, array_field_set, field_list, relationship_count = odv.process_tdr_schema(target_schema, \"file\")\n",
    "\n",
    "    # Loop through target schema tables, identify dangling foreign keys, and create new ingest records for them\n",
    "    logging.info(\"Attempting to identify and remediate dangling foreign keys...\")\n",
    "    client = bigquery.Client()\n",
    "    for table in table_set:\n",
    "        table_log = table + \": \"\n",
    "        records_dict, identify_str, identify_status = identify_dangling_fks(params, table, client, bq_project, bq_schema, field_list, array_field_set)\n",
    "        table_log = table_log + identify_str\n",
    "        ingest_status = \"\"\n",
    "        if records_dict:\n",
    "            ingest_str, ingest_status = ingest_records(params, dataset_id, table, records_dict)\n",
    "            table_log = table_log + \" - \" + ingest_str\n",
    "        if return_log == \"\":\n",
    "            return_log = table_log\n",
    "        else:\n",
    "            return_log = return_log + \"; \" + table_log\n",
    "        if identify_status == \"Error\" or ingest_status == \"Error\":\n",
    "            fail_count += 1\n",
    "    if fail_count > 0:\n",
    "        return_status = \"Error\"\n",
    "    return return_log, return_status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "# dataset_id = \"2b595fcb-9eb2-44ce-b72c-760c1e34c0a4\"\n",
    "# mapping_target = \"anvil\"\n",
    "# from google.cloud import storage\n",
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "# blob = bucket.blob(\"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(mapping_target, dataset_id))\n",
    "# target_schema = json.loads(blob.download_as_string(client=None))\n",
    "# output, status = resolve_dangling_fks(params, dataset_id, target_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
