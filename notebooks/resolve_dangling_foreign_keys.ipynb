{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version\n",
    "#print('Version 1.0.0: 10/19/2022 11:59am - Nate Calvanese - Initial version')\n",
    "#print('Version 1.0.1: 10/24/2022 3:18pm - Nate Calvanese - Added pass through of params dict')\n",
    "#print('Version 1.0.2: 02/07/2024 12:54pm - Nate Calvanese - Added logic to handle part_of_dataset required field')\n",
    "#print('Version 1.0.3: 03/12/2024 12:12pm - Nate Calvanese - Fixed a bug introduced in V1.0.2 update')\n",
    "#print('Version 1.0.4: 09/26/2024 9:28am - Nate Calvanese - Improved logic for handling part_of_dataset field')\n",
    "print('Version 1.0.5: 5/7/2025 4:15pm - Nate Calvanese - Updated to support dev')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import data_repo_client\n",
    "from google.cloud import bigquery\n",
    "import ingest_pipeline_utilities as utils\n",
    "import output_data_validation as odv\n",
    "import logging\n",
    "from time import sleep\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ingest_records(params, dataset_id, table, records_dict):\n",
    "    logging.info(\"Missing key values found for {}: {} new records to ingest\".format(table, str(len(records_dict))))\n",
    "    logging.info(\"Submitting dangling foreign key ingest for {}.\".format(table))\n",
    "    \n",
    "    # Build, submit, and monitor ingest request\n",
    "    ingest_request = {\n",
    "        \"table\": table,\n",
    "        \"profile_id\": params[\"profile_id\"],\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"array\",\n",
    "        \"load_tag\": \"Dangling foreign key ingest for {} in {}\".format(table, dataset_id),\n",
    "        \"records\": records_dict\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client(params[\"tdr_url\"])\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request), params[\"run_env\"])\n",
    "            logging.info(\"Dangling foreign key ingest for {} succeeded: {}\".format(table, str(ingest_request_result)[0:1000]))\n",
    "            return_str = \"Ingest Succeeded\"\n",
    "            status = \"Success\"\n",
    "            return return_str, status\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on dangling foreign key ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying dangling foreign key ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Logging error.\")\n",
    "                return_str = \"Ingest Failed ({})\".format(str(e))\n",
    "                status = \"Error\"\n",
    "                return return_str, status\n",
    "\n",
    "def identify_dangling_fks(params, table, client, bq_project, bq_schema, field_list, array_field_set, dataset_name):\n",
    "    for column_entry in field_list:\n",
    "        if column_entry[\"table\"] == table and column_entry[\"is_primary_key\"] == True and (len(column_entry[\"joins_from\"]) > 0):\n",
    "            logging.info(\"Identifying dangling foreign keys for {}...\".format(table))\n",
    "            \n",
    "            # Construct a CTE that includes all foreign keys that reference the primary key\n",
    "            table_name = column_entry[\"table\"]\n",
    "            col_name = column_entry[\"column\"]\n",
    "            counter = 0\n",
    "            cte_query = \"WITH temp_fks AS (\"\n",
    "            source_col_list = []\n",
    "            for entry in column_entry[\"joins_from\"]:\n",
    "                cte_query_segment = \"\"\n",
    "                counter += 1\n",
    "                source_table = entry[\"table\"]\n",
    "                source_column = entry[\"column\"]\n",
    "                source_table_col = entry[\"table\"] + \".\" + entry[\"column\"]\n",
    "                if counter > 1:\n",
    "                    cte_query_segment = \"UNION ALL \"\n",
    "                if source_table_col in array_field_set:\n",
    "                    cte_query_segment += \"SELECT DISTINCT {tar_col} FROM `{project}.{schema}.{table}` CROSS JOIN UNNEST({src_col}) AS {tar_col}\".format(project = bq_project, schema = bq_schema, table = source_table, src_col = source_column, tar_col = col_name)\n",
    "                else:\n",
    "                    cte_query_segment += \"SELECT DISTINCT {src_col} as {tar_col}  FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = source_table, src_col = source_column, tar_col = col_name)\n",
    "                cte_query = cte_query + cte_query_segment + \" \"\n",
    "            cte_query = cte_query + \")\"\n",
    "            \n",
    "            # Construct the query to identify foreign keys not present in the primary key field\n",
    "            if table_name in \"anvil_donor\":\n",
    "                dataset_select = f\", `dsp-data-ingest.transform_resources`.uuid_hash_value('{dataset_name}') AS part_of_dataset_id\"\n",
    "            elif table_name == \"anvil_biosample\":\n",
    "                dataset_select = f\", [`dsp-data-ingest.transform_resources`.uuid_hash_value('{dataset_name}')] AS part_of_dataset_id\"\n",
    "            else:\n",
    "                dataset_select = \"\"\n",
    "            dangling_fk_query = \"\"\"{cte}, base AS (SELECT DISTINCT src.{col} AS {col},\n",
    "                           FROM temp_fks src LEFT JOIN `{project}.{schema}.{table}` tar ON src.{col} = tar.{col}\n",
    "                           WHERE src.{col} IS NOT NULL AND tar.{col} IS NULL)\n",
    "                           SELECT {col}{mod} FROM base\"\"\".format(cte = cte_query, project = bq_project, schema = bq_schema, table = table_name, col = col_name, mod = dataset_select)\n",
    "#             print(table_name)\n",
    "#             print(dangling_fk_query)\n",
    "            \n",
    "            # Execute the query and convert results to a dict\n",
    "            try:\n",
    "                df = client.query(dangling_fk_query).result().to_dataframe()\n",
    "                records_json = df.to_json(orient=\"records\") \n",
    "                record_dict = json.loads(records_json)\n",
    "                return record_dict, \"Missing {} values\".format(str(len(record_dict))), \"Success\"\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "                return [], \"Error retrieving missing values ({})\".format(str(e)), \"Error\"\n",
    "    return [], \"No foreign keys identified\", \"Success\"  \n",
    "\n",
    "def resolve_dangling_fks(params, dataset_id, target_schema):\n",
    "    # Build return log items\n",
    "    return_log = \"\"\n",
    "    fail_count = 0\n",
    "    return_status = \"Success\"\n",
    "    \n",
    "    # Establish TDR API client and retrieve the schema for the specified dataset\n",
    "    logging.info(\"Attempting to identify the TDR object, and collect and parse its schema...\")\n",
    "    api_client = utils.refresh_tdr_api_client(params[\"tdr_url\"])\n",
    "    full_tdr_schema, bq_project, bq_schema, skip_bq_queries = odv.retrieve_tdr_schema(dataset_id, \"dataset\", api_client)\n",
    "    if skip_bq_queries:\n",
    "        return \"Error retrieving BQ project and schema\", \"Error\"\n",
    "    table_set, array_field_set, field_list, relationship_count = odv.process_tdr_schema(target_schema, \"file\")\n",
    "    \n",
    "    # Retrieve dataset name\n",
    "    dataset_name = \"\"\n",
    "    try:\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        dataset_name = response[\"name\"]\n",
    "        dataset_name = re.sub(\"(_[0-9]+$)\", \"\", dataset_name)\n",
    "    except Exception as e:\n",
    "        return \"Error retrieving dataset details\", \"Error\"\n",
    "\n",
    "    # Loop through target schema tables, identify dangling foreign keys, and create new ingest records for them\n",
    "    logging.info(\"Attempting to identify and remediate dangling foreign keys...\")\n",
    "    client = bigquery.Client()\n",
    "    for table in table_set:\n",
    "        table_log = table + \": \"\n",
    "        records_dict, identify_str, identify_status = identify_dangling_fks(params, table, client, bq_project, bq_schema, field_list, array_field_set, dataset_name)\n",
    "        table_log = table_log + identify_str\n",
    "        ingest_status = \"\"\n",
    "        if records_dict:\n",
    "            ingest_str, ingest_status = ingest_records(params, dataset_id, table, records_dict)\n",
    "            table_log = table_log + \" - \" + ingest_str\n",
    "        if return_log == \"\":\n",
    "            return_log = table_log\n",
    "        else:\n",
    "            return_log = return_log + \"; \" + table_log\n",
    "        if identify_status == \"Error\" or ingest_status == \"Error\":\n",
    "            fail_count += 1\n",
    "    if fail_count > 0:\n",
    "        return_status = \"Error\"\n",
    "    return return_log, return_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "# params = {}\n",
    "# params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "# params[\"run_env\"] = \"prod\"\n",
    "# params[\"tdr_url\"] = \"https://data.terra.bio\"\n",
    "# #dataset_id = \"d239dd7b-8d10-4960-aa91-8f8ede641e25\" #anvil_donor example\n",
    "# #dataset_id = \"ae4c80c8-a946-49cb-b376-81b4749f3221\" #anvil_biosample example\n",
    "# dataset_id = \"6d18aafc-0240-499c-902e-a72a5b98ff0a\"\n",
    "# mapping_target = \"anvil\"\n",
    "# from google.cloud import storage\n",
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "# blob = bucket.blob(\"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(mapping_target, dataset_id))\n",
    "# target_schema = json.loads(blob.download_as_string(client=None))\n",
    "# output, status = resolve_dangling_fks(params, dataset_id, target_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
