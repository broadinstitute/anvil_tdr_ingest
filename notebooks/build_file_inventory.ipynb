{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print('Version 1.0.0: 09/08/2022 07:56pm - Nate Calvanese - Initial Version')\n",
    "#print('Version 1.0.1: 09/26/2022 01:41pm - Nate Calvanese - Fixed bug when MD5 was not available')\n",
    "#print('Version 1.0.1: 09/26/2022 09:26pm - Nate Calvanese - Updated to be able to point to multiple buckets at once')\n",
    "#print('Version 1.0.2: 09/29/2022 02:59pm - Nate Calvanese - Added progress logging and retry logic for cloud storage calls')\n",
    "#print('Version 1.0.3: 10/04/2022 4:26pm - Nate Calvanese - Flattened target file path to help with ingest performance')\n",
    "print('Version 1.0.4: 10/12/2022 10:26am - Nate Calvanese - Added support for specifying additional buckets/dirs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables\n",
    "\n",
    "# Imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "import re\n",
    "import uuid\n",
    "import logging\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "\n",
    "# Function to return objects in specified bucket\n",
    "def get_objects_list(bucket_name, user_proj, dirs_to_include=[], dirs_to_exclude=[]):\n",
    "    \n",
    "    # Collect list of objects/blobs from bucket \n",
    "    obj_list = []\n",
    "    storage_client = storage.Client()\n",
    "    storage_bucket = storage_client.bucket(bucket_name, user_project=user_proj)\n",
    "    objects = list(storage_client.list_blobs(storage_bucket))\n",
    "    \n",
    "    # Loop through list of objects and append names to final list based on the directories to include and exclude\n",
    "    for obj in objects:\n",
    "        # Process inclusion\n",
    "        if len(dirs_to_include) > 0:\n",
    "            for entry in dirs_to_include:\n",
    "                entry_str = (entry + \"/\").replace(\"//\", \"/\")\n",
    "                if entry_str in obj.name:\n",
    "                    obj_list.append(obj.name)\n",
    "        else:\n",
    "            obj_list.append(obj.name)\n",
    "        # Process exclusions\n",
    "        if len(dirs_to_exclude) > 0:\n",
    "            for entry in dirs_to_exclude:\n",
    "                entry_str = (entry + \"/\").replace(\"//\", \"/\")\n",
    "                if entry_str in obj.name and obj.name in obj_list:\n",
    "                    obj_list.remove(obj.name)\n",
    "    return obj_list\n",
    "\n",
    "# Function to return object metadata\n",
    "def get_object(bucket_name, user_proj, object_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name, user_project=user_proj)\n",
    "    obj = bucket.get_blob(object_name)\n",
    "    return obj\n",
    "\n",
    "# Function to pull full file extension (including compression extensions)\n",
    "def get_full_file_ext(filepath):\n",
    "    full_ext_string = filepath\n",
    "    compression_extension = ''\n",
    "    compression_extensions = ['.7z', '.zip', '.gz', '.tar.gz', '.tgz']\n",
    "    for item in compression_extensions:\n",
    "        pattern = item + '$'\n",
    "        if re.search(pattern, full_ext_string):\n",
    "            full_ext_string = re.sub(pattern, '', full_ext_string)\n",
    "            compression_extension = item\n",
    "            break\n",
    "    full_ext_string = os.path.splitext(full_ext_string)[1] + compression_extension\n",
    "    return full_ext_string\n",
    "\n",
    "# Function to build file inventory\n",
    "def build_inventory(params):\n",
    "\n",
    "    # Collect parameters\n",
    "    data_files_src_buckets = params[\"data_files_src_buckets\"]\n",
    "    user_project = params[\"google_project\"]\n",
    "    file_inventory_dir = params[\"file_inventory_dir\"]\n",
    "    \n",
    "    # Initialize variables\n",
    "    record_list = []\n",
    "    retry_count = 0\n",
    "    prior_record_list_len = 0\n",
    "\n",
    "    # Loop through object list to construct inventory entry for each non-directory object \n",
    "    if data_files_src_buckets == None:\n",
    "        data_files_src_buckets[ws_bucket_name] = {\n",
    "            \"include_dirs\": [],\n",
    "            \"exclude_dirs\": []\n",
    "        }\n",
    "    for bucket, criteria in data_files_src_buckets.items():\n",
    "        # Get object list for bucket in question\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                object_list = get_objects_list(bucket, user_project, criteria[\"include_dirs\"], criteria[\"exclude_dirs\"])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt_counter += 1\n",
    "                if attempt_counter <= 2:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise Exception(str(e))\n",
    "        # Loop through object list for bucket in question\n",
    "        object_count = len(object_list)\n",
    "        if object_count >= 100:\n",
    "            object_count_decile = object_count//10\n",
    "            log_deciles = True\n",
    "        else:\n",
    "            object_count_decile = object_count\n",
    "            log_deciles = False\n",
    "        logging.info(\"Recording inventory entries from {} ({} objects total)\".format(bucket, str(object_count)))\n",
    "        for entry in object_list:\n",
    "            if not re.search('/$', entry):\n",
    "                # Get object to build inventory entry record\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        # Collect information from object\n",
    "                        entry_obj = get_object(bucket, user_project, entry)\n",
    "                        entry_obj_uri = \"gs://\" + bucket + \"/\" + entry_obj.name\n",
    "                        entry_obj_id_str = \"\".join(filter(None, [entry_obj_uri, entry_obj.md5_hash]))\n",
    "                        entry_obj_id = str(uuid.uuid5(uuid.NAMESPACE_OID, str(entry_obj_id_str)))\n",
    "                        entry_obj_file_name = os.path.split(entry_obj.name)[1]\n",
    "                        entry_obj_path = entry_obj.name\n",
    "                        entry_obj_full_ext = get_full_file_ext(entry_obj_file_name)\n",
    "                        # Construct fileref object\n",
    "                        fileref_obj = {}\n",
    "                        fileref_obj['sourcePath'] = entry_obj_uri\n",
    "                        fileref_obj['targetPath'] = \"/\" + entry_obj_path.replace(\"/\", \"_\")\n",
    "                        fileref_obj['description'] = f\"Ingest of {entry_obj_uri}\"\n",
    "                        fileref_obj['mimeType'] = entry_obj.content_type if entry_obj.content_type else \"application/octet-stream\"\n",
    "                        # Construct inventory entry record and append to record list\n",
    "                        entry_obj_record = []\n",
    "                        entry_obj_record = [entry_obj_id, entry_obj_file_name, entry_obj_path, entry_obj_uri, entry_obj.content_type, entry_obj_full_ext, entry_obj.size, entry_obj.crc32c, entry_obj.md5_hash, fileref_obj]  \n",
    "                        record_list.append(entry_obj_record)\n",
    "                        record_list_len = len(record_list) - prior_record_list_len\n",
    "                        if log_deciles:\n",
    "                            if record_list_len%object_count_decile == 0:\n",
    "                                completed_perc = (record_list_len//object_count_decile) * 10\n",
    "                                logging.info(\"{} files recorded (~{}%)\".format(str(record_list_len), str(completed_perc)))\n",
    "                        else:\n",
    "                            if record_list_len == object_count_decile:\n",
    "                                logging.info(\"{} files recorded (~100%)\".format(str(record_list_len)))\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        attempt_counter += 1\n",
    "                        retry_count += 1\n",
    "                        if attempt_counter <= 2:\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise Exception(str(e))\n",
    "        prior_record_list_len = len(record_list)\n",
    "    \n",
    "    # Build inventory dataframe, drop duplicates, and build JSON object\n",
    "    logging.info(\"All inventory entries recorded ({} objects total).\".format(str(len(record_list))))\n",
    "    column_list = ['file_id', 'name', 'path', 'uri', 'content_type', 'full_extension', 'size_in_bytes', 'crc32c', 'md5_hash', 'file_ref']\n",
    "    df_file_inventory = pd.DataFrame(record_list, columns = column_list)\n",
    "    df_file_inventory.drop_duplicates(['name', 'md5_hash'], keep='first', inplace=True, ignore_index=True)\n",
    "    file_inventory = df_file_inventory.to_dict(orient='records')\n",
    "    \n",
    "    # Write out inventory as file\n",
    "    destination_file = \"file_inventory.tsv\"\n",
    "    df_file_inventory.to_csv(destination_file, index=False, sep='\\t')\n",
    "    !gsutil cp $destination_file $ws_bucket/$file_inventory_dir/ 2> stdout\n",
    "    \n",
    "    return file_inventory, retry_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"data_files_src_buckets\"] = {\n",
    "#     \"fc-secure-0aedc988-3736-496c-b7ac-20cca5b3ceb9\": {\n",
    "#         \"include_dirs\": [], # Leave empty to include all\n",
    "#         \"exclude_dirs\": []\n",
    "#     },\n",
    "#     \"fc-secure-4859bab0-bf7e-4eb0-8ded-c6caeb89feba\": {\n",
    "#         \"include_dirs\": [\"146623\", \"146629\"], # Leave empty to include all\n",
    "#         \"exclude_dirs\": []\n",
    "#     },\n",
    "#     \"fc-secure-34f13712-8698-47cb-9b1e-a1b87fae14fa\": {\n",
    "#         \"include_dirs\": [], # Leave empty to include all\n",
    "#         \"exclude_dirs\": [\"199168\"]\n",
    "#     } \n",
    "# }\n",
    "# params[\"google_project\"] = \"terra-349c8d95\"\n",
    "# params[\"file_inventory_dir\"] = \"ingest_pipeline/input/test/data_files/file_inventory\"\n",
    "# inventory, retry_count = build_inventory(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(retry_count)\n",
    "# print(inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
