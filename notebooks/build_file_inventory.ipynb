{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print('Version 1.0.0: 09/08/2022 07:56pm - Nate Calvanese - Initial Version')\n",
    "#print('Version 1.0.1: 09/26/2022 01:41pm - Nate Calvanese - Fixed bug when MD5 was not available')\n",
    "#print('Version 1.0.1: 09/26/2022 09:26pm - Nate Calvanese - Updated to be able to point to multiple buckets at once')\n",
    "#print('Version 1.0.2: 09/29/2022 02:59pm - Nate Calvanese - Added progress logging and retry logic for cloud storage calls')\n",
    "#print('Version 1.0.3: 10/04/2022 4:26pm - Nate Calvanese - Flattened target file path to help with ingest performance')\n",
    "#print('Version 1.0.4: 10/12/2022 10:26am - Nate Calvanese - Added support for specifying additional buckets/dirs')\n",
    "#print('Version 1.0.5: 3/6/2023 3:39pm - Nate Calvanese - Added support for remote file references')\n",
    "#print('Version 2.0.0: 3/7/2023 9:32pm - Nate Calvanese - Massive performance improvement with use of gsutil parsing')\n",
    "#print('Version 2.0.1: 3/23/2023 8:29pm - Nate Calvanese - Added support for a global file exclusion')\n",
    "#print('Version 2.0.2: 5/25/2023 9:29am - Nate Calvanese - Updated target path logic to better support remote file references')\n",
    "#print('Version 2.0.3: 10/6/2023 9:29am - Nate Calvanese - Tweaked file extension parsing logic')\n",
    "#print('Version 2.0.4: 4/12/2024 2:30pm - Nate Calvanese - Fixed target path logic to remove unsupported characters')\n",
    "print('Version 2.0.4: 10/18/2024 2:19pm - Nate Calvanese - Updated get_objects_list function to not use fuzzy matching for full file paths')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables\n",
    "\n",
    "# Imports\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from google.cloud import storage\n",
    "import re\n",
    "import uuid\n",
    "import logging\n",
    "import subprocess\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "\n",
    "# Function to parse gsutil ls -L contents into dict structure\n",
    "def parse_ls_output(subprocess_output):\n",
    "    records = []\n",
    "    file_dict = {}\n",
    "    for line in subprocess_output.split(\"\\n\"):\n",
    "        if line[0:2] == \"gs\":\n",
    "            if file_dict:\n",
    "                records.append(file_dict)\n",
    "            file_dict = {}\n",
    "            if \"/:\" not in line:\n",
    "                file_dict[\"GlobalPath\"] = re.sub(\":$\", \"\", line)\n",
    "        else:\n",
    "            if file_dict:\n",
    "                if \"Content-Length:\" in line:\n",
    "                    clm = re.match(\"\\s*Content-Length:\\s*([0-9]+)\", line).group(1)\n",
    "                    file_dict[\"Size\"] = clm\n",
    "                elif \"Content-Type:\" in line:\n",
    "                    ctm = re.match(\"\\s*Content-Type:\\s*(.*)\", line).group(1)\n",
    "                    file_dict[\"Type\"] = ctm \n",
    "                elif \"Hash (crc32c):\" in line:\n",
    "                    crcm = re.match(\"\\s*Hash \\(crc32c\\):\\s*(.*)\", line).group(1)\n",
    "                    file_dict[\"crc32c\"] = crcm\n",
    "                elif \"Hash (md5):\" in line:\n",
    "                    md5m = re.match(\"\\s*Hash \\(md5\\):\\s*(.*)\", line).group(1)\n",
    "                    file_dict[\"md5\"] = md5m\n",
    "                elif \"Update time:\" in line:\n",
    "                    updm = re.match(\"\\s*Update time:\\s*(.*)\", line).group(1)\n",
    "                    file_dict[\"Modified\"] = updm\n",
    "    if file_dict:\n",
    "        records.append(file_dict)\n",
    "    return records\n",
    "\n",
    "# Function to return objects in specified bucket\n",
    "def get_objects_list(bucket_name, user_proj, dirs_to_include=[], dirs_to_exclude=[]):\n",
    "    \n",
    "    # Collect list of objects/blobs from bucket \n",
    "    obj_list = []\n",
    "    storage_client = storage.Client()\n",
    "    storage_bucket = storage_client.bucket(bucket_name, user_project=user_proj)\n",
    "    objects = list(storage_client.list_blobs(storage_bucket))\n",
    "    \n",
    "    # Loop through list of objects and append names to final list based on the directories to include and exclude\n",
    "    for obj in objects:\n",
    "        # Process inclusion\n",
    "        if len(dirs_to_include) > 0:\n",
    "            for entry in dirs_to_include:\n",
    "                if \".\" in entry:\n",
    "                    entry_str = entry\n",
    "                    if entry_str == obj.name:\n",
    "                        obj_list.append(obj.name)\n",
    "                else:\n",
    "                    entry_str = (entry + \"/\").replace(\"//\", \"/\")\n",
    "                    if entry_str in obj.name:\n",
    "                        obj_list.append(obj.name)\n",
    "        else:\n",
    "            obj_list.append(obj.name)\n",
    "        # Process exclusions\n",
    "        if len(dirs_to_exclude) > 0:\n",
    "            for entry in dirs_to_exclude:\n",
    "                if \".\" in entry:\n",
    "                    entry_str = entry\n",
    "                    if entry_str == obj.name and obj.name in obj_list:\n",
    "                        obj_list.remove(obj.name)\n",
    "                else:\n",
    "                    entry_str = (entry + \"/\").replace(\"//\", \"/\")\n",
    "                    if entry_str in obj.name and obj.name in obj_list:\n",
    "                        obj_list.remove(obj.name)\n",
    "    return obj_list\n",
    "\n",
    "# Function to return object metadata\n",
    "def get_object(bucket_name, user_proj, object_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name, user_project=user_proj)\n",
    "    obj = bucket.get_blob(object_name)\n",
    "    return obj\n",
    "\n",
    "# Function to pull full file extension (including compression extensions)\n",
    "def get_full_file_ext(filepath):\n",
    "    full_ext_string = filepath\n",
    "    compression_extension = ''\n",
    "    compression_extensions = ['.7z', '.zip', '.gz', '.tar.gz', '.tgz', '.bgz', '.tar']\n",
    "    for item in compression_extensions:\n",
    "        pattern = \"\\\\\" + item + \"$\"\n",
    "        if re.search(pattern, full_ext_string):\n",
    "            full_ext_string = re.sub(pattern, '', full_ext_string)\n",
    "            compression_extension = item\n",
    "            break\n",
    "    full_ext_string = os.path.splitext(full_ext_string)[1] + compression_extension\n",
    "    return full_ext_string\n",
    "\n",
    "# Function to build file inventory\n",
    "def build_inventory(params):\n",
    "\n",
    "    # Collect parameters\n",
    "    data_files_src_buckets = params[\"data_files_src_buckets\"]\n",
    "    user_project = params[\"google_project\"]\n",
    "    file_inventory_dir = params[\"file_inventory_dir\"]\n",
    "    global_file_exclusions = params[\"global_file_exclusions\"]\n",
    "    \n",
    "    # Initialize variables\n",
    "    record_list = []\n",
    "    retry_count = 0\n",
    "    prior_record_list_len = 0\n",
    "\n",
    "    # Loop through object list to construct inventory entry for each non-directory object \n",
    "    if data_files_src_buckets == None:\n",
    "        data_files_src_buckets[ws_bucket_name] = {\n",
    "            \"include_dirs\": [],\n",
    "            \"exclude_dirs\": []\n",
    "        }\n",
    "    for bucket, criteria in data_files_src_buckets.items():\n",
    "        \n",
    "        # Get object list for bucket in question\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                object_list = get_objects_list(bucket, user_project, criteria[\"include_dirs\"], criteria[\"exclude_dirs\"])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt_counter += 1\n",
    "                if attempt_counter <= 2:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise Exception(str(e))\n",
    "        \n",
    "        # Loop through object list for bucket in question\n",
    "        object_count = len(object_list)\n",
    "        if object_count >= 100:\n",
    "            object_count_decile = object_count//10\n",
    "            log_deciles = True\n",
    "        else:\n",
    "            object_count_decile = object_count\n",
    "            log_deciles = False\n",
    "        logging.info(\"Recording inventory entries from {} ({} objects total)\".format(bucket, str(object_count)))\n",
    "        \n",
    "        # For buckets without criteria specified, pull file metadata information using gsutil parsing\n",
    "        if len(criteria[\"include_dirs\"]) == 0 and len(criteria[\"exclude_dirs\"]) == 0:\n",
    "            gcs_path = \"gs://\" + bucket\n",
    "            cmd = f\"gsutil -u anvil-datastorage ls -L {gcs_path}/**\"\n",
    "            output = subprocess.check_output(cmd, shell=True, universal_newlines=True)\n",
    "            records = parse_ls_output(output)\n",
    "            df = pd.DataFrame(records)\n",
    "            for index, row in df.iterrows():\n",
    "                # Collect information from object\n",
    "                row.mask(row.isna(), other=None, inplace=True)\n",
    "                entry_obj_uri = row[\"GlobalPath\"]\n",
    "                entry_obj_id_str = \"\".join(filter(None, [entry_obj_uri, row[\"md5\"]]))\n",
    "                entry_obj_id = str(uuid.uuid5(uuid.NAMESPACE_OID, str(entry_obj_id_str)))\n",
    "                entry_obj_file_name = os.path.basename(row[\"GlobalPath\"])\n",
    "                entry_obj_path = row[\"GlobalPath\"].replace(gcs_path + \"/\", \"\")\n",
    "                entry_obj_full_ext = get_full_file_ext(entry_obj_file_name)\n",
    "                if row[\"Type\"] == \"None\":\n",
    "                    entry_obj_type = None\n",
    "                else:\n",
    "                    entry_obj_type = row[\"Type\"]\n",
    "                # Construct fileref object\n",
    "                fileref_obj = {}\n",
    "                fileref_obj['sourcePath'] = entry_obj_uri\n",
    "                fileref_obj['targetPath'] = \"/\" + entry_obj_path.replace(\"/\", \"_\").replace(\"#\", \"\").replace(\"?\", \"\")\n",
    "                fileref_obj['description'] = f\"Ingest of {entry_obj_uri}\"\n",
    "                fileref_obj['mimeType'] = entry_obj_type if entry_obj_type else \"application/octet-stream\"\n",
    "                # Construct inventory entry record and append to record list\n",
    "                entry_obj_record = []\n",
    "                entry_obj_record = [entry_obj_id, entry_obj_file_name, entry_obj_path, entry_obj_uri, entry_obj_type, entry_obj_full_ext, row[\"Size\"], row[\"crc32c\"], row[\"md5\"], fileref_obj, fileref_obj['targetPath']]  \n",
    "                record_list.append(entry_obj_record)\n",
    "                record_list_len = len(record_list) - prior_record_list_len\n",
    "                if log_deciles:\n",
    "                    if record_list_len%object_count_decile == 0:\n",
    "                        completed_perc = (record_list_len//object_count_decile) * 10\n",
    "                        logging.info(\"{} files recorded (~{}%)\".format(str(record_list_len), str(completed_perc)))\n",
    "                else:\n",
    "                    if record_list_len == object_count_decile:\n",
    "                        logging.info(\"{} files recorded (~100%)\".format(str(record_list_len)))\n",
    "            prior_record_list_len = len(record_list)\n",
    "        \n",
    "        # For buckets with criteria specified, pull file metadata information using GCS API calls\n",
    "        else:\n",
    "            for entry in object_list:\n",
    "                if not re.search('/$', entry):\n",
    "                    # Get object to build inventory entry record\n",
    "                    attempt_counter = 0\n",
    "                    while True:\n",
    "                        try:\n",
    "                            # Collect information from object\n",
    "                            entry_obj = get_object(bucket, user_project, entry)\n",
    "                            entry_obj_uri = \"gs://\" + bucket + \"/\" + entry_obj.name\n",
    "                            entry_obj_id_str = \"\".join(filter(None, [entry_obj_uri, entry_obj.md5_hash]))\n",
    "                            entry_obj_id = str(uuid.uuid5(uuid.NAMESPACE_OID, str(entry_obj_id_str)))\n",
    "                            entry_obj_file_name = os.path.split(entry_obj.name)[1]\n",
    "                            entry_obj_path = entry_obj.name\n",
    "                            entry_obj_full_ext = get_full_file_ext(entry_obj_file_name)\n",
    "                            # Construct fileref object\n",
    "                            fileref_obj = {}\n",
    "                            fileref_obj['sourcePath'] = entry_obj_uri\n",
    "                            fileref_obj['targetPath'] = \"/\" + entry_obj_path.replace(\"/\", \"_\").replace(\"#\", \"\").replace(\"?\", \"\")\n",
    "                            fileref_obj['description'] = f\"Ingest of {entry_obj_uri}\"\n",
    "                            fileref_obj['mimeType'] = entry_obj.content_type if entry_obj.content_type else \"application/octet-stream\"\n",
    "                            # Construct inventory entry record and append to record list\n",
    "                            entry_obj_record = []\n",
    "                            entry_obj_record = [entry_obj_id, entry_obj_file_name, entry_obj_path, entry_obj_uri, entry_obj.content_type, entry_obj_full_ext, entry_obj.size, entry_obj.crc32c, entry_obj.md5_hash, fileref_obj, fileref_obj['targetPath']]  \n",
    "                            record_list.append(entry_obj_record)\n",
    "                            record_list_len = len(record_list) - prior_record_list_len\n",
    "                            if log_deciles:\n",
    "                                if record_list_len%object_count_decile == 0:\n",
    "                                    completed_perc = (record_list_len//object_count_decile) * 10\n",
    "                                    logging.info(\"{} files recorded (~{}%)\".format(str(record_list_len), str(completed_perc)))\n",
    "                            else:\n",
    "                                if record_list_len == object_count_decile:\n",
    "                                    logging.info(\"{} files recorded (~100%)\".format(str(record_list_len)))\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            attempt_counter += 1\n",
    "                            retry_count += 1\n",
    "                            if attempt_counter <= 2:\n",
    "                                continue\n",
    "                            else:\n",
    "                                raise Exception(str(e))\n",
    "            prior_record_list_len = len(record_list)\n",
    "    \n",
    "    # Remove file inventory records that match a global exclusion term\n",
    "    logging.info(\"All inventory entries recorded ({} objects total).\".format(str(len(record_list))))\n",
    "    global_exclusion_text = \"; \".join(global_file_exclusions)\n",
    "    logging.info(\"Removing file objects in global file exclusion list: \" + global_exclusion_text)\n",
    "    for term in global_file_exclusions:\n",
    "        temp_record_list = []\n",
    "        for record in record_list:\n",
    "            if term not in record[3]:\n",
    "                temp_record_list.append(record)\n",
    "        record_list = []\n",
    "        record_list = temp_record_list.copy()\n",
    "    logging.info(f\"{str(len(record_list))} objects remain after file removal.\")\n",
    "    \n",
    "    # Build inventory dataframe, drop duplicates, and build JSON object\n",
    "    column_list = ['file_id', 'name', 'path', 'uri', 'content_type', 'full_extension', 'size_in_bytes', 'crc32c', 'md5_hash', 'file_ref', 'target_path']\n",
    "    df_file_inventory = pd.DataFrame(record_list, columns = column_list) \n",
    "    df_file_inventory.drop_duplicates(['target_path', 'md5_hash', 'size_in_bytes'], keep='first', inplace=True, ignore_index=True)\n",
    "    df_file_inventory.drop(columns=['target_path'], inplace=True)\n",
    "    file_inventory = df_file_inventory.to_dict(orient='records')\n",
    "    \n",
    "    # Write out inventory as file\n",
    "    destination_file = \"file_inventory.tsv\"\n",
    "    df_file_inventory.to_csv(destination_file, index=False, sep='\\t')\n",
    "    !gsutil cp $destination_file $ws_bucket/$file_inventory_dir/ 2> stdout\n",
    "    \n",
    "    return file_inventory, retry_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"data_files_src_buckets\"] = {\n",
    "# #     \"fc-a9e7890c-3902-4647-8b82-273490a7ce54\": {\n",
    "# #         \"include_dirs\": [], # Leave empty to include all\n",
    "# #         \"exclude_dirs\": []\n",
    "# #     },\n",
    "# #     \"fc-secure-4859bab0-bf7e-4eb0-8ded-c6caeb89feba\": {\n",
    "# #         \"include_dirs\": [\"146623\", \"146629\"], # Leave empty to include all\n",
    "# #         \"exclude_dirs\": []\n",
    "# #     },\n",
    "# #     \"fc-secure-34f13712-8698-47cb-9b1e-a1b87fae14fa\": {\n",
    "# #         \"include_dirs\": [], # Leave empty to include all\n",
    "# #         \"exclude_dirs\": [\"199168\"]\n",
    "# #     },\n",
    "# #     \"fc-secure-4984306f-6ceb-48a0-87d5-a6c4ef499867\": {\n",
    "# #         \"include_dirs\": [], # Leave empty to include all\n",
    "# #         \"exclude_dirs\": []\n",
    "# #     },\n",
    "#     \"fc-4310e737-a388-4a10-8c9e-babe06aaf0cf\": {\n",
    "#         \"include_dirs\": [], # Leave empty to include all\n",
    "#         \"exclude_dirs\": []\n",
    "#     },\n",
    "# }\n",
    "# params[\"google_project\"] = \"terra-349c8d95\"\n",
    "# params[\"file_inventory_dir\"] = \"ingest_pipeline/input/test/data_files/file_inventory\"\n",
    "# params[\"global_file_exclusions\"] = [\"SubsetHailJointCall\", \".vds/\"]\n",
    "# inventory, retry_count = build_inventory(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(retry_count)\n",
    "# df_final = pd.DataFrame(inventory)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.colheader_justify', 'center')\n",
    "# pd.set_option('display.precision', 3)\n",
    "# # display(df_final)\n",
    "# output_file_path = \"file_inventory_test_1.tsv\"\n",
    "# df_final.to_csv(output_file_path, index=False, sep=\"\\t\")\n",
    "# !gsutil cp $output_file_path $ws_bucket/ingest_pipeline/resources/ 2> stdout\n",
    "# !rm $output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
