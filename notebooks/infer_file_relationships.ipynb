{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version\n",
    "#print('Version 1.0.0: 11/4/2022 8:45am - Nate Calvanese - Initial version')\n",
    "#print('Version 1.0.1: 11/8/2022 4:03pm - Nate Calvanese - Expanded query to cover additional cases')\n",
    "print('Version 1.0.2: 10/6/2023 11:20am - Nate Calvanese - Updated query and added validation logic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ingest_pipeline_utilities.ipynb\n",
      "Version 1.0.31: 9/1/2023 11:44am - Nate Calvanese - Updated dataset creation to conditionally enable secure monitoring by public status\n",
      "importing Jupyter notebook from source_files_creation.ipynb\n",
      "Version 1.0.9: 2/25/2023 3:15pm - Nate Calvanese - Replaced FAPI with utils functions\n",
      "importing Jupyter notebook from build_file_inventory.ipynb\n",
      "Version 2.0.3: 10/6/2023 9:29am - Nate Calvanese - Tweaked file extension parsing logic\n",
      "importing Jupyter notebook from process_table_data.ipynb\n",
      "Version: 1.0.9: 3/8/2023 12:09pm - Nate Calvanese - Performance improvements for file ref lookups\n",
      "importing Jupyter notebook from build_mapping_query.ipynb\n",
      "Version 1.0.12: 2/21/2023 2:50pm - Nate Calvanese - Added support for $BQ_DATASET substitution variable\n",
      "importing Jupyter notebook from output_data_validation.ipynb\n",
      "Version 2.0.6: 2/28/2023 11:33am -- Updated notebook to be usable in dev (removed TDR host hardcoding)\n",
      "importing Jupyter notebook from resolve_dangling_foreign_keys.ipynb\n",
      "Version 1.0.1: 10/24/2022 3:18pm - Nate Calvanese - Added pass through of params dict\n",
      "importing Jupyter notebook from infer_file_relationships.ipynb\n",
      "Version 1.0.2: 10/6/2023 11:20am - Nate Calvanese - Updated query and added validation logic\n",
      "importing Jupyter notebook from identify_supplementary_files.ipynb\n",
      "Version 1.0.2: 10/4/2023 10:40am - Nate Calvanese - Updated query logic and added validation\n"
     ]
    }
   ],
   "source": [
    "## imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import data_repo_client\n",
    "from google.cloud import bigquery\n",
    "import ingest_pipeline_utilities as utils\n",
    "import output_data_validation as odv\n",
    "import logging\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_records(params, dataset_id, table, records_dict):\n",
    "    logging.info(\"File relationships found: {} new records to ingest\".format(str(len(records_dict))))\n",
    "    logging.info(\"Submitting ingest for inferred file relationships.\")\n",
    "    \n",
    "    # Build, submit, and monitor ingest request\n",
    "    ingest_request = {\n",
    "        \"table\": table,\n",
    "        \"profile_id\": params[\"profile_id\"],\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"array\",\n",
    "        \"load_tag\": \"File relationships inference ingest for {}\".format(dataset_id),\n",
    "        \"records\": records_dict\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"File relationships inference ingest succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            return_str = \"Ingest Succeeded: {}\".format(str(ingest_request_result)[0:1000])\n",
    "            status = \"Success\"\n",
    "            return return_str, status\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on file relationships inference ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying file relationships inference ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Logging error.\")\n",
    "                return_str = \"Ingest Failed ({})\".format(str(e))\n",
    "                status = \"Error\"\n",
    "                return return_str, status\n",
    "\n",
    "def infer_file_relationships(params, dataset_id):\n",
    "    \n",
    "    # Establish TDR API client and retrieve the schema for the specified dataset\n",
    "    logging.info(\"Attempting to identify the TDR object, and the necessary attributes...\")\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    full_tdr_schema, bq_project, bq_schema, skip_bq_queries = odv.retrieve_tdr_schema(dataset_id, \"dataset\", api_client)\n",
    "    if skip_bq_queries:\n",
    "        return \"Error retrieving BQ project and schema\", \"Error\"\n",
    "    \n",
    "    # Check files for duplicate names\n",
    "    logging.info(\"Determining file relationship inference method...\")\n",
    "    client = bigquery.Client()\n",
    "    file_query = \"\"\"\n",
    "        SELECT COUNT(*) file_count, COUNT(DISTINCT file_name) AS distinct_file_names\n",
    "        FROM `{project}.{schema}.anvil_file`\n",
    "        \"\"\".format(project=bq_project, schema = bq_schema)\n",
    "    try:\n",
    "        df = client.query(file_query).result().to_dataframe()\n",
    "        method = \"file_name\"\n",
    "        if not df.empty:\n",
    "            file_count = df[\"file_count\"].values[0]\n",
    "            distinct_name_count = df[\"distinct_file_names\"].values[0] \n",
    "            if file_count != distinct_name_count:\n",
    "                method = \"file_path\"\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "        return \"Error during query execution: {}\".format(str(e)), \"Error\"\n",
    "    if method == \"file_name\":\n",
    "        infer_file_rel_query = \"\"\"WITH existing_activity_records\n",
    "                AS\n",
    "                (\n",
    "                  SELECT activity_id, used_file_id, generated_file_id\n",
    "                  FROM `{project}.{schema}.anvil_activity`,\n",
    "                  UNNEST(used_file_id) AS used_file_id,\n",
    "                  UNNEST(generated_file_id) AS generated_file_id \n",
    "                ), \n",
    "                potential_activity_records\n",
    "                AS\n",
    "                (\n",
    "                  SELECT *, ROW_NUMBER() OVER (PARTITION BY used_file_id, generated_file_id ORDER BY 1) AS RN\n",
    "                  FROM \n",
    "                  (\n",
    "                    SELECT t1.file_id AS used_file_id, t1.file_format AS used_file_format, t0.file_id AS generated_file_id, t0.file_format AS generated_file_format, \n",
    "                    ARRAY_CONCAT(t0.source_datarepo_row_ids, t1.source_datarepo_row_ids) AS source_datarepo_row_ids\n",
    "                    FROM `{project}.{schema}.anvil_file` t0\n",
    "                      INNER JOIN `{project}.{schema}.anvil_file` t1\n",
    "                      ON REPLACE(t0.file_name, t0.file_format, '') = t1.file_name\n",
    "                      AND t0.file_name <> t1.file_name\n",
    "                    WHERE t0.file_id IS NOT NULL\n",
    "                    UNION ALL\n",
    "                    SELECT t1.file_id AS used_file_id, t1.file_format AS used_file_format, t0.file_id AS generated_file_id, t0.file_format AS generated_file_format, \n",
    "                    ARRAY_CONCAT(t0.source_datarepo_row_ids, t1.source_datarepo_row_ids) AS source_datarepo_row_ids\n",
    "                    FROM `{project}.{schema}.anvil_file` t0\n",
    "                      INNER JOIN `{project}.{schema}.anvil_file` t1\n",
    "                      ON REPLACE(t0.file_name, t0.file_format, '') = REPLACE(t1.file_name, t1.file_format, '')\n",
    "                      AND t0.file_name <> t1.file_name \n",
    "                      AND ((t1.file_format = '.cram' AND t0.file_format = '.crai') OR (t1.file_format = '.bam' AND t0.file_format = '.bai'))\n",
    "                    WHERE t0.file_id IS NOT NULL\n",
    "                  )\n",
    "                )\n",
    "                SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.generated_file_id||t0.used_file_id) AS activity_id, \n",
    "                CASE WHEN generated_file_format IN ('.crai', '.bai', '.tbi', '.csi') THEN 'Indexing'\n",
    "                      WHEN generated_file_format = '.md5' THEN 'Checksum'\n",
    "                      ELSE 'Unknown'\n",
    "                END AS activity_type,\n",
    "                [t0.used_file_id] AS used_file_id,\n",
    "                [t0.generated_file_id] AS generated_file_id,\n",
    "                t0.source_datarepo_row_ids\n",
    "                FROM potential_activity_records t0\n",
    "                  LEFT JOIN existing_activity_records t1\n",
    "                  ON t0.used_file_id = t1.used_file_id AND t0.generated_file_id = t1.generated_file_id\n",
    "                WHERE t1.activity_id IS NULL\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "    else:\n",
    "        infer_file_rel_query = \"\"\"WITH existing_activity_records\n",
    "                AS\n",
    "                (\n",
    "                  SELECT activity_id, used_file_id, generated_file_id\n",
    "                  FROM `{project}.{schema}.anvil_activity`,\n",
    "                  UNNEST(used_file_id) AS used_file_id,\n",
    "                  UNNEST(generated_file_id) AS generated_file_id \n",
    "                ), \n",
    "                potential_activity_records\n",
    "                AS\n",
    "                (\n",
    "                  SELECT *, ROW_NUMBER() OVER (PARTITION BY used_file_id, generated_file_id ORDER BY 1) AS RN\n",
    "                  FROM \n",
    "                  (\n",
    "                    SELECT t1.file_ref AS used_file_id, t1.full_extension AS used_file_format, t0.file_ref AS generated_file_id, t0.full_extension AS generated_file_format, \n",
    "                    [t0.datarepo_row_id, t1.datarepo_row_id] AS source_datarepo_row_ids\n",
    "                    FROM `{project}.{schema}.file_inventory` t0\n",
    "                      INNER JOIN `{project}.{schema}.file_inventory` t1\n",
    "                      ON REPLACE(t0.path, t0.full_extension, '') = t1.path\n",
    "                      AND t0.path <> t1.path \n",
    "                    WHERE t0.file_ref IS NOT NULL\n",
    "                    UNION ALL\n",
    "                    SELECT t1.file_ref AS used_file_id, t1.full_extension AS used_file_format, t0.file_ref AS generated_file_id, t0.full_extension AS generated_file_format, \n",
    "                    [t0.datarepo_row_id, t1.datarepo_row_id] AS source_datarepo_row_ids\n",
    "                    FROM `{project}.{schema}.file_inventory` t0\n",
    "                      INNER JOIN `{project}.{schema}.file_inventory` t1\n",
    "                      ON REPLACE(t0.path, t0.full_extension, '') = REPLACE(t1.path, t1.full_extension, '')\n",
    "                      AND t0.path <> t1.path \n",
    "                      AND ((t1.full_extension = '.cram' AND t0.full_extension = '.crai') OR (t1.full_extension = '.bam' AND t0.full_extension = '.bai'))\n",
    "                    WHERE t0.file_ref IS NOT NULL\n",
    "                  )\n",
    "                )\n",
    "                SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.generated_file_id||t0.used_file_id) AS activity_id, \n",
    "                CASE WHEN generated_file_format IN ('.crai', '.bai', '.tbi', '.csi') THEN 'Indexing'\n",
    "                      WHEN generated_file_format = '.md5' THEN 'Checksum'\n",
    "                      ELSE 'Unknown'\n",
    "                END AS activity_type,\n",
    "                [t0.used_file_id] AS used_file_id,\n",
    "                [t0.generated_file_id] AS generated_file_id,\n",
    "                t0.source_datarepo_row_ids\n",
    "                FROM potential_activity_records t0\n",
    "                  LEFT JOIN existing_activity_records t1\n",
    "                  ON t0.used_file_id = t1.used_file_id AND t0.generated_file_id = t1.generated_file_id\n",
    "                WHERE t1.activity_id IS NULL\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "        \n",
    "    # Construct records to ingest\n",
    "    logging.info(\"Attempting to infer and ingest file relationships...\")\n",
    "    try:\n",
    "        df = client.query(infer_file_rel_query).result().to_dataframe()\n",
    "        records_dict = df.to_dict(orient=\"records\")\n",
    "        final_records_dict = []\n",
    "        for record in records_dict:\n",
    "            inner_dict = {}\n",
    "            for key, val in record.items():\n",
    "                if isinstance(val, np.ndarray):\n",
    "                    inner_dict[key] = val.tolist()\n",
    "                else:\n",
    "                    inner_dict[key] = val\n",
    "            final_records_dict.append(inner_dict)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "        return \"Error during query execution: {}\".format(str(e)), \"Error\"\n",
    "    \n",
    "    # Ingest records\n",
    "    if final_records_dict:\n",
    "        ingest_str, ingest_status = ingest_records(params, dataset_id, \"anvil_activity\", final_records_dict)\n",
    "    else:\n",
    "        logging.info(\"No new file relationships found, no records to ingest.\")\n",
    "        return \"No new file relationships found, no records to ingest.\", \"Success\"     \n",
    "    \n",
    "    # Validate ingested file relationships\n",
    "    logging.info(\"Validating ingested file relationships...\")\n",
    "    client = bigquery.Client()\n",
    "    validation_query = \"\"\"\n",
    "        WITH activity_flattened AS\n",
    "        (\n",
    "          SELECT DISTINCT generated_file, activity_type, used_file\n",
    "          FROM `{project}.{schema}.anvil_activity`\n",
    "            CROSS JOIN UNNEST(used_file_id) AS used_file\n",
    "            CROSS JOIN UNNEST(generated_file_id) AS generated_file\n",
    "          WHERE ARRAY_LENGTH(used_biosample_id) = 0\n",
    "        ),\n",
    "        activity_agg AS\n",
    "        (\n",
    "          SELECT generated_file, activity_type, COUNT(DISTINCT used_file)\n",
    "          FROM activity_flattened\n",
    "          GROUP BY generated_file, activity_type\n",
    "          HAVING COUNT(DISTINCT used_file) > 1\n",
    "        )\n",
    "        SELECT *\n",
    "        FROM \n",
    "        (\n",
    "          SELECT 'Files generated from multiple file activities (Activity Type - All)' AS metric, COUNT(DISTINCT generated_file) AS result \n",
    "          FROM activity_agg\n",
    "          UNION ALL\n",
    "          SELECT 'Files generated from multiple file activities (Activity Type - ' || activity_type || ')' AS metric, COUNT(DISTINCT generated_file) AS result \n",
    "          FROM activity_agg\n",
    "          GROUP BY activity_type\n",
    "        )\n",
    "        ORDER BY metric\n",
    "        \"\"\".format(project = bq_project, schema = bq_schema)\n",
    "    try:\n",
    "        df = client.query(validation_query).result().to_dataframe()\n",
    "        if df.empty or (len(df) == 1 and df[\"result\"].values[0] == 0):\n",
    "            logging.info(\"No failures reported in validation of ingested file relationships.\")\n",
    "        else:\n",
    "            records_json = json.loads(df.to_json(orient='records'))\n",
    "            total_file_count = 0\n",
    "            index_file_count = 0\n",
    "            checksum_file_count = 0\n",
    "            unknown_file_count = 0\n",
    "            for record in records_json:\n",
    "                if record[\"metric\"] == \"Files generated from multiple file activities (Activity Type - All)\":\n",
    "                    total_file_count = record[\"result\"]\n",
    "                elif record[\"metric\"] == \"Files generated from multiple file activities (Activity Type - Indexing)\":\n",
    "                    index_file_count = record[\"result\"]\n",
    "                elif record[\"metric\"] == \"Files generated from multiple file activities (Activity Type - Checksum)\":\n",
    "                    checksum_file_count = record[\"result\"]\n",
    "                else:\n",
    "                    unknown_file_count = record[\"result\"]\n",
    "            err_msg = f\"Errors found when validating ingested file relationships (files generated from multiple file activities). All: {str(total_file_count)} Indexing Activities: {str(index_file_count)} Checksum Activities: {str(checksum_file_count)} Unknown Activities: {str(unknown_file_count)}\"\n",
    "            logging.error(err_msg)\n",
    "            return err_msg, \"Error\"   \n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "        return \"Error during query execution: {}\".format(str(e)), \"Error\" \n",
    "    \n",
    "    # Return success message if no failures recorded\n",
    "    logging.info(\"File relationships ingested and validated successfully.\")\n",
    "    return ingest_str, ingest_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "# dataset_id = \"8fbfea50-6a71-4b19-98e9-f95e3a8594c7\"\n",
    "# output, status = infer_file_relationships(params, dataset_id)\n",
    "# print(status)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
