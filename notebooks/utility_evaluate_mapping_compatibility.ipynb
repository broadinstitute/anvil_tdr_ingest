{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print(\"Version 1.0.0: 09/15/2022 2:06pm - Nate Calvanese - First version created\")\n",
    "#print(\"Version 1.0.1: 09/16/2022 3:10pm - Nate Calvanese - Shifted from transform to mapping compatibility\")\n",
    "#print(\"Version 1.0.2: 10/14/2022 7:40pm - Nate Calvanese - Added compatibility evaluation and support for multiple mapping specs\")\n",
    "#print(\"Version 1.0.3: 10/18/2022 1:33pm - Nate Calvanese - Encoded column names to match mapping specifications\")\n",
    "print(\"Version 1.0.4: 10/20/2022 11:50am - Nate Calvanese - Added ability to pull schemas for workspaces missing from workspace_schemas.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables\n",
    "# imports\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from firecloud import api as fapi\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_mapping_query as bmq\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "print(f\"workspace name = {ws_name}\")\n",
    "print(f\"workspace project = {ws_project}\")\n",
    "print(f\"workspace bucket = {ws_bucket}\")\n",
    "print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs:\n",
    "\n",
    "# Mapping specification to evaluate\n",
    "mapping_target = \"anvil\"\n",
    "mapping_target_spec_list = [\"cmg_ext_2\", \"gtex_ext_2\"]\n",
    "\n",
    "# Any known data_file_refs, so file ref fields can be properly evaluated\n",
    "data_file_refs = {   \n",
    "    \"sequencing\": [{\n",
    "        \"column\": \"sequencing_id\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"sequencing_id_file_id\"\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the workspace_schemas.csv file into dataframe, clean column names, and convert to dict\n",
    "ws_schema_filepath = ws_bucket + '/ingest_pipeline/resources/mapping_compatibility/workspace_schemas.csv'\n",
    "df = pd.read_csv(ws_schema_filepath)\n",
    "#int_dict = df[df[\"workspace_name\"] == 'anvil_ccdg_asc_ndd_daly_talkowski_barbosa_asd_exome'].to_dict(orient=\"records\")\n",
    "int_dict = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Derived pipeline tables: Should be added to every source workspace schema\n",
    "file_inv_dict = {\"name\": \"file_inventory\",\n",
    "                 \"columns\": [\n",
    "                     {\"name\": \"file_id\"},\n",
    "                     {\"name\": \"name\"},\n",
    "                     {\"name\": \"path\"},\n",
    "                     {\"name\": \"uri\"},\n",
    "                     {\"name\": \"content_type\"},\n",
    "                     {\"name\": \"full_extension\"},\n",
    "                     {\"name\": \"size_in_bytes\"},\n",
    "                     {\"name\": \"crc32c\"},\n",
    "                     {\"name\": \"md5_hash\"},\n",
    "                     {\"name\": \"file_ref\"}\n",
    "                 ]}\n",
    "ws_attr_dict = {\"name\": \"workspace_attributes\",\n",
    "                 \"columns\": [\n",
    "                     {\"name\": \"attribute\"},\n",
    "                     {\"name\": \"value\"}\n",
    "                 ]}\n",
    "\n",
    "# Re-organize workspace dict into TDR-like schema to enable use of existing functions\n",
    "workspace_dict = {}\n",
    "for idx, record in enumerate(int_dict):\n",
    "    if idx == 0:\n",
    "        current_workspace = record[\"workspace_name\"]\n",
    "        previous_workspace = record[\"workspace_name\"]\n",
    "        current_table = record[\"table_name\"]\n",
    "        previous_table = record[\"table_name\"]\n",
    "        current_column = utils.encode_name(record[\"column_name\"])\n",
    "        table_list = []\n",
    "        table_dict = {}\n",
    "        table_dict[\"name\"] = current_table\n",
    "        table_dict[\"columns\"] = []\n",
    "        column_set = set()\n",
    "        column_dict = {}\n",
    "        entity_column = record[\"table_name\"] + \"_id\"\n",
    "        column_dict[\"name\"] = entity_column\n",
    "        column_set.add(entity_column)\n",
    "        table_dict[\"columns\"].append(column_dict)\n",
    "        column_dict = {}\n",
    "        column_dict[\"name\"] = current_column\n",
    "        column_set.add(current_column)\n",
    "        table_dict[\"columns\"].append(column_dict)\n",
    "    else:\n",
    "        current_workspace = record[\"workspace_name\"]\n",
    "        current_table = record[\"table_name\"]\n",
    "        current_column = utils.encode_name(record[\"column_name\"])\n",
    "        if current_workspace != previous_workspace:\n",
    "            table_list.append(table_dict)\n",
    "            table_list.append(file_inv_dict)\n",
    "            table_list.append(ws_attr_dict)\n",
    "            workspace_dict[previous_workspace] = {}\n",
    "            workspace_dict[previous_workspace][\"tables\"] = table_list\n",
    "            table_list = []\n",
    "            table_dict = {}\n",
    "            column_set = set()\n",
    "            table_dict[\"name\"] = current_table\n",
    "            table_dict[\"columns\"] = []\n",
    "            column_dict = {}\n",
    "            entity_column = record[\"table_name\"] + \"_id\"\n",
    "            column_dict[\"name\"] = entity_column\n",
    "            column_set.add(entity_column)\n",
    "            table_dict[\"columns\"].append(column_dict)\n",
    "            column_dict = {}\n",
    "            column_dict[\"name\"] = current_column\n",
    "            column_set.add(current_column)\n",
    "            table_dict[\"columns\"].append(column_dict)\n",
    "        else:\n",
    "            if current_table != previous_table:\n",
    "                for key, value in data_file_refs.items():\n",
    "                    if key.split(\".\")[0] == previous_table:\n",
    "                        for entry in value:\n",
    "                            if utils.encode_name(entry[\"column\"]) in column_set:\n",
    "                                if entry[\"create_new_field\"] == True:\n",
    "                                    column_dict = {}\n",
    "                                    column_dict[\"name\"] = utils.encode_name(entry[\"new_field_name\"])\n",
    "                                    column_set.add(utils.encode_name(entry[\"new_field_name\"]))\n",
    "                                    table_dict[\"columns\"].append(column_dict)\n",
    "                table_list.append(table_dict)\n",
    "                table_dict = {}\n",
    "                column_set = set()\n",
    "                table_dict[\"name\"] = current_table\n",
    "                table_dict[\"columns\"] = []\n",
    "                column_dict = {}\n",
    "                entity_column = record[\"table_name\"] + \"_id\"\n",
    "                column_dict[\"name\"] = entity_column\n",
    "                column_set.add(entity_column)\n",
    "                table_dict[\"columns\"].append(column_dict)\n",
    "                column_dict = {}\n",
    "                column_dict[\"name\"] = current_column\n",
    "                column_set.add(current_column)\n",
    "                table_dict[\"columns\"].append(column_dict)\n",
    "                previous_table = current_table\n",
    "            else:\n",
    "                column_dict = {}\n",
    "                column_dict[\"name\"] = current_column\n",
    "                column_set.add(current_column)\n",
    "                table_dict[\"columns\"].append(column_dict)\n",
    "        previous_workspace = current_workspace\n",
    "        previous_table = current_table\n",
    "    if idx == len(int_dict)-1:\n",
    "        for key, value in data_file_refs.items():\n",
    "            if key.split(\".\")[0] == previous_table:\n",
    "                for entry in value:\n",
    "                    if utils.encode_name(entry[\"column\"]) in column_set:\n",
    "                        if entry[\"create_new_field\"] == True:\n",
    "                            column_dict = {}\n",
    "                            column_dict[\"name\"] = utils.encode_name(entry[\"new_field_name\"])\n",
    "                            column_set.add(utils.encode_name(entry[\"new_field_name\"]))\n",
    "                            table_dict[\"columns\"].append(column_dict)\n",
    "        table_list.append(table_dict)\n",
    "        table_list.append(file_inv_dict)\n",
    "        table_list.append(ws_attr_dict)\n",
    "        workspace_dict[previous_workspace] = {}\n",
    "        workspace_dict[previous_workspace][\"tables\"] = table_list\n",
    "#print(json.dumps(workspace_dict))\n",
    "                              \n",
    "# Read in target schema\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "target_schema_dict = {}\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "    target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "#print(json.dumps(target_schema_dict))\n",
    "\n",
    "# Loop through mapping specifications for evaluation\n",
    "spec_dict = {}\n",
    "for mapping_target_spec in mapping_target_spec_list:\n",
    "\n",
    "\n",
    "    # Read in mapping specification\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "    mapping_spec = {}\n",
    "    try:\n",
    "        blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "        blob_string = blob.download_as_text(client=None)\n",
    "        blob_string = blob_string.replace(\"$DATASET_NAME\", \"Dataset\").replace(\"$PROJECT_NAME\", \"Project\") #UPDATE WITH REAL PARAMETERS\n",
    "        mapping_spec = json.loads(blob_string)\n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "    #print(json.dumps(mapping_spec))\n",
    "\n",
    "    # Loop through workspaces and evaluate mapping compatibility \n",
    "    detail_dict = {}\n",
    "    for ws_key in workspace_dict:\n",
    "        entity_list = []\n",
    "        for entity in mapping_spec[\"entities\"]:\n",
    "            entity_dict = {}\n",
    "            record_set_list = []\n",
    "            target_table = {}\n",
    "            for table in target_schema_dict[\"tables\"]:\n",
    "                if table[\"name\"] == entity[\"name\"]:\n",
    "                    target_table = table\n",
    "            if target_table:\n",
    "                for record_set in entity[\"record_sets\"]:\n",
    "                    record_set_dict = {}\n",
    "                    record_set_dict[\"record_set\"] = record_set[\"name\"]\n",
    "                    if bmq.validate_record_set(record_set, workspace_dict[ws_key], target_table):\n",
    "                        record_set_dict[\"can_run\"] = True\n",
    "                    else:\n",
    "                        record_set_dict[\"can_run\"] = False\n",
    "                    record_set_dict[\"total_attrs\"] = len(record_set[\"attributes\"])\n",
    "                    valid_attr_count = 0\n",
    "                    invalid_attrs_set = set()\n",
    "                    for attribute in record_set[\"attributes\"]:\n",
    "                        if bmq.validate_attribute(attribute, workspace_dict[ws_key], target_table):\n",
    "                            valid_attr_count += 1\n",
    "                        else:\n",
    "                            invalid_attrs_set.add(target_table[\"name\"] + \".\" + attribute[\"name\"])\n",
    "                    record_set_dict[\"valid_attrs\"] = valid_attr_count\n",
    "                    record_set_dict[\"invalid_attrs_set\"] = list(invalid_attrs_set)\n",
    "                    record_set_list.append(record_set_dict)\n",
    "                entity_dict[entity[\"name\"]] = record_set_list\n",
    "                entity_list.append(entity_dict)\n",
    "        detail_dict[ws_key] = {}\n",
    "        detail_dict[ws_key][\"entities\"] = entity_list\n",
    "    #print(json.dumps(detail_dict))\n",
    "    spec_dict[mapping_target_spec] = {}\n",
    "    spec_dict[mapping_target_spec][\"detail_dict\"] = detail_dict\n",
    "\n",
    "    # Collect target tables and columns not in mapping specification\n",
    "    missing_table_set = set()\n",
    "    missing_column_set = set()\n",
    "    entity_table_list = [val[\"name\"] for val in mapping_spec[\"entities\"]]\n",
    "    entity_column_list = []\n",
    "    for entity in mapping_spec[\"entities\"]:\n",
    "        entity_name = entity[\"name\"]\n",
    "        for record_set in entity[\"record_sets\"]:\n",
    "            for attribute in record_set[\"attributes\"]:\n",
    "                attribute_name = entity_name + \".\" + attribute[\"name\"]\n",
    "                entity_column_list.append(attribute_name)\n",
    "    for table_entry in target_schema_dict[\"tables\"]:\n",
    "        if table_entry[\"name\"] not in entity_table_list:\n",
    "            missing_table_set.add(table_entry[\"name\"])\n",
    "        else:\n",
    "            for column_entry in table_entry[\"columns\"]:\n",
    "                column_name = table_entry[\"name\"] + \".\" + column_entry[\"name\"]\n",
    "                if column_name not in entity_column_list:\n",
    "                    missing_column_set.add(column_name)\n",
    "    spec_dict[mapping_target_spec][\"missing_table_set\"] = missing_table_set\n",
    "    spec_dict[mapping_target_spec][\"missing_column_set\"] = missing_column_set\n",
    "\n",
    "# Summarize mapping compatibility\n",
    "results_list = []\n",
    "for spec_key, spec_val in spec_dict.items():\n",
    "    for ws_key, value in spec_val[\"detail_dict\"].items():\n",
    "        workspace_results_list = []\n",
    "        can_run_set = set()\n",
    "        can_run_fully_set = set()\n",
    "        sum_valid_attrs = 0\n",
    "        sum_total_attrs = 0\n",
    "        invalid_attrs_list = []\n",
    "        for entities in value[\"entities\"]:\n",
    "            max_valid_attrs = 0\n",
    "            max_total_attrs = 0\n",
    "            for key, val in entities.items():\n",
    "                invalid_attrs_set = set(val[0][\"invalid_attrs_set\"])\n",
    "                for record_sets in val:\n",
    "                    if record_sets[\"can_run\"] == True:\n",
    "                        can_run_set.add(key)\n",
    "                    if record_sets[\"total_attrs\"] == record_sets[\"valid_attrs\"]:\n",
    "                        can_run_fully_set.add(key)\n",
    "                    if record_sets[\"valid_attrs\"] > max_valid_attrs:\n",
    "                        max_valid_attrs = record_sets[\"valid_attrs\"]\n",
    "                    if record_sets[\"total_attrs\"] > max_total_attrs:\n",
    "                        max_total_attrs = record_sets[\"total_attrs\"]\n",
    "                    invalid_attrs_set = invalid_attrs_set.union(set(record_sets[\"invalid_attrs_set\"]))\n",
    "                sum_valid_attrs += max_valid_attrs\n",
    "                sum_total_attrs += max_total_attrs\n",
    "                invalid_attrs_list.extend(list(invalid_attrs_set))\n",
    "        percent_valid_attrs = round(sum_valid_attrs/sum_total_attrs,2)\n",
    "        if \"donor\" in can_run_set and (\"biosample\" in can_run_set and \"biosample.donor_id\" not in invalid_attrs_list) and ((\"activity\" in can_run_set and \"activity.used_biosample_id\" not in invalid_attrs_list) or (\"sequencingactivity\" in can_run_set and \"sequencingactivity.used_biosample_id\" not in invalid_attrs_list)):\n",
    "            compatible = \"Y\"\n",
    "        else:\n",
    "            compatible = \"N\"\n",
    "        workspace_results_list.append(ws_key)\n",
    "        workspace_results_list.append(spec_key)\n",
    "        workspace_results_list.append(compatible)\n",
    "        workspace_results_list.append(len(can_run_set))\n",
    "        workspace_results_list.append(can_run_set)\n",
    "        workspace_results_list.append(len(can_run_fully_set))\n",
    "        workspace_results_list.append(can_run_fully_set)\n",
    "        workspace_results_list.append(sum_valid_attrs)\n",
    "        workspace_results_list.append(percent_valid_attrs)\n",
    "        workspace_results_list.append(invalid_attrs_list)\n",
    "        results_list.append(workspace_results_list)\n",
    "\n",
    "results_df = pd.DataFrame(results_list, columns = ['workspace', 'mapping_spec', 'compatible', 'can_run_count', 'can_run_entities', 'can_fully_run_count', 'can_fully_run_entities', 'cnt_valid_attrs', 'perc_valid_attrs', 'invalid_attr_list'])\n",
    "\n",
    "# Sort results dataframe and write out to tsv\n",
    "destination_dir = \"ingest_pipeline/resources/mapping_compatibility/output\"\n",
    "sorted_df = results_df.sort_values(['mapping_spec', 'compatible', 'can_run_count', 'perc_valid_attrs'], ascending=[True, False, False, False], ignore_index=True)\n",
    "output_file = \"mapping_compatibility_results.tsv\"\n",
    "sorted_df.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "!gsutil cp $output_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "!rm $output_file\n",
    "\n",
    "# Aggregate compatible mapping specs and write out to tsv\n",
    "agg_df = results_df[results_df[\"compatible\"] == \"Y\"].groupby('workspace').agg(compatible_mapping_specs=('mapping_spec', 'unique')).reset_index()\n",
    "output_file = \"mapping_compatibility_aggregation.tsv\"\n",
    "agg_df.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "!gsutil cp $output_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "!rm $output_file\n",
    "\n",
    "# Output results to the user\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mapping Compatibility Results for Mapping Target Specifications:\")\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Target tables not included in specifications:\")\n",
    "for spec_key, spec_val in spec_dict.items():\n",
    "    print(\"\\t\" + spec_key + \": \" + \", \".join(sorted(list(spec_val[\"missing_table_set\"]))))\n",
    "print(\"\\n\")\n",
    "print(\"Target fields not included in specification:\")\n",
    "for spec_key, spec_val in spec_dict.items():\n",
    "    print(\"\\t\" + spec_key + \": \" + \", \".join(sorted(list(spec_val[\"missing_column_set\"]))))\n",
    "print(\"\\n\")\n",
    "print(\"Workspace evaluation against specifications:\")\n",
    "display(sorted_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print detailed results for specific workspace\n",
    "print(json.dumps(detail_dict[\"anvil_cmg_uwash_ds-hfa\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print workspace dict for specific workspace\n",
    "print(json.dumps(workspace_dict[\"anvil_gtex_bcm_gru_corsivs\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for fileref fields\n",
    "ws_project = \"anvil-datastorage\"\n",
    "data_file_refs = {}\n",
    "ws_name_list = [\n",
    "\"1000G-high-coverage-2019\"\n",
    "]\n",
    "file_ref_set = set()\n",
    "for ws_name in ws_name_list:\n",
    "    file_ref_list = []\n",
    "    try:\n",
    "        ws_attributes = utils.get_workspace_attributes(ws_project, ws_name)\n",
    "        workspace_bucket = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\" \n",
    "        file_ref_list, data_file_refs, remote_list = utils.find_and_add_fileref_fields(ws_project, ws_name, workspace_bucket, data_file_refs)\n",
    "        for entry in file_ref_list:\n",
    "            file_ref_set.add(entry)\n",
    "    except:\n",
    "        continue\n",
    "print(sorted(list(file_ref_set)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collecting the schema for a specific workspace\n",
    "from firecloud import api as fapi\n",
    "ws_project = \"anvil-datastorage\"\n",
    "ws_name_list = [\n",
    "    \"AnVIL_NIMH_CIRM_FCDI_ConvergentNeuro_McCarroll_Eggan_GRU_Arrays\", \n",
    "    \"AnVIL_CSER_KidsCanSeq_GRU\"\n",
    "]\n",
    "schema_fields = []\n",
    "\n",
    "# Loop through workspaces\n",
    "for ws_name in ws_name_list:\n",
    "\n",
    "    # Collect and record all entity types in workspace\n",
    "    response_etypes = fapi.list_entity_types(ws_project, ws_name)\n",
    "    dict_all_etypes = json.loads(response_etypes.text)\n",
    "    etypes_list = [key for key in dict_all_etypes.keys()]\n",
    "\n",
    "    # Loop through entity types and parse result to build schema\n",
    "    if etypes_list:\n",
    "        for etype in etypes_list:\n",
    "            column_set = set()\n",
    "            entities_resp = fapi.get_entities(ws_project, ws_name, etype)\n",
    "            entities_dict = json.loads(entities_resp.text)\n",
    "            for i in range(0, 10):\n",
    "                column_set.add(entities_dict[i][\"entityType\"] + \"_id\")\n",
    "                for attr_key in entities_dict[i][\"attributes\"]:\n",
    "                    column_set.add(attr_key)\n",
    "            for column in column_set:\n",
    "                column_entry = []\n",
    "                column_entry = [ws_name.lower(), etype.lower(), column.lower()]\n",
    "                schema_fields.append(column_entry)\n",
    "\n",
    "# Convert to dataframe and display\n",
    "df = pd.DataFrame(schema_fields, columns = [\"workspace_name\", \"table_name\", \"column_name\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
