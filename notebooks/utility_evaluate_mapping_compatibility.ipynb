{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1.0.1: 09/16/2022 3:10pm - Nate Calvanese - Shifted from transform to mapping compatibility\n"
     ]
    }
   ],
   "source": [
    "# Version History\n",
    "#print(\"Version 1.0.0: 09/15/2022 2:06pm - Nate Calvanese - First version created\")\n",
    "print(\"Version 1.0.1: 09/16/2022 3:10pm - Nate Calvanese - Shifted from transform to mapping compatibility\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ingest_pipeline_utilities.ipynb\n",
      "Version 1.0.7: 09/26/2022 4:35pm - Nate Calvanese - Added schema reconciliation functionality to TDR dataset patching\n",
      "importing Jupyter notebook from source_files_creation.ipynb\n",
      "Version 1.0.3: 09/23/2022 11:53am - Nate Calvanese - Appended source workspace to file names\n",
      "importing Jupyter notebook from build_file_inventory.ipynb\n",
      "Version 1.0.3: 10/03/2022 12:15pm - Nate Calvanese - Flattened target file path to help with ingest performance\n",
      "importing Jupyter notebook from process_table_data.ipynb\n",
      "Version: 1.0.1: 9/16/2022 10:57am - Nate Calvanese - Fixed bug in file_inventory table creation\n",
      "importing Jupyter notebook from build_mapping_query.ipynb\n",
      "Version 1.0.5: 09/21/2022 11:58am - Nate Calvanese - Made multi-column array agg return array with distinct values\n",
      "importing Jupyter notebook from output_data_validation.ipynb\n",
      "Version 2.0.2: 09/14/2022 10:23am -- Made output directory and validation schema more configurable\n",
      "workspace name = anvil_workspace_ingest_resources\n",
      "workspace project = dsp-data-ingest\n",
      "workspace bucket = gs://fc-bc3dad8b-b3f9-43c7-b100-e8ed59b27f43\n",
      "workspace bucket name = fc-bc3dad8b-b3f9-43c7-b100-e8ed59b27f43\n"
     ]
    }
   ],
   "source": [
    "## Imports and environment variables\n",
    "# imports\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_mapping_query as bmq\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "print(f\"workspace name = {ws_name}\")\n",
    "print(f\"workspace project = {ws_project}\")\n",
    "print(f\"workspace bucket = {ws_bucket}\")\n",
    "print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs:\n",
    "\n",
    "# Mapping specification to evaluate\n",
    "mapping_target = \"anvil\"\n",
    "mapping_target_spec = \"cmg_ext_1\"\n",
    "\n",
    "# Any known data_file_refs, so file ref fields can be properly evaluated\n",
    "data_file_refs = {   \n",
    "    \"ws_sequencing.tsv\": [{\n",
    "        \"column\": \"sequencing_id\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"sequencing_id_file_id\"\n",
    "    }, {\n",
    "        \"column\": \"seq_filename\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"seq_filename_file_id\"\n",
    "    }, {\n",
    "        \"column\": \"capture_region_bed_file\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"capture_region_bed_file_file_id\"\n",
    "    }, {\n",
    "        \"column\": \"file_id\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"file_id_file_id\"\n",
    "    }, {\n",
    "        \"column\": \"cram\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"cram_file_id\"\n",
    "    }], \n",
    "    \"ws_sample.tsv\": [{\n",
    "        \"column\": \"crai\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"crai_file_id\"\n",
    "    }, {\n",
    "        \"column\": \"cram\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"cram_file_id\"\n",
    "    },{\n",
    "        \"column\": \"bai\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"bai_file_id\"\n",
    "    },{\n",
    "        \"column\": \"bam\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"bam_file_id\"\n",
    "    },{\n",
    "        \"column\": \"csi\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"csi_file_id\"\n",
    "    },{\n",
    "        \"column\": \"tbi\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"tbi_file_id\"\n",
    "    },{\n",
    "        \"column\": \"vcf\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"vcf_file_id\"\n",
    "    },{\n",
    "        \"column\": \"cram_id\",\n",
    "        \"method\": \"file_path_match\",\n",
    "        \"match_multiple_files\": True, \n",
    "        \"match_regex\": None,\n",
    "        \"create_new_field\": True,\n",
    "        \"new_field_name\": \"cram_id_file_id\"\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------\n",
      "Mapping Compatibility Results for Mapping Target Specification: anvil/cmg_ext_1\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Target tables not included in specification:\n",
      "\tassayactivity, antibody, variantcallingactivity, alignmentactivity\n",
      "\n",
      "\n",
      "Target fields not included in specification:\n",
      "\tdonor.source_datarepo_row_ids, diagnosis.diagnosis_age_upper_bound, biosample.diagnosis_id, sequencingactivity.source_datarepo_row_ids, diagnosis.diagnosis_age_lower_bound, biosample.apriori_cell_type, project.funded_by, file.reference_assembly, biosample.donor_age_at_collection_lower_bound, activity.source_datarepo_row_ids, diagnosis.source_datarepo_row_ids, project.principal_investigator, donor.genetic_ancestry, sequencingactivity.assay_type, diagnosis.phenopacket, biosample.donor_age_at_collection_unit, biosample.donor_age_at_collection_upper_bound, dataset.principal_investigator, dataset.source_datarepo_row_ids, diagnosis.diagnosis_age_unit, file.data_modality, dataset.data_modality, donor.organism_type, project.source_datarepo_row_ids, biosample.source_datarepo_row_ids, sequencingactivity.data_modality, file.source_datarepo_row_ids, biosample.disease, activity.used_file_id\n",
      "\n",
      "\n",
      "Workspace evaluation against specification:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>workspace</th>\n",
       "      <th>can_run_count</th>\n",
       "      <th>can_run_entities</th>\n",
       "      <th>can_fully_run_count</th>\n",
       "      <th>can_fully_run_entities</th>\n",
       "      <th>cnt_valid_attrs</th>\n",
       "      <th>perc_valid_attrs</th>\n",
       "      <th>invalid_attr_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anvil_cmg_uwash_gru</td>\n",
       "      <td>7</td>\n",
       "      <td>{sequencingactivity, diagnosis, dataset, project, biosample, file, donor}</td>\n",
       "      <td>6</td>\n",
       "      <td>{sequencingactivity, dataset, project, biosample, file, donor}</td>\n",
       "      <td>35</td>\n",
       "      <td>0.9</td>\n",
       "      <td>[diagnosis.onset_age_lower_bound, diagnosis.onset_age_upper_bound, diagnosis.onset_age_unit, activity.generated_file_id]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       workspace        can_run_count                              can_run_entities                               can_fully_run_count                      can_fully_run_entities                      cnt_valid_attrs  perc_valid_attrs                                                     invalid_attr_list                                                    \n",
       "0  anvil_cmg_uwash_gru        7        {sequencingactivity, diagnosis, dataset, project, biosample, file, donor}           6           {sequencingactivity, dataset, project, biosample, file, donor}        35                0.9        [diagnosis.onset_age_lower_bound, diagnosis.onset_age_upper_bound, diagnosis.onset_age_unit, activity.generated_file_id]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the workspace_schemas.csv file into dataframe, clean column names, and convert to dict\n",
    "ws_schema_filepath = ws_bucket + '/ingest_pipeline/resources/mapping_compatibility/workspace_schemas.csv'\n",
    "df = pd.read_csv(ws_schema_filepath)\n",
    "int_dict = df[df[\"workspace_name\"] == 'anvil_cmg_uwash_gru'].to_dict(orient=\"records\")\n",
    "#int_dict = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Derived pipeline tables: Should be added to every source workspace schema\n",
    "file_inv_dict = {\"name\": \"ws_file_inventory\",\n",
    "                 \"columns\": [\n",
    "                     {\"name\": \"file_id\"},\n",
    "                     {\"name\": \"name\"},\n",
    "                     {\"name\": \"path\"},\n",
    "                     {\"name\": \"uri\"},\n",
    "                     {\"name\": \"content_type\"},\n",
    "                     {\"name\": \"full_extension\"},\n",
    "                     {\"name\": \"size_in_bytes\"},\n",
    "                     {\"name\": \"crc32c\"},\n",
    "                     {\"name\": \"md5_hash\"},\n",
    "                     {\"name\": \"file_ref\"}\n",
    "                 ]}\n",
    "ws_attr_dict = {\"name\": \"ws_workspace_attributes\",\n",
    "                 \"columns\": [\n",
    "                     {\"name\": \"attribute\"},\n",
    "                     {\"name\": \"value\"}\n",
    "                 ]}\n",
    "\n",
    "# Re-organize workspace dict into TDR-like schema to enable use of existing functions\n",
    "workspace_dict = {}\n",
    "for idx, record in enumerate(int_dict):\n",
    "    if idx == 0:\n",
    "        current_workspace = record[\"workspace_name\"]\n",
    "        previous_workspace = record[\"workspace_name\"]\n",
    "        current_table = \"ws_\" + record[\"table_name\"]\n",
    "        previous_table = \"ws_\" + record[\"table_name\"]\n",
    "        current_column = record[\"column_name\"]\n",
    "        table_list = []\n",
    "        table_dict = {}\n",
    "        table_dict[\"name\"] = current_table\n",
    "        table_dict[\"columns\"] = []\n",
    "        column_set = set()\n",
    "        column_dict = {}\n",
    "        entity_column = record[\"table_name\"] + \"_id\"\n",
    "        column_dict[\"name\"] = entity_column\n",
    "        column_set.add(entity_column)\n",
    "        table_dict[\"columns\"].append(column_dict)\n",
    "        column_dict = {}\n",
    "        column_dict[\"name\"] = current_column\n",
    "        column_set.add(current_column)\n",
    "        table_dict[\"columns\"].append(column_dict)\n",
    "    else:\n",
    "        current_workspace = record[\"workspace_name\"]\n",
    "        current_table = \"ws_\" + record[\"table_name\"]\n",
    "        current_column = record[\"column_name\"]\n",
    "        if current_workspace != previous_workspace:\n",
    "            table_list.append(table_dict)\n",
    "            table_list.append(file_inv_dict)\n",
    "            table_list.append(ws_attr_dict)\n",
    "            workspace_dict[previous_workspace] = {}\n",
    "            workspace_dict[previous_workspace][\"tables\"] = table_list\n",
    "            table_list = []\n",
    "            table_dict = {}\n",
    "            column_set = set()\n",
    "            table_dict[\"name\"] = current_table\n",
    "            table_dict[\"columns\"] = []\n",
    "            column_dict = {}\n",
    "            entity_column = record[\"table_name\"] + \"_id\"\n",
    "            column_dict[\"name\"] = entity_column\n",
    "            column_set.add(entity_column)\n",
    "            table_dict[\"columns\"].append(column_dict)\n",
    "            column_dict = {}\n",
    "            column_dict[\"name\"] = current_column\n",
    "            column_set.add(current_column)\n",
    "            table_dict[\"columns\"].append(column_dict)\n",
    "        else:\n",
    "            if current_table != previous_table:\n",
    "                for key, value in data_file_refs.items():\n",
    "                    if key.split(\".\")[0] == previous_table:\n",
    "                        for entry in value:\n",
    "                            if entry[\"column\"] in column_set:\n",
    "                                if entry[\"create_new_field\"] == True:\n",
    "                                    column_dict = {}\n",
    "                                    column_dict[\"name\"] = entry[\"new_field_name\"]\n",
    "                                    column_set.add(entry[\"new_field_name\"])\n",
    "                                    table_dict[\"columns\"].append(column_dict)\n",
    "                table_list.append(table_dict)\n",
    "                table_dict = {}\n",
    "                column_set = set()\n",
    "                table_dict[\"name\"] = current_table\n",
    "                table_dict[\"columns\"] = []\n",
    "                column_dict = {}\n",
    "                entity_column = record[\"table_name\"] + \"_id\"\n",
    "                column_dict[\"name\"] = entity_column\n",
    "                column_set.add(entity_column)\n",
    "                table_dict[\"columns\"].append(column_dict)\n",
    "                column_dict = {}\n",
    "                column_dict[\"name\"] = current_column\n",
    "                column_set.add(current_column)\n",
    "                table_dict[\"columns\"].append(column_dict)\n",
    "                previous_table = current_table\n",
    "            else:\n",
    "                column_dict = {}\n",
    "                column_dict[\"name\"] = current_column\n",
    "                column_set.add(current_column)\n",
    "                table_dict[\"columns\"].append(column_dict)\n",
    "        previous_workspace = current_workspace\n",
    "        previous_table = current_table\n",
    "    if idx == len(int_dict)-1:\n",
    "        for key, value in data_file_refs.items():\n",
    "            if key.split(\".\")[0] == previous_table:\n",
    "                for entry in value:\n",
    "                    if entry[\"column\"] in column_set:\n",
    "                        if entry[\"create_new_field\"] == True:\n",
    "                            column_dict = {}\n",
    "                            column_dict[\"name\"] = entry[\"new_field_name\"]\n",
    "                            column_set.add(entry[\"new_field_name\"])\n",
    "                            table_dict[\"columns\"].append(column_dict)\n",
    "        table_list.append(table_dict)\n",
    "        table_list.append(file_inv_dict)\n",
    "        table_list.append(ws_attr_dict)\n",
    "        workspace_dict[previous_workspace] = {}\n",
    "        workspace_dict[previous_workspace][\"tables\"] = table_list\n",
    "#print(json.dumps(workspace_dict))\n",
    "                              \n",
    "# Read in mapping specification and target schema\n",
    "target_schema_dict = {}\n",
    "mapping_spec = {}\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "    target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "#print(json.dumps(target_schema_dict))\n",
    "try:\n",
    "    blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "    blob_string = blob.download_as_text(client=None)\n",
    "    blob_string = blob_string.replace(\"$DATASET_NAME\", \"Dataset\").replace(\"$PROJECT_NAME\", \"Project\") #UPDATE WITH REAL PARAMETERS\n",
    "    mapping_spec = json.loads(blob_string)\n",
    "except Exception as e:\n",
    "    print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "#print(json.dumps(mapping_spec))\n",
    "\n",
    "# Loop through workspaces and evaluate mapping compatibility \n",
    "detail_dict = {}\n",
    "for ws_key in workspace_dict:\n",
    "    entity_list = []\n",
    "    for entity in mapping_spec[\"entities\"]:\n",
    "        entity_dict = {}\n",
    "        record_set_list = []\n",
    "        target_table = {}\n",
    "        for table in target_schema_dict[\"tables\"]:\n",
    "            if table[\"name\"] == entity[\"name\"]:\n",
    "                target_table = table\n",
    "        if target_table:\n",
    "            for record_set in entity[\"record_sets\"]:\n",
    "                record_set_dict = {}\n",
    "                record_set_dict[\"record_set\"] = record_set[\"name\"]\n",
    "                if bmq.validate_record_set(record_set, workspace_dict[ws_key], target_table):\n",
    "                    record_set_dict[\"can_run\"] = True\n",
    "                else:\n",
    "                    record_set_dict[\"can_run\"] = False\n",
    "                record_set_dict[\"total_attrs\"] = len(record_set[\"attributes\"])\n",
    "                valid_attr_count = 0\n",
    "                invalid_attrs_set = set()\n",
    "                for attribute in record_set[\"attributes\"]:\n",
    "                    if bmq.validate_attribute(attribute, workspace_dict[ws_key], target_table):\n",
    "                        valid_attr_count += 1\n",
    "                    else:\n",
    "                        invalid_attrs_set.add(target_table[\"name\"] + \".\" + attribute[\"name\"])\n",
    "                record_set_dict[\"valid_attrs\"] = valid_attr_count\n",
    "                record_set_dict[\"invalid_attrs_set\"] = list(invalid_attrs_set)\n",
    "                record_set_list.append(record_set_dict)\n",
    "            entity_dict[entity[\"name\"]] = record_set_list\n",
    "            entity_list.append(entity_dict)\n",
    "    detail_dict[ws_key] = {}\n",
    "    detail_dict[ws_key][\"entities\"] = entity_list\n",
    "#print(json.dumps(detail_dict))\n",
    "\n",
    "# Collect target tables and columns not in mapping specification\n",
    "missing_table_set = set()\n",
    "missing_column_set = set()\n",
    "entity_table_list = [val[\"name\"] for val in mapping_spec[\"entities\"]]\n",
    "entity_column_list = []\n",
    "for entity in mapping_spec[\"entities\"]:\n",
    "    entity_name = entity[\"name\"]\n",
    "    for record_set in entity[\"record_sets\"]:\n",
    "        for attribute in record_set[\"attributes\"]:\n",
    "            attribute_name = entity_name + \".\" + attribute[\"name\"]\n",
    "            entity_column_list.append(attribute_name)\n",
    "for table_entry in target_schema_dict[\"tables\"]:\n",
    "    if table_entry[\"name\"] not in entity_table_list:\n",
    "        missing_table_set.add(table_entry[\"name\"])\n",
    "    else:\n",
    "        for column_entry in table_entry[\"columns\"]:\n",
    "            column_name = table_entry[\"name\"] + \".\" + column_entry[\"name\"]\n",
    "            if column_name not in entity_column_list:\n",
    "                missing_column_set.add(column_name)\n",
    "\n",
    "# Summarize mapping compatibility\n",
    "results_list = []\n",
    "for ws_key, value in detail_dict.items():\n",
    "    workspace_results_list = []\n",
    "    can_run_set = set()\n",
    "    can_run_fully_set = set()\n",
    "    sum_valid_attrs = 0\n",
    "    sum_total_attrs = 0\n",
    "    invalid_attrs_list = []\n",
    "    for entities in value[\"entities\"]:\n",
    "        max_valid_attrs = 0\n",
    "        max_total_attrs = 0\n",
    "        for key, val in entities.items():\n",
    "            invalid_attrs_set = set(val[0][\"invalid_attrs_set\"])\n",
    "            for record_sets in val:\n",
    "                if record_sets[\"can_run\"] == True:\n",
    "                    can_run_set.add(key)\n",
    "                if record_sets[\"total_attrs\"] == record_sets[\"valid_attrs\"]:\n",
    "                    can_run_fully_set.add(key)\n",
    "                if record_sets[\"valid_attrs\"] > max_valid_attrs:\n",
    "                    max_valid_attrs = record_sets[\"valid_attrs\"]\n",
    "                if record_sets[\"total_attrs\"] > max_total_attrs:\n",
    "                    max_total_attrs = record_sets[\"total_attrs\"]\n",
    "                invalid_attrs_set = invalid_attrs_set.union(set(record_sets[\"invalid_attrs_set\"]))\n",
    "            sum_valid_attrs += max_valid_attrs\n",
    "            sum_total_attrs += max_total_attrs\n",
    "            invalid_attrs_list.extend(list(invalid_attrs_set))\n",
    "    percent_valid_attrs = round(sum_valid_attrs/sum_total_attrs,2)\n",
    "    workspace_results_list.append(ws_key)\n",
    "    workspace_results_list.append(len(can_run_set))\n",
    "    workspace_results_list.append(can_run_set)\n",
    "    workspace_results_list.append(len(can_run_fully_set))\n",
    "    workspace_results_list.append(can_run_fully_set)\n",
    "    workspace_results_list.append(sum_valid_attrs)\n",
    "    workspace_results_list.append(percent_valid_attrs)\n",
    "    workspace_results_list.append(invalid_attrs_list)\n",
    "    results_list.append(workspace_results_list)\n",
    "\n",
    "results_df = pd.DataFrame(results_list, columns = ['workspace', 'can_run_count', 'can_run_entities', 'can_fully_run_count', 'can_fully_run_entities', 'cnt_valid_attrs', 'perc_valid_attrs', 'invalid_attr_list'])\n",
    "\n",
    "# Sort results dataframe and write out to tsv\n",
    "sorted_df = results_df.sort_values(['can_run_count', 'perc_valid_attrs'], ascending=[False, False], ignore_index=True)\n",
    "output_file = \"mapping_compatibility_results.tsv\"\n",
    "destination_dir = \"ingest_pipeline/resources/mapping_compatibility/output\"\n",
    "sorted_df.to_csv(output_file, index=False, sep=\"\\t\")\n",
    "!gsutil cp $output_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "!rm $output_file\n",
    "\n",
    "# Output results to the user\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mapping Compatibility Results for Mapping Target Specification: {}/{}\".format(mapping_target, mapping_target_spec))\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n",
    "print(\"Target tables not included in specification:\")\n",
    "print(\"\\t\" + \", \".join(list(missing_table_set)))\n",
    "print(\"\\n\")\n",
    "print(\"Target fields not included in specification:\")\n",
    "print(\"\\t\" + \", \".join(list(missing_column_set)))\n",
    "print(\"\\n\")\n",
    "print(\"Workspace evaluation against specification:\")\n",
    "display(sorted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"donor\": [\n",
      "        {\n",
      "          \"record_set\": \"default\",\n",
      "          \"can_run\": true,\n",
      "          \"total_attrs\": 5,\n",
      "          \"valid_attrs\": 3,\n",
      "          \"invalid_attrs_set\": [\n",
      "            \"donor.reported_ethnicity\",\n",
      "            \"donor.diagnosis_id\"\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"biosample\": [\n",
      "        {\n",
      "          \"record_set\": \"default\",\n",
      "          \"can_run\": false,\n",
      "          \"total_attrs\": 5,\n",
      "          \"valid_attrs\": 2,\n",
      "          \"invalid_attrs_set\": [\n",
      "            \"biosample.biosample_type\",\n",
      "            \"biosample.anatomical_site\",\n",
      "            \"biosample.biosample_id\"\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"diagnosis\": [\n",
      "        {\n",
      "          \"record_set\": \"default\",\n",
      "          \"can_run\": false,\n",
      "          \"total_attrs\": 6,\n",
      "          \"valid_attrs\": 1,\n",
      "          \"invalid_attrs_set\": [\n",
      "            \"diagnosis.onset_age_unit\",\n",
      "            \"diagnosis.disease\",\n",
      "            \"diagnosis.onset_age_lower_bound\",\n",
      "            \"diagnosis.diagnosis_id\",\n",
      "            \"diagnosis.onset_age_upper_bound\"\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"sequencingactivity\": [\n",
      "        {\n",
      "          \"record_set\": \"default\",\n",
      "          \"can_run\": true,\n",
      "          \"total_attrs\": 4,\n",
      "          \"valid_attrs\": 4,\n",
      "          \"invalid_attrs_set\": []\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"file\": [\n",
      "        {\n",
      "          \"record_set\": \"default\",\n",
      "          \"can_run\": true,\n",
      "          \"total_attrs\": 4,\n",
      "          \"valid_attrs\": 4,\n",
      "          \"invalid_attrs_set\": []\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"dataset\": [\n",
      "        {\n",
      "          \"record_set\": \"default\",\n",
      "          \"can_run\": true,\n",
      "          \"total_attrs\": 6,\n",
      "          \"valid_attrs\": 6,\n",
      "          \"invalid_attrs_set\": []\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"project\": [\n",
      "        {\n",
      "          \"record_set\": \"default\",\n",
      "          \"can_run\": true,\n",
      "          \"total_attrs\": 4,\n",
      "          \"valid_attrs\": 4,\n",
      "          \"invalid_attrs_set\": []\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print detailed results for specific workspace\n",
    "print(json.dumps(detail_dict[\"anvil_cmg_uwash_ds-hfa\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tables\": [\n",
      "    {\n",
      "      \"name\": \"ws_sample\",\n",
      "      \"columns\": [\n",
      "        {\n",
      "          \"name\": \"sample_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"dbgap_sample_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"sample_provider\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"subject_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"submitter_id\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ws_sequencing\",\n",
      "      \"columns\": [\n",
      "        {\n",
      "          \"name\": \"sequencing_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"alignment_method\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"analyte_type\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"capture_region_bed_file\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"data_processing_pipeline\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"date_data_generation\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"exome_capture_platform\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"fastq\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"functional_equivalence_standard\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"library_prep_kit_method\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"number_of_independent_libraries\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"read_length\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"reference_genome_build\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"sample_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"sequencer_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"sequencing_assay\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"sequencing_platform\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"sequencing_strategy\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"sex\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"submitter_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"target_depth\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"target_insert_size\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"tissue_source\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"sequencing_id_file_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"capture_region_bed_file_file_id\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ws_subject\",\n",
      "      \"columns\": [\n",
      "        {\n",
      "          \"name\": \"subject_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"consent\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"dbgap_study_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"dbgap_subject_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"dbgap_submission\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"sequencing_center\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"sex\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"study_nickname\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"submitter_id\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ws_file_inventory\",\n",
      "      \"columns\": [\n",
      "        {\n",
      "          \"name\": \"file_id\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"name\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"path\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"uri\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"content_type\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"full_extension\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"size_in_bytes\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"crc32c\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"md5_hash\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"file_ref\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"ws_workspace_attributes\",\n",
      "      \"columns\": [\n",
      "        {\n",
      "          \"name\": \"attribute\"\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"value\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print workspace dict for specific workspace\n",
    "print(json.dumps(workspace_dict[\"anvil_gtex_bcm_gru_corsivs\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
