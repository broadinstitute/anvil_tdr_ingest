{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional modules (one time effort per cloud environment)\n",
    "!pip install --upgrade import_ipynb data_repo_client urllib3 xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import data_repo_client\n",
    "from google.cloud import bigquery\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_file_inventory as bfi\n",
    "import identify_supplementary_files as isf\n",
    "import logging\n",
    "from time import sleep\n",
    "import datetime\n",
    "from google.cloud import storage\n",
    "import math\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create new snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to create new full view snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {}\n",
    "params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\", \"auth-domain\"]\n",
    "params[\"anvil_schema_version\"] = \"ANV5\"\n",
    "\n",
    "# Loop through datasets and create new snapshot\n",
    "dataset_id_run_list = [\n",
    "'8fbfea50-6a71-4b19-98e9-f95e3a8594c7',\n",
    "'5627cdbb-22a0-436f-a7a4-34d7ce21bb45',\n",
    "'6dca0ce9-37b3-4b0a-93bd-7d3f21b0edf3',\n",
    "'8b2b1c92-66cf-403c-8eb0-03b523d1550e',\n",
    "'595b6755-e7ae-4e83-af2e-693c089aeec3',\n",
    "'416b8daa-9537-46db-ae7b-3f5ff5f01dc3',\n",
    "'84ac0d05-4be5-43e9-973e-ef999144d802',\n",
    "'732eaae3-b509-4a7a-8961-09d861e55253',\n",
    "'c6f3bd64-ea67-488f-904f-f0bdf6320b5c',\n",
    "'544f643d-b19f-4aa0-a6ec-a90e1a8681d6',\n",
    "'d239dd7b-8d10-4960-aa91-8f8ede641e25',\n",
    "'2d434f2c-6aaa-46b2-ada9-de4b887e13d3',\n",
    "'c1644d4e-06e2-4fa8-95f1-5c1da5831257',\n",
    "'f85ea65e-1943-4bd6-a541-71c5d8465ca9',\n",
    "'2cbe079d-e7ab-47d8-836e-454a71440297',\n",
    "'280c5d6f-39a3-4d1d-aad2-a174451cd9b2',\n",
    "'488a38ee-f996-482d-a562-a4474f5594de',\n",
    "'79c58bfb-3188-442b-9166-a50f28fcfae5',\n",
    "'28e73469-12d4-493b-bf6f-83359c1f69c5',\n",
    "'c2fd0797-ca41-49a1-b485-a4bedac00613',\n",
    "'51daecbd-37fa-4a58-8625-b6fad65acf27',\n",
    "'5afc14bf-d7ca-4a62-b7aa-5104fa846888',\n",
    "'69178fa1-87d4-4ecc-bc0e-7347c3678635',\n",
    "'15ae6390-6f6d-4fd8-9a51-ecf988676c4d',\n",
    "'3a3100bb-369e-47c1-a77c-2cacb7cf020d',\n",
    "'95788aa7-c897-4ae8-9166-4b8fc1fc5342',\n",
    "]\n",
    "results = []\n",
    "for dataset in dataset_id_run_list:\n",
    "    dataset_id = dataset\n",
    "    try:\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        dataset_name = dataset_info[\"name\"]\n",
    "        phs_id = dataset_info[\"phs_id\"]\n",
    "        consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "        auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "        src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "    except:\n",
    "        dataset_name = \"\"\n",
    "    if dataset_name:\n",
    "        params[\"ws_bucket\"] = ws_bucket\n",
    "        params[\"dataset_id\"] = dataset_id\n",
    "        params[\"dataset_name\"] = dataset_name\n",
    "        params[\"phs_id\"] = phs_id\n",
    "        params[\"consent_name\"] = consent_name\n",
    "        params[\"auth_domains\"] = auth_domains\n",
    "        params[\"pipeline_results\"] = []\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "        params[\"snapshot_name\"] = params[\"dataset_name\"] + \"_\" + params[\"anvil_schema_version\"] + \"_\" + current_datetime_string \n",
    "        utils.create_and_share_snapshot(params)\n",
    "        int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "        errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "        if len(errors) > 0:\n",
    "            results.append([dataset_id, \"Error\", \"\"])\n",
    "        else:\n",
    "            snapshot_id = re.search(\"{'id': '([a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12})'\", str(int_df_results[int_df_results[\"Task\"]==\"Create and Share Snapshot\"][\"Message\"]))[1]\n",
    "            results.append([dataset_id, \"Success\", snapshot_id])\n",
    "results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\", \"snapshot_id\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Verify Snapshots Have Properly Formatted DRS URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_snapshot_drs_format(snapshot_id):\n",
    "    \n",
    "    # Retrieve snapshot information\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = snapshots_api.retrieve_snapshot(id=snapshot_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        return \"Failure - Issue Retrieving Snapshot Info\"\n",
    "    \n",
    "    # Determine if field exists for dataset, continue if so, fail otherwise\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"SELECT COUNT(file_ref) AS rec_cnt, COUNT(CASE WHEN file_ref LIKE '%drs://drs.anv0:v2_%' THEN file_ref END) AS valid_cnt\n",
    "                FROM `{project}.{dataset}.anvil_file`\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        df = client.query(query).result().to_dataframe()\n",
    "        if df[\"rec_cnt\"].values[0] == df[\"valid_cnt\"].values[0]:\n",
    "            return \"Success\"\n",
    "        else:\n",
    "            rec_cnt = df[\"rec_cnt\"].values[0]\n",
    "            valid_cnt = df[\"valid_cnt\"].values[0]\n",
    "            return f\"Failure: Only {valid_cnt} of {rec_cnt} records properly formatted\"\n",
    "    except Exception as e:\n",
    "        return \"Failure - BigQuery Error\"\n",
    "\n",
    "# Loop through datasets and validate is_supplementary field\n",
    "snapshot_id_list = [\n",
    "'c3d22305-b3f2-4561-a5b9-bed82ee742f4',\n",
    "'9fe2abd4-70b4-4eee-b00d-38726ced8620',\n",
    "'5329c25e-ccad-435d-9250-6fcc3ff88472',\n",
    "'ced601b2-9a11-40e9-8067-241e5a5996ed',\n",
    "'8165245c-2003-4ec7-bf57-731959022d47',\n",
    "'737d454c-88be-477f-ae2c-ef473e2106ce',\n",
    "'3bdbad9e-f9d4-4442-8606-791d490bf0af',\n",
    "'cd19195f-25a0-44b1-b47d-ec99141833fc',\n",
    "'b897e519-ba8b-4758-a263-6d57bd3b8e2b',\n",
    "'1d385cfc-4bed-4f52-8f7b-ea54fc44b4f7',\n",
    "'02d25240-823f-4b1d-8562-95385716a453',\n",
    "'1974a21b-c409-4736-a3d7-e195fa96c4eb',\n",
    "'99b46287-4790-492c-8a12-bea33f0f927c',\n",
    "'c6ef5822-3929-4ae7-b5bc-dc27528bf226',\n",
    "'08d19a7e-b868-4766-9f7e-d879d972cbd7',\n",
    "'35186e6d-2728-4a8e-b0ad-6b34d0fe480c',\n",
    "'b0d176bf-d094-4e33-a34b-b83a94de86ea',\n",
    "'cc6bacc8-29fa-4d97-8856-79f52ea50c6f',\n",
    "'85b721da-ad8e-4d82-93f0-0988f94af22e',\n",
    "'407c7800-3ab4-4b13-ba45-c6c13c1c2278',\n",
    "'2529f127-cff5-43ff-b879-06bc0e3468ff',\n",
    "'b511be0b-7dc5-4767-a891-37f43d04a5a5',\n",
    "'a7e031c3-62d4-46db-b2e2-0bdca6bbad65',\n",
    "'5bba97dc-d6ab-4329-912f-148c8b807056',\n",
    "'9cf61d88-d096-4981-b0c6-99db77554c01',\n",
    "'4c722626-c559-4f5a-84bd-8d7d46983e1e',\n",
    "'7c237e08-3329-4e64-bd2a-063be290e78b',\n",
    "'4117144f-92e7-454f-9263-dad5e128cadb',\n",
    "'ce2e7235-26e6-470f-8e05-298193b7f53d',\n",
    "'6df525e1-b143-4e6f-b667-80c783ae1b66',\n",
    "'92666b7c-4d50-4530-88e9-ea2d3da9d07a',\n",
    "'42644c25-fa23-4b4e-8fcc-907cd8dcef60',\n",
    "'155c11a9-638a-45c8-b172-7cf2e3e16fe6',\n",
    "'b3da9fec-08ad-4496-a9ac-1411388fb5cc',\n",
    "'0de07296-e3ff-4fe6-9183-9f421484197c',\n",
    "'1b6273c6-7769-4daf-abee-93b11b322c73',\n",
    "'ea50255a-45a4-4846-82e3-02b4f46f5b17',\n",
    "'eb7045e1-2286-49f1-bce6-21b5d7fa5c32',\n",
    "'b763c288-4132-434a-a6c9-25ad51b9d961',\n",
    "'b67702a8-307d-4b20-835e-c0245d0761e5',\n",
    "'88548251-e59e-4bc3-b71a-f1e9e2369919',\n",
    "'d3dc5627-503b-48a5-ad79-31ab6c2fd417',\n",
    "'ec14f8cd-5b1b-4124-a235-f11159984c7c',\n",
    "'6d9e1212-4fa6-4632-be8a-75c45a474dd3',\n",
    "'667eac9b-4e90-413d-80f3-d857b9829ab7',\n",
    "'c091ea30-1862-4b1f-8e92-087b441472c3',\n",
    "'43c86818-9bfe-46f2-9ae4-4a55a7baef1f',\n",
    "'ebdaca04-ef29-42f3-8486-a94dade81bf8',\n",
    "'f8781fbf-5fef-4481-8819-3df1bc724b7f',\n",
    "'830df9ed-e4a6-4c9a-a97a-aa080fb030e4',\n",
    "'84703c54-a9dd-400c-9701-2fc40922e3e3',\n",
    "'c1c674dd-056a-470c-8874-bf70d8fae3a8',\n",
    "'6a5b3be6-d1de-4f23-a431-b08e7ab231b8',\n",
    "'ffe34538-3ddd-48de-b4a2-94f9b2dad086',\n",
    "'2c6de04e-104d-42c8-8448-97d74985dacb',\n",
    "'2a1882d9-88ca-4849-bcc1-f6914f593407',\n",
    "'bf2f4106-cee9-419c-b4d1-d7b03a6293d5',\n",
    "'a6c36f5e-b86c-4164-85ae-8bf0df2e4a90',\n",
    "'7c19d852-e36a-4353-afea-10e501601d9a',\n",
    "'00297802-e20a-413f-b389-a6f764b6600e',\n",
    "'b8a455eb-827d-43a0-a89b-5d017747140f',\n",
    "'3e85b06a-a6ea-4ce8-a655-44b1fce12138',\n",
    "'9321b908-f2e4-437b-b53e-ed81754dcace',\n",
    "'172bada7-f1c5-41c4-836d-05381beaed9a',\n",
    "'133e902c-5ff0-4119-8078-db3e15006844',\n",
    "'452bcafd-ab45-4e24-b5e0-13fcf22b0755',\n",
    "'5e547934-c339-410e-a013-dfefed50f4b8',\n",
    "'ffa84feb-ca0e-43d3-a04d-a402a8e24a3b',\n",
    "'ff27037e-cb52-44ef-8979-f6e7ac3ed6f6',\n",
    "'c853d4c0-d4be-433d-964e-e30bdc35480e',\n",
    "'8fbe2def-b8ad-4b2d-90c9-0dd4517c67e1',\n",
    "'03e54581-8fd3-47c3-9143-55368d2e4e86',\n",
    "'9efae3c7-904c-48a8-939a-e82b46005ae1',\n",
    "'5955a235-5be6-47bc-8303-ed0c4e68f501',\n",
    "'e04edfef-69f8-47ff-8df9-dfff0e9218d2',\n",
    "'f2a7be5a-4f7a-4a96-935e-ca7592855b45',\n",
    "'7c90289b-be3e-4c9b-917a-d5e27d95dc15',\n",
    "'0f46a588-b4ff-4a69-99e9-0a0bcf052522',\n",
    "'cdd689fd-10f3-4cfa-b738-46549e689cac',\n",
    "'eb7948be-1007-4b0e-b9b6-a5c40bbb9596',\n",
    "'f20753f0-d09a-4b47-bffe-8f24ec354761',\n",
    "'4cff04f4-eff9-4a62-bc6e-691accfbd328',\n",
    "'9a61b980-4a33-465a-bc50-1aba00bc2cf6',\n",
    "'90fe2016-e79c-456c-a5f9-3a31149fcd65',\n",
    "'a4c62d7f-34f0-4e2e-9e46-c762d3ab0ff2',\n",
    "'28dc8121-5e55-46c2-8313-681de2298986',\n",
    "'dcc578ed-44bb-458f-8ff5-a78ca83f4616',\n",
    "'aa42debe-3747-4dcd-8bc9-24eb90673fa5',\n",
    "'5208772d-21f9-46b0-8167-0b05b57296b8',\n",
    "'a2da748b-fec8-4e10-88ee-de32cbe8dee1',\n",
    "'26df2a34-b10d-4361-ba2b-d9f966d09f61',\n",
    "'dd00a8ba-ac49-481b-8d79-0e440adafd77',\n",
    "'0df983d7-ed5e-44d2-acf1-686822b0cc7e',\n",
    "'28559e94-ed57-48c8-bc8b-6cc4ad659a61',\n",
    "'8b385bd3-52aa-48b9-be33-41f4d3fd4531',\n",
    "'ce1bf5c3-525e-455d-a1e9-dd5f3d68c9d3',\n",
    "'d0a6aa4c-821c-4bba-b53b-4f230ca3cda4',\n",
    "'d9e817a2-6657-433b-8b2f-73790561725c',\n",
    "'33c854eb-d228-4a82-8324-5e455ed1e447',\n",
    "]\n",
    "results = []\n",
    "for snapshot_id in snapshot_id_list:\n",
    "    status = validate_snapshot_drs_format(snapshot_id) \n",
    "    results.append([snapshot_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"snapshot_id\", \"validation_status\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Add and populate anvil_file.is_supplementary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Script to patch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set base parameters\n",
    "params = {}\n",
    "params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "\n",
    "# Loop through datasets and process is_supplementary field\n",
    "dataset_id_list = [\n",
    "'8b2b1c92-66cf-403c-8eb0-03b523d1550e',\n",
    "'595b6755-e7ae-4e83-af2e-693c089aeec3',\n",
    "'84ac0d05-4be5-43e9-973e-ef999144d802',\n",
    "'732eaae3-b509-4a7a-8961-09d861e55253',\n",
    "'544f643d-b19f-4aa0-a6ec-a90e1a8681d6',\n",
    "'f85ea65e-1943-4bd6-a541-71c5d8465ca9',\n",
    "'280c5d6f-39a3-4d1d-aad2-a174451cd9b2',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    logging.info(f\"Patching dataset_id: {dataset_id}\")\n",
    "    params[\"t_output_dir\"] = \"ingest_pipeline/output/transformed/anvil/{}/table_data\".format(dataset_id)\n",
    "    output, status = isf.identify_supplementary_files(params, dataset_id)\n",
    "    results.append([dataset_id, status, output])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\", \"output\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Script to validate patch worked properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_supp_file_flg(dataset_id):\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        return \"Failure - Issue Retrieving Dataset Info\"\n",
    "    \n",
    "    # Determine if field exists for dataset, continue if so, fail otherwise\n",
    "    field_found = False\n",
    "    for table in src_schema_dict[\"tables\"]:\n",
    "        if table[\"name\"] == \"anvil_file\":\n",
    "            for col in table[\"columns\"]:\n",
    "                if col[\"name\"] == \"is_supplementary\":\n",
    "                    field_found = True\n",
    "                    break\n",
    "            break\n",
    "    if field_found == False:\n",
    "        return \"Failure - is_supplementary field not found\"\n",
    "    else:\n",
    "        client = bigquery.Client()\n",
    "        # Check field population\n",
    "        query = \"\"\"SELECT COUNT(*) AS rec_cnt, COUNT(is_supplementary) AS populated_cnt\n",
    "                    FROM `{project}.{dataset}.anvil_file`\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "        try:\n",
    "            df = client.query(query).result().to_dataframe()\n",
    "            if df[\"rec_cnt\"].values[0] == df[\"populated_cnt\"].values[0]:\n",
    "                pass\n",
    "            else:\n",
    "                return \"Failure - is_supplementary field not populated\"\n",
    "        except Exception as e:\n",
    "            return \"Failure - BigQuery Error\"\n",
    "        # Check field logic\n",
    "        validation_query = \"\"\"\n",
    "            WITH activity_agg\n",
    "            AS\n",
    "            (\n",
    "              SELECT used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_activity`\n",
    "              UNION ALL \n",
    "              SELECT [] AS used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_alignmentactivity`\n",
    "              UNION ALL \n",
    "              SELECT used_biosample_id, generated_file_id, [] AS used_file_id FROM `{project}.{dataset}.anvil_assayactivity`\n",
    "              UNION ALL \n",
    "              SELECT used_biosample_id, generated_file_id, [] AS used_file_id FROM `{project}.{dataset}.anvil_sequencingactivity`\n",
    "              UNION ALL \n",
    "              SELECT [] AS used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_variantcallingactivity`\n",
    "            ),\n",
    "            activity_exp \n",
    "            AS\n",
    "            (\n",
    "              SELECT file_id, int_file_id, biosample_id\n",
    "              FROM activity_agg\n",
    "                  LEFT JOIN UNNEST(used_biosample_id) AS biosample_id\n",
    "                  LEFT JOIN UNNEST(generated_file_id) as file_id\n",
    "                  LEFT JOIN UNNEST(used_file_id) as int_file_id\n",
    "            ),\n",
    "            activity_exp_tagged\n",
    "            AS\n",
    "            (\n",
    "              SELECT a.file_id, b.is_supplementary AS file_id_supp, int_file_id, c.is_supplementary AS int_file_id_supp, biosample_id\n",
    "              FROM activity_exp a\n",
    "                  LEFT JOIN  `{project}.{dataset}.anvil_file` b\n",
    "                  ON a.file_id = b.file_id\n",
    "                  LEFT JOIN  `{project}.{dataset}.anvil_file` c\n",
    "                  ON a.int_file_id = c.file_id \n",
    "            )\n",
    "            SELECT CASE WHEN file_id_supp = TRUE AND biosample_id IS NOT NULL THEN 'Supplemental File Linked to BioSample' WHEN (file_id_supp = TRUE AND int_file_id_supp = FALSE) OR (file_id_supp = FALSE AND int_file_id_supp = TRUE) THEN 'Supplemental File Linked to Non-Supplemental File' ELSE 'No Issue Found' END AS finding, COUNT(*) AS occurrences\n",
    "            FROM activity_exp_tagged\n",
    "            GROUP by finding\n",
    "            \"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "        try:\n",
    "            df = client.query(validation_query).result().to_dataframe()\n",
    "            records_json = json.loads(df.to_json(orient='records'))\n",
    "            supp_linked_to_biosample = 0\n",
    "            supp_linked_to_nonsupp = 0\n",
    "            non_issue_links = 0\n",
    "            for record in records_json:\n",
    "                if record[\"finding\"] == \"Supplemental File Linked to BioSample\":\n",
    "                    supp_linked_to_biosample = record[\"occurrences\"]\n",
    "                elif record[\"finding\"] == \"Supplemental File Linked to Non-Supplemental File\":\n",
    "                    supp_linked_to_nonsupp = record[\"occurrences\"]\n",
    "                else:\n",
    "                    non_issue_links = record[\"occurrences\"]\n",
    "            if supp_linked_to_biosample > 0 or supp_linked_to_nonsupp > 0:\n",
    "                err_msg = f\"Failure - Errors found when validating supplementary files flagged in the TDR dataset: Supplemental Files Linked to a Biosample: {str(supp_linked_to_biosample)} Supplemental Files Linked to a Non-Supplemental File: {str(supp_linked_to_nonsupp)} Links with No Issues: {str(non_issue_links)}\"\n",
    "                return err_msg\n",
    "        except Exception as e:\n",
    "            return \"Failure - BigQuery Error\"\n",
    "        return \"Success\"  \n",
    "\n",
    "# Loop through datasets and validate is_supplementary field\n",
    "dataset_id_list = [\n",
    "'8b2b1c92-66cf-403c-8eb0-03b523d1550e',\n",
    "'595b6755-e7ae-4e83-af2e-693c089aeec3',\n",
    "'84ac0d05-4be5-43e9-973e-ef999144d802',\n",
    "'732eaae3-b509-4a7a-8961-09d861e55253',\n",
    "'544f643d-b19f-4aa0-a6ec-a90e1a8681d6',\n",
    "'f85ea65e-1943-4bd6-a541-71c5d8465ca9',\n",
    "'280c5d6f-39a3-4d1d-aad2-a174451cd9b2',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    logging.info(f\"Validating dataset_id: {dataset_id}\")\n",
    "    status = validate_supp_file_flg(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"validation_status\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Attempt to populate anvil_donor.organism_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to patch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def populate_organism_type(dataset_id):\n",
    "    logging.info(f\"Processing anvil_donor.organism_type for Dataset ID = {dataset_id}\")\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    logging.info(\"Retrieving necessary information from TDR.\")\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving information from TDR. Exiting function. Error: {}\".format(e))\n",
    "        return \"Failure\"\n",
    "\n",
    "    # Re-process anvil_donor data to include organism_type (where available)\n",
    "    logging.info(\"Re-processing existing anvil_donor data to include organism_type value.\")\n",
    "    client = bigquery.Client()\n",
    "    target_file = \"anvil_donor.json\"\n",
    "    destination_dir = \"ingest_pipeline/output/transformed/anvil/{}/table_data\".format(dataset_id)\n",
    "    query = \"\"\"SELECT donor_id, \n",
    "    (SELECT MAX(CASE WHEN REGEXP_CONTAINS(value, '(h37|h38|h39|hg16|hg17|hg18|hg19|hs37|hs38|b37)') THEN 'Homo sapiens' END) AS organism_type FROM `{project}.{dataset}.workspace_attributes` WHERE attribute = 'library:reference') AS organism_type,\n",
    "    part_of_dataset_id, phenotypic_sex, reported_ethnicity, genetic_ancestry, source_datarepo_row_ids\n",
    "    FROM `{project}.{dataset}.anvil_donor`\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        df = client.query(query).result().to_dataframe()\n",
    "        records_json = df.to_json(orient='records') \n",
    "        records_list = json.loads(records_json)\n",
    "        records_cnt = len(records_list)\n",
    "        with open(target_file, 'w') as outfile:\n",
    "            for idx, val in enumerate(records_list):\n",
    "                json.dump(val, outfile)\n",
    "                if idx < (records_cnt - 1):\n",
    "                    outfile.write('\\n')\n",
    "        !gsutil cp $target_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "        !rm $target_file\n",
    "        logging.info(\"Successfully created new anvil_donor.json file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error creating new anvil_donor.json file. Exiting function. Error: {}\".format(str(e)))\n",
    "        return \"Failure\"\n",
    "\n",
    "    # Ingest updated anvil_donor data\n",
    "    logging.info(\"Ingesting updated anvil_donor data into TDR dataset.\")\n",
    "    source_full_file_path = \"{}/{}/{}\".format(ws_bucket, destination_dir, \"anvil_donor.json\")\n",
    "    ingest_request = {\n",
    "        \"table\": \"anvil_donor\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"json\",\n",
    "        \"load_tag\": \"Ingest for {}\".format(dataset_id),\n",
    "        \"path\": source_full_file_path\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"Ingest from file anvil_donor.json succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Dataset Ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Dataset Ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Exiting function.\")\n",
    "                return \"Failure\"\n",
    "\n",
    "    # Return success message if no failures recorded\n",
    "    logging.info(\"Function completed successfully.\")\n",
    "    return \"Success\"\n",
    "\n",
    "# Loop through datasets and process supplementary_file_flag\n",
    "dataset_id_list = [\n",
    "'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = populate_organism_type(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to examine organism_type population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_organism_type(dataset_id):\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        return \"Failure - Issue Retrieving Dataset Info\"\n",
    "    \n",
    "    # Determine if field exists for dataset, continue if so, fail otherwise\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"SELECT COUNT(organism_type) AS populated_cnt\n",
    "                FROM `{project}.{dataset}.anvil_donor`\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        df = client.query(query).result().to_dataframe()\n",
    "        if df[\"populated_cnt\"].values[0] > 0:\n",
    "            return \"Success - Field Populated\"\n",
    "        else:\n",
    "            return \"Success - Field Not Populated\"\n",
    "    except Exception as e:\n",
    "        return \"Failure - BigQuery Error\"\n",
    "\n",
    "# Loop through datasets and validate is_supplementary field\n",
    "dataset_id_list = [\n",
    "'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = validate_organism_type(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"validation_status\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Update references to md5-added files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to collect all datarepo rows for a particular table within a dataset\n",
    "def collect_all_datarepo_rows(dataset_id, table_name):\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving BQ project and schema: {}\".format(str(e)))\n",
    "    client = bigquery.Client()\n",
    "    query = \"SELECT datarepo_row_id FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = table_name)\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        results = [row[\"datarepo_row_id\"] for row in query_job]\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving datarepo_row_id list: {}\".format(str(e)))\n",
    "        raise Exception(e)\n",
    "\n",
    "# Function to delete rows from a dataset\n",
    "def delete_old_records(dataset_id, table, datarepo_row_ids):\n",
    "    logging.info(f\"Attempting to delete original {table} records.\")\n",
    "    if datarepo_row_ids:\n",
    "        data_deletion_payload = {\n",
    "            \"deleteType\": \"soft\",\n",
    "            \"specType\": \"jsonArray\",\n",
    "            \"tables\": [{\n",
    "              \"tableName\": table,\n",
    "              \"jsonArraySpec\": {\n",
    "                \"rowIds\": datarepo_row_ids\n",
    "              }\n",
    "            }]\n",
    "        }\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        try:\n",
    "            data_deletion_result, job_id = utils.wait_for_tdr_job(datasets_api.apply_dataset_data_deletion(id=dataset_id, data_deletion_request=data_deletion_payload))\n",
    "            logging.info(\"Result: {}\".format(data_deletion_result))\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error: {}\".format(str(e)))\n",
    "            raise Exception(e)\n",
    "    else:\n",
    "        logging.info(\"No datarepo_row_ids specified for deletion.\")\n",
    "\n",
    "def ingest_updated_records(profile_id, dataset_id, table, records_dict):\n",
    "    logging.info(f\"Submitting ingest for updated {table} records.\")\n",
    "    \n",
    "    # Build, submit, and monitor ingest request\n",
    "    ingest_request = {\n",
    "        \"table\": table,\n",
    "        \"profile_id\": profile_id,\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"array\",\n",
    "        \"bulkMode\": False,\n",
    "        \"load_tag\": f\"File ref fields patch for {table} in {dataset_id}\",\n",
    "        \"records\": records_dict\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"Ingest succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            status = \"Success\"\n",
    "            return\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 1:\n",
    "                logging.info(\"Retrying ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Logging error.\")\n",
    "                status = \"Error\"\n",
    "                raise Exception(e)\n",
    "                \n",
    "def update_recs_w_file_refs(dataset_id):\n",
    "    logging.info(f\"Processing md5-added files for Dataset ID = {dataset_id}\")\n",
    "\n",
    "    ## Retrieve dataset information\n",
    "    logging.info(\"Retrieving necessary information from TDR.\")\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving information from TDR. Exiting function. Error: {}\".format(e))\n",
    "        return \"Failure - Pre-processing\"\n",
    "\n",
    "    ## Parse TDR schema to identify file reference fields\n",
    "    table_dict = {}\n",
    "    for table in src_schema_dict[\"tables\"]:\n",
    "        if table[\"name\"] in [\"file_inventory\", \"anvil_file\"]:\n",
    "            continue\n",
    "        else:\n",
    "            col_list = []\n",
    "            for column in table[\"columns\"]:\n",
    "                if column[\"datatype\"] == \"fileref\":\n",
    "                    col_list.append([column[\"name\"], column[\"array_of\"]])\n",
    "            if col_list:\n",
    "                table_dict[table[\"name\"]] = col_list\n",
    "\n",
    "    ## Loop through tables and re-process impacted records\n",
    "    for table in table_dict.keys():\n",
    "        logging.info(f\"Processing updates for {table}.\")\n",
    "        # Retrieve relevant records from BigQuery\n",
    "        col_list = []\n",
    "        old_cols = \"\"\n",
    "        new_cols = \"\"\n",
    "        join_clause = \"\"\n",
    "        where_clause = \"\"\n",
    "        for idx, col in enumerate(table_dict[table]):\n",
    "            column_name = col[0]\n",
    "            col_list.append(column_name)\n",
    "            if idx == 0: \n",
    "                old_cols += column_name\n",
    "                where_clause += f\"t.{column_name} IN (SELECT file_ref FROM file_list)\"\n",
    "            else:\n",
    "                old_cols += \", \" + column_name\n",
    "                where_clause += f\" OR t.{column_name} IN (SELECT file_ref FROM file_list)\"\n",
    "            new_cols += f\", CASE WHEN t{idx}.source_name IS NOT NULL THEN TO_JSON(STRUCT(t{idx}.source_name AS sourcePath, t{idx}.target_path AS targetPath)) END AS {column_name}\"\n",
    "            join_clause += f\" LEFT JOIN load_hist t{idx} ON t.{column_name} = t{idx}.file_id\"\n",
    "\n",
    "        query = \"\"\"WITH \n",
    "            file_list AS (SELECT * FROM `{project}.{dataset}.file_inventory` WHERE md5_hash IS NULL),\n",
    "            load_hist AS (SELECT * FROM `{project}.{dataset}.datarepo_load_history` WHERE state = 'succeeded')\n",
    "            SELECT t.* EXCEPT({old_cols}){new_cols}\n",
    "            FROM `{project}.{dataset}.{table}` t {joins} WHERE {where}\"\"\".format(project=bq_project, dataset=bq_dataset, table=table, old_cols=old_cols, new_cols=new_cols, joins=join_clause, where=where_clause)\n",
    "        try:\n",
    "            client = bigquery.Client()\n",
    "            res = client.query(query).result()\n",
    "            if res.total_rows > 0:\n",
    "                logging.info(f\"{res.total_rows} records to process.\")\n",
    "                df = res.to_dataframe()\n",
    "                records_json = df.to_json(orient='records')\n",
    "                records_list = json.loads(records_json)\n",
    "            else:\n",
    "                logging.info(\"No records to process.\")\n",
    "                records_list = []\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error retrieving update records from BigQuery: {str(e)}\")\n",
    "            return \"Failure - Table Processing\"\n",
    "        # Ingest updated records back to TDR dataset\n",
    "        try:\n",
    "            datarepo_row_ids = []\n",
    "            for record in records_list:\n",
    "                datarepo_row_ids.append(record.pop(\"datarepo_row_id\", None))\n",
    "                for col in col_list:\n",
    "                    record[col] = json.loads(record[col])\n",
    "            if records_list:\n",
    "                ingest_updated_records(\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", dataset_id, table, records_list)\n",
    "                delete_old_records(dataset_id, table, datarepo_row_ids)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error replacing TDR records: {str(e)}\")\n",
    "            return \"Failure - Table Processing\"\n",
    "        \n",
    "    ## Re-process file_inventory\n",
    "    logging.info(f\"Processing updates for file_inventory.\")\n",
    "    # Retrieve relevant records from BigQuery\n",
    "    query = \"\"\"WITH \n",
    "        file_list AS (SELECT file_ref FROM `{project}.{dataset}.file_inventory` WHERE md5_hash IS NULL),\n",
    "        load_hist AS (SELECT * FROM `{project}.{dataset}.datarepo_load_history` WHERE state = 'succeeded')\n",
    "        SELECT t1.*, CASE WHEN t2.source_name IS NOT NULL THEN TO_JSON(STRUCT(t2.source_name AS sourcePath, t2.target_path AS targetPath)) END AS file_ref\n",
    "        FROM `{project}.{dataset}.file_inventory` t1\n",
    "          INNER JOIN load_hist t2 ON t1.file_ref = t2.file_id\n",
    "        WHERE file_ref IN (SELECT file_ref FROM file_list)\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        client = bigquery.Client()\n",
    "        res = client.query(query).result()\n",
    "        if res.total_rows > 0:\n",
    "            logging.info(f\"{res.total_rows} records to process.\")\n",
    "            df = res.to_dataframe()\n",
    "            records_json = df.to_json(orient='records')\n",
    "            records_list = json.loads(records_json)\n",
    "        else:\n",
    "            logging.info(\"No records to process.\")\n",
    "            records_list = []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving update records from BigQuery: {str(e)}\")\n",
    "        return \"Failure - File Inventory Processing\"\n",
    "    # Loop through records and update md5_hash from GCS metadata\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        datarepo_row_ids = []\n",
    "        for record in records_list:\n",
    "            bucket = re.match('gs:\\/\\/([a-z0-9\\-]+)', record[\"uri\"]).group(1)\n",
    "            obj = re.match('gs:\\/\\/[a-z0-9\\-]+\\/([A-Za-z0-9\\-_\\/\\.]+)', record[\"uri\"]).group(1)\n",
    "            bucket = storage_client.bucket(bucket, user_project=\"anvil-datastorage\")\n",
    "            blob = bucket.get_blob(obj)\n",
    "            record[\"md5_hash\"] = blob.md5_hash\n",
    "            datarepo_row_ids.append(record.pop(\"datarepo_row_id\", None))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving file metadata from GCS: {str(e)}\")\n",
    "        return \"Failure - File Inventory Processing\"\n",
    "    # Ingest updated records back to TDR dataset\n",
    "    try:\n",
    "        if records_list:\n",
    "            ingest_updated_records(\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", dataset_id, \"file_inventory\", records_list)\n",
    "            delete_old_records(dataset_id, \"file_inventory\", datarepo_row_ids)         \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error replacing TDR records: {str(e)}\")\n",
    "        return \"Failure - File Inventory Processing\"\n",
    "\n",
    "    ## Empty anvil_% tables\n",
    "    logging.info(\"Clearing out existing anvil_% tables\")\n",
    "    table_list = [\"anvil_activity\", \"anvil_alignmentactivity\", \"anvil_antibody\", \"anvil_assayactivity\", \"anvil_biosample\", \"anvil_diagnosis\", \"anvil_donor\", \"anvil_file\", \"anvil_sequencingactivity\", \"anvil_variantcallingactivity\"]\n",
    "    for table in table_list:\n",
    "        try:\n",
    "            datarepo_row_ids = collect_all_datarepo_rows(dataset_id, table)\n",
    "            if datarepo_row_ids:\n",
    "                delete_old_records(dataset_id, table, datarepo_row_ids)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error clearing out existing anvil_% records: {str(e)}\")\n",
    "            return \"Failure - anvil_% Record Deletion\"\n",
    "    \n",
    "    ## Re-run T pipeline without validation\n",
    "    params = {}\n",
    "    params[\"ws_name\"] = ws_name\n",
    "    params[\"ws_project\"] = ws_project\n",
    "    params[\"ws_bucket\"] = ws_bucket\n",
    "    params[\"ws_bucket_name\"] = ws_bucket_name\n",
    "    params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\" \n",
    "    params[\"mapping_target\"] = \"anvil\"\n",
    "    params[\"skip_transforms\"] = False\n",
    "    params[\"transform_list_override\"] = [] # Leave empty to run transforms for all files, otherwise populate with target table names \n",
    "    params[\"skip_schema_extension\"] = False\n",
    "    params[\"skip_ingests\"] = False\n",
    "    params[\"ingest_list_override\"] = [] # Leave empty to run ingests for all files, otherwise populate with target table names\n",
    "    params[\"skip_file_relation_inference\"] = False\n",
    "    params[\"skip_dangling_fk_resolution\"] = False\n",
    "    params[\"skip_supplementary_file_identification\"] = False\n",
    "    params[\"skip_snapshot_creation\"] = False\n",
    "    params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "    params[\"skip_data_validation\"] = True\n",
    "    try:\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        dataset_name = dataset_info[\"name\"]\n",
    "        phs_id = dataset_info[\"phs_id\"]\n",
    "        consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "        auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "        src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "    except:\n",
    "        dataset_name = \"\"\n",
    "        return \"Failure - Dataset Retrieval for T Pipeline\"\n",
    "    if dataset_name:\n",
    "        params[\"dataset_id\"] = dataset_id\n",
    "        params[\"dataset_name\"] = dataset_name\n",
    "        params[\"phs_id\"] = phs_id\n",
    "        params[\"consent_name\"] = consent_name\n",
    "        params[\"auth_domains\"] = auth_domains\n",
    "        utils.run_t_pipeline(params)\n",
    "    \n",
    "    # Return success message if no failures recorded\n",
    "    logging.info(\"Function completed successfully.\")\n",
    "    return \"Success\"\n",
    "\n",
    "# Loop through datasets and process md5 updates\n",
    "dataset_id_list = [\n",
    "'700303c2-fcef-48a5-9900-096bf34e2d83',\n",
    "'a715c70d-da92-43ee-a851-1a27277909a2',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = update_recs_w_file_refs(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Testing\n",
    "# dataset_id = 'bc6075ac-5cfe-4613-8601-36ceb614939e'\n",
    "\n",
    "# logging.info(f\"Processing md5-added files for Dataset ID = {dataset_id}\")\n",
    "\n",
    "# ## Retrieve dataset information\n",
    "# logging.info(\"Retrieving necessary information from TDR.\")\n",
    "# src_schema_dict = {}\n",
    "# api_client = utils.refresh_tdr_api_client()\n",
    "# datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "# try:\n",
    "#     response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "#     src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "#     bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "#     bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "# except Exception as e:\n",
    "#     logging.error(\"Error retrieving information from TDR. Exiting function. Error: {}\".format(e))\n",
    "#     #return \"Failure - Pre-processing\"\n",
    "\n",
    "# ## Parse TDR schema to identify file reference fields\n",
    "# table_dict = {}\n",
    "# for table in src_schema_dict[\"tables\"]:\n",
    "#     if table[\"name\"] in [\"file_inventory\", \"anvil_file\"]:\n",
    "#         continue\n",
    "#     else:\n",
    "#         col_list = []\n",
    "#         for column in table[\"columns\"]:\n",
    "#             if column[\"datatype\"] == \"fileref\":\n",
    "#                 col_list.append([column[\"name\"], column[\"array_of\"]])\n",
    "#         if col_list:\n",
    "#             table_dict[table[\"name\"]] = col_list\n",
    "\n",
    "# ## Loop through tables and re-process impacted records\n",
    "# for table in table_dict.keys():\n",
    "#     logging.info(f\"Processing updates for {table}.\")\n",
    "#     # Retrieve relevant records from BigQuery\n",
    "#     col_list = []\n",
    "#     old_cols = \"\"\n",
    "#     new_cols = \"\"\n",
    "#     join_clause = \"\"\n",
    "#     where_clause = \"\"\n",
    "#     for idx, col in enumerate(table_dict[table]):\n",
    "#         column_name = col[0]\n",
    "#         col_list.append(column_name)\n",
    "#         if idx == 0: \n",
    "#             old_cols += column_name\n",
    "#             where_clause += f\"t.{column_name} IN (SELECT file_ref FROM file_list)\"\n",
    "#         else:\n",
    "#             old_cols += \", \" + column_name\n",
    "#             where_clause += f\" OR t.{column_name} IN (SELECT file_ref FROM file_list)\"\n",
    "#         new_cols += f\", CASE WHEN t{idx}.source_name IS NOT NULL THEN TO_JSON(STRUCT(t{idx}.source_name AS sourcePath, t{idx}.target_path AS targetPath)) END AS {column_name}\"\n",
    "#         join_clause += f\" LEFT JOIN load_hist t{idx} ON t.{column_name} = t{idx}.file_id\"\n",
    "\n",
    "#     query = \"\"\"WITH \n",
    "#         file_list AS (SELECT * FROM `{project}.{dataset}.file_inventory` WHERE md5_hash IS NULL),\n",
    "#         load_hist AS (SELECT * FROM `{project}.{dataset}.datarepo_load_history` WHERE state = 'succeeded')\n",
    "#         SELECT t.* EXCEPT({old_cols}){new_cols}\n",
    "#         FROM `{project}.{dataset}.{table}` t {joins} WHERE {where}\"\"\".format(project=bq_project, dataset=bq_dataset, table=table, old_cols=old_cols, new_cols=new_cols, joins=join_clause, where=where_clause)\n",
    "#     try:\n",
    "#         client = bigquery.Client()\n",
    "#         res = client.query(query).result()\n",
    "#         if res.total_rows > 0:\n",
    "#             logging.info(f\"{res.total_rows} records to process.\")\n",
    "#             df = res.to_dataframe()\n",
    "#             records_json = df.to_json(orient='records')\n",
    "#             records_list = json.loads(records_json)\n",
    "#         else:\n",
    "#             logging.info(\"No records to process.\")\n",
    "#             records_list = []\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error retrieving update records from BigQuery: {str(e)}\")\n",
    "#         break\n",
    "#         #return \"Failure - Table Processing\"\n",
    "#     # Ingest updated records back to TDR dataset\n",
    "#     try:\n",
    "#         datarepo_row_ids = []\n",
    "#         for record in records_list:\n",
    "#             datarepo_row_ids.append(record.pop(\"datarepo_row_id\", None))\n",
    "#             for col in col_list:\n",
    "#                 record[col] = json.loads(record[col])\n",
    "#         if records_list:\n",
    "#             ingest_updated_records(\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", dataset_id, table, records_list)\n",
    "#             delete_old_records(dataset_id, table, datarepo_row_ids)\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error replacing TDR records: {str(e)}\")\n",
    "#         break\n",
    "#         #return \"Failure - Table Processing\"\n",
    "\n",
    "# # ## Re-process file_inventory\n",
    "# # logging.info(f\"Processing updates for file_inventory.\")\n",
    "# # # Retrieve relevant records from BigQuery\n",
    "# # query = \"\"\"WITH \n",
    "# #     file_list AS (SELECT file_ref FROM `{project}.{dataset}.file_inventory` WHERE md5_hash IS NULL),\n",
    "# #     load_hist AS (SELECT * FROM `{project}.{dataset}.datarepo_load_history` WHERE state = 'succeeded')\n",
    "# #     SELECT t1.*, CASE WHEN t2.source_name IS NOT NULL THEN TO_JSON(STRUCT(t2.source_name AS sourcePath, t2.target_path AS targetPath)) END AS file_ref\n",
    "# #     FROM `{project}.{dataset}.file_inventory` t1\n",
    "# #       INNER JOIN load_hist t2 ON t1.file_ref = t2.file_id\n",
    "# #     WHERE file_ref IN (SELECT file_ref FROM file_list)\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "# # try:\n",
    "# #     client = bigquery.Client()\n",
    "# #     res = client.query(query).result()\n",
    "# #     if res.total_rows > 0:\n",
    "# #         logging.info(f\"{res.total_rows} records to process.\")\n",
    "# #         df = res.to_dataframe()\n",
    "# #         records_json = df.to_json(orient='records')\n",
    "# #         records_list = json.loads(records_json)\n",
    "# #     else:\n",
    "# #         logging.info(\"No records to process.\")\n",
    "# #         records_list = []\n",
    "# # except Exception as e:\n",
    "# #     logging.error(f\"Error retrieving update records from BigQuery: {str(e)}\")\n",
    "# #     #return \"Failure - File Inventory Processing\"\n",
    "# # # Loop through records and update md5_hash from GCS metadata\n",
    "# # try:\n",
    "# #     storage_client = storage.Client()\n",
    "# #     datarepo_row_ids = []\n",
    "# #     for record in records_list:\n",
    "# #         bucket = re.match('gs:\\/\\/([a-z0-9\\-]+)', record[\"uri\"]).group(1)\n",
    "# #         obj = re.match('gs:\\/\\/[a-z0-9\\-]+\\/([A-Za-z0-9\\-_\\/\\.]+)', record[\"uri\"]).group(1)\n",
    "# #         bucket = storage_client.bucket(bucket, user_project=\"anvil-datastorage\")\n",
    "# #         blob = bucket.get_blob(obj)\n",
    "# #         record[\"md5_hash\"] = blob.md5_hash\n",
    "# #         datarepo_row_ids.append(record.pop(\"datarepo_row_id\", None))\n",
    "# # except Exception as e:\n",
    "# #     logging.error(f\"Error retrieving file metadata from GCS: {str(e)}\")\n",
    "# #     #return \"Failure - File Inventory Processing\"\n",
    "# # # Ingest updated records back to TDR dataset\n",
    "# # try:\n",
    "# #     if records_list:\n",
    "# #         ingest_updated_records(\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", dataset_id, \"file_inventory\", records_list)\n",
    "# #         delete_old_records(dataset_id, \"file_inventory\", datarepo_row_ids)         \n",
    "# # except Exception as e:\n",
    "# #     logging.error(f\"Error replacing TDR records: {str(e)}\")\n",
    "# #     #return \"Failure - File Inventory Processing\"\n",
    "\n",
    "# # ## Empty anvil_% tables\n",
    "# # logging.info(\"Clearing out existing anvil_% tables\")\n",
    "# # table_list = [\"anvil_activity\", \"anvil_alignmentactivity\", \"anvil_antibody\", \"anvil_assayactivity\", \"anvil_biosample\", \"anvil_diagnosis\", \"anvil_donor\", \"anvil_file\", \"anvil_sequencingactivity\", \"anvil_variantcallingactivity\"]\n",
    "# # for table in table_list:\n",
    "# #     try:\n",
    "# #         datarepo_row_ids = collect_all_datarepo_rows(dataset_id, table)\n",
    "# #         if datarepo_row_ids:\n",
    "# #             delete_old_records(dataset_id, table, datarepo_row_ids)\n",
    "# #     except Exception as e:\n",
    "# #         logging.error(f\"Error clearing out existing anvil_% records: {str(e)}\")\n",
    "# #         break\n",
    "# #         #return \"Failure - anvil_% Record Deletion\"\n",
    "\n",
    "# # ## Re-run T pipeline without validation\n",
    "# # params = {}\n",
    "# # params[\"ws_name\"] = ws_name\n",
    "# # params[\"ws_project\"] = ws_project\n",
    "# # params[\"ws_bucket\"] = ws_bucket\n",
    "# # params[\"ws_bucket_name\"] = ws_bucket_name\n",
    "# # params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\" \n",
    "# # params[\"mapping_target\"] = \"anvil\"\n",
    "# # params[\"skip_transforms\"] = False\n",
    "# # params[\"transform_list_override\"] = [] # Leave empty to run transforms for all files, otherwise populate with target table names \n",
    "# # params[\"skip_schema_extension\"] = False\n",
    "# # params[\"skip_ingests\"] = False\n",
    "# # params[\"ingest_list_override\"] = [] # Leave empty to run ingests for all files, otherwise populate with target table names\n",
    "# # params[\"skip_file_relation_inference\"] = False\n",
    "# # params[\"skip_dangling_fk_resolution\"] = False\n",
    "# # params[\"skip_supplementary_file_identification\"] = False\n",
    "# # params[\"skip_snapshot_creation\"] = False\n",
    "# # params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "# # params[\"skip_data_validation\"] = True\n",
    "# # try:\n",
    "# #     api_client = utils.refresh_tdr_api_client()\n",
    "# #     datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "# #     dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "# #     dataset_name = dataset_info[\"name\"]\n",
    "# #     phs_id = dataset_info[\"phs_id\"]\n",
    "# #     consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "# #     auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "# #     src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "# # except:\n",
    "# #     dataset_name = \"\"\n",
    "# #     return \"Failure - Dataset Retrieval for T Pipeline\"\n",
    "# # if dataset_name:\n",
    "# #     params[\"dataset_id\"] = dataset_id\n",
    "# #     params[\"dataset_name\"] = dataset_name\n",
    "# #     params[\"phs_id\"] = phs_id\n",
    "# #     params[\"consent_name\"] = consent_name\n",
    "# #     params[\"auth_domains\"] = auth_domains\n",
    "# #     utils.run_t_pipeline(params)\n",
    "\n",
    "# # Return success message if no failures recorded\n",
    "# logging.info(\"Function completed successfully.\")\n",
    "# #return \"Success\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for idx, record in enumerate(records_list):\n",
    "#     if record[\"library_2_estimated_library_size\"]:\n",
    "#         print(str(idx) + \" - \" + str(record[\"library_2_estimated_library_size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# records_list[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new supplementary workspace files to TDR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to identify new supplementary files and ingest them to TDR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_supplementary_files(dataset_id):\n",
    "    \n",
    "    # Retrieve dataset details\n",
    "    logging.info(\"Retrieving dataset details.\")\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    dataset_details = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "    try:\n",
    "        source_workspaces = dataset_details[\"properties\"][\"source_workspaces\"]\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        return \"Failure - Issue Retrieving Dataset Info\"\n",
    "    \n",
    "    # Use source workspace(s) to find workspace bucket(s) to look for new files\n",
    "    logging.info(\"Determining source workspace bucket(s).\")\n",
    "    data_files_src_buckets = {}\n",
    "    for ws in source_workspaces:\n",
    "        try:\n",
    "            ws_attributes = utils.get_workspace_attributes(\"anvil-datastorage\", ws)\n",
    "            src_bucket = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "            if not src_bucket:\n",
    "                return \"Failure - Issue Retrieving Source Buckets\"\n",
    "            elif src_bucket not in data_files_src_buckets:\n",
    "                data_files_src_buckets[src_bucket] = {\n",
    "                    \"include_dirs\": [],\n",
    "                    \"exclude_dirs\": []\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return \"Failure - Issue Retrieving Source Buckets\"\n",
    "    \n",
    "    # Pull existing file inventory from BigQuery\n",
    "    logging.info(\"Pulling existing file inventory records.\")\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"SELECT uri FROM `{project}.{schema}.file_inventory`\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "    file_list = []\n",
    "    try:\n",
    "        output = client.query(query).result()\n",
    "        if output.total_rows > 0:\n",
    "            for row in output:\n",
    "                file_list.append(row.uri)\n",
    "    except Exception as e:\n",
    "            return \"Failure - Issue Retrieving Existing File Inventory Records\"\n",
    "        \n",
    "    # Build file inventory from workspace bucket(s)\n",
    "    logging.info(\"Building new file inventory.\")\n",
    "    params = {}\n",
    "    params[\"data_files_src_buckets\"] = data_files_src_buckets\n",
    "    params[\"google_project\"] = \"terra-349c8d95\"\n",
    "    params[\"file_inventory_dir\"] = \"ingest_pipeline/input/temp/data_files/file_inventory\"\n",
    "    inventory, retry_count = bfi.build_inventory(params)\n",
    "    \n",
    "    # Diff files to ingest\n",
    "    logging.info(\"Diffing new and existing file inventory records.\")\n",
    "    ingest_list = []\n",
    "    for file in inventory:\n",
    "        if file[\"uri\"] not in file_list:\n",
    "            ingest_list.append(file)\n",
    "    df_inventory = pd.DataFrame(ingest_list)\n",
    "    records_dict = df_inventory.to_dict(orient=\"records\")\n",
    "    return records_dict\n",
    "    \n",
    "    # Build, submit, and monitor ingest request\n",
    "    ingest_request = {\n",
    "        \"table\": \"file_inventory\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"array\",\n",
    "        \"load_tag\": \"Supplementary file ingest for {}\".format(dataset_id),\n",
    "        \"records\": records_dict\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            return \"Success\"\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on file relationships inference ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                return f\"Failure - Ingest error: {str(e)}\"\n",
    "    \n",
    "# # Loop through datasets and ingest additional files if necessary\n",
    "# dataset_id_list = [\n",
    "# 'd74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "# ]\n",
    "# results = []\n",
    "# for dataset_id in dataset_id_list:\n",
    "#     status = ingest_supplementary_files(dataset_id) \n",
    "#     results.append([dataset_id, status])\n",
    "#     results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "# display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_id = 'bf9108b6-bebc-4b3b-8517-6a2cce5f7d89'\n",
    "\n",
    "# Retrieve dataset details\n",
    "logging.info(\"Retrieving dataset details.\")\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "dataset_details = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "try:\n",
    "    source_workspaces = dataset_details[\"properties\"][\"source_workspaces\"]\n",
    "    bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "    bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "except Exception as e:\n",
    "    print(\"Failure - Issue Retrieving Dataset Info\") \n",
    "\n",
    "# Use source workspace(s) to find workspace bucket(s) to look for new files\n",
    "logging.info(\"Determining source workspace bucket(s).\")\n",
    "data_files_src_buckets = {}\n",
    "for ws in source_workspaces:\n",
    "    try:\n",
    "        ws_attributes = utils.get_workspace_attributes(\"anvil-datastorage\", ws)\n",
    "        src_bucket = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "        if not src_bucket:\n",
    "            print(\"Failure - Issue Retrieving Source Buckets\")\n",
    "        elif src_bucket not in data_files_src_buckets:\n",
    "            data_files_src_buckets[src_bucket] = {\n",
    "                \"include_dirs\": [],\n",
    "                \"exclude_dirs\": []\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(\"Failure - Issue Retrieving Source Buckets\")\n",
    "\n",
    "# Pull existing file inventory from BigQuery\n",
    "logging.info(\"Pulling existing file inventory records.\")\n",
    "client = bigquery.Client()\n",
    "query = \"\"\"SELECT uri FROM `{project}.{schema}.file_inventory`\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "print(query)\n",
    "file_list = []\n",
    "try:\n",
    "    output = client.query(query).result()\n",
    "    if output.total_rows > 0:\n",
    "        for row in output:\n",
    "            file_list.append(row.uri)\n",
    "except Exception as e:\n",
    "        print(\"Failure - Issue Retrieving Existing File Inventory Records\")\n",
    "\n",
    "# Build file inventory from workspace bucket(s)\n",
    "logging.info(\"Building new file inventory.\")\n",
    "params = {}\n",
    "params[\"data_files_src_buckets\"] = data_files_src_buckets\n",
    "params[\"google_project\"] = \"terra-349c8d95\"\n",
    "params[\"file_inventory_dir\"] = \"ingest_pipeline/input/temp/data_files/file_inventory\"\n",
    "inventory, retry_count = bfi.build_inventory(params)\n",
    "\n",
    "# Diff files to ingest\n",
    "logging.info(\"Diffing new and existing file inventory records.\")\n",
    "ingest_list = []\n",
    "for file in inventory:\n",
    "    if file[\"uri\"] not in file_list:\n",
    "        ingest_list.append(file)\n",
    "df_inventory = pd.DataFrame(ingest_list)\n",
    "records_list = df_inventory.to_dict(orient=\"records\")\n",
    "records_cnt = len(records_list)\n",
    "logging.info(f\"New file inventory records to ingest: {records_cnt}\")\n",
    "\n",
    "# Break records to ingest into chunks if necessary\n",
    "chunk_size = 100000\n",
    "chunk_cnt = math.ceil(records_cnt/chunk_size)\n",
    "for i in range(0, chunk_cnt):\n",
    "    if i == 0:\n",
    "        start_row = 0\n",
    "        end_row = chunk_size\n",
    "    else:\n",
    "        start_row = (i*chunk_size) + 1\n",
    "        end_row = min((i+1)*chunk_size, records_cnt)\n",
    "    # Write out chunk to file for ingest\n",
    "    destination_file = \"file_inventory_\" + str(i) + \".json\"\n",
    "    with open(destination_file, \"w\") as outfile:\n",
    "        for idx, val in enumerate(records_list):\n",
    "            if idx >= start_row and idx <= end_row:\n",
    "                json.dump(val, outfile)\n",
    "                if idx < end_row:\n",
    "                    outfile.write(\"\\n\")\n",
    "    !gsutil cp $destination_file $ws_bucket/ingest_pipeline/input/temp 2> stdout   \n",
    "    # Build, submit, and monitor ingest request\n",
    "    logging.info(f\"Ingesting new file inventory records into TDR (chunk #{i}).\")\n",
    "    ingest_request = {\n",
    "        \"table\": \"file_inventory\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"json\",\n",
    "        \"load_tag\": \"Supplementary file ingest for {}\".format(dataset_id),\n",
    "        \"bulkMode\": True,\n",
    "        \"path\": f\"{ws_bucket}/ingest_pipeline/input/temp/{destination_file}\"\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            print(\"Success\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on new file inventory records ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Failure - Ingest error (chunk #{i}): {str(e)}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
