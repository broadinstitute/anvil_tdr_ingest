{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional modules (one time effort per cloud environment)\n",
    "!pip install --upgrade import_ipynb data_repo_client urllib3 xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ingest_pipeline_utilities.ipynb\n",
      "Version 1.0.25: 3/10/2023 8:46am - Nate Calvanese - Turned on the predictable file IDs dataset creation parameter\n",
      "importing Jupyter notebook from source_files_creation.ipynb\n",
      "Version 1.0.9: 2/25/2023 3:15pm - Nate Calvanese - Replaced FAPI with utils functions\n",
      "importing Jupyter notebook from build_file_inventory.ipynb\n",
      "Version 2.0.0: 3/7/2022 9:32pm - Nate Calvanese - Massive performance improvement with use of gsutil parsing\n",
      "importing Jupyter notebook from process_table_data.ipynb\n",
      "Version: 1.0.9: 3/8/2023 12:09pm - Nate Calvanese - Performance improvements for file ref lookups\n",
      "importing Jupyter notebook from build_mapping_query.ipynb\n",
      "Version 1.0.12: 2/21/2023 2:50pm - Nate Calvanese - Added support for $BQ_DATASET substitution variable\n",
      "importing Jupyter notebook from output_data_validation.ipynb\n",
      "Version 2.0.6: 2/28/2023 11:33am -- Updated notebook to be usable in dev (removed TDR host hardcoding)\n",
      "importing Jupyter notebook from resolve_dangling_foreign_keys.ipynb\n",
      "Version 1.0.1: 10/24/2022 3:18pm - Nate Calvanese - Added pass through of params dict\n",
      "importing Jupyter notebook from infer_file_relationships.ipynb\n",
      "Version 1.0.1: 11/8/2022 4:03pm - Nate Calvanese - Expanded query to cover additional cases\n",
      "importing Jupyter notebook from identify_supplementary_files.ipynb\n",
      "Version 1.0.0: 2/22/2023 8:30pm - Nate Calvanese - Initial version\n"
     ]
    }
   ],
   "source": [
    "## imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import data_repo_client\n",
    "from google.cloud import bigquery\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_file_inventory as bfi\n",
    "import logging\n",
    "from time import sleep\n",
    "import datetime\n",
    "from google.cloud import storage\n",
    "import math\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create new snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to create new full view snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/08/2023 02:51:43 PM - INFO: Creating full-view snapshot.\n",
      "03/08/2023 02:51:43 PM - INFO: Attempting to lookup consent code using PHS: 693 and Consent Name: DS-BAV-IRB-PUB-PD.\n",
      "03/08/2023 02:51:44 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: 1O3X4HkcQmWYRPED-mDb_g\n",
      "03/08/2023 02:52:55 PM - INFO: Snapshot Creation succeeded: {'id': 'e7cba2c4-6b44-4d70-9449-472a1e095a65', 'name': 'ANVIL_CMG_UWASH_DS_BAV_IRB_PUB_RD_20221020_ANV5_202303081451', 'description': 'Full view snapshot of ANVIL_CMG_UWASH_DS_BAV_IRB_PUB_RD_20221020', 'createdDate': '2023-03-08T14:52:04.020771Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'DS-BAV-IRB-PUB-PD', 'phsId': 'phs000693', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-aa67671a', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}\n",
      "03/08/2023 02:52:56 PM - INFO: Creating full-view snapshot.\n",
      "03/08/2023 02:52:56 PM - INFO: Attempting to lookup consent code using PHS: 693 and Consent Name: DS-EP.\n",
      "03/08/2023 02:52:56 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: w6gWCSgHSxazOPzcBTSS_g\n",
      "03/08/2023 02:54:18 PM - INFO: Snapshot Creation succeeded: {'id': '388f45e2-a4ef-48e5-9b69-0128db5a25e2', 'name': 'ANVIL_CMG_UWash_DS_EP_20221020_ANV5_202303081452', 'description': 'Full view snapshot of ANVIL_CMG_UWash_DS_EP_20221020', 'createdDate': '2023-03-08T14:53:17.214689Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'c2', 'phsId': 'phs000693', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-080b2c9e', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}\n",
      "03/08/2023 02:54:18 PM - INFO: Creating full-view snapshot.\n",
      "03/08/2023 02:54:18 PM - INFO: Attempting to lookup consent code using PHS: 693 and Consent Name: HMB-IRB.\n",
      "03/08/2023 02:54:18 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: sPx66rq5R6mpQQMB4pwqVg\n",
      "03/08/2023 02:55:30 PM - INFO: Snapshot Creation succeeded: {'id': '293429af-d91d-4af7-8d8b-cb33aab4a055', 'name': 'ANVIL_CMG_UWASH_HMB_IRB_20221020_ANV5_202303081454', 'description': 'Full view snapshot of ANVIL_CMG_UWASH_HMB_IRB_20221020', 'createdDate': '2023-03-08T14:54:38.463784Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'c28', 'phsId': 'phs000693', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-3d4c42f7', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}\n",
      "03/08/2023 02:55:30 PM - INFO: Creating full-view snapshot.\n",
      "03/08/2023 02:55:30 PM - INFO: Attempting to lookup consent code using PHS: 693 and Consent Name: HMB.\n",
      "03/08/2023 02:55:30 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: CpLQru7RRauY0Ke5_EIhOQ\n",
      "03/08/2023 02:56:51 PM - INFO: Snapshot Creation succeeded: {'id': '32427168-2013-43bb-a100-89e1b38c8998', 'name': 'ANVIL_CMG_UWASH_HMB_20221020_ANV5_202303081455', 'description': 'Full view snapshot of ANVIL_CMG_UWASH_HMB_20221020', 'createdDate': '2023-03-08T14:55:48.992326Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'c9', 'phsId': 'phs000693', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-6a5b13ea', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}\n",
      "03/08/2023 02:56:51 PM - INFO: Creating full-view snapshot.\n",
      "03/08/2023 02:56:51 PM - INFO: Attempting to lookup consent code using PHS: 693 and Consent Name: DS-HFA.\n",
      "03/08/2023 02:56:51 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: _ErSF0x4RlyM31I9glPUCw\n",
      "03/08/2023 02:58:02 PM - INFO: Snapshot Creation succeeded: {'id': '1949b996-2f50-4c66-8656-ee1aea6c9b80', 'name': 'ANVIL_CMG_UWASH_DS_HFA_20221020_ANV5_202303081456', 'description': 'Full view snapshot of ANVIL_CMG_UWASH_DS_HFA_20221020', 'createdDate': '2023-03-08T14:57:10.114881Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'c21', 'phsId': 'phs000693', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-333dd883', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}\n",
      "03/08/2023 02:58:02 PM - INFO: Creating full-view snapshot.\n",
      "03/08/2023 02:58:02 PM - INFO: Attempting to lookup consent code using PHS: 693 and Consent Name: GRU-IRB.\n",
      "03/08/2023 02:58:02 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: v42zY7zITe--i2d0IQUK5g\n",
      "03/08/2023 02:59:34 PM - INFO: Snapshot Creation succeeded: {'id': '19997cd4-25ed-46a9-be6c-77049f1c74eb', 'name': 'ANVIL_CMG_UWash_GRU_IRB_20221020_ANV5_202303081458', 'description': 'Full view snapshot of ANVIL_CMG_UWash_GRU_IRB_20221020', 'createdDate': '2023-03-08T14:58:21.909014Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'c30', 'phsId': 'phs000693', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-ec9365be', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}\n",
      "03/08/2023 02:59:34 PM - INFO: Creating full-view snapshot.\n",
      "03/08/2023 02:59:34 PM - INFO: Attempting to lookup consent code using PHS: 693 and Consent Name: DS-NBIA.\n",
      "03/08/2023 02:59:34 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: tdhaG-srSmOu37qakp7HfQ\n",
      "03/08/2023 03:01:15 PM - INFO: Snapshot Creation succeeded: {'id': '392d09d1-3e69-46bc-84be-679e7bf52d1a', 'name': 'ANVIL_CMG_UWASH_DS_NBIA_20221020_ANV5_202303081459', 'description': 'Full view snapshot of ANVIL_CMG_UWASH_DS_NBIA_20221020', 'createdDate': '2023-03-08T14:59:52.872407Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'c27', 'phsId': 'phs000693', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-b968cbdb', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}\n",
      "03/08/2023 03:01:15 PM - INFO: Creating full-view snapshot.\n",
      "03/08/2023 03:01:15 PM - INFO: Attempting to lookup consent code using PHS: 693 and Consent Name: DS-BDIS.\n",
      "03/08/2023 03:01:15 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: sx2QITIHTSep5TGsnTQq9A\n",
      "03/08/2023 03:02:26 PM - INFO: Snapshot Creation succeeded: {'id': '2ae00e5c-4aef-4a1e-9eca-d8d0747b5348', 'name': 'ANVIL_CMG_UWASH_DS_BDIS_20221020_ANV5_202303081501', 'description': 'Full view snapshot of ANVIL_CMG_UWASH_DS_BDIS_20221020', 'createdDate': '2023-03-08T15:01:34.357766Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'c34', 'phsId': 'phs000693', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-b4e0bfd5', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/08/2023 03:02:26 PM - INFO: Creating full-view snapshot.\n",
      "03/08/2023 03:02:26 PM - INFO: Attempting to lookup consent code using PHS: 424 and Consent Name: GRU.\n",
      "03/08/2023 03:02:26 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: vB1Kkk4tRQK02UFuxi8Stg\n",
      "03/08/2023 03:22:55 PM - ERROR: Error on Snapshot Creation: Job vB1Kkk4tRQK02UFuxi8Stg failed: (500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 08 Mar 2023 15:22:55 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'eyR5B5MD', 'Content-Type': 'application/json', 'Content-Length': '74', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Failed operation with no exception reported\",\"errorDetail\":[]}\n",
      "\n",
      "03/08/2023 03:22:55 PM - INFO: Retrying Snapshot Creation (attempt #1)...\n",
      "TDR Job ID: dnTt8jHkRxaFI_-z64pPrA\n",
      "03/08/2023 03:23:37 PM - ERROR: Error on Snapshot Creation: Job dnTt8jHkRxaFI_-z64pPrA failed: (400)\n",
      "Reason: \n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 08 Mar 2023 15:23:37 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'eObPx4n8', 'Content-Type': 'application/json', 'Content-Length': '150', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Snapshot name or id already exists: ANVIL_GTEx_V8_hg38_20221013_ANV5_202303081502, 3b4d8ed1-2108-4b5d-9d4d-7ad8f9d4233d\",\"errorDetail\":[]}\n",
      "\n",
      "03/08/2023 03:23:37 PM - ERROR: Maximum number of retries exceeded. Recording error to pipeline results.\n",
      "03/08/2023 03:23:37 PM - INFO: Creating full-view snapshot.\n",
      "03/08/2023 03:23:37 PM - WARNING: Unable to lookup consent code. Consent name and/or PHS ID missing for lookup.\n",
      "03/08/2023 03:23:37 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: UBTqvZj9Tf6CZ6eYfnJ28g\n",
      "03/08/2023 03:27:29 PM - INFO: Snapshot Creation succeeded: {'id': '0377b6a0-a203-46f3-a4e4-0238b21ce141', 'name': 'ANVIL_1000G_high_coverage_2019_20221019_ANV5_202303081523', 'description': 'Full view snapshot of ANVIL_1000G_high_coverage_2019_20221019', 'createdDate': '2023-03-08T15:23:56.638368Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'NRES', 'phsId': '', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-1ba591a6', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>run_status</th>\n",
       "      <th>snapshot_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77ae9412-cd65-4bfb-9afb-29b77280ac33</td>\n",
       "      <td>Success</td>\n",
       "      <td>e7cba2c4-6b44-4d70-9449-472a1e095a65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9613ea77-4dd4-457c-b8b8-8d5fcde8fe20</td>\n",
       "      <td>Success</td>\n",
       "      <td>388f45e2-a4ef-48e5-9b69-0128db5a25e2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f3d925d8-3aeb-49d0-8157-a6fa4c1a5056</td>\n",
       "      <td>Success</td>\n",
       "      <td>293429af-d91d-4af7-8d8b-cb33aab4a055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1fe3865a-429f-43a1-bd19-1ebf023f452a</td>\n",
       "      <td>Success</td>\n",
       "      <td>32427168-2013-43bb-a100-89e1b38c8998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f7b1556b-950d-4f75-b570-c84f3d760119</td>\n",
       "      <td>Success</td>\n",
       "      <td>1949b996-2f50-4c66-8656-ee1aea6c9b80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f9b0ceaf-2a8f-4cfd-8d36-2107f47393cf</td>\n",
       "      <td>Success</td>\n",
       "      <td>19997cd4-25ed-46a9-be6c-77049f1c74eb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ef573845-3b10-48e6-8201-9579fa26d0e1</td>\n",
       "      <td>Success</td>\n",
       "      <td>392d09d1-3e69-46bc-84be-679e7bf52d1a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f1e1ef01-d52d-423e-a65b-3a1d26c7ee9d</td>\n",
       "      <td>Success</td>\n",
       "      <td>2ae00e5c-4aef-4a1e-9eca-d8d0747b5348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>024c790a-3370-49b8-bef2-aee12afbd45e</td>\n",
       "      <td>Error</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bf9108b6-bebc-4b3b-8517-6a2cce5f7d89</td>\n",
       "      <td>Success</td>\n",
       "      <td>0377b6a0-a203-46f3-a4e4-0238b21ce141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dataset_id              run_status              snapshot_id             \n",
       "0  77ae9412-cd65-4bfb-9afb-29b77280ac33   Success   e7cba2c4-6b44-4d70-9449-472a1e095a65\n",
       "1  9613ea77-4dd4-457c-b8b8-8d5fcde8fe20   Success   388f45e2-a4ef-48e5-9b69-0128db5a25e2\n",
       "2  f3d925d8-3aeb-49d0-8157-a6fa4c1a5056   Success   293429af-d91d-4af7-8d8b-cb33aab4a055\n",
       "3  1fe3865a-429f-43a1-bd19-1ebf023f452a   Success   32427168-2013-43bb-a100-89e1b38c8998\n",
       "4  f7b1556b-950d-4f75-b570-c84f3d760119   Success   1949b996-2f50-4c66-8656-ee1aea6c9b80\n",
       "5  f9b0ceaf-2a8f-4cfd-8d36-2107f47393cf   Success   19997cd4-25ed-46a9-be6c-77049f1c74eb\n",
       "6  ef573845-3b10-48e6-8201-9579fa26d0e1   Success   392d09d1-3e69-46bc-84be-679e7bf52d1a\n",
       "7  f1e1ef01-d52d-423e-a65b-3a1d26c7ee9d   Success   2ae00e5c-4aef-4a1e-9eca-d8d0747b5348\n",
       "8  024c790a-3370-49b8-bef2-aee12afbd45e     Error                                       \n",
       "9  bf9108b6-bebc-4b3b-8517-6a2cce5f7d89   Success   0377b6a0-a203-46f3-a4e4-0238b21ce141"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {}\n",
    "params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\"]\n",
    "params[\"anvil_schema_version\"] = \"ANV5\"\n",
    "\n",
    "# Loop through datasets and create new snapshot\n",
    "dataset_id_run_list = [\n",
    "    #'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "    '77ae9412-cd65-4bfb-9afb-29b77280ac33',\n",
    "    '9613ea77-4dd4-457c-b8b8-8d5fcde8fe20',\n",
    "    'f3d925d8-3aeb-49d0-8157-a6fa4c1a5056',\n",
    "    '1fe3865a-429f-43a1-bd19-1ebf023f452a',\n",
    "    'f7b1556b-950d-4f75-b570-c84f3d760119',\n",
    "    'f9b0ceaf-2a8f-4cfd-8d36-2107f47393cf',\n",
    "    'ef573845-3b10-48e6-8201-9579fa26d0e1',\n",
    "    'f1e1ef01-d52d-423e-a65b-3a1d26c7ee9d',\n",
    "    '024c790a-3370-49b8-bef2-aee12afbd45e',\n",
    "    'bf9108b6-bebc-4b3b-8517-6a2cce5f7d89'\n",
    "]\n",
    "results = []\n",
    "for dataset in dataset_id_run_list:\n",
    "    dataset_id = dataset\n",
    "    try:\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        dataset_name = dataset_info[\"name\"]\n",
    "        phs_id = dataset_info[\"phs_id\"]\n",
    "        consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "        auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "        src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "    except:\n",
    "        dataset_name = \"\"\n",
    "    if dataset_name:\n",
    "        params[\"ws_bucket\"] = ws_bucket\n",
    "        params[\"dataset_id\"] = dataset_id\n",
    "        params[\"dataset_name\"] = dataset_name\n",
    "        params[\"phs_id\"] = phs_id\n",
    "        params[\"consent_name\"] = consent_name\n",
    "        params[\"auth_domains\"] = auth_domains\n",
    "        params[\"pipeline_results\"] = []\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "        params[\"snapshot_name\"] = params[\"dataset_name\"] + \"_\" + params[\"anvil_schema_version\"] + \"_\" + current_datetime_string \n",
    "        utils.create_and_share_snapshot(params)\n",
    "        int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "        errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "        if len(errors) > 0:\n",
    "            results.append([dataset_id, \"Error\", \"\"])\n",
    "        else:\n",
    "            snapshot_id = re.search(\"{'id': '([a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12})'\", str(int_df_results[int_df_results[\"Task\"]==\"Create and Share Snapshot\"][\"Message\"]))[1]\n",
    "            results.append([dataset_id, \"Success\", snapshot_id])\n",
    "results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\", \"snapshot_id\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Add and populate anvil_file.is_supplementary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to patch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_and_populate_supp_file_flg(dataset_id):\n",
    "    logging.info(f\"Processing anvil_file.is_supplementary for Dataset ID = {dataset_id}\")\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    logging.info(\"Retrieving necessary information from TDR.\")\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving information from TDR. Exiting function. Error: {}\".format(e))\n",
    "        return \"Failure\"\n",
    "    \n",
    "    # Determine if field needs to be added, and add if so\n",
    "    logging.info(\"Adding anvil_file.is_supplementary to dataset schema, if necessary.\")\n",
    "    field_found = False\n",
    "    for table in src_schema_dict[\"tables\"]:\n",
    "        if table[\"name\"] == \"anvil_file\":\n",
    "            for col in table[\"columns\"]:\n",
    "                if col[\"name\"] == \"is_supplementary\":\n",
    "                    field_found = True\n",
    "                    logging.info(\"Field already found! Skipping schema update.\")\n",
    "                    break\n",
    "            break\n",
    "    if field_found == False:\n",
    "        logging.info(\"Field not found. Running dataset schema update.\")\n",
    "        schema_update_request = {\n",
    "            \"description\": \"Adding is_supplementary column to anvil_file\",\n",
    "            \"changes\": {\n",
    "                \"addColumns\": [\n",
    "                  {\n",
    "                    \"tableName\": \"anvil_file\",\n",
    "                    \"columns\": [\n",
    "                      {\n",
    "                        \"name\": \"is_supplementary\",\n",
    "                        \"datatype\": \"boolean\",\n",
    "                        \"array_of\": False,\n",
    "                        \"required\": False\n",
    "                      }\n",
    "                    ]\n",
    "                  }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                schema_update_result, job_id = utils.wait_for_tdr_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "                logging.info(\"Dataset schema update succeeded!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error on dataset schema update: {}\".format(str(e)))\n",
    "                attempt_counter += 1\n",
    "                if attempt_counter < 2:\n",
    "                    logging.info(\"Retrying dataset schema update (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(15)\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Unable to update dataset schema. Exiting function.\")\n",
    "                    return \"Failure\"\n",
    "        \n",
    "    # Re-process anvil_file data to include is_supplementary (where appropriate) and ingest into TDR dataset (as replace)\n",
    "    logging.info(\"Re-processing existing anvil_file data to include is_supplementary value.\")\n",
    "    client = bigquery.Client()\n",
    "    target_file = \"anvil_file.json\"\n",
    "    destination_dir = \"ingest_pipeline/output/transformed/anvil/{}/table_data\".format(dataset_id)\n",
    "    query = \"\"\"BEGIN\n",
    "        \n",
    "        CREATE TEMPORARY TABLE activity_exp AS WITH activity_agg\n",
    "        AS\n",
    "        (\n",
    "          SELECT used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_activity`\n",
    "          UNION ALL \n",
    "          SELECT [] AS used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_alignmentactivity`\n",
    "          UNION ALL \n",
    "          SELECT used_biosample_id, generated_file_id, [] AS used_file_id FROM `{project}.{dataset}.anvil_assayactivity`\n",
    "          UNION ALL \n",
    "          SELECT used_biosample_id, generated_file_id, [] AS used_file_id FROM `{project}.{dataset}.anvil_sequencingactivity`\n",
    "          UNION ALL \n",
    "          SELECT [] AS used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_variantcallingactivity`\n",
    "        )\n",
    "        SELECT file_id, int_file_id, biosample_id\n",
    "        FROM activity_agg\n",
    "            LEFT JOIN UNNEST(used_biosample_id) AS biosample_id\n",
    "            LEFT JOIN UNNEST(generated_file_id) as file_id\n",
    "            LEFT JOIN UNNEST(used_file_id) as int_file_id\n",
    "        ;\n",
    "        \n",
    "        CREATE TEMPORARY TABLE act_exp_lookup\n",
    "        AS\n",
    "        (\n",
    "            SELECT file_id, MAX(biosample_id) AS biosample_id\n",
    "          FROM\n",
    "          (\n",
    "            --Level 1:\n",
    "            SELECT file_id, biosample_id\n",
    "            FROM activity_exp\n",
    "            WHERE int_file_id IS NULL AND file_id IS NOT NULl AND biosample_id IS NOT NULL\n",
    "            --Level 2:\n",
    "            UNION ALL\n",
    "            SELECT a2.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "            WHERE a2.int_file_id IS NOT NULL AND a2.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "            --Level 3:\n",
    "            UNION ALL\n",
    "            SELECT a3.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "              LEFT JOIN activity_exp a3\n",
    "              ON a2.file_id = a3.int_file_id\n",
    "            WHERE a3.int_file_id IS NOT NULL AND a3.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "            --Level 4:\n",
    "            UNION ALL\n",
    "            SELECT a4.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "              LEFT JOIN activity_exp a3\n",
    "              ON a2.file_id = a3.int_file_id\n",
    "              LEFT JOIN activity_exp a4\n",
    "              ON a3.file_id = a4.int_file_id\n",
    "            WHERE a4.int_file_id IS NOT NULL AND a4.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "            --Level 5:\n",
    "            UNION ALL\n",
    "            SELECT a5.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "              LEFT JOIN activity_exp a3\n",
    "              ON a2.file_id = a3.int_file_id\n",
    "              LEFT JOIN activity_exp a4\n",
    "              ON a3.file_id = a4.int_file_id\n",
    "              LEFT JOIN activity_exp a5\n",
    "              ON a4.file_id = a5.int_file_id\n",
    "            WHERE a5.int_file_id IS NOT NULL AND a5.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "          )\n",
    "          GROUP BY file_id\n",
    "        );\n",
    "        \n",
    "        SELECT t1.file_id, data_modality, file_format, file_size, file_md5sum, reference_assembly, file_name, file_ref, source_datarepo_row_ids,\n",
    "        CASE WHEN t2.biosample_id IS NULL THEN TRUE ELSE FALSE END AS is_supplementary\n",
    "        FROM `{project}.{dataset}.anvil_file` t1\n",
    "          LEFT JOIN act_exp_lookup t2\n",
    "          ON t1.file_id = t2.file_id\n",
    "        ;\n",
    "        \n",
    "        END\n",
    "        \"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        df = client.query(query).result().to_dataframe()\n",
    "        records_json = df.to_json(orient='records') \n",
    "        records_list = json.loads(records_json)\n",
    "        records_cnt = len(records_list)\n",
    "        with open(target_file, 'w') as outfile:\n",
    "            for idx, val in enumerate(records_list):\n",
    "                json.dump(val, outfile)\n",
    "                if idx < (records_cnt - 1):\n",
    "                    outfile.write('\\n')\n",
    "        !gsutil cp $target_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "        !rm $target_file\n",
    "        logging.info(\"Successfully created new anvil_file.json file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error creating new anvil_file.json file. Exiting function. Error: {}\".format(str(e)))\n",
    "        return \"Failure\"\n",
    "\n",
    "    # Ingest updated anvil_file data\n",
    "    logging.info(\"Ingesting updated anvil_file data into TDR dataset.\")\n",
    "    source_full_file_path = \"{}/{}/{}\".format(ws_bucket, destination_dir, \"anvil_file.json\")\n",
    "    ingest_request = {\n",
    "        \"table\": \"anvil_file\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"json\",\n",
    "        \"load_tag\": \"Ingest for {}\".format(dataset_id),\n",
    "        \"path\": source_full_file_path\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"Ingest from file anvil_file.json succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Dataset Ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Dataset Ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Exiting function.\")\n",
    "                return \"Failure\"\n",
    "    \n",
    "    # Return success message if no failures recorded\n",
    "    logging.info(\"Function completed successfully.\")\n",
    "    return \"Success\"\n",
    "\n",
    "# Loop through datasets and process is_supplementary field\n",
    "dataset_id_list = [\n",
    "'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = add_and_populate_supp_file_flg(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "display(results_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to validate patch worked properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_supp_file_flg(dataset_id):\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        return \"Failure - Issue Retrieving Dataset Info\"\n",
    "    \n",
    "    # Determine if field exists for dataset, continue if so, fail otherwise\n",
    "    field_found = False\n",
    "    for table in src_schema_dict[\"tables\"]:\n",
    "        if table[\"name\"] == \"anvil_file\":\n",
    "            for col in table[\"columns\"]:\n",
    "                if col[\"name\"] == \"is_supplementary\":\n",
    "                    field_found = True\n",
    "                    break\n",
    "            break\n",
    "    if field_found == False:\n",
    "        return \"Failure - is_supplementary field not found\"\n",
    "    else:\n",
    "        client = bigquery.Client()\n",
    "        query = \"\"\"SELECT COUNT(*) AS rec_cnt, COUNT(is_supplementary) AS populated_cnt\n",
    "                    FROM `{project}.{dataset}.anvil_file`\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "        try:\n",
    "            df = client.query(query).result().to_dataframe()\n",
    "            if df[\"rec_cnt\"].values[0] == df[\"populated_cnt\"].values[0]:\n",
    "                return \"Success\"\n",
    "        except Exception as e:\n",
    "            return \"Failure - BigQuery Error\"\n",
    "\n",
    "# Loop through datasets and validate is_supplementary field\n",
    "dataset_id_list = [\n",
    "'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = validate_supp_file_flg(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"validation_status\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Attempt to populate anvil_donor.organism_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to patch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def populate_organism_type(dataset_id):\n",
    "    logging.info(f\"Processing anvil_donor.organism_type for Dataset ID = {dataset_id}\")\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    logging.info(\"Retrieving necessary information from TDR.\")\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving information from TDR. Exiting function. Error: {}\".format(e))\n",
    "        return \"Failure\"\n",
    "\n",
    "    # Re-process anvil_donor data to include organism_type (where available)\n",
    "    logging.info(\"Re-processing existing anvil_donor data to include organism_type value.\")\n",
    "    client = bigquery.Client()\n",
    "    target_file = \"anvil_donor.json\"\n",
    "    destination_dir = \"ingest_pipeline/output/transformed/anvil/{}/table_data\".format(dataset_id)\n",
    "    query = \"\"\"SELECT donor_id, \n",
    "    (SELECT MAX(CASE WHEN REGEXP_CONTAINS(value, '(h37|h38|h39|hg16|hg17|hg18|hg19|hs37|hs38|b37)') THEN 'Homo sapiens' END) AS organism_type FROM `{project}.{dataset}.workspace_attributes` WHERE attribute = 'library:reference') AS organism_type,\n",
    "    part_of_dataset_id, phenotypic_sex, reported_ethnicity, genetic_ancestry, source_datarepo_row_ids\n",
    "    FROM `{project}.{dataset}.anvil_donor`\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        df = client.query(query).result().to_dataframe()\n",
    "        records_json = df.to_json(orient='records') \n",
    "        records_list = json.loads(records_json)\n",
    "        records_cnt = len(records_list)\n",
    "        with open(target_file, 'w') as outfile:\n",
    "            for idx, val in enumerate(records_list):\n",
    "                json.dump(val, outfile)\n",
    "                if idx < (records_cnt - 1):\n",
    "                    outfile.write('\\n')\n",
    "        !gsutil cp $target_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "        !rm $target_file\n",
    "        logging.info(\"Successfully created new anvil_donor.json file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error creating new anvil_donor.json file. Exiting function. Error: {}\".format(str(e)))\n",
    "        return \"Failure\"\n",
    "\n",
    "    # Ingest updated anvil_donor data\n",
    "    logging.info(\"Ingesting updated anvil_donor data into TDR dataset.\")\n",
    "    source_full_file_path = \"{}/{}/{}\".format(ws_bucket, destination_dir, \"anvil_donor.json\")\n",
    "    ingest_request = {\n",
    "        \"table\": \"anvil_donor\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"json\",\n",
    "        \"load_tag\": \"Ingest for {}\".format(dataset_id),\n",
    "        \"path\": source_full_file_path\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"Ingest from file anvil_donor.json succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Dataset Ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Dataset Ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Exiting function.\")\n",
    "                return \"Failure\"\n",
    "\n",
    "    # Return success message if no failures recorded\n",
    "    logging.info(\"Function completed successfully.\")\n",
    "    return \"Success\"\n",
    "\n",
    "# Loop through datasets and process supplementary_file_flag\n",
    "dataset_id_list = [\n",
    "'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = populate_organism_type(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to examine organism_type population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_organism_type(dataset_id):\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        return \"Failure - Issue Retrieving Dataset Info\"\n",
    "    \n",
    "    # Determine if field exists for dataset, continue if so, fail otherwise\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"SELECT COUNT(organism_type) AS populated_cnt\n",
    "                FROM `{project}.{dataset}.anvil_donor`\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        df = client.query(query).result().to_dataframe()\n",
    "        if df[\"populated_cnt\"].values[0] > 0:\n",
    "            return \"Success - Field Populated\"\n",
    "        else:\n",
    "            return \"Success - Field Not Populated\"\n",
    "    except Exception as e:\n",
    "        return \"Failure - BigQuery Error\"\n",
    "\n",
    "# Loop through datasets and validate is_supplementary field\n",
    "dataset_id_list = [\n",
    "'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = validate_organism_type(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"validation_status\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update references to md5-added files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/10/2023 08:45:58 PM - INFO: Processing md5-added files for Dataset ID = 2f2de074-0e24-4b70-b3d2-83afacb28f7f\n",
      "03/10/2023 08:45:58 PM - INFO: Retrieving necessary information from TDR.\n",
      "03/10/2023 08:45:58 PM - INFO: Processing updates for sample.\n",
      "03/10/2023 08:46:02 PM - INFO: 2908 records to process.\n",
      "03/10/2023 08:46:03 PM - INFO: Submitting ingest for updated sample records.\n",
      "TDR Job ID: WaQ0I1FDSwWuBP8cJdVlyg\n",
      "03/10/2023 09:44:07 PM - ERROR: Error on ingest: Job WaQ0I1FDSwWuBP8cJdVlyg failed: (400)\n",
      "Reason: \n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Fri, 10 Mar 2023 21:44:06 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'a0JdAR7n', 'Content-Type': 'application/json', 'Content-Length': '922', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Ingest failed with 3 errors - see error details\",\"errorDetail\":[\"BigQueryError: reason=invalid message=Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 33; errors: 1. Please look into the errors[] collection for more details. File: gs://datarepo-ff5b78c7-bq-scratch-bucket/WaQ0I1FDSwWuBP8cJdVlyg/ingest-scratch.json\",\"BigQueryError: reason=invalid message=Error while reading data, error message: JSON processing encountered too many errors, giving up. Rows: 33; errors: 1; max bad: 0; error percent: 0\",\"BigQueryError: reason=invalid message=Error while reading data, error message: JSON parsing error in row starting at position 75523: Could not convert value 'number_value: \\t \\\"3.4270905E7\\\"' to integer. Field: library_2_estimated_library_size; Value: 3.4270905E7 File: gs://datarepo-ff5b78c7-bq-scratch-bucket/WaQ0I1FDSwWuBP8cJdVlyg/ingest-scratch.json\"]}\n",
      "\n",
      "03/10/2023 09:44:07 PM - ERROR: Maximum number of retries exceeded. Logging error.\n",
      "03/10/2023 09:44:07 PM - ERROR: Error replacing TDR records: Job WaQ0I1FDSwWuBP8cJdVlyg failed: (400)\n",
      "Reason: \n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Fri, 10 Mar 2023 21:44:06 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'a0JdAR7n', 'Content-Type': 'application/json', 'Content-Length': '922', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Ingest failed with 3 errors - see error details\",\"errorDetail\":[\"BigQueryError: reason=invalid message=Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 33; errors: 1. Please look into the errors[] collection for more details. File: gs://datarepo-ff5b78c7-bq-scratch-bucket/WaQ0I1FDSwWuBP8cJdVlyg/ingest-scratch.json\",\"BigQueryError: reason=invalid message=Error while reading data, error message: JSON processing encountered too many errors, giving up. Rows: 33; errors: 1; max bad: 0; error percent: 0\",\"BigQueryError: reason=invalid message=Error while reading data, error message: JSON parsing error in row starting at position 75523: Could not convert value 'number_value: \\t \\\"3.4270905E7\\\"' to integer. Field: library_2_estimated_library_size; Value: 3.4270905E7 File: gs://datarepo-ff5b78c7-bq-scratch-bucket/WaQ0I1FDSwWuBP8cJdVlyg/ingest-scratch.json\"]}\n",
      "\n",
      "03/10/2023 09:44:07 PM - INFO: Processing md5-added files for Dataset ID = ab76b5ca-e464-4063-b949-853f61036370\n",
      "03/10/2023 09:44:07 PM - INFO: Retrieving necessary information from TDR.\n",
      "03/10/2023 09:44:07 PM - INFO: Processing updates for sample.\n",
      "03/10/2023 09:44:10 PM - INFO: 17 records to process.\n",
      "03/10/2023 09:44:11 PM - INFO: Submitting ingest for updated sample records.\n",
      "TDR Job ID: VGBLtxetRTyVdS1dK10oBw\n",
      "03/10/2023 09:45:22 PM - INFO: Ingest succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'sample', 'path': None, 'load_tag': 'File ref fields patch for sample in ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 17, 'bad_row_count': 0, 'load_result': {'loadSummary': {'loadTag': 'File ref fields patch for sample in ab76b5ca-e464-4063-b949-853f61036370', 'jobId': 'VGBLtxetRTyVdS1dK10oBw', 'totalFiles': 0, 'succeededFiles': 0, 'failedFiles': 0, 'notTriedFiles': 0}, 'loadFileResults': []}}\n",
      "03/10/2023 09:45:22 PM - INFO: Attempting to delete original sample records.\n",
      "TDR Job ID: y3wkjr4SQQCSjVGoufC4PA\n",
      "03/10/2023 09:45:32 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/10/2023 09:45:32 PM - INFO: Processing updates for file_inventory.\n",
      "03/10/2023 09:45:33 PM - INFO: 18 records to process.\n",
      "03/10/2023 09:45:35 PM - INFO: Submitting ingest for updated file_inventory records.\n",
      "TDR Job ID: TxK_VeqxTtiiPENx5LgJ2w\n",
      "03/10/2023 09:46:06 PM - INFO: Ingest succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'file_inventory', 'path': None, 'load_tag': 'File ref fields patch for file_inventory in ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 18, 'bad_row_count': 0, 'load_result': None}\n",
      "03/10/2023 09:46:06 PM - INFO: Attempting to delete original file_inventory records.\n",
      "TDR Job ID: qshz2zx3Timg_EZytNDPFA\n",
      "03/10/2023 09:46:17 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/10/2023 09:46:17 PM - INFO: Clearing out existing anvil_% tables\n",
      "03/10/2023 09:46:18 PM - INFO: Attempting to delete original anvil_activity records.\n",
      "TDR Job ID: 3NeSqKnXSie4fuJmqofXVQ\n",
      "03/10/2023 09:46:28 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/10/2023 09:46:34 PM - INFO: Attempting to delete original anvil_biosample records.\n",
      "TDR Job ID: tTBADrNHRlesMxza1ESDaA\n",
      "03/10/2023 09:46:44 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/10/2023 09:46:47 PM - INFO: Attempting to delete original anvil_donor records.\n",
      "TDR Job ID: ivMHqA2RTACTxUtrNPZIUg\n",
      "03/10/2023 09:46:57 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/10/2023 09:46:58 PM - INFO: Attempting to delete original anvil_file records.\n",
      "TDR Job ID: xoPrXzyBSXO3RTPBG6zzGw\n",
      "03/10/2023 09:47:09 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/10/2023 09:47:12 PM - INFO: Starting Transformation (T) Pipeline for ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370).\n",
      "03/10/2023 09:47:12 PM - INFO: Attempting to confirm transform artifact retrieval (target schema and transform queries).\n",
      "03/10/2023 09:47:12 PM - INFO: Transform artifact retrieval confirmed.\n",
      "03/10/2023 09:47:12 PM - INFO: Running transformed files creation.\n",
      "03/10/2023 09:47:12 PM - INFO: Running transform for target table: anvil_activity\n",
      "03/10/2023 09:47:18 PM - INFO: Running transform for target table: anvil_biosample\n",
      "03/10/2023 09:47:24 PM - INFO: Running transform for target table: anvil_dataset\n",
      "03/10/2023 09:47:29 PM - INFO: Running transform for target table: anvil_donor\n",
      "03/10/2023 09:47:35 PM - INFO: Running transform for target table: anvil_file\n",
      "03/10/2023 09:47:41 PM - INFO: Running transform for target table: anvil_project\n",
      "03/10/2023 09:47:47 PM - INFO: Running TDR schema extension.\n",
      "03/10/2023 09:47:47 PM - INFO: Retrieving current TDR schema to determine new tables and relationships to add.\n",
      "03/10/2023 09:47:48 PM - INFO: No new tables or relationships to add to the TDR schema.\n",
      "03/10/2023 09:47:48 PM - INFO: Running dataset ingests\n",
      "03/10/2023 09:47:48 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_activity.json...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/10/2023 09:47:48 PM - INFO: Running ingest from anvil_activity.json to table anvil_activity.\n",
      "TDR Job ID: CBACKO-NSj6G7RdJ1sTCSg\n",
      "03/10/2023 09:48:08 PM - INFO: Ingest from file anvil_activity.json succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_activity', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_activity.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 55, 'bad_row_count': 0, 'load_result': None}\n",
      "03/10/2023 09:48:08 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_biosample.json...\n",
      "03/10/2023 09:48:08 PM - INFO: Running ingest from anvil_biosample.json to table anvil_biosample.\n",
      "TDR Job ID: oDn1bEUtTv6HmfjHa8_12Q\n",
      "03/10/2023 09:48:29 PM - INFO: Ingest from file anvil_biosample.json succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_biosample', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_biosample.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 55, 'bad_row_count': 0, 'load_result': None}\n",
      "03/10/2023 09:48:29 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_dataset.json...\n",
      "03/10/2023 09:48:29 PM - INFO: Running ingest from anvil_dataset.json to table anvil_dataset.\n",
      "TDR Job ID: ORuJSOTpSlehGwTU7Tn4iQ\n",
      "03/10/2023 09:48:50 PM - INFO: Ingest from file anvil_dataset.json succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_dataset', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_dataset.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "03/10/2023 09:48:50 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_donor.json...\n",
      "03/10/2023 09:48:50 PM - INFO: Running ingest from anvil_donor.json to table anvil_donor.\n",
      "TDR Job ID: HEJvCnnQT3au3fIYyOEktg\n",
      "03/10/2023 09:49:10 PM - INFO: Ingest from file anvil_donor.json succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_donor', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_donor.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 55, 'bad_row_count': 0, 'load_result': None}\n",
      "03/10/2023 09:49:10 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_file.json...\n",
      "03/10/2023 09:49:10 PM - INFO: Running ingest from anvil_file.json to table anvil_file.\n",
      "TDR Job ID: GfNi-Y-zSVW8dtawsQ32_g\n",
      "03/10/2023 09:49:31 PM - INFO: Ingest from file anvil_file.json succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_file.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 171, 'bad_row_count': 0, 'load_result': None}\n",
      "03/10/2023 09:49:31 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_project.json...\n",
      "03/10/2023 09:49:31 PM - INFO: Running ingest from anvil_project.json to table anvil_project.\n",
      "TDR Job ID: z1GmQGDYSaCA8xXqBQVC0g\n",
      "03/10/2023 09:50:02 PM - INFO: Ingest from file anvil_project.json succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_project', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_project.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "03/10/2023 09:50:02 PM - INFO: Running file relationships inference.\n",
      "03/10/2023 09:50:02 PM - INFO: Attempting to identify the TDR object, and the necessary attributes...\n",
      "03/10/2023 09:50:02 PM - INFO: Attempting to infer and ingest file relationships...\n",
      "03/10/2023 09:50:05 PM - INFO: File relationships found: 111 new records to ingest\n",
      "03/10/2023 09:50:05 PM - INFO: Submitting ingest for inferred file relationships.\n",
      "TDR Job ID: Hww0HER4T36gdkwjNPwUUg\n",
      "03/10/2023 09:50:36 PM - INFO: File relationships inference ingest succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_activity', 'path': None, 'load_tag': 'File relationships inference ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 111, 'bad_row_count': 0, 'load_result': None}\n",
      "03/10/2023 09:50:36 PM - INFO: Running dangling foreign key resolution.\n",
      "03/10/2023 09:50:36 PM - INFO: Attempting to identify the TDR object, and collect and parse its schema...\n",
      "03/10/2023 09:50:37 PM - INFO: Attempting to identify and remediate dangling foreign keys...\n",
      "03/10/2023 09:50:37 PM - INFO: Identifying dangling foreign keys for anvil_biosample...\n",
      "03/10/2023 09:50:39 PM - INFO: Identifying dangling foreign keys for anvil_dataset...\n",
      "03/10/2023 09:50:41 PM - INFO: Identifying dangling foreign keys for anvil_file...\n",
      "03/10/2023 09:50:43 PM - INFO: Identifying dangling foreign keys for anvil_donor...\n",
      "03/10/2023 09:50:45 PM - INFO: Identifying dangling foreign keys for anvil_antibody...\n",
      "03/10/2023 09:50:47 PM - INFO: Running supplementary file identification.\n",
      "03/10/2023 09:50:47 PM - INFO: Attempting to identify the TDR object, and the necessary attributes...\n",
      "03/10/2023 09:50:47 PM - INFO: Adding anvil_file.is_supplementary to dataset schema, if necessary.\n",
      "03/10/2023 09:50:47 PM - INFO: Field already found! Skipping schema update.\n",
      "03/10/2023 09:50:47 PM - INFO: Re-processing existing anvil_file data to include is_supplementary value.\n",
      "03/10/2023 09:50:56 PM - INFO: Successfully created new anvil_file.json file.\n",
      "03/10/2023 09:50:56 PM - INFO: Ingesting updated anvil_file data into TDR dataset.\n",
      "TDR Job ID: CgaYe1VBQha-EH6B3K-MBw\n",
      "03/10/2023 09:51:26 PM - INFO: Ingest from file anvil_file.json succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_file.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 171, 'bad_row_count': 0, 'load_result': None}\n",
      "03/10/2023 09:51:26 PM - INFO: Supplementary file identification ran successfully.\n",
      "03/10/2023 09:51:26 PM - INFO: Running snapshot creation\n",
      "03/10/2023 09:51:26 PM - INFO: Creating full-view snapshot.\n",
      "03/10/2023 09:51:26 PM - INFO: Attempting to lookup consent code using PHS: 298 and Consent Name: TBD.\n",
      "03/10/2023 09:51:26 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: ZXAlXK-FQKeLMBThRF8n6g\n",
      "03/10/2023 09:52:47 PM - INFO: Snapshot Creation succeeded: {'id': '11e8f80b-451d-49e3-87fd-b8a9310ac822', 'name': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024_ANV4_202303102147', 'description': 'Full view snapshot of ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'createdDate': '2023-03-10T21:51:46.088656Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'TBD', 'phsId': 'phs000298', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-35115bd0', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/10/2023 09:52:47 PM - INFO: Skipping output data validation on user request.\n",
      "03/10/2023 09:52:47 PM - INFO: The ingest pipeline has completed for ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370).\n",
      "03/10/2023 09:52:56 PM - INFO: Pipeline Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Time</th>\n",
       "      <th>Step</th>\n",
       "      <th>Task</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:47:12</td>\n",
       "      <td>Transform Artifact Retrieval</td>\n",
       "      <td>Confirm Transform Artifact Retrieval</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:47:18</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_activity.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:47:24</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_biosample.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:47:29</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_dataset.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:47:35</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_donor.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:47:41</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_file.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:47:47</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_project.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:47:48</td>\n",
       "      <td>TDR Schema Extension</td>\n",
       "      <td>Extend TDR Schema</td>\n",
       "      <td>Success</td>\n",
       "      <td>No new tables or relationships to add to the TDR schema.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:48:08</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_activity - File: anvil_activity.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: CBACKO-NSj6G7RdJ1sTCSg - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_activity', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_activity.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 55, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:48:29</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_biosample - File: anvil_biosample.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: oDn1bEUtTv6HmfjHa8_12Q - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_biosample', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_biosample.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 55, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:48:50</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_dataset - File: anvil_dataset.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: ORuJSOTpSlehGwTU7Tn4iQ - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_dataset', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_dataset.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:49:10</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_donor - File: anvil_donor.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: HEJvCnnQT3au3fIYyOEktg - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_donor', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_donor.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 55, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:49:31</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_file - File: anvil_file.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: GfNi-Y-zSVW8dtawsQ32_g - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_file.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 171, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:50:02</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_project - File: anvil_project.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: z1GmQGDYSaCA8xXqBQVC0g - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_project', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_project.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:50:36</td>\n",
       "      <td>File Relationship Inference</td>\n",
       "      <td>Infer and Ingest File Relationships</td>\n",
       "      <td>Success</td>\n",
       "      <td>Ingest Succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_activity', 'path': None, 'load_tag': 'File relationships inference ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 111, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:50:47</td>\n",
       "      <td>Dangling Foreign Key Resolution</td>\n",
       "      <td>Resolve Dangling Foreign Keys</td>\n",
       "      <td>Success</td>\n",
       "      <td>anvil_sequencingactivity: No foreign keys identified; anvil_biosample: Missing 0 values; anvil_assayactivity: No foreign keys identified; anvil_dataset: Missing 0 values; anvil_alignmentactivity: No foreign keys identified; anvil_file: Missing 0 values; anvil_project: No foreign keys identified; anvil_variantcallingactivity: No foreign keys identified; anvil_donor: Missing 0 values; anvil_antibody: Missing 0 values; anvil_diagnosis: No foreign keys identified; anvil_activity: No foreign keys identified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:51:26</td>\n",
       "      <td>Supplementary File Identification</td>\n",
       "      <td>Identify and Update Supplementary Files</td>\n",
       "      <td>Success</td>\n",
       "      <td>Ingest Succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_file.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 171, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:52:47</td>\n",
       "      <td>Snapshot Creation</td>\n",
       "      <td>Create and Share Snapshot</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: ZXAlXK-FQKeLMBThRF8n6g - Truncated Response: {'id': '11e8f80b-451d-49e3-87fd-b8a9310ac822', 'name': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024_ANV4_202303102147', 'description': 'Full view snapshot of ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'createdDate': '2023-03-10T21:51:46.088656Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'TBD', 'phsId': 'phs000298', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-35115bd0', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)</td>\n",
       "      <td>2023-03-10 21:52:47</td>\n",
       "      <td>Output Data Validation</td>\n",
       "      <td>Profile and Validate Data</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>User request</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Dataset                                                        Time                        Step                                        Task                          Status   \\\n",
       "0   ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:47:12       Transform Artifact Retrieval                 Confirm Transform Artifact Retrieval  Success   \n",
       "1   ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:47:18         Transformed Files Creation                            File: anvil_activity.json  Success   \n",
       "2   ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:47:24         Transformed Files Creation                           File: anvil_biosample.json  Success   \n",
       "3   ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:47:29         Transformed Files Creation                             File: anvil_dataset.json  Success   \n",
       "4   ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:47:35         Transformed Files Creation                               File: anvil_donor.json  Success   \n",
       "5   ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:47:41         Transformed Files Creation                                File: anvil_file.json  Success   \n",
       "6   ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:47:47         Transformed Files Creation                             File: anvil_project.json  Success   \n",
       "7   ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:47:48               TDR Schema Extension                                    Extend TDR Schema  Success   \n",
       "8   ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:48:08                    Dataset Ingests    Table: anvil_activity - File: anvil_activity.json  Success   \n",
       "9   ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:48:29                    Dataset Ingests  Table: anvil_biosample - File: anvil_biosample.json  Success   \n",
       "10  ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:48:50                    Dataset Ingests      Table: anvil_dataset - File: anvil_dataset.json  Success   \n",
       "11  ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:49:10                    Dataset Ingests          Table: anvil_donor - File: anvil_donor.json  Success   \n",
       "12  ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:49:31                    Dataset Ingests            Table: anvil_file - File: anvil_file.json  Success   \n",
       "13  ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:50:02                    Dataset Ingests      Table: anvil_project - File: anvil_project.json  Success   \n",
       "14  ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:50:36        File Relationship Inference                  Infer and Ingest File Relationships  Success   \n",
       "15  ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:50:47    Dangling Foreign Key Resolution                        Resolve Dangling Foreign Keys  Success   \n",
       "16  ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:51:26  Supplementary File Identification              Identify and Update Supplementary Files  Success   \n",
       "17  ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:52:47                  Snapshot Creation                            Create and Share Snapshot  Success   \n",
       "18  ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024 (ab76b5ca-e464-4063-b949-853f61036370)  2023-03-10 21:52:47             Output Data Validation                            Profile and Validate Data  Skipped   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                       Message                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         No new tables or relationships to add to the TDR schema.  \n",
       "8                                                                                                                                                                                                                                                                                                                                                                 Job_ID: CBACKO-NSj6G7RdJ1sTCSg - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_activity', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_activity.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 55, 'bad_row_count': 0, 'load_result': None}  \n",
       "9                                                                                                                                                                                                                                                                                                                                                               Job_ID: oDn1bEUtTv6HmfjHa8_12Q - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_biosample', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_biosample.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 55, 'bad_row_count': 0, 'load_result': None}  \n",
       "10                                                                                                                                                                                                                                                                                                                                                                   Job_ID: ORuJSOTpSlehGwTU7Tn4iQ - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_dataset', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_dataset.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                      Job_ID: HEJvCnnQT3au3fIYyOEktg - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_donor', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_donor.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 55, 'bad_row_count': 0, 'load_result': None}  \n",
       "12                                                                                                                                                                                                                                                                                                                                                                       Job_ID: GfNi-Y-zSVW8dtawsQ32_g - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_file.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 171, 'bad_row_count': 0, 'load_result': None}  \n",
       "13                                                                                                                                                                                                                                                                                                                                                                   Job_ID: z1GmQGDYSaCA8xXqBQVC0g - Truncated Response: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_project', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_project.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}  \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Ingest Succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_activity', 'path': None, 'load_tag': 'File relationships inference ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 111, 'bad_row_count': 0, 'load_result': None}  \n",
       "15                                                                                                                                                                                                                                                                                                                                                     anvil_sequencingactivity: No foreign keys identified; anvil_biosample: Missing 0 values; anvil_assayactivity: No foreign keys identified; anvil_dataset: Missing 0 values; anvil_alignmentactivity: No foreign keys identified; anvil_file: Missing 0 values; anvil_project: No foreign keys identified; anvil_variantcallingactivity: No foreign keys identified; anvil_donor: Missing 0 values; anvil_antibody: Missing 0 values; anvil_diagnosis: No foreign keys identified; anvil_activity: No foreign keys identified  \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                          Ingest Succeeded: {'dataset_id': 'ab76b5ca-e464-4063-b949-853f61036370', 'dataset': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/ab76b5ca-e464-4063-b949-853f61036370/table_data/anvil_file.json', 'load_tag': 'Ingest for ab76b5ca-e464-4063-b949-853f61036370', 'row_count': 171, 'bad_row_count': 0, 'load_result': None}  \n",
       "17  Job_ID: ZXAlXK-FQKeLMBThRF8n6g - Truncated Response: {'id': '11e8f80b-451d-49e3-87fd-b8a9310ac822', 'name': 'ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024_ANV4_202303102147', 'description': 'Full view snapshot of ANVIL_ccdg_asc_ndd_daly_talkowski_gurrieri_asd_exome_20221024', 'createdDate': '2023-03-10T21:51:46.088656Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'TBD', 'phsId': 'phs000298', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-35115bd0', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': False}  \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    User request  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/10/2023 09:52:56 PM - INFO: Function completed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>run_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2f2de074-0e24-4b70-b3d2-83afacb28f7f</td>\n",
       "      <td>Failure - Table Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ab76b5ca-e464-4063-b949-853f61036370</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dataset_id                       run_status        \n",
       "0  2f2de074-0e24-4b70-b3d2-83afacb28f7f  Failure - Table Processing\n",
       "1  ab76b5ca-e464-4063-b949-853f61036370                     Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to collect all datarepo rows for a particular table within a dataset\n",
    "def collect_all_datarepo_rows(dataset_id, table_name):\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving BQ project and schema: {}\".format(str(e)))\n",
    "    client = bigquery.Client()\n",
    "    query = \"SELECT datarepo_row_id FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = table_name)\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        results = [row[\"datarepo_row_id\"] for row in query_job]\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving datarepo_row_id list: {}\".format(str(e)))\n",
    "        raise Exception(e)\n",
    "\n",
    "# Function to delete rows from a dataset\n",
    "def delete_old_records(dataset_id, table, datarepo_row_ids):\n",
    "    logging.info(f\"Attempting to delete original {table} records.\")\n",
    "    if datarepo_row_ids:\n",
    "        data_deletion_payload = {\n",
    "            \"deleteType\": \"soft\",\n",
    "            \"specType\": \"jsonArray\",\n",
    "            \"tables\": [{\n",
    "              \"tableName\": table,\n",
    "              \"jsonArraySpec\": {\n",
    "                \"rowIds\": datarepo_row_ids\n",
    "              }\n",
    "            }]\n",
    "        }\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        try:\n",
    "            data_deletion_result, job_id = utils.wait_for_tdr_job(datasets_api.apply_dataset_data_deletion(id=dataset_id, data_deletion_request=data_deletion_payload))\n",
    "            logging.info(\"Result: {}\".format(data_deletion_result))\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error: {}\".format(str(e)))\n",
    "            raise Exception(e)\n",
    "    else:\n",
    "        logging.info(\"No datarepo_row_ids specified for deletion.\")\n",
    "\n",
    "def ingest_updated_records(profile_id, dataset_id, table, records_dict):\n",
    "    logging.info(f\"Submitting ingest for updated {table} records.\")\n",
    "    \n",
    "    # Build, submit, and monitor ingest request\n",
    "    ingest_request = {\n",
    "        \"table\": table,\n",
    "        \"profile_id\": profile_id,\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"array\",\n",
    "        \"bulkMode\": False,\n",
    "        \"load_tag\": f\"File ref fields patch for {table} in {dataset_id}\",\n",
    "        \"records\": records_dict\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"Ingest succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            status = \"Success\"\n",
    "            return\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 1:\n",
    "                logging.info(\"Retrying ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Logging error.\")\n",
    "                status = \"Error\"\n",
    "                raise Exception(e)\n",
    "                \n",
    "def update_recs_w_file_refs(dataset_id):\n",
    "    logging.info(f\"Processing md5-added files for Dataset ID = {dataset_id}\")\n",
    "\n",
    "    ## Retrieve dataset information\n",
    "    logging.info(\"Retrieving necessary information from TDR.\")\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving information from TDR. Exiting function. Error: {}\".format(e))\n",
    "        return \"Failure - Pre-processing\"\n",
    "\n",
    "    ## Parse TDR schema to identify file reference fields\n",
    "    table_dict = {}\n",
    "    for table in src_schema_dict[\"tables\"]:\n",
    "        if table[\"name\"] in [\"file_inventory\", \"anvil_file\"]:\n",
    "            continue\n",
    "        else:\n",
    "            col_list = []\n",
    "            for column in table[\"columns\"]:\n",
    "                if column[\"datatype\"] == \"fileref\":\n",
    "                    col_list.append([column[\"name\"], column[\"array_of\"]])\n",
    "            if col_list:\n",
    "                table_dict[table[\"name\"]] = col_list\n",
    "\n",
    "    ## Loop through tables and re-process impacted records\n",
    "    for table in table_dict.keys():\n",
    "        logging.info(f\"Processing updates for {table}.\")\n",
    "        # Retrieve relevant records from BigQuery\n",
    "        col_list = []\n",
    "        old_cols = \"\"\n",
    "        new_cols = \"\"\n",
    "        join_clause = \"\"\n",
    "        where_clause = \"\"\n",
    "        for idx, col in enumerate(table_dict[table]):\n",
    "            column_name = col[0]\n",
    "            col_list.append(column_name)\n",
    "            if idx == 0: \n",
    "                old_cols += column_name\n",
    "                where_clause += f\"t.{column_name} IN (SELECT file_ref FROM file_list)\"\n",
    "            else:\n",
    "                old_cols += \", \" + column_name\n",
    "                where_clause += f\" OR t.{column_name} IN (SELECT file_ref FROM file_list)\"\n",
    "            new_cols += f\", CASE WHEN t{idx}.source_name IS NOT NULL THEN TO_JSON(STRUCT(t{idx}.source_name AS sourcePath, t{idx}.target_path AS targetPath)) END AS {column_name}\"\n",
    "            join_clause += f\" LEFT JOIN load_hist t{idx} ON t.{column_name} = t{idx}.file_id\"\n",
    "\n",
    "        query = \"\"\"WITH \n",
    "            file_list AS (SELECT * FROM `{project}.{dataset}.file_inventory` WHERE md5_hash IS NULL),\n",
    "            load_hist AS (SELECT * FROM `{project}.{dataset}.datarepo_load_history` WHERE state = 'succeeded')\n",
    "            SELECT t.* EXCEPT({old_cols}){new_cols}\n",
    "            FROM `{project}.{dataset}.{table}` t {joins} WHERE {where}\"\"\".format(project=bq_project, dataset=bq_dataset, table=table, old_cols=old_cols, new_cols=new_cols, joins=join_clause, where=where_clause)\n",
    "        try:\n",
    "            client = bigquery.Client()\n",
    "            res = client.query(query).result()\n",
    "            if res.total_rows > 0:\n",
    "                logging.info(f\"{res.total_rows} records to process.\")\n",
    "                df = res.to_dataframe()\n",
    "                records_json = df.to_json(orient='records')\n",
    "                records_list = json.loads(records_json)\n",
    "            else:\n",
    "                logging.info(\"No records to process.\")\n",
    "                records_list = []\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error retrieving update records from BigQuery: {str(e)}\")\n",
    "            return \"Failure - Table Processing\"\n",
    "        # Ingest updated records back to TDR dataset\n",
    "        try:\n",
    "            datarepo_row_ids = []\n",
    "            for record in records_list:\n",
    "                datarepo_row_ids.append(record.pop(\"datarepo_row_id\", None))\n",
    "                for col in col_list:\n",
    "                    record[col] = json.loads(record[col])\n",
    "            if records_list:\n",
    "                ingest_updated_records(\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", dataset_id, table, records_list)\n",
    "                delete_old_records(dataset_id, table, datarepo_row_ids)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error replacing TDR records: {str(e)}\")\n",
    "            return \"Failure - Table Processing\"\n",
    "        \n",
    "    ## Re-process file_inventory\n",
    "    logging.info(f\"Processing updates for file_inventory.\")\n",
    "    # Retrieve relevant records from BigQuery\n",
    "    query = \"\"\"WITH \n",
    "        file_list AS (SELECT file_ref FROM `{project}.{dataset}.file_inventory` WHERE md5_hash IS NULL),\n",
    "        load_hist AS (SELECT * FROM `{project}.{dataset}.datarepo_load_history` WHERE state = 'succeeded')\n",
    "        SELECT t1.*, CASE WHEN t2.source_name IS NOT NULL THEN TO_JSON(STRUCT(t2.source_name AS sourcePath, t2.target_path AS targetPath)) END AS file_ref\n",
    "        FROM `{project}.{dataset}.file_inventory` t1\n",
    "          INNER JOIN load_hist t2 ON t1.file_ref = t2.file_id\n",
    "        WHERE file_ref IN (SELECT file_ref FROM file_list)\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        client = bigquery.Client()\n",
    "        res = client.query(query).result()\n",
    "        if res.total_rows > 0:\n",
    "            logging.info(f\"{res.total_rows} records to process.\")\n",
    "            df = res.to_dataframe()\n",
    "            records_json = df.to_json(orient='records')\n",
    "            records_list = json.loads(records_json)\n",
    "        else:\n",
    "            logging.info(\"No records to process.\")\n",
    "            records_list = []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving update records from BigQuery: {str(e)}\")\n",
    "        return \"Failure - File Inventory Processing\"\n",
    "    # Loop through records and update md5_hash from GCS metadata\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        datarepo_row_ids = []\n",
    "        for record in records_list:\n",
    "            bucket = re.match('gs:\\/\\/([a-z0-9\\-]+)', record[\"uri\"]).group(1)\n",
    "            obj = re.match('gs:\\/\\/[a-z0-9\\-]+\\/([A-Za-z0-9\\-_\\/\\.]+)', record[\"uri\"]).group(1)\n",
    "            bucket = storage_client.bucket(bucket, user_project=\"anvil-datastorage\")\n",
    "            blob = bucket.get_blob(obj)\n",
    "            record[\"md5_hash\"] = blob.md5_hash\n",
    "            datarepo_row_ids.append(record.pop(\"datarepo_row_id\", None))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving file metadata from GCS: {str(e)}\")\n",
    "        return \"Failure - File Inventory Processing\"\n",
    "    # Ingest updated records back to TDR dataset\n",
    "    try:\n",
    "        if records_list:\n",
    "            ingest_updated_records(\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", dataset_id, \"file_inventory\", records_list)\n",
    "            delete_old_records(dataset_id, \"file_inventory\", datarepo_row_ids)         \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error replacing TDR records: {str(e)}\")\n",
    "        return \"Failure - File Inventory Processing\"\n",
    "\n",
    "    ## Empty anvil_% tables\n",
    "    logging.info(\"Clearing out existing anvil_% tables\")\n",
    "    table_list = [\"anvil_activity\", \"anvil_alignmentactivity\", \"anvil_antibody\", \"anvil_assayactivity\", \"anvil_biosample\", \"anvil_diagnosis\", \"anvil_donor\", \"anvil_file\", \"anvil_sequencingactivity\", \"anvil_variantcallingactivity\"]\n",
    "    for table in table_list:\n",
    "        try:\n",
    "            datarepo_row_ids = collect_all_datarepo_rows(dataset_id, table)\n",
    "            if datarepo_row_ids:\n",
    "                delete_old_records(dataset_id, table, datarepo_row_ids)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error clearing out existing anvil_% records: {str(e)}\")\n",
    "            return \"Failure - anvil_% Record Deletion\"\n",
    "    \n",
    "    ## Re-run T pipeline without validation\n",
    "    params = {}\n",
    "    params[\"ws_name\"] = ws_name\n",
    "    params[\"ws_project\"] = ws_project\n",
    "    params[\"ws_bucket\"] = ws_bucket\n",
    "    params[\"ws_bucket_name\"] = ws_bucket_name\n",
    "    params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\" \n",
    "    params[\"mapping_target\"] = \"anvil\"\n",
    "    params[\"skip_transforms\"] = False\n",
    "    params[\"transform_list_override\"] = [] # Leave empty to run transforms for all files, otherwise populate with target table names \n",
    "    params[\"skip_schema_extension\"] = False\n",
    "    params[\"skip_ingests\"] = False\n",
    "    params[\"ingest_list_override\"] = [] # Leave empty to run ingests for all files, otherwise populate with target table names\n",
    "    params[\"skip_file_relation_inference\"] = False\n",
    "    params[\"skip_dangling_fk_resolution\"] = False\n",
    "    params[\"skip_supplementary_file_identification\"] = False\n",
    "    params[\"skip_snapshot_creation\"] = False\n",
    "    params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "    params[\"skip_data_validation\"] = True\n",
    "    try:\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        dataset_name = dataset_info[\"name\"]\n",
    "        phs_id = dataset_info[\"phs_id\"]\n",
    "        consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "        auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "        src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "    except:\n",
    "        dataset_name = \"\"\n",
    "        return \"Failure - Dataset Retrieval for T Pipeline\"\n",
    "    if dataset_name:\n",
    "        params[\"dataset_id\"] = dataset_id\n",
    "        params[\"dataset_name\"] = dataset_name\n",
    "        params[\"phs_id\"] = phs_id\n",
    "        params[\"consent_name\"] = consent_name\n",
    "        params[\"auth_domains\"] = auth_domains\n",
    "        utils.run_t_pipeline(params)\n",
    "    \n",
    "    # Return success message if no failures recorded\n",
    "    logging.info(\"Function completed successfully.\")\n",
    "    return \"Success\"\n",
    "\n",
    "# Loop through datasets and process md5 updates\n",
    "dataset_id_list = [\n",
    "'672b617f-936e-440a-a735-80f94798aed1',\n",
    "'29b16d47-bac7-4439-9ae2-c1866de36896',\n",
    "'516ceb43-1378-4c02-88fc-a1d2a2258d59',\n",
    "'8991fc1f-d056-47be-80e9-dca3cb94ea25',\n",
    "'c383a955-ec11-40cd-908a-10c855120c4a',\n",
    "'c7e9afbe-47be-4d31-bc5c-2b989bd3aa17',\n",
    "'30f7621c-bff8-434a-90df-06f6e56e6031',\n",
    "'10e173bd-e597-4ba2-b3cf-c959a38269f0',\n",
    "'1fef2a90-dcdb-4501-a259-e7f81e4b2726',\n",
    "'4a597ecd-3c6f-4a77-b927-cbc0e586a3e3',\n",
    "'847b8433-c837-4c9c-ab30-704ca22cfeae',\n",
    "'b164c54b-5bd7-4347-addf-0fd8b56f3254',\n",
    "'9cb6aa04-01dc-4a9c-aa70-bfb7549d3db9',\n",
    "'c2fd0797-ca41-49a1-b485-a4bedac00613',\n",
    "'db8e3965-c129-431c-957e-6ddc0fe8c5d3',\n",
    "'1481ed22-df3c-49b7-bfb8-8e814858c386',\n",
    "'75949c06-891a-4dd5-94c8-82afbe47df03',\n",
    "'13d5cde2-9044-4d32-ad45-2aeee48c79d0',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = update_recs_w_file_refs(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Add new supplementary workspace files to TDR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to identify new supplementary files and ingest them to TDR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ingest_supplementary_files(dataset_id):\n",
    "    \n",
    "    # Retrieve dataset details\n",
    "    logging.info(\"Retrieving dataset details.\")\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    dataset_details = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "    try:\n",
    "        source_workspaces = dataset_details[\"properties\"][\"source_workspaces\"]\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        return \"Failure - Issue Retrieving Dataset Info\"\n",
    "    \n",
    "    # Use source workspace(s) to find workspace bucket(s) to look for new files\n",
    "    logging.info(\"Determining source workspace bucket(s).\")\n",
    "    data_files_src_buckets = {}\n",
    "    for ws in source_workspaces:\n",
    "        try:\n",
    "            ws_attributes = utils.get_workspace_attributes(\"anvil-datastorage\", ws)\n",
    "            src_bucket = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "            if not src_bucket:\n",
    "                return \"Failure - Issue Retrieving Source Buckets\"\n",
    "            elif src_bucket not in data_files_src_buckets:\n",
    "                data_files_src_buckets[src_bucket] = {\n",
    "                    \"include_dirs\": [],\n",
    "                    \"exclude_dirs\": []\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return \"Failure - Issue Retrieving Source Buckets\"\n",
    "    \n",
    "    # Pull existing file inventory from BigQuery\n",
    "    logging.info(\"Pulling existing file inventory records.\")\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"SELECT uri FROM `{project}.{schema}.file_inventory`\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "    file_list = []\n",
    "    try:\n",
    "        output = client.query(query).result()\n",
    "        if output.total_rows > 0:\n",
    "            for row in output:\n",
    "                file_list.append(row.uri)\n",
    "    except Exception as e:\n",
    "            return \"Failure - Issue Retrieving Existing File Inventory Records\"\n",
    "        \n",
    "    # Build file inventory from workspace bucket(s)\n",
    "    logging.info(\"Building new file inventory.\")\n",
    "    params = {}\n",
    "    params[\"data_files_src_buckets\"] = data_files_src_buckets\n",
    "    params[\"google_project\"] = \"terra-349c8d95\"\n",
    "    params[\"file_inventory_dir\"] = \"ingest_pipeline/input/temp/data_files/file_inventory\"\n",
    "    inventory, retry_count = bfi.build_inventory(params)\n",
    "    \n",
    "    # Diff files to ingest\n",
    "    logging.info(\"Diffing new and existing file inventory records.\")\n",
    "    ingest_list = []\n",
    "    for file in inventory:\n",
    "        if file[\"uri\"] not in file_list:\n",
    "            ingest_list.append(file)\n",
    "    df_inventory = pd.DataFrame(ingest_list)\n",
    "    records_dict = df_inventory.to_dict(orient=\"records\")\n",
    "    return records_dict\n",
    "    \n",
    "    # Build, submit, and monitor ingest request\n",
    "    ingest_request = {\n",
    "        \"table\": \"file_inventory\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"array\",\n",
    "        \"load_tag\": \"Supplementary file ingest for {}\".format(dataset_id),\n",
    "        \"records\": records_dict\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            return \"Success\"\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on file relationships inference ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                return f\"Failure - Ingest error: {str(e)}\"\n",
    "    \n",
    "# # Loop through datasets and ingest additional files if necessary\n",
    "# dataset_id_list = [\n",
    "# 'd74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "# ]\n",
    "# results = []\n",
    "# for dataset_id in dataset_id_list:\n",
    "#     status = ingest_supplementary_files(dataset_id) \n",
    "#     results.append([dataset_id, status])\n",
    "#     results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "# display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/08/2023 12:05:45 AM - INFO: Retrieving dataset details.\n",
      "03/08/2023 12:05:45 AM - INFO: Determining source workspace bucket(s).\n",
      "03/08/2023 12:05:46 AM - INFO: Pulling existing file inventory records.\n",
      "SELECT uri FROM `datarepo-cdb3d9d8.datarepo_ANVIL_1000G_high_coverage_2019_20221019.file_inventory`\n",
      "03/08/2023 12:05:48 AM - INFO: Building new file inventory.\n",
      "03/08/2023 12:07:32 AM - INFO: Recording inventory entries from fc-56ac46ea-efc4-4683-b6d5-6d95bed41c5e (999613 objects total)\n",
      "03/08/2023 12:26:50 AM - INFO: 99961 files recorded (~10%)\n",
      "03/08/2023 12:28:09 AM - INFO: 199922 files recorded (~20%)\n",
      "03/08/2023 12:29:24 AM - INFO: 299883 files recorded (~30%)\n",
      "03/08/2023 12:30:40 AM - INFO: 399844 files recorded (~40%)\n",
      "03/08/2023 12:31:57 AM - INFO: 499805 files recorded (~50%)\n",
      "03/08/2023 12:33:12 AM - INFO: 599766 files recorded (~60%)\n",
      "03/08/2023 12:34:27 AM - INFO: 699727 files recorded (~70%)\n",
      "03/08/2023 12:35:43 AM - INFO: 799688 files recorded (~80%)\n",
      "03/08/2023 12:36:59 AM - INFO: 899649 files recorded (~90%)\n",
      "03/08/2023 12:38:16 AM - INFO: 999610 files recorded (~100%)\n",
      "03/08/2023 12:38:16 AM - INFO: All inventory entries recorded (999613 objects total).\n",
      "03/08/2023 12:39:28 AM - INFO: Diffing new and existing file inventory records.\n",
      "03/08/2023 12:49:00 AM - INFO: New file inventory records to ingest: 973597\n",
      "03/08/2023 12:49:10 AM - INFO: Ingesting new file inventory records into TDR (chunk #0).\n",
      "TDR Job ID: mslcIjGVSyuSyZJvj4mwRw\n",
      "03/08/2023 01:39:48 AM - ERROR: Error on new file inventory records ingest: Job mslcIjGVSyuSyZJvj4mwRw failed: (401)\n",
      "Reason: \n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Wed, 08 Mar 2023 01:39:48 GMT', 'Server': 'Apache', 'X-Frame-Options': 'SAMEORIGIN', 'Access-Control-Allow-Headers': 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization,Accept,Referer,X-App-Id,Origin', 'Access-Control-Allow-Methods': 'GET,POST,DELETE,PUT,PATCH,OPTIONS,HEAD', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=31536000;includeSubDomains', 'Cache-Control': 'no-cache,no-store,must-revalidate', 'X-Request-ID': 'eRPYDrX7', 'Content-Type': 'application/json', 'Content-Length': '193', 'Vary': 'Origin', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
      "HTTP response body: {\"message\":\"Encountered IamUnauthorizedException while running AuthorizeBillingProfileUseStep.\",\"errorDetail\":[\"The step failed for an unknown reason.\",\"Please contact the TDR team for help.\"]}\n",
      "\n",
      "TDR Job ID: 2Qn3FDSAS7adA5fhqm64HQ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_194/1568316878.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mapi_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh_tdr_api_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mdatasets_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_repo_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetsApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mingest_request_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_tdr_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mingest_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mingest_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Success\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anvil_workspace_ingest_resources_dev/edit/ingest_pipeline_utilities.ipynb\u001b[0m in \u001b[0;36mwait_for_tdr_job\u001b[0;34m(job_model)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_id = 'bf9108b6-bebc-4b3b-8517-6a2cce5f7d89'\n",
    "\n",
    "# Retrieve dataset details\n",
    "logging.info(\"Retrieving dataset details.\")\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "dataset_details = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "try:\n",
    "    source_workspaces = dataset_details[\"properties\"][\"source_workspaces\"]\n",
    "    bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "    bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "except Exception as e:\n",
    "    print(\"Failure - Issue Retrieving Dataset Info\") \n",
    "\n",
    "# Use source workspace(s) to find workspace bucket(s) to look for new files\n",
    "logging.info(\"Determining source workspace bucket(s).\")\n",
    "data_files_src_buckets = {}\n",
    "for ws in source_workspaces:\n",
    "    try:\n",
    "        ws_attributes = utils.get_workspace_attributes(\"anvil-datastorage\", ws)\n",
    "        src_bucket = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "        if not src_bucket:\n",
    "            print(\"Failure - Issue Retrieving Source Buckets\")\n",
    "        elif src_bucket not in data_files_src_buckets:\n",
    "            data_files_src_buckets[src_bucket] = {\n",
    "                \"include_dirs\": [],\n",
    "                \"exclude_dirs\": []\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(\"Failure - Issue Retrieving Source Buckets\")\n",
    "\n",
    "# Pull existing file inventory from BigQuery\n",
    "logging.info(\"Pulling existing file inventory records.\")\n",
    "client = bigquery.Client()\n",
    "query = \"\"\"SELECT uri FROM `{project}.{schema}.file_inventory`\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "print(query)\n",
    "file_list = []\n",
    "try:\n",
    "    output = client.query(query).result()\n",
    "    if output.total_rows > 0:\n",
    "        for row in output:\n",
    "            file_list.append(row.uri)\n",
    "except Exception as e:\n",
    "        print(\"Failure - Issue Retrieving Existing File Inventory Records\")\n",
    "\n",
    "# Build file inventory from workspace bucket(s)\n",
    "logging.info(\"Building new file inventory.\")\n",
    "params = {}\n",
    "params[\"data_files_src_buckets\"] = data_files_src_buckets\n",
    "params[\"google_project\"] = \"terra-349c8d95\"\n",
    "params[\"file_inventory_dir\"] = \"ingest_pipeline/input/temp/data_files/file_inventory\"\n",
    "inventory, retry_count = bfi.build_inventory(params)\n",
    "\n",
    "# Diff files to ingest\n",
    "logging.info(\"Diffing new and existing file inventory records.\")\n",
    "ingest_list = []\n",
    "for file in inventory:\n",
    "    if file[\"uri\"] not in file_list:\n",
    "        ingest_list.append(file)\n",
    "df_inventory = pd.DataFrame(ingest_list)\n",
    "records_list = df_inventory.to_dict(orient=\"records\")\n",
    "records_cnt = len(records_list)\n",
    "logging.info(f\"New file inventory records to ingest: {records_cnt}\")\n",
    "\n",
    "# Break records to ingest into chunks if necessary\n",
    "chunk_size = 100000\n",
    "chunk_cnt = math.ceil(records_cnt/chunk_size)\n",
    "for i in range(0, chunk_cnt):\n",
    "    if i == 0:\n",
    "        start_row = 0\n",
    "        end_row = chunk_size\n",
    "    else:\n",
    "        start_row = (i*chunk_size) + 1\n",
    "        end_row = min((i+1)*chunk_size, records_cnt)\n",
    "    # Write out chunk to file for ingest\n",
    "    destination_file = \"file_inventory_\" + str(i) + \".json\"\n",
    "    with open(destination_file, \"w\") as outfile:\n",
    "        for idx, val in enumerate(records_list):\n",
    "            if idx >= start_row and idx <= end_row:\n",
    "                json.dump(val, outfile)\n",
    "                if idx < end_row:\n",
    "                    outfile.write(\"\\n\")\n",
    "    !gsutil cp $destination_file $ws_bucket/ingest_pipeline/input/temp 2> stdout   \n",
    "    # Build, submit, and monitor ingest request\n",
    "    logging.info(f\"Ingesting new file inventory records into TDR (chunk #{i}).\")\n",
    "    ingest_request = {\n",
    "        \"table\": \"file_inventory\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"json\",\n",
    "        \"load_tag\": \"Supplementary file ingest for {}\".format(dataset_id),\n",
    "        \"bulkMode\": True,\n",
    "        \"path\": f\"{ws_bucket}/ingest_pipeline/input/temp/{destination_file}\"\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            print(\"Success\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on new file inventory records ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Failure - Ingest error (chunk #{i}): {str(e)}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
