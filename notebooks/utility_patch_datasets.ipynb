{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional modules (one time effort per cloud environment)\n",
    "!pip install --upgrade import_ipynb data_repo_client urllib3 xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ingest_pipeline_utilities.ipynb\n",
      "Version 1.0.26: 3/24/2023 8:31am - Nate Calvanese - Turned on the global file IDs snapshot creation parameter\n",
      "importing Jupyter notebook from source_files_creation.ipynb\n",
      "Version 1.0.9: 2/25/2023 3:15pm - Nate Calvanese - Replaced FAPI with utils functions\n",
      "importing Jupyter notebook from build_file_inventory.ipynb\n",
      "Version 2.0.1: 3/23/2022 8:29pm - Nate Calvanese - Added support for a global file exclusion\n",
      "importing Jupyter notebook from process_table_data.ipynb\n",
      "Version: 1.0.9: 3/8/2023 12:09pm - Nate Calvanese - Performance improvements for file ref lookups\n",
      "importing Jupyter notebook from build_mapping_query.ipynb\n",
      "Version 1.0.12: 2/21/2023 2:50pm - Nate Calvanese - Added support for $BQ_DATASET substitution variable\n",
      "importing Jupyter notebook from output_data_validation.ipynb\n",
      "Version 2.0.6: 2/28/2023 11:33am -- Updated notebook to be usable in dev (removed TDR host hardcoding)\n",
      "importing Jupyter notebook from resolve_dangling_foreign_keys.ipynb\n",
      "Version 1.0.1: 10/24/2022 3:18pm - Nate Calvanese - Added pass through of params dict\n",
      "importing Jupyter notebook from infer_file_relationships.ipynb\n",
      "Version 1.0.1: 11/8/2022 4:03pm - Nate Calvanese - Expanded query to cover additional cases\n",
      "importing Jupyter notebook from identify_supplementary_files.ipynb\n",
      "Version 1.0.1: 3/13/2023 2:14pm - Nate Calvanese - Updated query for identifying records to flag\n"
     ]
    }
   ],
   "source": [
    "## imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import data_repo_client\n",
    "from google.cloud import bigquery\n",
    "import ingest_pipeline_utilities as utils\n",
    "import build_file_inventory as bfi\n",
    "import logging\n",
    "from time import sleep\n",
    "import datetime\n",
    "from google.cloud import storage\n",
    "import math\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create new snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to create new full view snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {}\n",
    "params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\"\n",
    "params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\"]\n",
    "params[\"anvil_schema_version\"] = \"ANV5\"\n",
    "\n",
    "# Loop through datasets and create new snapshot\n",
    "dataset_id_run_list = [\n",
    "    'ee4a96ab-ffe0-4fd6-bb69-2921f6e944d0',\n",
    "]\n",
    "results = []\n",
    "for dataset in dataset_id_run_list:\n",
    "    dataset_id = dataset\n",
    "    try:\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        dataset_name = dataset_info[\"name\"]\n",
    "        phs_id = dataset_info[\"phs_id\"]\n",
    "        consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "        auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "        src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "    except:\n",
    "        dataset_name = \"\"\n",
    "    if dataset_name:\n",
    "        params[\"ws_bucket\"] = ws_bucket\n",
    "        params[\"dataset_id\"] = dataset_id\n",
    "        params[\"dataset_name\"] = dataset_name\n",
    "        params[\"phs_id\"] = phs_id\n",
    "        params[\"consent_name\"] = consent_name\n",
    "        params[\"auth_domains\"] = auth_domains\n",
    "        params[\"pipeline_results\"] = []\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "        params[\"snapshot_name\"] = params[\"dataset_name\"] + \"_\" + params[\"anvil_schema_version\"] + \"_\" + current_datetime_string \n",
    "        utils.create_and_share_snapshot(params)\n",
    "        int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "        errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "        if len(errors) > 0:\n",
    "            results.append([dataset_id, \"Error\", \"\"])\n",
    "        else:\n",
    "            snapshot_id = re.search(\"{'id': '([a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12})'\", str(int_df_results[int_df_results[\"Task\"]==\"Create and Share Snapshot\"][\"Message\"]))[1]\n",
    "            results.append([dataset_id, \"Success\", snapshot_id])\n",
    "results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\", \"snapshot_id\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Add and populate anvil_file.is_supplementary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to patch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_and_populate_supp_file_flg(dataset_id):\n",
    "    logging.info(f\"Processing anvil_file.is_supplementary for Dataset ID = {dataset_id}\")\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    logging.info(\"Retrieving necessary information from TDR.\")\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving information from TDR. Exiting function. Error: {}\".format(e))\n",
    "        return \"Failure\"\n",
    "    \n",
    "    # Determine if field needs to be added, and add if so\n",
    "    logging.info(\"Adding anvil_file.is_supplementary to dataset schema, if necessary.\")\n",
    "    field_found = False\n",
    "    for table in src_schema_dict[\"tables\"]:\n",
    "        if table[\"name\"] == \"anvil_file\":\n",
    "            for col in table[\"columns\"]:\n",
    "                if col[\"name\"] == \"is_supplementary\":\n",
    "                    field_found = True\n",
    "                    logging.info(\"Field already found! Skipping schema update.\")\n",
    "                    break\n",
    "            break\n",
    "    if field_found == False:\n",
    "        logging.info(\"Field not found. Running dataset schema update.\")\n",
    "        schema_update_request = {\n",
    "            \"description\": \"Adding is_supplementary column to anvil_file\",\n",
    "            \"changes\": {\n",
    "                \"addColumns\": [\n",
    "                  {\n",
    "                    \"tableName\": \"anvil_file\",\n",
    "                    \"columns\": [\n",
    "                      {\n",
    "                        \"name\": \"is_supplementary\",\n",
    "                        \"datatype\": \"boolean\",\n",
    "                        \"array_of\": False,\n",
    "                        \"required\": False\n",
    "                      }\n",
    "                    ]\n",
    "                  }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                schema_update_result, job_id = utils.wait_for_tdr_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "                logging.info(\"Dataset schema update succeeded!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error on dataset schema update: {}\".format(str(e)))\n",
    "                attempt_counter += 1\n",
    "                if attempt_counter < 2:\n",
    "                    logging.info(\"Retrying dataset schema update (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(15)\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Unable to update dataset schema. Exiting function.\")\n",
    "                    return \"Failure\"\n",
    "        \n",
    "    # Re-process anvil_file data to include is_supplementary (where appropriate) and ingest into TDR dataset (as replace)\n",
    "    logging.info(\"Re-processing existing anvil_file data to include is_supplementary value.\")\n",
    "    client = bigquery.Client()\n",
    "    target_file = \"anvil_file.json\"\n",
    "    destination_dir = \"ingest_pipeline/output/transformed/anvil/{}/table_data\".format(dataset_id)\n",
    "    query = \"\"\"BEGIN\n",
    "        \n",
    "        CREATE TEMPORARY TABLE activity_exp AS WITH activity_agg\n",
    "        AS\n",
    "        (\n",
    "          SELECT used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_activity`\n",
    "          UNION ALL \n",
    "          SELECT [] AS used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_alignmentactivity`\n",
    "          UNION ALL \n",
    "          SELECT used_biosample_id, generated_file_id, [] AS used_file_id FROM `{project}.{dataset}.anvil_assayactivity`\n",
    "          UNION ALL \n",
    "          SELECT used_biosample_id, generated_file_id, [] AS used_file_id FROM `{project}.{dataset}.anvil_sequencingactivity`\n",
    "          UNION ALL \n",
    "          SELECT [] AS used_biosample_id, generated_file_id, used_file_id FROM `{project}.{dataset}.anvil_variantcallingactivity`\n",
    "        )\n",
    "        SELECT file_id, int_file_id, biosample_id\n",
    "        FROM activity_agg\n",
    "            LEFT JOIN UNNEST(used_biosample_id) AS biosample_id\n",
    "            LEFT JOIN UNNEST(generated_file_id) as file_id\n",
    "            LEFT JOIN UNNEST(used_file_id) as int_file_id\n",
    "        ;\n",
    "        \n",
    "        CREATE TEMPORARY TABLE act_exp_lookup\n",
    "        AS\n",
    "        (\n",
    "            SELECT file_id, MAX(biosample_id) AS biosample_id\n",
    "          FROM\n",
    "          (\n",
    "            --Level 1:\n",
    "            SELECT file_id, biosample_id\n",
    "            FROM activity_exp\n",
    "            WHERE int_file_id IS NULL AND file_id IS NOT NULl AND biosample_id IS NOT NULL\n",
    "            --Level 2:\n",
    "            UNION ALL\n",
    "            SELECT a2.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "            WHERE a2.int_file_id IS NOT NULL AND a2.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "            --Level 3:\n",
    "            UNION ALL\n",
    "            SELECT a3.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "              LEFT JOIN activity_exp a3\n",
    "              ON a2.file_id = a3.int_file_id\n",
    "            WHERE a3.int_file_id IS NOT NULL AND a3.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "            --Level 4:\n",
    "            UNION ALL\n",
    "            SELECT a4.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "              LEFT JOIN activity_exp a3\n",
    "              ON a2.file_id = a3.int_file_id\n",
    "              LEFT JOIN activity_exp a4\n",
    "              ON a3.file_id = a4.int_file_id\n",
    "            WHERE a4.int_file_id IS NOT NULL AND a4.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "            --Level 5:\n",
    "            UNION ALL\n",
    "            SELECT a5.file_id, a1.biosample_id\n",
    "            FROM activity_exp a1\n",
    "              LEFT JOIN activity_exp a2\n",
    "              ON a1.file_id = a2.int_file_id\n",
    "              LEFT JOIN activity_exp a3\n",
    "              ON a2.file_id = a3.int_file_id\n",
    "              LEFT JOIN activity_exp a4\n",
    "              ON a3.file_id = a4.int_file_id\n",
    "              LEFT JOIN activity_exp a5\n",
    "              ON a4.file_id = a5.int_file_id\n",
    "            WHERE a5.int_file_id IS NOT NULL AND a5.file_id IS NOT NULL AND a1.biosample_id IS NOT NULL\n",
    "          )\n",
    "          GROUP BY file_id\n",
    "        );\n",
    "        \n",
    "        SELECT t1.file_id, data_modality, file_format, file_size, file_md5sum, reference_assembly, file_name, file_ref, source_datarepo_row_ids,\n",
    "        CASE WHEN t2.biosample_id IS NULL THEN TRUE ELSE FALSE END AS is_supplementary\n",
    "        FROM `{project}.{dataset}.anvil_file` t1\n",
    "          LEFT JOIN act_exp_lookup t2\n",
    "          ON t1.file_id = t2.file_id\n",
    "        ;\n",
    "        \n",
    "        END\n",
    "        \"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        df = client.query(query).result().to_dataframe()\n",
    "        records_json = df.to_json(orient='records') \n",
    "        records_list = json.loads(records_json)\n",
    "        records_cnt = len(records_list)\n",
    "        with open(target_file, 'w') as outfile:\n",
    "            for idx, val in enumerate(records_list):\n",
    "                json.dump(val, outfile)\n",
    "                if idx < (records_cnt - 1):\n",
    "                    outfile.write('\\n')\n",
    "        !gsutil cp $target_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "        !rm $target_file\n",
    "        logging.info(\"Successfully created new anvil_file.json file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error creating new anvil_file.json file. Exiting function. Error: {}\".format(str(e)))\n",
    "        return \"Failure\"\n",
    "\n",
    "    # Ingest updated anvil_file data\n",
    "    logging.info(\"Ingesting updated anvil_file data into TDR dataset.\")\n",
    "    source_full_file_path = \"{}/{}/{}\".format(ws_bucket, destination_dir, \"anvil_file.json\")\n",
    "    ingest_request = {\n",
    "        \"table\": \"anvil_file\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"json\",\n",
    "        \"load_tag\": \"Ingest for {}\".format(dataset_id),\n",
    "        \"path\": source_full_file_path\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"Ingest from file anvil_file.json succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Dataset Ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Dataset Ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Exiting function.\")\n",
    "                return \"Failure\"\n",
    "    \n",
    "    # Return success message if no failures recorded\n",
    "    logging.info(\"Function completed successfully.\")\n",
    "    return \"Success\"\n",
    "\n",
    "# Loop through datasets and process is_supplementary field\n",
    "dataset_id_list = [\n",
    "'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = add_and_populate_supp_file_flg(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "display(results_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to validate patch worked properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_supp_file_flg(dataset_id):\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        return \"Failure - Issue Retrieving Dataset Info\"\n",
    "    \n",
    "    # Determine if field exists for dataset, continue if so, fail otherwise\n",
    "    field_found = False\n",
    "    for table in src_schema_dict[\"tables\"]:\n",
    "        if table[\"name\"] == \"anvil_file\":\n",
    "            for col in table[\"columns\"]:\n",
    "                if col[\"name\"] == \"is_supplementary\":\n",
    "                    field_found = True\n",
    "                    break\n",
    "            break\n",
    "    if field_found == False:\n",
    "        return \"Failure - is_supplementary field not found\"\n",
    "    else:\n",
    "        client = bigquery.Client()\n",
    "        query = \"\"\"SELECT COUNT(*) AS rec_cnt, COUNT(is_supplementary) AS populated_cnt\n",
    "                    FROM `{project}.{dataset}.anvil_file`\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "        try:\n",
    "            df = client.query(query).result().to_dataframe()\n",
    "            if df[\"rec_cnt\"].values[0] == df[\"populated_cnt\"].values[0]:\n",
    "                return \"Success\"\n",
    "        except Exception as e:\n",
    "            return \"Failure - BigQuery Error\"\n",
    "\n",
    "# Loop through datasets and validate is_supplementary field\n",
    "dataset_id_list = [\n",
    "'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = validate_supp_file_flg(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"validation_status\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Attempt to populate anvil_donor.organism_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to patch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def populate_organism_type(dataset_id):\n",
    "    logging.info(f\"Processing anvil_donor.organism_type for Dataset ID = {dataset_id}\")\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    logging.info(\"Retrieving necessary information from TDR.\")\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving information from TDR. Exiting function. Error: {}\".format(e))\n",
    "        return \"Failure\"\n",
    "\n",
    "    # Re-process anvil_donor data to include organism_type (where available)\n",
    "    logging.info(\"Re-processing existing anvil_donor data to include organism_type value.\")\n",
    "    client = bigquery.Client()\n",
    "    target_file = \"anvil_donor.json\"\n",
    "    destination_dir = \"ingest_pipeline/output/transformed/anvil/{}/table_data\".format(dataset_id)\n",
    "    query = \"\"\"SELECT donor_id, \n",
    "    (SELECT MAX(CASE WHEN REGEXP_CONTAINS(value, '(h37|h38|h39|hg16|hg17|hg18|hg19|hs37|hs38|b37)') THEN 'Homo sapiens' END) AS organism_type FROM `{project}.{dataset}.workspace_attributes` WHERE attribute = 'library:reference') AS organism_type,\n",
    "    part_of_dataset_id, phenotypic_sex, reported_ethnicity, genetic_ancestry, source_datarepo_row_ids\n",
    "    FROM `{project}.{dataset}.anvil_donor`\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        df = client.query(query).result().to_dataframe()\n",
    "        records_json = df.to_json(orient='records') \n",
    "        records_list = json.loads(records_json)\n",
    "        records_cnt = len(records_list)\n",
    "        with open(target_file, 'w') as outfile:\n",
    "            for idx, val in enumerate(records_list):\n",
    "                json.dump(val, outfile)\n",
    "                if idx < (records_cnt - 1):\n",
    "                    outfile.write('\\n')\n",
    "        !gsutil cp $target_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "        !rm $target_file\n",
    "        logging.info(\"Successfully created new anvil_donor.json file.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error creating new anvil_donor.json file. Exiting function. Error: {}\".format(str(e)))\n",
    "        return \"Failure\"\n",
    "\n",
    "    # Ingest updated anvil_donor data\n",
    "    logging.info(\"Ingesting updated anvil_donor data into TDR dataset.\")\n",
    "    source_full_file_path = \"{}/{}/{}\".format(ws_bucket, destination_dir, \"anvil_donor.json\")\n",
    "    ingest_request = {\n",
    "        \"table\": \"anvil_donor\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"json\",\n",
    "        \"load_tag\": \"Ingest for {}\".format(dataset_id),\n",
    "        \"path\": source_full_file_path\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"Ingest from file anvil_donor.json succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Dataset Ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Dataset Ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Exiting function.\")\n",
    "                return \"Failure\"\n",
    "\n",
    "    # Return success message if no failures recorded\n",
    "    logging.info(\"Function completed successfully.\")\n",
    "    return \"Success\"\n",
    "\n",
    "# Loop through datasets and process supplementary_file_flag\n",
    "dataset_id_list = [\n",
    "'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = populate_organism_type(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to examine organism_type population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def validate_organism_type(dataset_id):\n",
    "    \n",
    "    # Retrieve dataset information\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        return \"Failure - Issue Retrieving Dataset Info\"\n",
    "    \n",
    "    # Determine if field exists for dataset, continue if so, fail otherwise\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"SELECT COUNT(organism_type) AS populated_cnt\n",
    "                FROM `{project}.{dataset}.anvil_donor`\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        df = client.query(query).result().to_dataframe()\n",
    "        if df[\"populated_cnt\"].values[0] > 0:\n",
    "            return \"Success - Field Populated\"\n",
    "        else:\n",
    "            return \"Success - Field Not Populated\"\n",
    "    except Exception as e:\n",
    "        return \"Failure - BigQuery Error\"\n",
    "\n",
    "# Loop through datasets and validate is_supplementary field\n",
    "dataset_id_list = [\n",
    "'d74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = validate_organism_type(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"validation_status\"])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update references to md5-added files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/31/2023 01:41:51 PM - INFO: Processing md5-added files for Dataset ID = 700303c2-fcef-48a5-9900-096bf34e2d83\n",
      "03/31/2023 01:41:51 PM - INFO: Retrieving necessary information from TDR.\n",
      "03/31/2023 01:41:51 PM - INFO: Processing updates for sample.\n",
      "03/31/2023 01:41:54 PM - INFO: No records to process.\n",
      "03/31/2023 01:41:54 PM - INFO: Processing updates for file_inventory.\n",
      "03/31/2023 01:41:56 PM - INFO: 1 records to process.\n",
      "03/31/2023 01:41:57 PM - INFO: Submitting ingest for updated file_inventory records.\n",
      "TDR Job ID: Qf0n906NT1OCnyEMLf0mtg\n",
      "03/31/2023 01:42:28 PM - INFO: Ingest succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'file_inventory', 'path': None, 'load_tag': 'File ref fields patch for file_inventory in 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "03/31/2023 01:42:28 PM - INFO: Attempting to delete original file_inventory records.\n",
      "TDR Job ID: ykD-o-g0S2i6bXJvjzhuXg\n",
      "03/31/2023 01:42:38 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/31/2023 01:42:38 PM - INFO: Clearing out existing anvil_% tables\n",
      "03/31/2023 01:42:40 PM - INFO: Attempting to delete original anvil_activity records.\n",
      "TDR Job ID: 0qlDVlKdS7SKQmfQgnlxXQ\n",
      "03/31/2023 01:42:51 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/31/2023 01:42:56 PM - INFO: Attempting to delete original anvil_biosample records.\n",
      "TDR Job ID: KvqHJVh6SkS2GBEbCOCZSA\n",
      "03/31/2023 01:43:07 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/31/2023 01:43:09 PM - INFO: Attempting to delete original anvil_donor records.\n",
      "TDR Job ID: Iuq4MnoXQkKSWbq7fdk6wQ\n",
      "03/31/2023 01:43:19 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/31/2023 01:43:21 PM - INFO: Attempting to delete original anvil_file records.\n",
      "TDR Job ID: ka53AnGeTgigDmbHeWYaCg\n",
      "03/31/2023 01:43:31 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/31/2023 01:43:34 PM - INFO: Starting Transformation (T) Pipeline for ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83).\n",
      "03/31/2023 01:43:34 PM - INFO: Attempting to confirm transform artifact retrieval (target schema and transform queries).\n",
      "03/31/2023 01:43:34 PM - INFO: Transform artifact retrieval confirmed.\n",
      "03/31/2023 01:43:34 PM - INFO: Running transformed files creation.\n",
      "03/31/2023 01:43:34 PM - INFO: Running transform for target table: anvil_activity\n",
      "03/31/2023 01:43:45 PM - INFO: Running transform for target table: anvil_biosample\n",
      "03/31/2023 01:43:53 PM - INFO: Running transform for target table: anvil_dataset\n",
      "03/31/2023 01:43:58 PM - INFO: Running transform for target table: anvil_donor\n",
      "03/31/2023 01:44:05 PM - INFO: Running transform for target table: anvil_file\n",
      "03/31/2023 01:44:12 PM - INFO: Running transform for target table: anvil_project\n",
      "03/31/2023 01:44:18 PM - INFO: Running TDR schema extension.\n",
      "03/31/2023 01:44:18 PM - INFO: Retrieving current TDR schema to determine new tables and relationships to add.\n",
      "03/31/2023 01:44:18 PM - INFO: No new tables or relationships to add to the TDR schema.\n",
      "03/31/2023 01:44:18 PM - INFO: Running dataset ingests\n",
      "03/31/2023 01:44:18 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_activity.json...\n",
      "03/31/2023 01:44:18 PM - INFO: Running ingest from anvil_activity.json to table anvil_activity.\n",
      "TDR Job ID: iflXmnzUQsyo3igv9uipjQ\n",
      "03/31/2023 01:44:39 PM - INFO: Ingest from file anvil_activity.json succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_activity', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_activity.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1496, 'bad_row_count': 0, 'load_result': None}\n",
      "03/31/2023 01:44:39 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_biosample.json...\n",
      "03/31/2023 01:44:39 PM - INFO: Running ingest from anvil_biosample.json to table anvil_biosample.\n",
      "TDR Job ID: 4lJLTcMnSp-vQbj1duD0VQ\n",
      "03/31/2023 01:44:59 PM - INFO: Ingest from file anvil_biosample.json succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_biosample', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_biosample.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1496, 'bad_row_count': 0, 'load_result': None}\n",
      "03/31/2023 01:44:59 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_dataset.json...\n",
      "03/31/2023 01:44:59 PM - INFO: Running ingest from anvil_dataset.json to table anvil_dataset.\n",
      "TDR Job ID: 7ue-4sOoRVuZRaEa9dvCWw\n",
      "03/31/2023 01:45:20 PM - INFO: Ingest from file anvil_dataset.json succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_dataset', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_dataset.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "03/31/2023 01:45:20 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_donor.json...\n",
      "03/31/2023 01:45:20 PM - INFO: Running ingest from anvil_donor.json to table anvil_donor.\n",
      "TDR Job ID: ieiABkZKRv24OrIj9T3jpQ\n",
      "03/31/2023 01:45:51 PM - INFO: Ingest from file anvil_donor.json succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_donor', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_donor.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1496, 'bad_row_count': 0, 'load_result': None}\n",
      "03/31/2023 01:45:51 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_file.json...\n",
      "03/31/2023 01:45:51 PM - INFO: Running ingest from anvil_file.json to table anvil_file.\n",
      "TDR Job ID: FWIjf2Y6SyurYWp7uSpNSg\n",
      "03/31/2023 01:46:21 PM - INFO: Ingest from file anvil_file.json succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_file.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 4492, 'bad_row_count': 0, 'load_result': None}\n",
      "03/31/2023 01:46:21 PM - INFO: Checking for file ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_project.json...\n",
      "03/31/2023 01:46:22 PM - INFO: Running ingest from anvil_project.json to table anvil_project.\n",
      "TDR Job ID: A6tRZuBRRP2lN_Nutsjccw\n",
      "03/31/2023 01:46:52 PM - INFO: Ingest from file anvil_project.json succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_project', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_project.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}\n",
      "03/31/2023 01:46:52 PM - INFO: Running file relationships inference.\n",
      "03/31/2023 01:46:52 PM - INFO: Attempting to identify the TDR object, and the necessary attributes...\n",
      "03/31/2023 01:46:53 PM - INFO: Attempting to infer and ingest file relationships...\n",
      "03/31/2023 01:46:56 PM - INFO: File relationships found: 2993 new records to ingest\n",
      "03/31/2023 01:46:56 PM - INFO: Submitting ingest for inferred file relationships.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDR Job ID: RgWm3B9lTZahjbyVkSG44w\n",
      "03/31/2023 01:47:37 PM - INFO: File relationships inference ingest succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_activity', 'path': None, 'load_tag': 'File relationships inference ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 2993, 'bad_row_count': 0, 'load_result': None}\n",
      "03/31/2023 01:47:37 PM - INFO: Running dangling foreign key resolution.\n",
      "03/31/2023 01:47:37 PM - INFO: Attempting to identify the TDR object, and collect and parse its schema...\n",
      "03/31/2023 01:47:38 PM - INFO: Attempting to identify and remediate dangling foreign keys...\n",
      "03/31/2023 01:47:38 PM - INFO: Identifying dangling foreign keys for anvil_donor...\n",
      "03/31/2023 01:47:39 PM - INFO: Identifying dangling foreign keys for anvil_antibody...\n",
      "03/31/2023 01:47:41 PM - INFO: Identifying dangling foreign keys for anvil_biosample...\n",
      "03/31/2023 01:47:43 PM - INFO: Identifying dangling foreign keys for anvil_dataset...\n",
      "03/31/2023 01:47:44 PM - INFO: Identifying dangling foreign keys for anvil_file...\n",
      "03/31/2023 01:47:48 PM - INFO: Running supplementary file identification.\n",
      "03/31/2023 01:47:48 PM - INFO: Attempting to identify the TDR object, and the necessary attributes...\n",
      "03/31/2023 01:47:48 PM - INFO: Adding anvil_file.is_supplementary to dataset schema, if necessary.\n",
      "03/31/2023 01:47:48 PM - INFO: Field already found! Skipping schema update.\n",
      "03/31/2023 01:47:48 PM - INFO: Re-processing existing anvil_file data to include is_supplementary value.\n",
      "03/31/2023 01:48:02 PM - INFO: Successfully created new anvil_file.json file.\n",
      "03/31/2023 01:48:02 PM - INFO: Ingesting updated anvil_file data into TDR dataset.\n",
      "TDR Job ID: Nc2G7Dc8SEmL0GCKnzMQdw\n",
      "03/31/2023 01:48:33 PM - INFO: Ingest from file anvil_file.json succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_file.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 4492, 'bad_row_count': 0, 'load_result': None}\n",
      "03/31/2023 01:48:33 PM - INFO: Supplementary file identification ran successfully.\n",
      "03/31/2023 01:48:33 PM - INFO: Running snapshot creation\n",
      "03/31/2023 01:48:33 PM - INFO: Creating full-view snapshot.\n",
      "03/31/2023 01:48:33 PM - INFO: Attempting to lookup consent code using PHS: 1398 and Consent Name: GRU.\n",
      "03/31/2023 01:48:33 PM - INFO: Submitting snapshot request.\n",
      "TDR Job ID: OUl7J8bNSl2vK4b84nMy2Q\n",
      "03/31/2023 01:50:14 PM - INFO: Snapshot Creation succeeded: {'id': '5264be69-e3ed-4f1d-972a-875bc28564b5', 'name': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107_ANV4_202303311343', 'description': 'Full view snapshot of ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'createdDate': '2023-03-31T13:48:52.555648Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'c1', 'phsId': 'phs001398', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-9338c0d0', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': True}\n",
      "03/31/2023 01:50:14 PM - INFO: Skipping output data validation on user request.\n",
      "03/31/2023 01:50:14 PM - INFO: The ingest pipeline has completed for ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83).\n",
      "03/31/2023 01:50:22 PM - INFO: Pipeline Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Time</th>\n",
       "      <th>Step</th>\n",
       "      <th>Task</th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:43:34</td>\n",
       "      <td>Transform Artifact Retrieval</td>\n",
       "      <td>Confirm Transform Artifact Retrieval</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:43:45</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_activity.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:43:53</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_biosample.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:43:58</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_dataset.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:44:05</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_donor.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:44:12</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_file.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:44:18</td>\n",
       "      <td>Transformed Files Creation</td>\n",
       "      <td>File: anvil_project.json</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:44:18</td>\n",
       "      <td>TDR Schema Extension</td>\n",
       "      <td>Extend TDR Schema</td>\n",
       "      <td>Success</td>\n",
       "      <td>No new tables or relationships to add to the TDR schema.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:44:39</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_activity - File: anvil_activity.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: iflXmnzUQsyo3igv9uipjQ - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_activity', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_activity.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1496, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:44:59</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_biosample - File: anvil_biosample.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: 4lJLTcMnSp-vQbj1duD0VQ - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_biosample', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_biosample.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1496, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:45:20</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_dataset - File: anvil_dataset.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: 7ue-4sOoRVuZRaEa9dvCWw - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_dataset', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_dataset.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:45:51</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_donor - File: anvil_donor.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: ieiABkZKRv24OrIj9T3jpQ - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_donor', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_donor.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1496, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:46:21</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_file - File: anvil_file.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: FWIjf2Y6SyurYWp7uSpNSg - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_file.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 4492, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:46:52</td>\n",
       "      <td>Dataset Ingests</td>\n",
       "      <td>Table: anvil_project - File: anvil_project.json</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: A6tRZuBRRP2lN_Nutsjccw - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_project', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_project.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:47:37</td>\n",
       "      <td>File Relationship Inference</td>\n",
       "      <td>Infer and Ingest File Relationships</td>\n",
       "      <td>Success</td>\n",
       "      <td>Ingest Succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_activity', 'path': None, 'load_tag': 'File relationships inference ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 2993, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:47:48</td>\n",
       "      <td>Dangling Foreign Key Resolution</td>\n",
       "      <td>Resolve Dangling Foreign Keys</td>\n",
       "      <td>Success</td>\n",
       "      <td>anvil_activity: No foreign keys identified; anvil_sequencingactivity: No foreign keys identified; anvil_donor: Missing 0 values; anvil_antibody: Missing 0 values; anvil_assayactivity: No foreign keys identified; anvil_biosample: Missing 0 values; anvil_dataset: Missing 0 values; anvil_variantcallingactivity: No foreign keys identified; anvil_diagnosis: No foreign keys identified; anvil_file: Missing 0 values; anvil_project: No foreign keys identified; anvil_alignmentactivity: No foreign keys identified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:48:33</td>\n",
       "      <td>Supplementary File Identification</td>\n",
       "      <td>Identify and Update Supplementary Files</td>\n",
       "      <td>Success</td>\n",
       "      <td>Ingest Succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_file.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 4492, 'bad_row_count': 0, 'load_result': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:50:14</td>\n",
       "      <td>Snapshot Creation</td>\n",
       "      <td>Create and Share Snapshot</td>\n",
       "      <td>Success</td>\n",
       "      <td>Job_ID: OUl7J8bNSl2vK4b84nMy2Q - Truncated Response: {'id': '5264be69-e3ed-4f1d-972a-875bc28564b5', 'name': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107_ANV4_202303311343', 'description': 'Full view snapshot of ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'createdDate': '2023-03-31T13:48:52.555648Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'c1', 'phsId': 'phs001398', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-9338c0d0', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': True}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)</td>\n",
       "      <td>2023-03-31 13:50:14</td>\n",
       "      <td>Output Data Validation</td>\n",
       "      <td>Profile and Validate Data</td>\n",
       "      <td>Skipped</td>\n",
       "      <td>User request</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Dataset                                               Time                        Step                                        Task                          Status   \\\n",
       "0   ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:43:34       Transform Artifact Retrieval                 Confirm Transform Artifact Retrieval  Success   \n",
       "1   ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:43:45         Transformed Files Creation                            File: anvil_activity.json  Success   \n",
       "2   ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:43:53         Transformed Files Creation                           File: anvil_biosample.json  Success   \n",
       "3   ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:43:58         Transformed Files Creation                             File: anvil_dataset.json  Success   \n",
       "4   ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:44:05         Transformed Files Creation                               File: anvil_donor.json  Success   \n",
       "5   ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:44:12         Transformed Files Creation                                File: anvil_file.json  Success   \n",
       "6   ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:44:18         Transformed Files Creation                             File: anvil_project.json  Success   \n",
       "7   ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:44:18               TDR Schema Extension                                    Extend TDR Schema  Success   \n",
       "8   ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:44:39                    Dataset Ingests    Table: anvil_activity - File: anvil_activity.json  Success   \n",
       "9   ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:44:59                    Dataset Ingests  Table: anvil_biosample - File: anvil_biosample.json  Success   \n",
       "10  ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:45:20                    Dataset Ingests      Table: anvil_dataset - File: anvil_dataset.json  Success   \n",
       "11  ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:45:51                    Dataset Ingests          Table: anvil_donor - File: anvil_donor.json  Success   \n",
       "12  ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:46:21                    Dataset Ingests            Table: anvil_file - File: anvil_file.json  Success   \n",
       "13  ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:46:52                    Dataset Ingests      Table: anvil_project - File: anvil_project.json  Success   \n",
       "14  ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:47:37        File Relationship Inference                  Infer and Ingest File Relationships  Success   \n",
       "15  ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:47:48    Dangling Foreign Key Resolution                        Resolve Dangling Foreign Keys  Success   \n",
       "16  ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:48:33  Supplementary File Identification              Identify and Update Supplementary Files  Success   \n",
       "17  ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:50:14                  Snapshot Creation                            Create and Share Snapshot  Success   \n",
       "18  ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107 (700303c2-fcef-48a5-9900-096bf34e2d83)  2023-03-31 13:50:14             Output Data Validation                            Profile and Validate Data  Skipped   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                   Message                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 No new tables or relationships to add to the TDR schema.  \n",
       "8                                                                                                                                                                                                                                                                                                                                          Job_ID: iflXmnzUQsyo3igv9uipjQ - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_activity', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_activity.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1496, 'bad_row_count': 0, 'load_result': None}  \n",
       "9                                                                                                                                                                                                                                                                                                                                        Job_ID: 4lJLTcMnSp-vQbj1duD0VQ - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_biosample', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_biosample.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1496, 'bad_row_count': 0, 'load_result': None}  \n",
       "10                                                                                                                                                                                                                                                                                                                                              Job_ID: 7ue-4sOoRVuZRaEa9dvCWw - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_dataset', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_dataset.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}  \n",
       "11                                                                                                                                                                                                                                                                                                                                               Job_ID: ieiABkZKRv24OrIj9T3jpQ - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_donor', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_donor.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1496, 'bad_row_count': 0, 'load_result': None}  \n",
       "12                                                                                                                                                                                                                                                                                                                                                 Job_ID: FWIjf2Y6SyurYWp7uSpNSg - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_file.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 4492, 'bad_row_count': 0, 'load_result': None}  \n",
       "13                                                                                                                                                                                                                                                                                                                                              Job_ID: A6tRZuBRRP2lN_Nutsjccw - Truncated Response: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_project', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_project.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 1, 'bad_row_count': 0, 'load_result': None}  \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Ingest Succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_activity', 'path': None, 'load_tag': 'File relationships inference ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 2993, 'bad_row_count': 0, 'load_result': None}  \n",
       "15                                                                                                                                                                                                                                                                                                             anvil_activity: No foreign keys identified; anvil_sequencingactivity: No foreign keys identified; anvil_donor: Missing 0 values; anvil_antibody: Missing 0 values; anvil_assayactivity: No foreign keys identified; anvil_biosample: Missing 0 values; anvil_dataset: Missing 0 values; anvil_variantcallingactivity: No foreign keys identified; anvil_diagnosis: No foreign keys identified; anvil_file: Missing 0 values; anvil_project: No foreign keys identified; anvil_alignmentactivity: No foreign keys identified  \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                    Ingest Succeeded: {'dataset_id': '700303c2-fcef-48a5-9900-096bf34e2d83', 'dataset': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'table': 'anvil_file', 'path': 'gs://fc-2a9eefc3-0302-427f-9ac3-82f078741c03/ingest_pipeline/output/transformed/anvil/700303c2-fcef-48a5-9900-096bf34e2d83/table_data/anvil_file.json', 'load_tag': 'Ingest for 700303c2-fcef-48a5-9900-096bf34e2d83', 'row_count': 4492, 'bad_row_count': 0, 'load_result': None}  \n",
       "17  Job_ID: OUl7J8bNSl2vK4b84nMy2Q - Truncated Response: {'id': '5264be69-e3ed-4f1d-972a-875bc28564b5', 'name': 'ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107_ANV4_202303311343', 'description': 'Full view snapshot of ANVIL_CCDG_Broad_MI_BRAVE_GRU_WES_20221107', 'createdDate': '2023-03-31T13:48:52.555648Z', 'profileId': 'e0e03e48-5b96-45ec-baa4-8cc1ebf74c61', 'storage': [{'region': 'us-central1', 'cloudResource': 'bigquery', 'cloudPlatform': 'gcp'}, {'region': 'us-east4', 'cloudResource': 'firestore', 'cloudPlatform': 'gcp'}, {'region': 'us-central1', 'cloudResource': 'bucket', 'cloudPlatform': 'gcp'}], 'secureMonitoringEnabled': False, 'consentCode': 'c1', 'phsId': 'phs001398', 'cloudPlatform': 'gcp', 'dataProject': 'datarepo-9338c0d0', 'storageAccount': None, 'selfHosted': True, 'globalFileIds': True}  \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            User request  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/31/2023 01:50:22 PM - INFO: Function completed successfully.\n",
      "03/31/2023 01:50:22 PM - INFO: Processing md5-added files for Dataset ID = a715c70d-da92-43ee-a851-1a27277909a2\n",
      "03/31/2023 01:50:22 PM - INFO: Retrieving necessary information from TDR.\n",
      "03/31/2023 01:50:22 PM - INFO: Processing updates for pggb.\n",
      "03/31/2023 01:50:25 PM - INFO: 8 records to process.\n",
      "03/31/2023 01:50:26 PM - INFO: Submitting ingest for updated pggb records.\n",
      "TDR Job ID: 1fd9nwmgSyu1ChY0zxaJ1Q\n",
      "03/31/2023 01:51:09 PM - INFO: Ingest succeeded: {'dataset_id': 'a715c70d-da92-43ee-a851-1a27277909a2', 'dataset': 'ANVIL_HPRC_20230310', 'table': 'pggb', 'path': None, 'load_tag': 'File ref fields patch for pggb in a715c70d-da92-43ee-a851-1a27277909a2', 'row_count': 8, 'bad_row_count': 0, 'load_result': {'loadSummary': {'loadTag': 'File ref fields patch for pggb in a715c70d-da92-43ee-a851-1a27277909a2', 'jobId': '1fd9nwmgSyu1ChY0zxaJ1Q', 'totalFiles': 0, 'succeededFiles': 0, 'failedFiles': 0, 'notTriedFiles': 0}, 'loadFileResults': []}}\n",
      "03/31/2023 01:51:09 PM - INFO: Attempting to delete original pggb records.\n",
      "TDR Job ID: q5iwJQ-QRP2L8pJa7Of9Gw\n",
      "03/31/2023 01:51:19 PM - INFO: Result: {'objectState': 'deleted'}\n",
      "03/31/2023 01:51:19 PM - INFO: Processing updates for assembly_sample.\n",
      "03/31/2023 01:51:28 PM - INFO: No records to process.\n",
      "03/31/2023 01:51:28 PM - INFO: Processing updates for minigraph_cactus.\n",
      "03/31/2023 01:51:52 PM - INFO: 22016 records to process.\n",
      "03/31/2023 01:51:53 PM - ERROR: Error replacing TDR records: the JSON object must be str, bytes or bytearray, not NoneType\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>run_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>700303c2-fcef-48a5-9900-096bf34e2d83</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a715c70d-da92-43ee-a851-1a27277909a2</td>\n",
       "      <td>Failure - Table Processing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dataset_id                       run_status        \n",
       "0  700303c2-fcef-48a5-9900-096bf34e2d83                     Success\n",
       "1  a715c70d-da92-43ee-a851-1a27277909a2  Failure - Table Processing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to collect all datarepo rows for a particular table within a dataset\n",
    "def collect_all_datarepo_rows(dataset_id, table_name):\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\"]).to_dict()\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving BQ project and schema: {}\".format(str(e)))\n",
    "    client = bigquery.Client()\n",
    "    query = \"SELECT datarepo_row_id FROM `{project}.{schema}.{table}`\".format(project = bq_project, schema = bq_schema, table = table_name)\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        results = [row[\"datarepo_row_id\"] for row in query_job]\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving datarepo_row_id list: {}\".format(str(e)))\n",
    "        raise Exception(e)\n",
    "\n",
    "# Function to delete rows from a dataset\n",
    "def delete_old_records(dataset_id, table, datarepo_row_ids):\n",
    "    logging.info(f\"Attempting to delete original {table} records.\")\n",
    "    if datarepo_row_ids:\n",
    "        data_deletion_payload = {\n",
    "            \"deleteType\": \"soft\",\n",
    "            \"specType\": \"jsonArray\",\n",
    "            \"tables\": [{\n",
    "              \"tableName\": table,\n",
    "              \"jsonArraySpec\": {\n",
    "                \"rowIds\": datarepo_row_ids\n",
    "              }\n",
    "            }]\n",
    "        }\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        try:\n",
    "            data_deletion_result, job_id = utils.wait_for_tdr_job(datasets_api.apply_dataset_data_deletion(id=dataset_id, data_deletion_request=data_deletion_payload))\n",
    "            logging.info(\"Result: {}\".format(data_deletion_result))\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error: {}\".format(str(e)))\n",
    "            raise Exception(e)\n",
    "    else:\n",
    "        logging.info(\"No datarepo_row_ids specified for deletion.\")\n",
    "\n",
    "def ingest_updated_records(profile_id, dataset_id, table, records_dict):\n",
    "    logging.info(f\"Submitting ingest for updated {table} records.\")\n",
    "    \n",
    "    # Build, submit, and monitor ingest request\n",
    "    ingest_request = {\n",
    "        \"table\": table,\n",
    "        \"profile_id\": profile_id,\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"array\",\n",
    "        \"bulkMode\": False,\n",
    "        \"load_tag\": f\"File ref fields patch for {table} in {dataset_id}\",\n",
    "        \"records\": records_dict\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            logging.info(\"Ingest succeeded: {}\".format(str(ingest_request_result)[0:1000]))\n",
    "            status = \"Success\"\n",
    "            return\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 1:\n",
    "                logging.info(\"Retrying ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Logging error.\")\n",
    "                status = \"Error\"\n",
    "                raise Exception(e)\n",
    "                \n",
    "def update_recs_w_file_refs(dataset_id):\n",
    "    logging.info(f\"Processing md5-added files for Dataset ID = {dataset_id}\")\n",
    "\n",
    "    ## Retrieve dataset information\n",
    "    logging.info(\"Retrieving necessary information from TDR.\")\n",
    "    src_schema_dict = {}\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "        bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving information from TDR. Exiting function. Error: {}\".format(e))\n",
    "        return \"Failure - Pre-processing\"\n",
    "\n",
    "    ## Parse TDR schema to identify file reference fields\n",
    "    table_dict = {}\n",
    "    for table in src_schema_dict[\"tables\"]:\n",
    "        if table[\"name\"] in [\"file_inventory\", \"anvil_file\"]:\n",
    "            continue\n",
    "        else:\n",
    "            col_list = []\n",
    "            for column in table[\"columns\"]:\n",
    "                if column[\"datatype\"] == \"fileref\":\n",
    "                    col_list.append([column[\"name\"], column[\"array_of\"]])\n",
    "            if col_list:\n",
    "                table_dict[table[\"name\"]] = col_list\n",
    "\n",
    "    ## Loop through tables and re-process impacted records\n",
    "    for table in table_dict.keys():\n",
    "        logging.info(f\"Processing updates for {table}.\")\n",
    "        # Retrieve relevant records from BigQuery\n",
    "        col_list = []\n",
    "        old_cols = \"\"\n",
    "        new_cols = \"\"\n",
    "        join_clause = \"\"\n",
    "        where_clause = \"\"\n",
    "        for idx, col in enumerate(table_dict[table]):\n",
    "            column_name = col[0]\n",
    "            col_list.append(column_name)\n",
    "            if idx == 0: \n",
    "                old_cols += column_name\n",
    "                where_clause += f\"t.{column_name} IN (SELECT file_ref FROM file_list)\"\n",
    "            else:\n",
    "                old_cols += \", \" + column_name\n",
    "                where_clause += f\" OR t.{column_name} IN (SELECT file_ref FROM file_list)\"\n",
    "            new_cols += f\", CASE WHEN t{idx}.source_name IS NOT NULL THEN TO_JSON(STRUCT(t{idx}.source_name AS sourcePath, t{idx}.target_path AS targetPath)) END AS {column_name}\"\n",
    "            join_clause += f\" LEFT JOIN load_hist t{idx} ON t.{column_name} = t{idx}.file_id\"\n",
    "\n",
    "        query = \"\"\"WITH \n",
    "            file_list AS (SELECT * FROM `{project}.{dataset}.file_inventory` WHERE md5_hash IS NULL),\n",
    "            load_hist AS (SELECT * FROM `{project}.{dataset}.datarepo_load_history` WHERE state = 'succeeded')\n",
    "            SELECT t.* EXCEPT({old_cols}){new_cols}\n",
    "            FROM `{project}.{dataset}.{table}` t {joins} WHERE {where}\"\"\".format(project=bq_project, dataset=bq_dataset, table=table, old_cols=old_cols, new_cols=new_cols, joins=join_clause, where=where_clause)\n",
    "        try:\n",
    "            client = bigquery.Client()\n",
    "            res = client.query(query).result()\n",
    "            if res.total_rows > 0:\n",
    "                logging.info(f\"{res.total_rows} records to process.\")\n",
    "                df = res.to_dataframe()\n",
    "                records_json = df.to_json(orient='records')\n",
    "                records_list = json.loads(records_json)\n",
    "            else:\n",
    "                logging.info(\"No records to process.\")\n",
    "                records_list = []\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error retrieving update records from BigQuery: {str(e)}\")\n",
    "            return \"Failure - Table Processing\"\n",
    "        # Ingest updated records back to TDR dataset\n",
    "        try:\n",
    "            datarepo_row_ids = []\n",
    "            for record in records_list:\n",
    "                datarepo_row_ids.append(record.pop(\"datarepo_row_id\", None))\n",
    "                for col in col_list:\n",
    "                    record[col] = json.loads(record[col])\n",
    "            if records_list:\n",
    "                ingest_updated_records(\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", dataset_id, table, records_list)\n",
    "                delete_old_records(dataset_id, table, datarepo_row_ids)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error replacing TDR records: {str(e)}\")\n",
    "            return \"Failure - Table Processing\"\n",
    "        \n",
    "    ## Re-process file_inventory\n",
    "    logging.info(f\"Processing updates for file_inventory.\")\n",
    "    # Retrieve relevant records from BigQuery\n",
    "    query = \"\"\"WITH \n",
    "        file_list AS (SELECT file_ref FROM `{project}.{dataset}.file_inventory` WHERE md5_hash IS NULL),\n",
    "        load_hist AS (SELECT * FROM `{project}.{dataset}.datarepo_load_history` WHERE state = 'succeeded')\n",
    "        SELECT t1.*, CASE WHEN t2.source_name IS NOT NULL THEN TO_JSON(STRUCT(t2.source_name AS sourcePath, t2.target_path AS targetPath)) END AS file_ref\n",
    "        FROM `{project}.{dataset}.file_inventory` t1\n",
    "          INNER JOIN load_hist t2 ON t1.file_ref = t2.file_id\n",
    "        WHERE file_ref IN (SELECT file_ref FROM file_list)\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "    try:\n",
    "        client = bigquery.Client()\n",
    "        res = client.query(query).result()\n",
    "        if res.total_rows > 0:\n",
    "            logging.info(f\"{res.total_rows} records to process.\")\n",
    "            df = res.to_dataframe()\n",
    "            records_json = df.to_json(orient='records')\n",
    "            records_list = json.loads(records_json)\n",
    "        else:\n",
    "            logging.info(\"No records to process.\")\n",
    "            records_list = []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving update records from BigQuery: {str(e)}\")\n",
    "        return \"Failure - File Inventory Processing\"\n",
    "    # Loop through records and update md5_hash from GCS metadata\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        datarepo_row_ids = []\n",
    "        for record in records_list:\n",
    "            bucket = re.match('gs:\\/\\/([a-z0-9\\-]+)', record[\"uri\"]).group(1)\n",
    "            obj = re.match('gs:\\/\\/[a-z0-9\\-]+\\/([A-Za-z0-9\\-_\\/\\.]+)', record[\"uri\"]).group(1)\n",
    "            bucket = storage_client.bucket(bucket, user_project=\"anvil-datastorage\")\n",
    "            blob = bucket.get_blob(obj)\n",
    "            record[\"md5_hash\"] = blob.md5_hash\n",
    "            datarepo_row_ids.append(record.pop(\"datarepo_row_id\", None))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving file metadata from GCS: {str(e)}\")\n",
    "        return \"Failure - File Inventory Processing\"\n",
    "    # Ingest updated records back to TDR dataset\n",
    "    try:\n",
    "        if records_list:\n",
    "            ingest_updated_records(\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", dataset_id, \"file_inventory\", records_list)\n",
    "            delete_old_records(dataset_id, \"file_inventory\", datarepo_row_ids)         \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error replacing TDR records: {str(e)}\")\n",
    "        return \"Failure - File Inventory Processing\"\n",
    "\n",
    "    ## Empty anvil_% tables\n",
    "    logging.info(\"Clearing out existing anvil_% tables\")\n",
    "    table_list = [\"anvil_activity\", \"anvil_alignmentactivity\", \"anvil_antibody\", \"anvil_assayactivity\", \"anvil_biosample\", \"anvil_diagnosis\", \"anvil_donor\", \"anvil_file\", \"anvil_sequencingactivity\", \"anvil_variantcallingactivity\"]\n",
    "    for table in table_list:\n",
    "        try:\n",
    "            datarepo_row_ids = collect_all_datarepo_rows(dataset_id, table)\n",
    "            if datarepo_row_ids:\n",
    "                delete_old_records(dataset_id, table, datarepo_row_ids)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error clearing out existing anvil_% records: {str(e)}\")\n",
    "            return \"Failure - anvil_% Record Deletion\"\n",
    "    \n",
    "    ## Re-run T pipeline without validation\n",
    "    params = {}\n",
    "    params[\"ws_name\"] = ws_name\n",
    "    params[\"ws_project\"] = ws_project\n",
    "    params[\"ws_bucket\"] = ws_bucket\n",
    "    params[\"ws_bucket_name\"] = ws_bucket_name\n",
    "    params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\" \n",
    "    params[\"mapping_target\"] = \"anvil\"\n",
    "    params[\"skip_transforms\"] = False\n",
    "    params[\"transform_list_override\"] = [] # Leave empty to run transforms for all files, otherwise populate with target table names \n",
    "    params[\"skip_schema_extension\"] = False\n",
    "    params[\"skip_ingests\"] = False\n",
    "    params[\"ingest_list_override\"] = [] # Leave empty to run ingests for all files, otherwise populate with target table names\n",
    "    params[\"skip_file_relation_inference\"] = False\n",
    "    params[\"skip_dangling_fk_resolution\"] = False\n",
    "    params[\"skip_supplementary_file_identification\"] = False\n",
    "    params[\"skip_snapshot_creation\"] = False\n",
    "    params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "    params[\"skip_data_validation\"] = True\n",
    "    try:\n",
    "        api_client = utils.refresh_tdr_api_client()\n",
    "        datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "        dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "        dataset_name = dataset_info[\"name\"]\n",
    "        phs_id = dataset_info[\"phs_id\"]\n",
    "        consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "        auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "        src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "    except:\n",
    "        dataset_name = \"\"\n",
    "        return \"Failure - Dataset Retrieval for T Pipeline\"\n",
    "    if dataset_name:\n",
    "        params[\"dataset_id\"] = dataset_id\n",
    "        params[\"dataset_name\"] = dataset_name\n",
    "        params[\"phs_id\"] = phs_id\n",
    "        params[\"consent_name\"] = consent_name\n",
    "        params[\"auth_domains\"] = auth_domains\n",
    "        utils.run_t_pipeline(params)\n",
    "    \n",
    "    # Return success message if no failures recorded\n",
    "    logging.info(\"Function completed successfully.\")\n",
    "    return \"Success\"\n",
    "\n",
    "# Loop through datasets and process md5 updates\n",
    "dataset_id_list = [\n",
    "'700303c2-fcef-48a5-9900-096bf34e2d83',\n",
    "'a715c70d-da92-43ee-a851-1a27277909a2',\n",
    "]\n",
    "results = []\n",
    "for dataset_id in dataset_id_list:\n",
    "    status = update_recs_w_file_refs(dataset_id) \n",
    "    results.append([dataset_id, status])\n",
    "    results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing\n",
    "# dataset_id = 'bc6075ac-5cfe-4613-8601-36ceb614939e'\n",
    "\n",
    "# logging.info(f\"Processing md5-added files for Dataset ID = {dataset_id}\")\n",
    "\n",
    "# ## Retrieve dataset information\n",
    "# logging.info(\"Retrieving necessary information from TDR.\")\n",
    "# src_schema_dict = {}\n",
    "# api_client = utils.refresh_tdr_api_client()\n",
    "# datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "# try:\n",
    "#     response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "#     src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "#     bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "#     bq_dataset = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "# except Exception as e:\n",
    "#     logging.error(\"Error retrieving information from TDR. Exiting function. Error: {}\".format(e))\n",
    "#     #return \"Failure - Pre-processing\"\n",
    "\n",
    "# ## Parse TDR schema to identify file reference fields\n",
    "# table_dict = {}\n",
    "# for table in src_schema_dict[\"tables\"]:\n",
    "#     if table[\"name\"] in [\"file_inventory\", \"anvil_file\"]:\n",
    "#         continue\n",
    "#     else:\n",
    "#         col_list = []\n",
    "#         for column in table[\"columns\"]:\n",
    "#             if column[\"datatype\"] == \"fileref\":\n",
    "#                 col_list.append([column[\"name\"], column[\"array_of\"]])\n",
    "#         if col_list:\n",
    "#             table_dict[table[\"name\"]] = col_list\n",
    "\n",
    "# ## Loop through tables and re-process impacted records\n",
    "# for table in table_dict.keys():\n",
    "#     logging.info(f\"Processing updates for {table}.\")\n",
    "#     # Retrieve relevant records from BigQuery\n",
    "#     col_list = []\n",
    "#     old_cols = \"\"\n",
    "#     new_cols = \"\"\n",
    "#     join_clause = \"\"\n",
    "#     where_clause = \"\"\n",
    "#     for idx, col in enumerate(table_dict[table]):\n",
    "#         column_name = col[0]\n",
    "#         col_list.append(column_name)\n",
    "#         if idx == 0: \n",
    "#             old_cols += column_name\n",
    "#             where_clause += f\"t.{column_name} IN (SELECT file_ref FROM file_list)\"\n",
    "#         else:\n",
    "#             old_cols += \", \" + column_name\n",
    "#             where_clause += f\" OR t.{column_name} IN (SELECT file_ref FROM file_list)\"\n",
    "#         new_cols += f\", CASE WHEN t{idx}.source_name IS NOT NULL THEN TO_JSON(STRUCT(t{idx}.source_name AS sourcePath, t{idx}.target_path AS targetPath)) END AS {column_name}\"\n",
    "#         join_clause += f\" LEFT JOIN load_hist t{idx} ON t.{column_name} = t{idx}.file_id\"\n",
    "\n",
    "#     query = \"\"\"WITH \n",
    "#         file_list AS (SELECT * FROM `{project}.{dataset}.file_inventory` WHERE md5_hash IS NULL),\n",
    "#         load_hist AS (SELECT * FROM `{project}.{dataset}.datarepo_load_history` WHERE state = 'succeeded')\n",
    "#         SELECT t.* EXCEPT({old_cols}){new_cols}\n",
    "#         FROM `{project}.{dataset}.{table}` t {joins} WHERE {where}\"\"\".format(project=bq_project, dataset=bq_dataset, table=table, old_cols=old_cols, new_cols=new_cols, joins=join_clause, where=where_clause)\n",
    "#     try:\n",
    "#         client = bigquery.Client()\n",
    "#         res = client.query(query).result()\n",
    "#         if res.total_rows > 0:\n",
    "#             logging.info(f\"{res.total_rows} records to process.\")\n",
    "#             df = res.to_dataframe()\n",
    "#             records_json = df.to_json(orient='records')\n",
    "#             records_list = json.loads(records_json)\n",
    "#         else:\n",
    "#             logging.info(\"No records to process.\")\n",
    "#             records_list = []\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error retrieving update records from BigQuery: {str(e)}\")\n",
    "#         break\n",
    "#         #return \"Failure - Table Processing\"\n",
    "#     # Ingest updated records back to TDR dataset\n",
    "#     try:\n",
    "#         datarepo_row_ids = []\n",
    "#         for record in records_list:\n",
    "#             datarepo_row_ids.append(record.pop(\"datarepo_row_id\", None))\n",
    "#             for col in col_list:\n",
    "#                 record[col] = json.loads(record[col])\n",
    "#         if records_list:\n",
    "#             ingest_updated_records(\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", dataset_id, table, records_list)\n",
    "#             delete_old_records(dataset_id, table, datarepo_row_ids)\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error replacing TDR records: {str(e)}\")\n",
    "#         break\n",
    "#         #return \"Failure - Table Processing\"\n",
    "\n",
    "# # ## Re-process file_inventory\n",
    "# # logging.info(f\"Processing updates for file_inventory.\")\n",
    "# # # Retrieve relevant records from BigQuery\n",
    "# # query = \"\"\"WITH \n",
    "# #     file_list AS (SELECT file_ref FROM `{project}.{dataset}.file_inventory` WHERE md5_hash IS NULL),\n",
    "# #     load_hist AS (SELECT * FROM `{project}.{dataset}.datarepo_load_history` WHERE state = 'succeeded')\n",
    "# #     SELECT t1.*, CASE WHEN t2.source_name IS NOT NULL THEN TO_JSON(STRUCT(t2.source_name AS sourcePath, t2.target_path AS targetPath)) END AS file_ref\n",
    "# #     FROM `{project}.{dataset}.file_inventory` t1\n",
    "# #       INNER JOIN load_hist t2 ON t1.file_ref = t2.file_id\n",
    "# #     WHERE file_ref IN (SELECT file_ref FROM file_list)\"\"\".format(project=bq_project, dataset=bq_dataset)\n",
    "# # try:\n",
    "# #     client = bigquery.Client()\n",
    "# #     res = client.query(query).result()\n",
    "# #     if res.total_rows > 0:\n",
    "# #         logging.info(f\"{res.total_rows} records to process.\")\n",
    "# #         df = res.to_dataframe()\n",
    "# #         records_json = df.to_json(orient='records')\n",
    "# #         records_list = json.loads(records_json)\n",
    "# #     else:\n",
    "# #         logging.info(\"No records to process.\")\n",
    "# #         records_list = []\n",
    "# # except Exception as e:\n",
    "# #     logging.error(f\"Error retrieving update records from BigQuery: {str(e)}\")\n",
    "# #     #return \"Failure - File Inventory Processing\"\n",
    "# # # Loop through records and update md5_hash from GCS metadata\n",
    "# # try:\n",
    "# #     storage_client = storage.Client()\n",
    "# #     datarepo_row_ids = []\n",
    "# #     for record in records_list:\n",
    "# #         bucket = re.match('gs:\\/\\/([a-z0-9\\-]+)', record[\"uri\"]).group(1)\n",
    "# #         obj = re.match('gs:\\/\\/[a-z0-9\\-]+\\/([A-Za-z0-9\\-_\\/\\.]+)', record[\"uri\"]).group(1)\n",
    "# #         bucket = storage_client.bucket(bucket, user_project=\"anvil-datastorage\")\n",
    "# #         blob = bucket.get_blob(obj)\n",
    "# #         record[\"md5_hash\"] = blob.md5_hash\n",
    "# #         datarepo_row_ids.append(record.pop(\"datarepo_row_id\", None))\n",
    "# # except Exception as e:\n",
    "# #     logging.error(f\"Error retrieving file metadata from GCS: {str(e)}\")\n",
    "# #     #return \"Failure - File Inventory Processing\"\n",
    "# # # Ingest updated records back to TDR dataset\n",
    "# # try:\n",
    "# #     if records_list:\n",
    "# #         ingest_updated_records(\"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\", dataset_id, \"file_inventory\", records_list)\n",
    "# #         delete_old_records(dataset_id, \"file_inventory\", datarepo_row_ids)         \n",
    "# # except Exception as e:\n",
    "# #     logging.error(f\"Error replacing TDR records: {str(e)}\")\n",
    "# #     #return \"Failure - File Inventory Processing\"\n",
    "\n",
    "# # ## Empty anvil_% tables\n",
    "# # logging.info(\"Clearing out existing anvil_% tables\")\n",
    "# # table_list = [\"anvil_activity\", \"anvil_alignmentactivity\", \"anvil_antibody\", \"anvil_assayactivity\", \"anvil_biosample\", \"anvil_diagnosis\", \"anvil_donor\", \"anvil_file\", \"anvil_sequencingactivity\", \"anvil_variantcallingactivity\"]\n",
    "# # for table in table_list:\n",
    "# #     try:\n",
    "# #         datarepo_row_ids = collect_all_datarepo_rows(dataset_id, table)\n",
    "# #         if datarepo_row_ids:\n",
    "# #             delete_old_records(dataset_id, table, datarepo_row_ids)\n",
    "# #     except Exception as e:\n",
    "# #         logging.error(f\"Error clearing out existing anvil_% records: {str(e)}\")\n",
    "# #         break\n",
    "# #         #return \"Failure - anvil_% Record Deletion\"\n",
    "\n",
    "# # ## Re-run T pipeline without validation\n",
    "# # params = {}\n",
    "# # params[\"ws_name\"] = ws_name\n",
    "# # params[\"ws_project\"] = ws_project\n",
    "# # params[\"ws_bucket\"] = ws_bucket\n",
    "# # params[\"ws_bucket_name\"] = ws_bucket_name\n",
    "# # params[\"profile_id\"] = \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\" \n",
    "# # params[\"mapping_target\"] = \"anvil\"\n",
    "# # params[\"skip_transforms\"] = False\n",
    "# # params[\"transform_list_override\"] = [] # Leave empty to run transforms for all files, otherwise populate with target table names \n",
    "# # params[\"skip_schema_extension\"] = False\n",
    "# # params[\"skip_ingests\"] = False\n",
    "# # params[\"ingest_list_override\"] = [] # Leave empty to run ingests for all files, otherwise populate with target table names\n",
    "# # params[\"skip_file_relation_inference\"] = False\n",
    "# # params[\"skip_dangling_fk_resolution\"] = False\n",
    "# # params[\"skip_supplementary_file_identification\"] = False\n",
    "# # params[\"skip_snapshot_creation\"] = False\n",
    "# # params[\"snapshot_readers_list\"] = [\"azul-anvil-prod@firecloud.org\"] # Include \"auth-domain\" to add the auth domain(s) as a reader (if one exists)\n",
    "# # params[\"skip_data_validation\"] = True\n",
    "# # try:\n",
    "# #     api_client = utils.refresh_tdr_api_client()\n",
    "# #     datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "# #     dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "# #     dataset_name = dataset_info[\"name\"]\n",
    "# #     phs_id = dataset_info[\"phs_id\"]\n",
    "# #     consent_name = dataset_info[\"properties\"][\"consent_name\"]\n",
    "# #     auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "# #     src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "# # except:\n",
    "# #     dataset_name = \"\"\n",
    "# #     return \"Failure - Dataset Retrieval for T Pipeline\"\n",
    "# # if dataset_name:\n",
    "# #     params[\"dataset_id\"] = dataset_id\n",
    "# #     params[\"dataset_name\"] = dataset_name\n",
    "# #     params[\"phs_id\"] = phs_id\n",
    "# #     params[\"consent_name\"] = consent_name\n",
    "# #     params[\"auth_domains\"] = auth_domains\n",
    "# #     utils.run_t_pipeline(params)\n",
    "\n",
    "# # Return success message if no failures recorded\n",
    "# logging.info(\"Function completed successfully.\")\n",
    "# #return \"Success\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, record in enumerate(records_list):\n",
    "#     if record[\"library_2_estimated_library_size\"]:\n",
    "#         print(str(idx) + \" - \" + str(record[\"library_2_estimated_library_size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# records_list[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Add new supplementary workspace files to TDR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Script to identify new supplementary files and ingest them to TDR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ingest_supplementary_files(dataset_id):\n",
    "    \n",
    "    # Retrieve dataset details\n",
    "    logging.info(\"Retrieving dataset details.\")\n",
    "    api_client = utils.refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    dataset_details = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "    try:\n",
    "        source_workspaces = dataset_details[\"properties\"][\"source_workspaces\"]\n",
    "        bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "        bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "    except Exception as e:\n",
    "        return \"Failure - Issue Retrieving Dataset Info\"\n",
    "    \n",
    "    # Use source workspace(s) to find workspace bucket(s) to look for new files\n",
    "    logging.info(\"Determining source workspace bucket(s).\")\n",
    "    data_files_src_buckets = {}\n",
    "    for ws in source_workspaces:\n",
    "        try:\n",
    "            ws_attributes = utils.get_workspace_attributes(\"anvil-datastorage\", ws)\n",
    "            src_bucket = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "            if not src_bucket:\n",
    "                return \"Failure - Issue Retrieving Source Buckets\"\n",
    "            elif src_bucket not in data_files_src_buckets:\n",
    "                data_files_src_buckets[src_bucket] = {\n",
    "                    \"include_dirs\": [],\n",
    "                    \"exclude_dirs\": []\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return \"Failure - Issue Retrieving Source Buckets\"\n",
    "    \n",
    "    # Pull existing file inventory from BigQuery\n",
    "    logging.info(\"Pulling existing file inventory records.\")\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"SELECT uri FROM `{project}.{schema}.file_inventory`\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "    file_list = []\n",
    "    try:\n",
    "        output = client.query(query).result()\n",
    "        if output.total_rows > 0:\n",
    "            for row in output:\n",
    "                file_list.append(row.uri)\n",
    "    except Exception as e:\n",
    "            return \"Failure - Issue Retrieving Existing File Inventory Records\"\n",
    "        \n",
    "    # Build file inventory from workspace bucket(s)\n",
    "    logging.info(\"Building new file inventory.\")\n",
    "    params = {}\n",
    "    params[\"data_files_src_buckets\"] = data_files_src_buckets\n",
    "    params[\"google_project\"] = \"terra-349c8d95\"\n",
    "    params[\"file_inventory_dir\"] = \"ingest_pipeline/input/temp/data_files/file_inventory\"\n",
    "    inventory, retry_count = bfi.build_inventory(params)\n",
    "    \n",
    "    # Diff files to ingest\n",
    "    logging.info(\"Diffing new and existing file inventory records.\")\n",
    "    ingest_list = []\n",
    "    for file in inventory:\n",
    "        if file[\"uri\"] not in file_list:\n",
    "            ingest_list.append(file)\n",
    "    df_inventory = pd.DataFrame(ingest_list)\n",
    "    records_dict = df_inventory.to_dict(orient=\"records\")\n",
    "    return records_dict\n",
    "    \n",
    "    # Build, submit, and monitor ingest request\n",
    "    ingest_request = {\n",
    "        \"table\": \"file_inventory\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"array\",\n",
    "        \"load_tag\": \"Supplementary file ingest for {}\".format(dataset_id),\n",
    "        \"records\": records_dict\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            return \"Success\"\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on file relationships inference ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                return f\"Failure - Ingest error: {str(e)}\"\n",
    "    \n",
    "# # Loop through datasets and ingest additional files if necessary\n",
    "# dataset_id_list = [\n",
    "# 'd74b26d5-24bb-4696-84c3-bcd1f5f90b08',\n",
    "# ]\n",
    "# results = []\n",
    "# for dataset_id in dataset_id_list:\n",
    "#     status = ingest_supplementary_files(dataset_id) \n",
    "#     results.append([dataset_id, status])\n",
    "#     results_df = pd.DataFrame(results, columns = [\"dataset_id\", \"run_status\"])\n",
    "# display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_id = 'bf9108b6-bebc-4b3b-8517-6a2cce5f7d89'\n",
    "\n",
    "# Retrieve dataset details\n",
    "logging.info(\"Retrieving dataset details.\")\n",
    "api_client = utils.refresh_tdr_api_client()\n",
    "datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "dataset_details = datasets_api.retrieve_dataset(id=dataset_id, include=[\"ACCESS_INFORMATION\", \"PROPERTIES\"]).to_dict()\n",
    "try:\n",
    "    source_workspaces = dataset_details[\"properties\"][\"source_workspaces\"]\n",
    "    bq_project = dataset_details[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "    bq_schema = dataset_details[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "except Exception as e:\n",
    "    print(\"Failure - Issue Retrieving Dataset Info\") \n",
    "\n",
    "# Use source workspace(s) to find workspace bucket(s) to look for new files\n",
    "logging.info(\"Determining source workspace bucket(s).\")\n",
    "data_files_src_buckets = {}\n",
    "for ws in source_workspaces:\n",
    "    try:\n",
    "        ws_attributes = utils.get_workspace_attributes(\"anvil-datastorage\", ws)\n",
    "        src_bucket = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\"\n",
    "        if not src_bucket:\n",
    "            print(\"Failure - Issue Retrieving Source Buckets\")\n",
    "        elif src_bucket not in data_files_src_buckets:\n",
    "            data_files_src_buckets[src_bucket] = {\n",
    "                \"include_dirs\": [],\n",
    "                \"exclude_dirs\": []\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(\"Failure - Issue Retrieving Source Buckets\")\n",
    "\n",
    "# Pull existing file inventory from BigQuery\n",
    "logging.info(\"Pulling existing file inventory records.\")\n",
    "client = bigquery.Client()\n",
    "query = \"\"\"SELECT uri FROM `{project}.{schema}.file_inventory`\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "print(query)\n",
    "file_list = []\n",
    "try:\n",
    "    output = client.query(query).result()\n",
    "    if output.total_rows > 0:\n",
    "        for row in output:\n",
    "            file_list.append(row.uri)\n",
    "except Exception as e:\n",
    "        print(\"Failure - Issue Retrieving Existing File Inventory Records\")\n",
    "\n",
    "# Build file inventory from workspace bucket(s)\n",
    "logging.info(\"Building new file inventory.\")\n",
    "params = {}\n",
    "params[\"data_files_src_buckets\"] = data_files_src_buckets\n",
    "params[\"google_project\"] = \"terra-349c8d95\"\n",
    "params[\"file_inventory_dir\"] = \"ingest_pipeline/input/temp/data_files/file_inventory\"\n",
    "inventory, retry_count = bfi.build_inventory(params)\n",
    "\n",
    "# Diff files to ingest\n",
    "logging.info(\"Diffing new and existing file inventory records.\")\n",
    "ingest_list = []\n",
    "for file in inventory:\n",
    "    if file[\"uri\"] not in file_list:\n",
    "        ingest_list.append(file)\n",
    "df_inventory = pd.DataFrame(ingest_list)\n",
    "records_list = df_inventory.to_dict(orient=\"records\")\n",
    "records_cnt = len(records_list)\n",
    "logging.info(f\"New file inventory records to ingest: {records_cnt}\")\n",
    "\n",
    "# Break records to ingest into chunks if necessary\n",
    "chunk_size = 100000\n",
    "chunk_cnt = math.ceil(records_cnt/chunk_size)\n",
    "for i in range(0, chunk_cnt):\n",
    "    if i == 0:\n",
    "        start_row = 0\n",
    "        end_row = chunk_size\n",
    "    else:\n",
    "        start_row = (i*chunk_size) + 1\n",
    "        end_row = min((i+1)*chunk_size, records_cnt)\n",
    "    # Write out chunk to file for ingest\n",
    "    destination_file = \"file_inventory_\" + str(i) + \".json\"\n",
    "    with open(destination_file, \"w\") as outfile:\n",
    "        for idx, val in enumerate(records_list):\n",
    "            if idx >= start_row and idx <= end_row:\n",
    "                json.dump(val, outfile)\n",
    "                if idx < end_row:\n",
    "                    outfile.write(\"\\n\")\n",
    "    !gsutil cp $destination_file $ws_bucket/ingest_pipeline/input/temp 2> stdout   \n",
    "    # Build, submit, and monitor ingest request\n",
    "    logging.info(f\"Ingesting new file inventory records into TDR (chunk #{i}).\")\n",
    "    ingest_request = {\n",
    "        \"table\": \"file_inventory\",\n",
    "        \"profile_id\": \"e0e03e48-5b96-45ec-baa4-8cc1ebf74c61\",\n",
    "        \"ignore_unknown_values\": True,\n",
    "        \"resolve_existing_files\": True,\n",
    "        \"updateStrategy\": \"replace\",\n",
    "        \"format\": \"json\",\n",
    "        \"load_tag\": \"Supplementary file ingest for {}\".format(dataset_id),\n",
    "        \"bulkMode\": True,\n",
    "        \"path\": f\"{ws_bucket}/ingest_pipeline/input/temp/{destination_file}\"\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = utils.refresh_tdr_api_client()\n",
    "            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "            ingest_request_result, job_id = utils.wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "            print(\"Success\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on new file inventory records ingest: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Failure - Ingest error (chunk #{i}): {str(e)}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
