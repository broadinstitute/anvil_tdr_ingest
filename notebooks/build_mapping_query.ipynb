{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print('Version 1.0.0: 09/13/2022 03:31pm - Nate Calvanese - Initial Version')\n",
    "#print('Version 1.0.1: 09/14/2022 09:04pm - Nate Calvanese - Additional functionality to cover remainder of CMG')\n",
    "#print('Version 1.0.2: 09/16/2022 11:37am - Nate Calvanese - Fixed bug in multi-column array agg when source column was an array itself')\n",
    "#print('Version 1.0.3: 09/19/2022 9:40am - Nate Calvanese - Added in vocab map evaluation query construction')\n",
    "#print('Version 1.0.4: 09/20/2022 4:29pm - Nate Calvanese - Changed record set validation logic')\n",
    "#print('Version 1.0.5: 09/21/2022 11:58am - Nate Calvanese - Made multi-column array agg return array with distinct values')\n",
    "#print('Version 1.0.6: 10/6/2022 9:17pm - Nate Calvanese - Added support for multiple queries for the same table')\n",
    "#print('Version 1.0.7: 10/15/2022 3:08pm - Nate Calvanese - Added support for column aggregation logic')\n",
    "#print('Version 1.0.8: 10/24/2022 4:24pm - Nate Calvanese - Added UUID validation transform function')\n",
    "#print('Version 1.0.9: 10/26/2022 11:21am - Nate Calvanese - Fixed NULLIF issue with different data types')\n",
    "#print('Version 1.0.10: 10/31/2022 1:35pm - Nate Calvanese - Added run_criteria parameter to record sets and incorporated in record set validation')\n",
    "#print('Version 1.0.11: 11/2/2022 1:14pm - Nate Calvanese - Added join construction function to incorporate data type and array status')\n",
    "#print('Version 1.0.11: 2/8/2023 2:40pm - Nate Calvanese - Added SAFE_CAST tranformation function')\n",
    "#print('Version 1.0.12: 2/21/2023 2:50pm - Nate Calvanese - Added support for $BQ_DATASET substitution variable')\n",
    "#print('Version 1.0.13: 12/13/2023 9:47am - Nate Calvanese - Replaced deprecated append with pd.concat')\n",
    "#print('Version 1.0.13: 2/1/2024 4:16pm - Nate Calvanese - Updated logic to not include field in select statement when source table cant be joined to')\n",
    "#print('Version 1.0.14: 6/24/2024 1:07pm - Nate Calvanese - Added VALUE_FILTER transformation function')\n",
    "#print('Version 1.0.14: 8/1/2024 1:07pm - Nate Calvanese - Added support for vocab mapping array fields')\n",
    "#print('Version 1.0.15: 8/7/2024 12:02pm - Nate Calvanese - Improved support for vocab mapping array fields')\n",
    "#print('Version 1.0.16: 8/13/2024 4:37pm - Nate Calvanese - Updated bug in COALESCE logic and added SPLIT transform function')\n",
    "#print('Version 1.0.17: 9/19/2024 3:51pm - Nate Calvanese - Tweaked VOCAB_MAP function to trim whitespace when joining non-array values to the vocab table')\n",
    "print('Version 1.0.18: 10/23/2024 2:16pm - Nate Calvanese - Added support for queries without a source table (all hardcoded values)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ingest_pipeline_utilities.ipynb\n",
      "Version 1.0.45: 10/18/2024 2:09pm - Nate Calvanese - Fixed performance bug with find_and_add_fileref_fields function.\n",
      "importing Jupyter notebook from source_files_creation.ipynb\n",
      "Version 1.0.9: 2/25/2023 3:15pm - Nate Calvanese - Replaced FAPI with utils functions\n",
      "importing Jupyter notebook from build_file_inventory.ipynb\n",
      "Version 2.0.4: 10/18/2024 2:19pm - Nate Calvanese - Updated get_objects_list function to not use fuzzy matching for full file paths\n",
      "importing Jupyter notebook from process_table_data.ipynb\n",
      "Version: 1.0.10: 1/12/2024 11:25am - Nate Calvanese - Made max_combined_rec_ref_size configurable\n",
      "importing Jupyter notebook from build_mapping_query.ipynb\n",
      "Version 1.0.17: 9/19/2024 3:51pm - Nate Calvanese - Tweaked VOCAB_MAP function to trim whitespace when joining non-array values to the vocab table\n",
      "importing Jupyter notebook from output_data_validation.ipynb\n",
      "Version 2.0.8: 9/20/2024 9:06pm -- Added high-priority flags in the object returned by the function\n",
      "importing Jupyter notebook from resolve_dangling_foreign_keys.ipynb\n",
      "Version 1.0.4: 09/26/2024 9:28am - Nate Calvanese - Improved logic for handling part_of_dataset field\n",
      "importing Jupyter notebook from infer_file_relationships.ipynb\n",
      "Version 1.0.3: 12/11/2023 1:25pm - Nate Calvanese - Fixed bug in query logic to correct source_datarepo_row_ids\n",
      "importing Jupyter notebook from identify_supplementary_files.ipynb\n",
      "Version 1.0.2: 10/4/2023 10:40am - Nate Calvanese - Updated query logic and added validation\n"
     ]
    }
   ],
   "source": [
    "## Imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from itertools import combinations\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import ingest_pipeline_utilities as utils\n",
    "\n",
    "# Workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support Functions\n",
    "\n",
    "# Function to validate the syntax of a BQ SQL query\n",
    "def run_syntax_check(query):\n",
    "    try:\n",
    "        client = bigquery.Client()\n",
    "        client.query(query).result()\n",
    "        return \"Passed\"\n",
    "    except Exception as e:\n",
    "        return \"Failed: {}\".format(e)\n",
    "\n",
    "# Function to identify if a particular table and column is present in the source\n",
    "def confirm_column_exists(schema, table, column):\n",
    "    column_exists = False\n",
    "    for table_entry in schema[\"tables\"]:\n",
    "        if table_entry[\"name\"] == table:\n",
    "            for column_entry in table_entry[\"columns\"]:\n",
    "                if column_entry[\"name\"] == column:\n",
    "                    column_exists = True\n",
    "    return column_exists\n",
    "\n",
    "# Function to identify the datatype for a particular table and column in the source\n",
    "def return_column_attributes(schema, table, column):\n",
    "    column_datatype = \"string\"\n",
    "    column_array = False\n",
    "    for table_entry in schema[\"tables\"]:\n",
    "        if table_entry[\"name\"] == table:\n",
    "            for column_entry in table_entry[\"columns\"]:\n",
    "                if column_entry[\"name\"] == column:\n",
    "                    column_datatype = column_entry[\"datatype\"]\n",
    "                    column_array = column_entry[\"array_of\"]\n",
    "    return column_datatype, column_array\n",
    "\n",
    "# Function to apply table alias and qualified name for readability and testing\n",
    "def apply_table_alias(query_str, lookup_dict):\n",
    "    for key in lookup_dict:\n",
    "        query_str = query_str.replace(\" \" + key + \".\", \" \" + lookup_dict[key][\"alias\"] + \".\")\n",
    "        query_str = query_str.replace(\"(\" + key + \".\", \"(\" + lookup_dict[key][\"alias\"] + \".\")\n",
    "        query_str = query_str.replace(\"[\" + key + \".\", \"[\" + lookup_dict[key][\"alias\"] + \".\")\n",
    "        query_str = query_str.replace(\"FROM \" + key, \"FROM \" + lookup_dict[key][\"qualified_name\"] + \" \" + lookup_dict[key][\"alias\"])\n",
    "        query_str = query_str.replace(\"JOIN \" + key, \"JOIN \" + lookup_dict[key][\"qualified_name\"] + \" \" + lookup_dict[key][\"alias\"])\n",
    "    return query_str\n",
    "\n",
    "# Function to return table name from fully qualified field name\n",
    "def split_field_name(field):\n",
    "    field_split = field.split(\".\")\n",
    "    return field_split[0], \".\".join(field_split[1:])\n",
    "\n",
    "# Function to parse fields from string\n",
    "def parse_fields_from_string(input_str):\n",
    "    if input_str:\n",
    "        fields = re.search(r\"[(|\\s]([A-Za-z_0-9]+\\.[A-Za-z_0-9]+)[)|\\s]\", input_str)\n",
    "        if fields:\n",
    "            return fields.groups()\n",
    "\n",
    "# Function to validate whether an attribute should be processed -- Exists in target schema and appropriate source fields exist in source schema\n",
    "def validate_attribute(attr_dict, src_schema, tar_schema):\n",
    "    # Confirm attribute exists in target schema\n",
    "    target_field_exists = False\n",
    "    for col in tar_schema[\"columns\"]:\n",
    "        if attr_dict[\"name\"] == col[\"name\"]:\n",
    "            target_field_exists = True\n",
    "            break\n",
    "    if target_field_exists == False:\n",
    "        return False\n",
    "    # Confirm source field exists in source schema (single source field)\n",
    "    if len(attr_dict[\"source\"][\"fields\"]) == 1:\n",
    "        tab_name, col_name = split_field_name(attr_dict[\"source\"][\"fields\"][0])\n",
    "        valid_src = confirm_column_exists(src_schema, tab_name, col_name)\n",
    "        if not valid_src:\n",
    "            return False\n",
    "    # Confirm source fields exist in source schema, if marked as required or used in condition/transform/aggregation (multiple source fields)\n",
    "    elif len(attr_dict[\"source\"][\"fields\"]) > 1:\n",
    "        if attr_dict[\"source\"][\"all_fields_required\"] == True:\n",
    "            found_counter = 0\n",
    "            for fld in attr_dict[\"source\"][\"fields\"]:\n",
    "                tab_name, col_name = split_field_name(fld)\n",
    "                valid_src = confirm_column_exists(src_schema, tab_name, col_name)\n",
    "                if not valid_src:\n",
    "                    return False\n",
    "                else:\n",
    "                    found_counter += 1\n",
    "            if found_counter != len(attr_dict[\"source\"][\"fields\"]):\n",
    "                return False\n",
    "        else:\n",
    "            field_counter = 0\n",
    "            for fld in attr_dict[\"source\"][\"fields\"]:\n",
    "                tab_name, col_name = split_field_name(fld)\n",
    "                valid_src = confirm_column_exists(src_schema, tab_name, col_name)\n",
    "                if not valid_src:\n",
    "                    if attr_dict[\"source\"][\"under_condition\"] != None:\n",
    "                        if fld in attr_dict[\"source\"][\"under_condition\"]:\n",
    "                            return False\n",
    "                    for transform in attr_dict[\"source\"][\"with_transformation\"]:\n",
    "                        for parameter in transform[\"parameters\"]:\n",
    "                            if isinstance(parameter, str) and fld in parameter:\n",
    "                                return False\n",
    "                    if attr_dict[\"source\"][\"with_row_aggregation\"] != None:\n",
    "                        if fld in attr_dict[\"source\"][\"with_row_aggregation\"][\"partition_by\"] or fld in attr_dict[\"source\"][\"with_row_aggregation\"][\"order_by\"]:\n",
    "                            return False\n",
    "                else:\n",
    "                    field_counter += 1\n",
    "            if field_counter == 0:\n",
    "                return False\n",
    "    # Confirm any fields hardcoded in \"under_condition\", \"with_transformation\", or \"with_row_aggregation\" exist\n",
    "    if attr_dict[\"source\"][\"under_condition\"]:\n",
    "        fields = parse_fields_from_string(attr_dict[\"source\"][\"under_condition\"])\n",
    "        if fields:\n",
    "            for field in list(fields):\n",
    "                if field not in attr_dict[\"source\"][\"fields\"]:\n",
    "                    tab_name, col_name = split_field_name(field)\n",
    "                    valid_src = confirm_column_exists(src_schema, tab_name, col_name)\n",
    "                    if not valid_src:\n",
    "                        return False\n",
    "    for transform in attr_dict[\"source\"][\"with_transformation\"]:\n",
    "        for parameter in transform[\"parameters\"]:\n",
    "            fields = parse_fields_from_string(str(parameter)) \n",
    "            if fields:\n",
    "                for field in list(fields):\n",
    "                    if field not in attr_dict[\"source\"][\"fields\"]:\n",
    "                        tab_name, col_name = split_field_name(field)\n",
    "                        valid_src = confirm_column_exists(src_schema, tab_name, col_name)\n",
    "                        if not valid_src:\n",
    "                            return False\n",
    "    if attr_dict[\"source\"][\"with_row_aggregation\"]:\n",
    "        fields = parse_fields_from_string(attr_dict[\"source\"][\"with_row_aggregation\"][\"partition_by\"])\n",
    "        if fields:\n",
    "            for field in list(fields):\n",
    "                if field not in attr_dict[\"source\"][\"fields\"]:\n",
    "                    tab_name, col_name = split_field_name(field)\n",
    "                    valid_src = confirm_column_exists(src_schema, tab_name, col_name)\n",
    "                    if not valid_src:\n",
    "                        return False\n",
    "        fields = parse_fields_from_string(attr_dict[\"source\"][\"with_row_aggregation\"][\"order_by\"])\n",
    "        if fields:\n",
    "            for field in list(fields):\n",
    "                if field not in attr_dict[\"source\"][\"fields\"]:\n",
    "                    tab_name, col_name = split_field_name(field)\n",
    "                    valid_src = confirm_column_exists(src_schema, tab_name, col_name)\n",
    "                    if not valid_src:\n",
    "                        return False\n",
    "    # If everything checks out, return true\n",
    "    return True\n",
    "\n",
    "# Function to validate whether a query should be built for a record set -- Primary key attributes are all found and have valid specifications\n",
    "def validate_record_set(map_set, src_schema_dict, target_table):\n",
    "    target_pk_cols = target_table[\"primaryKey\"]\n",
    "    map_attr_list = [attr[\"name\"] for attr in map_set[\"attributes\"]]\n",
    "    # Confirm all pk_cols are present and have valid specifications for the record set\n",
    "    for pk in target_pk_cols:\n",
    "        if pk not in map_attr_list:\n",
    "            return False\n",
    "        else:\n",
    "            for attr in map_set[\"attributes\"]:\n",
    "                if attr[\"name\"] == pk and not validate_attribute(attr, src_schema_dict, target_table):\n",
    "                    return False\n",
    "    # Confirm any other columns marked as required have valid specifications for the record set\n",
    "    for attr in map_set[\"attributes\"]:\n",
    "        if attr[\"required\"] and not validate_attribute(attr, src_schema_dict, target_table):\n",
    "            return False\n",
    "    # Confirm run criteria for the record set has been met\n",
    "    if map_set.get(\"run_criteria\"):\n",
    "        rc_function = map_set[\"run_criteria\"][\"function\"]\n",
    "        rc_parameters = map_set[\"run_criteria\"][\"parameters\"]\n",
    "        if rc_function == \"ANY_FIELD_EXISTS\":\n",
    "            field_found = False\n",
    "            for field_entry in rc_parameters:\n",
    "                tab_name, col_name = split_field_name(field_entry)\n",
    "                field_found = confirm_column_exists(src_schema_dict, tab_name, col_name)\n",
    "                if field_found:\n",
    "                    break\n",
    "            if not field_found:\n",
    "                return False\n",
    "        elif rc_function == \"ALL_FIELDS_EXIST\":\n",
    "            field_found = True\n",
    "            for field_entry in rc_parameters:\n",
    "                tab_name, col_name = split_field_name(field_entry)\n",
    "                field_found = confirm_column_exists(src_schema_dict, tab_name, col_name)\n",
    "                if not field_found:\n",
    "                    return False\n",
    "    return True\n",
    "        \n",
    "# Function to find and return a direct relationship between two tables, if one exists\n",
    "def confirm_relationship(rel_list, tab1, tab2):\n",
    "    relationship_exists = False\n",
    "    relationship = {}\n",
    "    for rel in rel_list:\n",
    "        if (rel[\"_from\"][\"table\"] == tab1 and rel[\"to\"][\"table\"] == tab2) or (rel[\"_from\"][\"table\"] == tab2 and rel[\"to\"][\"table\"] == tab1):\n",
    "            relationship_exists = True\n",
    "            relationship = rel\n",
    "            break\n",
    "    return relationship_exists, relationship\n",
    "\n",
    "# Function to find the shortest path between two tables, if one exists\n",
    "def shortest_path(rel_list, tab1, tab2):\n",
    "    rels_to_use_list = []\n",
    "    # Check that relationship records exist for both tables\n",
    "    rel_table_set = set()\n",
    "    for rel in rel_list:\n",
    "        rel_table_set.add(rel[\"_from\"][\"table\"])\n",
    "        rel_table_set.add(rel[\"to\"][\"table\"])\n",
    "    if tab1 not in rel_table_set or tab2 not in rel_table_set:\n",
    "        return rels_to_use_list\n",
    "    # Check whether direct relationship exists between tables\n",
    "    direct_exists, direct_rel = confirm_relationship(rel_list, tab1, tab2)\n",
    "    if direct_exists:\n",
    "        rels_to_use_list.append(direct_rel)\n",
    "        return rels_to_use_list\n",
    "    # Check whether indirect relationship exists between tables\n",
    "    rel_table_list = list(rel_table_set)\n",
    "    if len(rel_table_list) > 0:\n",
    "        set_found = False\n",
    "        for i in range(3, len(rel_table_list)+1):\n",
    "            combination_list = list(combinations(rel_table_list, r=i))\n",
    "            for combo in combination_list:\n",
    "                rels_to_use_set = set()\n",
    "                rels_to_use_list = []\n",
    "                if tab1 in combo and tab2 in combo:\n",
    "                    path_exists = False\n",
    "                    for i2 in range(0, i):\n",
    "                        for i3 in range(0, i):\n",
    "                            rel_exists, rel = confirm_relationship(rel_list, combo[i2], combo[i3]) \n",
    "                            if i2 != i3 and rel_exists:\n",
    "                                rel_combo = min(combo[i2], combo[i3]) + \":\" + max(combo[i2], combo[i3])\n",
    "                                if rel_combo not in rels_to_use_set:\n",
    "                                    rels_to_use_set.add(rel_combo)\n",
    "                                    rels_to_use_list.append(rel)\n",
    "                                break\n",
    "                if len(rels_to_use_set) == i-1:\n",
    "                    set_found = True\n",
    "                    break\n",
    "            if set_found == True:\n",
    "                break\n",
    "    # Re-order relationships from tab2 to tab1\n",
    "    rels_to_use_list_ordered = []\n",
    "    tables_referenced_set = set()\n",
    "    tables_referenced_set.add(tab2)\n",
    "    while rels_to_use_list:\n",
    "        for rel_idx, rel in enumerate(rels_to_use_list):\n",
    "            if rel[\"_from\"][\"table\"] in tables_referenced_set and rel[\"to\"][\"table\"] in tables_referenced_set:\n",
    "                pass\n",
    "            elif rel[\"_from\"][\"table\"] in tables_referenced_set:\n",
    "                rels_to_use_list_ordered.append(rel)\n",
    "                tables_referenced_set.add(rel[\"to\"][\"table\"])\n",
    "                rels_to_use_list.pop(rel_idx)\n",
    "            elif rel[\"to\"][\"table\"] in tables_referenced_set:\n",
    "                rels_to_use_list_ordered.append(rel)\n",
    "                tables_referenced_set.add(rel[\"_from\"][\"table\"])\n",
    "                rels_to_use_list.pop(rel_idx)\n",
    "    return rels_to_use_list_ordered\n",
    "\n",
    "# Function to apply row aggregation logic\n",
    "def apply_row_agg_logic(stmt, rowagg_obj):\n",
    "    # Collect partition by parameter - If not present, exit function\n",
    "    if rowagg_obj[\"partition_by\"] == None or rowagg_obj[\"partition_by\"] == \"\":\n",
    "        return stmt\n",
    "    else:\n",
    "        partition_parameter = rowagg_obj[\"partition_by\"]\n",
    "    # Collect order by parameter\n",
    "    if rowagg_obj[\"order_by\"] == None or rowagg_obj[\"partition_by\"] == \"\":\n",
    "        order_parameter = \"1\"\n",
    "    else:\n",
    "        order_parameter = rowagg_obj[\"order_by\"]\n",
    "    # Apply aggregation function \n",
    "    if rowagg_obj[\"function\"] in (\"MAX\", \"MIN\"):\n",
    "        stmt = \"{function}({stmt}) OVER (PARTITION BY {part})\".format(function= rowagg_obj[\"function\"], stmt=stmt, part=partition_parameter, order=order_parameter)\n",
    "    elif rowagg_obj[\"function\"] in (\"FIRST_VALUE\", \"LAST_VALUE\"):\n",
    "        stmt = \"{function}({stmt} IGNORE NULLS) OVER (PARTITION BY {part} ORDER BY {order})\".format(function= rowagg_obj[\"function\"], stmt=stmt, part=partition_parameter, order=order_parameter)\n",
    "    return stmt\n",
    "\n",
    "# Function to parse a VOCAB_MAP select statement into its input expressions\n",
    "def parse_vocab_map_select(stmt):\n",
    "    try:\n",
    "        tar_expr = re.search(\"COALESCE\\((.*?)\\,\", stmt).group(1)\n",
    "        src_expr = re.search(\"COALESCE\\(.*\\, (.*)\\)\", stmt).group(1)\n",
    "        array = False\n",
    "    except:\n",
    "        tar_expr = re.search(\"CASE WHEN (.*) IS NOT NULL\", stmt).group(1)\n",
    "        src_expr = re.search(\"PARTITION BY .* ELSE (.*) END\\)$\", stmt).group(1)\n",
    "        array = True\n",
    "    return src_expr, tar_expr, array\n",
    "    \n",
    "# Function to apply transformation logic\n",
    "def apply_transformation_logic(stmt, attr_name, transform_obj, made_array, from_clause, target_table, src_schema_dict, record_set_map):\n",
    "    for transform in transform_obj:\n",
    "        if transform[\"function\"] == \"UUID\":\n",
    "            if made_array == False:\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.uuid_hash_value({})\".format(stmt)\n",
    "            else:\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.uuid_hash_array_values({})\".format(stmt)\n",
    "        elif transform[\"function\"] == \"HARDCODE\":\n",
    "            hardcode_param = transform[\"parameters\"][0].format(previous=stmt)\n",
    "            stmt = hardcode_param\n",
    "        elif transform[\"function\"] == \"CUSTOM\":\n",
    "            custom_logic_param = transform[\"parameters\"][0].format(previous=stmt)\n",
    "            array_param = transform[\"parameters\"][1]\n",
    "            stmt = custom_logic_param\n",
    "            if array_param == \"array\":\n",
    "                made_array = True\n",
    "        elif transform[\"function\"] == \"VOCAB_MAP\":\n",
    "            attribute_param = transform[\"parameters\"][0]\n",
    "            default_to_source_param = transform[\"parameters\"][1]\n",
    "            if made_array == False:\n",
    "                from_clause += \" LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = '{param}') vm_{param} ON TRIM(vm_{param}.source_value) = TRIM(CAST({stmt} AS STRING))\".format(param=attribute_param, stmt=stmt)\n",
    "                if default_to_source_param == True:\n",
    "                    stmt = \"COALESCE(vm_{param}.mapped_value, CAST({stmt} AS STRING))\".format(param=attribute_param, stmt=stmt)\n",
    "                else:\n",
    "                    stmt = \"vm_{param}.mapped_value\".format(param=attribute_param)\n",
    "            else:\n",
    "                from_clause += \" LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = '{param}') vm_{param} ON vm_{param}.source_value IN UNNEST({stmt})\".format(param=attribute_param, stmt=stmt)\n",
    "                entity = target_table[\"name\"]\n",
    "                for map_attr in record_set_map[\"attributes\"]:\n",
    "                    if map_attr[\"name\"] == entity.replace(\"anvil_\", \"\") + \"_id\":\n",
    "                        pk_stmt, temp_from, temp_where = construct_select(map_attr, target_table, src_schema_dict, \"\", \"\", record_set_map)\n",
    "                if default_to_source_param:\n",
    "                    stmt = \"`dsp-data-ingest.transform_resources`.dedupe_array(CASE WHEN vm_{param}.mapped_value IS NOT NULL THEN ARRAY_AGG(vm_{param}.mapped_value) OVER (PARTITION BY {pk_stmt}) ELSE {stmt} END)\".format(param=attribute_param, stmt=stmt, pk_stmt=pk_stmt)\n",
    "                else:\n",
    "                    stmt = \"`dsp-data-ingest.transform_resources`.dedupe_array(CASE WHEN vm_{param}.mapped_value IS NOT NULL THEN ARRAY_AGG(vm_{param}.mapped_value) OVER (PARTITION BY {pk_stmt}) ELSE [] END)\".format(param=attribute_param, stmt=stmt, pk_stmt=pk_stmt)\n",
    "        elif transform[\"function\"] == \"PREFIX\":\n",
    "            array_param = transform[\"parameters\"][0]\n",
    "            prefix_param = transform[\"parameters\"][1].format(previous=stmt)\n",
    "            input_param = transform[\"parameters\"][2].format(previous=stmt)\n",
    "            if array_param == \"array\":\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.prefix_array_values(\" + prefix_param + \", \" + input_param + \")\"\n",
    "            elif array_param == \"non-array\":\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.prefix_value(\" + prefix_param + \", \" + input_param + \")\"\n",
    "        elif transform[\"function\"] == \"VALIDATE_UUID\":\n",
    "            if made_array == False:\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.validate_uuid_value({})\".format(stmt)\n",
    "            else:\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.validate_uuid_array_values({})\".format(stmt)\n",
    "        elif transform[\"function\"] == \"EXPLODE\":\n",
    "            from_clause += \" CROSS JOIN UNNEST({}) {}\".format(stmt, attr_name)\n",
    "            stmt = attr_name\n",
    "            made_array = False\n",
    "        elif transform[\"function\"] == \"SAFE_CAST\":\n",
    "            tar_type_param = transform[\"parameters\"][0]\n",
    "            stmt = \"SAFE_CAST({stmt} AS {type})\".format(stmt=stmt, type=tar_type_param)\n",
    "        elif transform[\"function\"] == \"VALUE_FILTER\":\n",
    "            default_val_param = transform[\"parameters\"][0]\n",
    "            stmt = \"CASE WHEN {stmt} IN (SELECT file_extension FROM `dsp-data-ingest.transform_resources.acceptable_file_extensions`) THEN {stmt} ELSE {default} END\".format(stmt=stmt, default=default_val_param)\n",
    "        elif transform[\"function\"] == \"SPLIT\":\n",
    "            delim_param = transform[\"parameters\"][0]\n",
    "            stmt = \"SPLIT(CAST({stmt} AS STRING), '{delim_param}')\".format(stmt=stmt, delim_param=delim_param)\n",
    "            made_array = True\n",
    "    return stmt, made_array, from_clause\n",
    "\n",
    "# Function to construct select statement for a specific field\n",
    "def construct_select(attr_dict, tar_schema, src_schema, from_clause, where_clause, record_set_map):\n",
    "    made_array = False\n",
    "    # Add target field attributes to attribute mapping\n",
    "    for col in tar_schema[\"columns\"]:\n",
    "        if attr_dict[\"name\"] == col[\"name\"]:\n",
    "            attr_dict[\"datatype\"] = col[\"datatype\"]\n",
    "            attr_dict[\"array_of\"] = col[\"array_of\"]\n",
    "            if attr_dict[\"required\"] == False:\n",
    "                if col.get(\"required\") and col[\"required\"] == True:\n",
    "                    attr_dict[\"required\"] = True\n",
    "    for pk_col in tar_schema[\"primaryKey\"]:\n",
    "        if attr_dict[\"name\"] == pk_col:\n",
    "            attr_dict[\"required\"] = True\n",
    "    \n",
    "    # Set base attribute based on number of source fields, target field type, and specified column aggregation\n",
    "    if len(attr_dict[\"source\"][\"fields\"]) == 0:\n",
    "        attr_stmt = \"\"\n",
    "    elif len(attr_dict[\"source\"][\"fields\"]) == 1:\n",
    "        attr_stmt = attr_dict[\"source\"][\"fields\"][0]\n",
    "    else:\n",
    "        # Create filtered field list for use in column aggregation functions as needed\n",
    "        filtered_field_list = []\n",
    "        for field in attr_dict[\"source\"][\"fields\"]:\n",
    "            tab_name, col_name = split_field_name(field)\n",
    "            if confirm_column_exists(src_schema, tab_name, col_name):\n",
    "                for src_tab in src_schema[\"tables\"]:\n",
    "                    if src_tab[\"name\"] == tab_name:\n",
    "                        for src_col in src_tab[\"columns\"]:\n",
    "                            if src_col[\"name\"] == col_name:\n",
    "                                if src_col[\"array_of\"] == True:\n",
    "                                    array_field = f\"\"\"IF(ARRAY_TO_STRING({field}, ',') NOT IN ('', '-', '\"', \"'\"), ARRAY_TO_STRING({field}, ','), NULL)\"\"\"\n",
    "                                    filtered_field_list.append(array_field)\n",
    "                                else:\n",
    "                                    if src_col[\"datatype\"] in [\"string\", \"fileref\"]:\n",
    "                                        non_array_field = f\"\"\"IF(NULLIF({field}, '') NOT IN ('', '-', '\"', \"'\"), NULLIF({field}, ''), NULL)\"\"\"\n",
    "                                        filtered_field_list.append(non_array_field)\n",
    "                                    else:\n",
    "                                        non_array_field = f\"CAST({field} AS STRING)\"\n",
    "                                        filtered_field_list.append(non_array_field)\n",
    "        # Field Coalesce column aggregation logic\n",
    "        if attr_dict[\"source\"][\"with_column_aggregation\"] == \"FIELD_COALESCE\":\n",
    "            attr_stmt = \"\"\n",
    "            for field in attr_dict[\"source\"][\"fields\"]:\n",
    "                tab_name, col_name = split_field_name(field)\n",
    "                if confirm_column_exists(src_schema, tab_name, col_name):\n",
    "                    attr_stmt = field\n",
    "                    break\n",
    "        # Coalesce column aggregation logic\n",
    "        elif attr_dict[\"source\"][\"with_column_aggregation\"] == \"COALESCE\":\n",
    "            array_cnt = 0\n",
    "            non_array_cnt = 0\n",
    "            for field in attr_dict[\"source\"][\"fields\"]:\n",
    "                tab_name, col_name = split_field_name(field)\n",
    "                column_datatype, column_array = return_column_attributes(src_schema, tab_name, col_name)\n",
    "                if column_array == True:\n",
    "                    array_cnt += 1\n",
    "                else:\n",
    "                    non_array_cnt += 1\n",
    "            if array_cnt == 0:\n",
    "                field_list_str = ', '.join(filtered_field_list)\n",
    "                coalesce_stmt = \"COALESCE({})\".format(field_list_str)\n",
    "                attr_stmt = \"SPLIT((SELECT STRING_AGG(DISTINCT col, ',') FROM UNNEST(SPLIT(TRIM(FORMAT('%t', (SELECT AS STRUCT {})), '()'), ', ')) AS col WHERE NOT UPPER(col) = 'NULL'), ',')\".format(coalesce_stmt)\n",
    "                made_array = True\n",
    "                if attr_dict[\"array_of\"] == False:\n",
    "                    attr_stmt = \"ARRAY_TO_STRING({}, ', ')\".format(attr_stmt)\n",
    "                    made_array = False\n",
    "            else:\n",
    "                attr_stmt = \"ERROR - COALESCE specified for array fields\"\n",
    "        # Default column aggregation logic\n",
    "        else:\n",
    "            field_list_str = ', '.join(filtered_field_list)\n",
    "            attr_stmt = \"SPLIT((SELECT STRING_AGG(DISTINCT col, ',') FROM UNNEST(SPLIT(TRIM(FORMAT('%t', (SELECT AS STRUCT {})), '()'), ', ')) AS col WHERE NOT UPPER(col) = 'NULL'), ',')\".format(field_list_str)\n",
    "            made_array = True\n",
    "            if attr_dict[\"array_of\"] == False:\n",
    "                attr_stmt = \"ARRAY_TO_STRING({}, ', ')\".format(attr_stmt)\n",
    "                made_array = False\n",
    "    \n",
    "    # Apply transformations\n",
    "    if len(attr_dict[\"source\"][\"with_transformation\"]) > 0:\n",
    "        attr_stmt, made_array, from_clause = apply_transformation_logic(attr_stmt, attr_dict[\"name\"], attr_dict[\"source\"][\"with_transformation\"], made_array, from_clause, tar_schema, src_schema, record_set_map)\n",
    "    \n",
    "    # Apply case logic    \n",
    "    if attr_dict[\"source\"][\"under_condition\"] != None:\n",
    "        attr_stmt = \"CASE WHEN \" + attr_dict[\"source\"][\"under_condition\"] + \" THEN {} END\".format(attr_stmt)\n",
    "    \n",
    "    # Apply row aggregation logic\n",
    "    if attr_dict[\"source\"][\"with_row_aggregation\"] != None:\n",
    "        attr_stmt = apply_row_agg_logic(attr_stmt, attr_dict[\"source\"][\"with_row_aggregation\"])\n",
    "    \n",
    "    # Construct final select clauses\n",
    "    if attr_dict[\"array_of\"] == True and made_array == False:\n",
    "        attr_stmt = \"CASE WHEN {attr} IS NOT NULL THEN [{attr}] ELSE [] END\".format(attr=attr_stmt)\n",
    "    elif attr_dict[\"array_of\"] == False and made_array == True:\n",
    "        attr_stmt = \"ARRAY_TO_STRING({}, ', ')\".format(attr_stmt)\n",
    "    \n",
    "    # Extend the where_clause for required fields\n",
    "    if attr_dict[\"required\"] == True:\n",
    "        if attr_dict[\"array_of\"] == False:\n",
    "            where_clause += \" AND {} IS NOT NULL\".format(attr_stmt)\n",
    "        else:\n",
    "            where_clause += \" AND ARRAY_LENGTH({}) > 0\".format(attr_stmt)\n",
    "    \n",
    "    # Returned the altered clauses\n",
    "    return attr_stmt, from_clause, where_clause\n",
    "\n",
    "# Function to construct join statement between a specific set of fields\n",
    "def construct_join(schema, base_table, base_field, join_table, join_field, join_type=\"LEFT\"):\n",
    "    b_type, b_array = return_column_attributes(schema, base_table, base_field)\n",
    "    j_type, j_array = return_column_attributes(schema, join_table, join_field) \n",
    "    if b_array == False and j_array == False:\n",
    "        if b_type == j_type:\n",
    "            join_clause = \" \" + join_type + \" JOIN \" + base_table + \" ON \" + base_table + \".\" + base_field + \" = \" + join_table + \".\" + join_field\n",
    "        else:\n",
    "            join_clause = \" \" + join_type + \" JOIN \" + base_table + \" ON CAST(\" + base_table + \".\" + base_field + \" AS STRING) = CAST(\" + join_table + \".\" + join_field + \" AS STRING)\"\n",
    "    elif b_array == False and j_array == True:\n",
    "        if b_type == j_type:\n",
    "            join_clause = \" \" + join_type + \" JOIN \" + base_table + \" ON \" + base_table + \".\" + base_field + \" IN UNNEST(\" + join_table + \".\" + join_field + \")\"\n",
    "    elif b_array == True and j_array == False:\n",
    "        if b_type == j_type:\n",
    "            join_clause = \" \" + join_type + \" JOIN \" + base_table + \" ON \" + join_table + \".\" + join_field + \" IN UNNEST(\" + base_table + \".\" + base_field + \")\"\n",
    "    return join_clause\n",
    "\n",
    "# Function to update aliases in a mapping specification\n",
    "def update_mapping_spec_aliases(mapping_spec, src_schema_dict):\n",
    "    if mapping_spec.get(\"aliases\"):\n",
    "        # Create full field list from src_schema_dict\n",
    "        existing_field_list = []\n",
    "        for table in src_schema_dict[\"tables\"]:\n",
    "            table_name = table[\"name\"]\n",
    "            for column in table[\"columns\"]:\n",
    "                field_name = table_name + \".\" + column[\"name\"]\n",
    "                existing_field_list.append(field_name)\n",
    "        # Loop through aliases in mapping_spec\n",
    "        for field, aliases in mapping_spec[\"aliases\"].items():\n",
    "            if field in existing_field_list:\n",
    "                continue\n",
    "            for alias in aliases:\n",
    "                if alias in existing_field_list:\n",
    "                    mapping_spec_str = json.dumps(mapping_spec).replace(field, alias)\n",
    "                    mapping_spec = json.loads(mapping_spec_str)\n",
    "                    break\n",
    "    # Return final dictionary\n",
    "    return mapping_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main Function - Building mapping transformation queries\n",
    "def build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema):\n",
    "\n",
    "    # Initialize function variables\n",
    "    dataset = bq_project + \".\" + bq_schema\n",
    "    query_dict = {\"query\": None, \"syntax_check\": None} \n",
    "\n",
    "    # Identify the entity of interest and it's place in the mapping specification\n",
    "    entity_map_idx = None\n",
    "    entity = target_table[\"name\"]\n",
    "    for idx, entry in enumerate(mapping_spec[\"entities\"]):\n",
    "        if entry[\"name\"] == entity:\n",
    "            entity_map_idx = idx\n",
    "    if entity_map_idx == None:\n",
    "        return query_dict\n",
    "\n",
    "    # Start with mapping_specification, locate the entity of interest, and loop through the record sets associated with it\n",
    "    final_query_set = set()\n",
    "    for set_idx, map_set in enumerate(mapping_spec[\"entities\"][entity_map_idx][\"record_sets\"]):\n",
    "        \n",
    "        # Validate whether record set should be processed or skipped\n",
    "        if not validate_record_set(map_set, src_schema_dict, target_table):\n",
    "            continue\n",
    "\n",
    "        # Initialize query variables\n",
    "        select_clause = \"SELECT \"\n",
    "        source_row_agg = \"\"\n",
    "        from_clause = \"\"\n",
    "        where_clause = \"WHERE 1=1\"\n",
    "        table_alias_lookup = {}\n",
    "        \n",
    "        # Loop through and identify valid attributes in the record set\n",
    "        attribute_set = set()\n",
    "        for attr_idx, attr in enumerate(map_set[\"attributes\"]):\n",
    "            if validate_attribute(attr, src_schema_dict, target_table):\n",
    "                attribute_set.add(attr[\"name\"])\n",
    "        \n",
    "        # Loop through columns in target schema and build attribute, either from record set (if valid) or default logic\n",
    "        for column_entry in target_table[\"columns\"]:\n",
    "            if column_entry[\"name\"] in [\"source_datarepo_row_ids\"]:\n",
    "                continue\n",
    "            elif column_entry[\"name\"] not in attribute_set:\n",
    "                if column_entry[\"array_of\"] == True:\n",
    "                    select_clause += \"[] AS {}, \".format(column_entry[\"name\"])\n",
    "                else:\n",
    "                    select_clause += \"NULL AS {}, \".format(column_entry[\"name\"])\n",
    "            else:\n",
    "                for attr_idx, attr in enumerate(map_set[\"attributes\"]):\n",
    "                    if attr[\"name\"] == column_entry[\"name\"]:\n",
    "                        # Build/extend from clause based on source fields\n",
    "                        missing_relationship = False\n",
    "                        for fld_idx, fld in enumerate(attr[\"source\"][\"fields\"]):\n",
    "                            tab_name, col_name = split_field_name(fld)\n",
    "                            if confirm_column_exists(src_schema_dict, tab_name, col_name):\n",
    "                                if from_clause == \"\":\n",
    "                                    base_table, base_field = split_field_name(attr[\"source\"][\"fields\"][0])\n",
    "                                    table_iter = 0\n",
    "                                    table_alias_lookup = {base_table: {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + base_table + \"`\"}}\n",
    "                                    from_clause = \"FROM \" + base_table\n",
    "                                    source_row_agg = \"ARRAY_AGG(DISTINCT '{}:'||t0.datarepo_row_id)\".format(base_table)\n",
    "                                    source_row_agg = \"['{}:'||t{}.datarepo_row_id]\".format(base_table, str(table_iter))\n",
    "                                elif tab_name not in table_alias_lookup:\n",
    "                                    join_clause = \"\"\n",
    "                                    relationships = shortest_path(src_schema_dict[\"relationships\"], tab_name, base_table)\n",
    "                                    if relationships:\n",
    "                                        for rel in relationships:\n",
    "                                            if rel[\"_from\"][\"table\"] not in table_alias_lookup:\n",
    "                                                join_clause = construct_join(src_schema_dict, rel[\"_from\"][\"table\"], rel[\"_from\"][\"column\"], rel[\"to\"][\"table\"], rel[\"to\"][\"column\"], \"LEFT\")\n",
    "                                                from_clause += join_clause\n",
    "                                                table_iter += 1\n",
    "                                                table_alias_lookup[rel[\"_from\"][\"table\"]] = {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + rel[\"_from\"][\"table\"] + \"`\"}\n",
    "                                                source_row_agg += \", CASE WHEN t{}.datarepo_row_id IS NOT NULL THEN ['{}:'||t{}.datarepo_row_id] ELSE [] END\".format(str(table_iter), rel[\"_from\"][\"table\"], str(table_iter))\n",
    "                                            elif rel[\"to\"][\"table\"] not in table_alias_lookup:\n",
    "                                                join_clause = construct_join(src_schema_dict, rel[\"to\"][\"table\"], rel[\"to\"][\"column\"], rel[\"_from\"][\"table\"], rel[\"_from\"][\"column\"], \"LEFT\")\n",
    "                                                from_clause += join_clause\n",
    "                                                table_iter += 1\n",
    "                                                table_alias_lookup[rel[\"to\"][\"table\"]] = {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + rel[\"to\"][\"table\"] + \"`\"}\n",
    "                                                source_row_agg += \", CASE WHEN t{}.datarepo_row_id IS NOT NULL THEN ['{}:'||t{}.datarepo_row_id] ELSE [] END\".format(str(table_iter), rel[\"to\"][\"table\"], str(table_iter))\n",
    "                                    else:\n",
    "                                        missing_relationship = True\n",
    "                                        \n",
    "                        # Build select clause for attribute \n",
    "                        if not missing_relationship:\n",
    "                            attr_select, from_clause, where_clause = construct_select(attr, target_table, src_schema_dict, from_clause, where_clause, map_set)\n",
    "                            select_clause += \"{stmt} AS {name}, \".format(stmt=attr_select, name=attr[\"name\"])\n",
    "             \n",
    "        # Build final queries and add to final_query_set\n",
    "        if from_clause:\n",
    "            select_clause += \"ARRAY_CONCAT({}) AS source_datarepo_row_ids, \".format(source_row_agg)\n",
    "        else:\n",
    "            select_clause += \"[] AS source_datarepo_row_ids, \"\n",
    "            from_clause = \"FROM (SELECT 1)\"\n",
    "        select_clause += \"{} AS record_set_priority\".format(set_idx)\n",
    "        initial_root_query = select_clause + \" \" + from_clause + \" \" + where_clause\n",
    "        final_root_query = apply_table_alias(initial_root_query, table_alias_lookup)\n",
    "        final_query_set.add(final_root_query)\n",
    "\n",
    "    # Convert final_query_set into a union of queries if multiple record sets are present\n",
    "    final_entity_query = \"\"\n",
    "    if len(final_query_set) == 0:\n",
    "        return query_dict\n",
    "    else:\n",
    "        for idx, entry in enumerate(final_query_set):\n",
    "            if idx == 0:\n",
    "                final_entity_query += entry\n",
    "            else:\n",
    "                final_entity_query += \" UNION ALL \" + entry\n",
    "\n",
    "    # Add deduplication logic \n",
    "    pk_col_string = \", \".join(target_table[\"primaryKey\"])\n",
    "    deduped_entity_query = \"SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY {pk_cols} ORDER BY record_set_priority, 1) AS rn FROM ({query})) WHERE rn = 1\".format(pk_cols=pk_col_string, query=final_entity_query)\n",
    "    query_dict[\"query\"] = deduped_entity_query\n",
    "    #print(deduped_entity_query)\n",
    "\n",
    "    # Run syntax check and return query\n",
    "    smoke_test_query = deduped_entity_query.replace(\"WHERE 1=1\", \"WHERE 1=0\")\n",
    "    query_dict[\"syntax_check\"] = run_syntax_check(smoke_test_query)\n",
    "    return query_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Secondary function -- Building and executing vocab mapping validation\n",
    "def evaluate_vocab_mapping(mapping_spec, src_schema_dict, target_schema_dict, bq_project, bq_schema):\n",
    "    \n",
    "    # Initialize function variables\n",
    "    dataset = bq_project + \".\" + bq_schema\n",
    "    \n",
    "    # Determine valid vocab mapping attributes\n",
    "    vocab_map_valid_attr_set = set()\n",
    "    for entity in mapping_spec[\"entities\"]:\n",
    "        target_table = {}\n",
    "        for table_entry in target_schema_dict[\"tables\"]:\n",
    "            if table_entry[\"name\"] == entity[\"name\"]:\n",
    "                target_table = table_entry\n",
    "        for record_set in entity[\"record_sets\"]:\n",
    "            if validate_record_set(record_set, src_schema_dict, target_table): \n",
    "                for attribute in record_set[\"attributes\"]:\n",
    "                    for transform in attribute[\"source\"][\"with_transformation\"]:\n",
    "                        if transform[\"function\"] == \"VOCAB_MAP\":\n",
    "                            if validate_attribute(attribute, src_schema_dict, target_table):\n",
    "                                vocab_map_valid_attr_set.add(entity[\"name\"] + \".\" + attribute[\"name\"])\n",
    "    \n",
    "    # Loop through valid vocab mapping attributes and build mapping queries\n",
    "    eval_query_set = set()\n",
    "    for entity in mapping_spec[\"entities\"]:\n",
    "        target_table = {}\n",
    "        for table_entry in target_schema_dict[\"tables\"]:\n",
    "            if table_entry[\"name\"] == entity[\"name\"]:\n",
    "                target_table = table_entry\n",
    "        for record_set in entity[\"record_sets\"]:\n",
    "            for attr in record_set[\"attributes\"]:\n",
    "                if (entity[\"name\"] + \".\" + attr[\"name\"]) in vocab_map_valid_attr_set:\n",
    "        \n",
    "                    # Initialize query variables\n",
    "                    select_clause = \"SELECT \"\n",
    "                    from_clause = \"\"\n",
    "                    where_clause = \"WHERE 1=1\"\n",
    "                    group_by_clause = \"GROUP BY \"\n",
    "\n",
    "                    # Build/extend from clause based on source fields\n",
    "                    for fld_idx, fld in enumerate(attr[\"source\"][\"fields\"]):\n",
    "                        tab_name, col_name = split_field_name(fld)\n",
    "                        if confirm_column_exists(src_schema_dict, tab_name, col_name):\n",
    "                            if from_clause == \"\":\n",
    "                                base_table, base_field = split_field_name(attr[\"source\"][\"fields\"][0])\n",
    "                                table_iter = 0\n",
    "                                table_alias_lookup = {base_table: {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + base_table + \"`\"}}\n",
    "                                from_clause = \"FROM \" + base_table\n",
    "                            elif tab_name not in table_alias_lookup:\n",
    "                                join_clause = \"\"\n",
    "                                relationships = shortest_path(src_schema_dict[\"relationships\"], tab_name, base_table)\n",
    "                                for rel in relationships:\n",
    "                                    if rel[\"_from\"][\"table\"] not in table_alias_lookup:\n",
    "                                        join_clause = construct_join(src_schema_dict, rel[\"_from\"][\"table\"], rel[\"_from\"][\"column\"], rel[\"to\"][\"table\"], rel[\"to\"][\"column\"], \"LEFT\")\n",
    "                                        from_clause += join_clause\n",
    "                                        table_iter += 1\n",
    "                                        table_alias_lookup[rel[\"_from\"][\"table\"]] = {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + rel[\"_from\"][\"table\"] + \"`\"}\n",
    "                                    elif rel[\"to\"][\"table\"] not in table_alias_lookup:\n",
    "                                        join_clause = construct_join(src_schema_dict, rel[\"to\"][\"table\"], rel[\"to\"][\"column\"], rel[\"_from\"][\"table\"], rel[\"_from\"][\"column\"], \"LEFT\")\n",
    "                                        from_clause += join_clause\n",
    "                                        table_iter += 1\n",
    "                                        table_alias_lookup[rel[\"to\"][\"table\"]] = {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + rel[\"to\"][\"table\"] + \"`\"}\n",
    "\n",
    "                    # Build select clause for attribute\n",
    "                    attr_transforms = attr[\"source\"][\"with_transformation\"]\n",
    "                    for idx, transform in enumerate(attr_transforms):\n",
    "                        if transform[\"function\"] == \"VOCAB_MAP\":\n",
    "                            attr[\"source\"][\"with_transformation\"][idx][\"parameters\"][1] = True\n",
    "                    attr_select, from_clause, where_clause = construct_select(attr, target_table, src_schema_dict, from_clause, where_clause, record_set)\n",
    "                    src_expr, map_expr, array_src = parse_vocab_map_select(attr_select)\n",
    "                    attr_name = attr[\"name\"]\n",
    "                    select_clause += f\"'{attr_name}' AS attribute, {src_expr} AS source_value, {map_expr} AS mapped_value, t0.datarepo_row_id\"\n",
    "                    \n",
    "                    # Build final queries and add to final_query_set\n",
    "                    initial_root_query = select_clause + \" \" + from_clause + \" \" + where_clause\n",
    "                    if array_src:\n",
    "                        interim_root_query = f\"\"\"SELECT t0.attribute, flattened_source_value AS source_value, vm_{attr_name}.mapped_value AS mapped_value, COUNT(DISTINCT datarepo_row_id) AS record_count FROM ({initial_root_query}) t0 CROSS JOIN UNNEST(source_value) AS flattened_source_value LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = '{attr_name}') vm_{attr_name} ON vm_{attr_name}.source_value = flattened_source_value GROUP BY t0.attribute, flattened_source_value, vm_{attr_name}.mapped_value\"\"\"\n",
    "                    else:\n",
    "                        interim_root_query = f\"\"\"SELECT attribute, source_value, mapped_value, COUNT(*) AS record_count FROM ({initial_root_query}) GROUP BY attribute, source_value, mapped_value\"\"\"\n",
    "                    final_root_query = apply_table_alias(interim_root_query, table_alias_lookup)\n",
    "                    eval_query_set.add(final_root_query)   \n",
    "    \n",
    "    # Convert eval_query_set into a union of queries if vocab map fields are present\n",
    "    client = bigquery.Client()\n",
    "    df = pd.DataFrame(columns = [\"attribute\", \"source_value\", \"mapped_value\", \"record_count\"])\n",
    "    final_eval_query = \"\"\n",
    "    if len(eval_query_set) == 0:\n",
    "        return df\n",
    "    else:\n",
    "        for idx, entry in enumerate(eval_query_set):\n",
    "            if idx == 0:\n",
    "                final_eval_query += entry\n",
    "            else:\n",
    "                final_eval_query += \" UNION ALL \" + entry \n",
    "        final_eval_query += \" ORDER BY attribute, record_count DESC\"\n",
    "    \n",
    "    # Execute query and return dataframe with results\n",
    "    try:\n",
    "        df2 = client.query(final_eval_query).result().to_dataframe()\n",
    "        df = pd.concat([df, df2])\n",
    "    except Exception as e:\n",
    "        print(\"Error during query execution: {}\".format(str(e)))\n",
    "    return df\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>target_table</th>\n",
       "      <th>query</th>\n",
       "      <th>syntax_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anvil_activity</td>\n",
       "      <td>SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY activity_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value(`dsp-data-ingest.transform_resources`.prefix_value('activity', t0.file_id)) AS activity_id, 'Unknown' AS activity_type, [] AS used_file_id, CASE WHEN `dsp-data-ingest.transform_resources`.validate_uuid_value(t0.file_ref) IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.validate_uuid_value(t0.file_ref)] ELSE [] END AS generated_file_id, CASE WHEN `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.associated_biosample_id) IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.uuid_hash_value(t0.associated_biosample_id)] ELSE [] END AS used_biosample_id, ARRAY_CONCAT(['file:'||t0.datarepo_row_id]) AS source_datarepo_row_ids, 0 AS record_set_priority FROM `datarepo-d344de2a.datarepo_ANVIL_HudsonAlpha_LR_v1_GRU_20241018.file` t0 WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value(`dsp-data-ingest.transform_resources`.prefix_value('activity', t0.file_id)) IS NOT NULL AND ARRAY_LENGTH(CASE WHEN `dsp-data-ingest.transform_resources`.validate_uuid_value(t0.file_ref) IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.validate_uuid_value(t0.file_ref)] ELSE [] END) &gt; 0 AND ARRAY_LENGTH(CASE WHEN `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.associated_biosample_id) IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.uuid_hash_value(t0.associated_biosample_id)] ELSE [] END) &gt; 0)) WHERE rn = 1</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anvil_alignmentactivity</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anvil_antibody</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anvil_assayactivity</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anvil_biosample</td>\n",
       "      <td>SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY biosample_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.biosample_id) AS biosample_id, vm_anatomical_site.mapped_value AS anatomical_site, [] AS apriori_cell_type, vm_biosample_type.mapped_value AS biosample_type, NULL AS disease, t0.donor_age_at_collection_unit AS donor_age_at_collection_unit, SAFE_CAST(t0.donor_age_at_collection_lower_bound AS INT64) AS donor_age_at_collection_lower_bound, SAFE_CAST(t0.donor_age_at_collection_upper_bound AS INT64) AS donor_age_at_collection_upper_bound, CASE WHEN `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id) IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id)] ELSE [] END AS donor_id, CASE WHEN `dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU') IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU')] ELSE [] END AS part_of_dataset_id, ARRAY_CONCAT(['biosample:'||t0.datarepo_row_id]) AS source_datarepo_row_ids, 0 AS record_set_priority FROM `datarepo-d344de2a.datarepo_ANVIL_HudsonAlpha_LR_v1_GRU_20241018.biosample` t0 LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = 'anatomical_site') vm_anatomical_site ON TRIM(vm_anatomical_site.source_value) = TRIM(CAST(t0.anatomical_site AS STRING)) LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = 'biosample_type') vm_biosample_type ON TRIM(vm_biosample_type.source_value) = TRIM(CAST(t0.biosample_type AS STRING)) WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.biosample_id) IS NOT NULL)) WHERE rn = 1</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anvil_dataset</td>\n",
       "      <td>SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY dataset_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU') AS dataset_id, CASE WHEN 'GRU' IS NOT NULL THEN ['GRU'] ELSE [] END AS consent_group, CASE WHEN 'GRU' IS NOT NULL THEN ['GRU'] ELSE [] END AS data_use_permission, [] AS owner, [] AS principal_investigator, CASE WHEN 'phs003537' IS NOT NULL THEN ['phs003537'] ELSE [] END AS registered_identifier, 'ANVIL_HudsonAlpha_LR_v1_GRU' AS title, SPLIT(CAST((SELECT FIRST_VALUE(mapped_value) OVER (ORDER BY 1) FROM `dsp-data-ingest.transform_resources.dataset_map` WHERE attribute = 'data_modality' and dataset = 'ANVIL_HudsonAlpha_LR_v1_GRU') AS STRING), ',') AS data_modality, [] AS source_datarepo_row_ids, 0 AS record_set_priority FROM (SELECT 1) WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU') IS NOT NULL)) WHERE rn = 1</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>anvil_diagnosis</td>\n",
       "      <td>SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY diagnosis_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value(`dsp-data-ingest.transform_resources`.prefix_value('diagnosis', t0.donor_id)) AS diagnosis_id, `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id) AS donor_id, CASE WHEN vm_disease.mapped_value IS NOT NULL THEN [vm_disease.mapped_value] ELSE [] END AS disease, NULL AS diagnosis_age_unit, NULL AS diagnosis_age_lower_bound, NULL AS diagnosis_age_upper_bound, NULL AS onset_age_unit, NULL AS onset_age_lower_bound, NULL AS onset_age_upper_bound, [] AS phenotype, [] AS phenopacket, ARRAY_CONCAT(['biosample:'||t0.datarepo_row_id]) AS source_datarepo_row_ids, 0 AS record_set_priority FROM `datarepo-d344de2a.datarepo_ANVIL_HudsonAlpha_LR_v1_GRU_20241018.biosample` t0 LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = 'disease') vm_disease ON TRIM(vm_disease.source_value) = TRIM(CAST(t0.disease AS STRING)) WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value(`dsp-data-ingest.transform_resources`.prefix_value('diagnosis', t0.donor_id)) IS NOT NULL AND `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id) IS NOT NULL AND ARRAY_LENGTH(CASE WHEN vm_disease.mapped_value IS NOT NULL THEN [vm_disease.mapped_value] ELSE [] END) &gt; 0)) WHERE rn = 1</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>anvil_donor</td>\n",
       "      <td>SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY donor_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id) AS donor_id, vm_organism_type.mapped_value AS organism_type, `dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU') AS part_of_dataset_id, vm_phenotypic_sex.mapped_value AS phenotypic_sex, [] AS reported_ethnicity, [] AS genetic_ancestry, ARRAY_CONCAT(['donor:'||t0.datarepo_row_id]) AS source_datarepo_row_ids, 0 AS record_set_priority FROM `datarepo-d344de2a.datarepo_ANVIL_HudsonAlpha_LR_v1_GRU_20241018.donor` t0 LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = 'organism_type') vm_organism_type ON TRIM(vm_organism_type.source_value) = TRIM(CAST(t0.organism_type AS STRING)) LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = 'phenotypic_sex') vm_phenotypic_sex ON TRIM(vm_phenotypic_sex.source_value) = TRIM(CAST(t0.phenotypic_sex AS STRING)) WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id) IS NOT NULL)) WHERE rn = 1</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>anvil_file</td>\n",
       "      <td>SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY file_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT t0.file_ref AS file_id, CASE WHEN t0.data_modality IS NOT NULL THEN [t0.data_modality] ELSE [] END AS data_modality, CASE WHEN CASE WHEN t0.file_format NOT LIKE '.%' THEN '.'||LOWER(t0.file_format) ELSE t0.file_format END IN (SELECT file_extension FROM `dsp-data-ingest.transform_resources.acceptable_file_extensions`) THEN CASE WHEN t0.file_format NOT LIKE '.%' THEN '.'||LOWER(t0.file_format) ELSE t0.file_format END ELSE 'Other' END AS file_format, t0.file_size AS file_size, t0.file_md5sum AS file_md5sum, CASE WHEN t0.reference_assembly IS NOT NULL THEN [t0.reference_assembly] ELSE [] END AS reference_assembly, t0.file_name AS file_name, t0.file_ref AS file_ref, NULL AS is_supplementary, ARRAY_CONCAT(['file:'||t0.datarepo_row_id]) AS source_datarepo_row_ids, 0 AS record_set_priority FROM `datarepo-d344de2a.datarepo_ANVIL_HudsonAlpha_LR_v1_GRU_20241018.file` t0 WHERE 1=1 AND t0.file_ref IS NOT NULL)) WHERE rn = 1</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anvil_project</td>\n",
       "      <td>SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY project_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value('HudsonAlpha Long Read Sequencing Data of Individuals with Rare Suspected Genetic Conditions') AS project_id, [] AS funded_by, CASE WHEN `dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU') IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU')] ELSE [] END AS generated_dataset_id, [] AS principal_investigator, 'HudsonAlpha Long Read Sequencing Data of Individuals with Rare Suspected Genetic Conditions' AS title, CASE WHEN 'phs003537' IS NOT NULL THEN ['phs003537'] ELSE [] END AS registered_identifier, [] AS source_datarepo_row_ids, 0 AS record_set_priority FROM (SELECT 1) WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value('HudsonAlpha Long Read Sequencing Data of Individuals with Rare Suspected Genetic Conditions') IS NOT NULL)) WHERE rn = 1</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>anvil_sequencingactivity</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>anvil_variantcallingactivity</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target_table          \\\n",
       "0                 anvil_activity   \n",
       "1        anvil_alignmentactivity   \n",
       "2                 anvil_antibody   \n",
       "3            anvil_assayactivity   \n",
       "4                anvil_biosample   \n",
       "5                  anvil_dataset   \n",
       "6                anvil_diagnosis   \n",
       "7                    anvil_donor   \n",
       "8                     anvil_file   \n",
       "9                  anvil_project   \n",
       "10      anvil_sequencingactivity   \n",
       "11  anvil_variantcallingactivity   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                query                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\\n",
       "0                                                                                                                                                                                                                                            SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY activity_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value(`dsp-data-ingest.transform_resources`.prefix_value('activity', t0.file_id)) AS activity_id, 'Unknown' AS activity_type, [] AS used_file_id, CASE WHEN `dsp-data-ingest.transform_resources`.validate_uuid_value(t0.file_ref) IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.validate_uuid_value(t0.file_ref)] ELSE [] END AS generated_file_id, CASE WHEN `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.associated_biosample_id) IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.uuid_hash_value(t0.associated_biosample_id)] ELSE [] END AS used_biosample_id, ARRAY_CONCAT(['file:'||t0.datarepo_row_id]) AS source_datarepo_row_ids, 0 AS record_set_priority FROM `datarepo-d344de2a.datarepo_ANVIL_HudsonAlpha_LR_v1_GRU_20241018.file` t0 WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value(`dsp-data-ingest.transform_resources`.prefix_value('activity', t0.file_id)) IS NOT NULL AND ARRAY_LENGTH(CASE WHEN `dsp-data-ingest.transform_resources`.validate_uuid_value(t0.file_ref) IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.validate_uuid_value(t0.file_ref)] ELSE [] END) > 0 AND ARRAY_LENGTH(CASE WHEN `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.associated_biosample_id) IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.uuid_hash_value(t0.associated_biosample_id)] ELSE [] END) > 0)) WHERE rn = 1   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
       "4   SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY biosample_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.biosample_id) AS biosample_id, vm_anatomical_site.mapped_value AS anatomical_site, [] AS apriori_cell_type, vm_biosample_type.mapped_value AS biosample_type, NULL AS disease, t0.donor_age_at_collection_unit AS donor_age_at_collection_unit, SAFE_CAST(t0.donor_age_at_collection_lower_bound AS INT64) AS donor_age_at_collection_lower_bound, SAFE_CAST(t0.donor_age_at_collection_upper_bound AS INT64) AS donor_age_at_collection_upper_bound, CASE WHEN `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id) IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id)] ELSE [] END AS donor_id, CASE WHEN `dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU') IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU')] ELSE [] END AS part_of_dataset_id, ARRAY_CONCAT(['biosample:'||t0.datarepo_row_id]) AS source_datarepo_row_ids, 0 AS record_set_priority FROM `datarepo-d344de2a.datarepo_ANVIL_HudsonAlpha_LR_v1_GRU_20241018.biosample` t0 LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = 'anatomical_site') vm_anatomical_site ON TRIM(vm_anatomical_site.source_value) = TRIM(CAST(t0.anatomical_site AS STRING)) LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = 'biosample_type') vm_biosample_type ON TRIM(vm_biosample_type.source_value) = TRIM(CAST(t0.biosample_type AS STRING)) WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.biosample_id) IS NOT NULL)) WHERE rn = 1   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY dataset_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU') AS dataset_id, CASE WHEN 'GRU' IS NOT NULL THEN ['GRU'] ELSE [] END AS consent_group, CASE WHEN 'GRU' IS NOT NULL THEN ['GRU'] ELSE [] END AS data_use_permission, [] AS owner, [] AS principal_investigator, CASE WHEN 'phs003537' IS NOT NULL THEN ['phs003537'] ELSE [] END AS registered_identifier, 'ANVIL_HudsonAlpha_LR_v1_GRU' AS title, SPLIT(CAST((SELECT FIRST_VALUE(mapped_value) OVER (ORDER BY 1) FROM `dsp-data-ingest.transform_resources.dataset_map` WHERE attribute = 'data_modality' and dataset = 'ANVIL_HudsonAlpha_LR_v1_GRU') AS STRING), ',') AS data_modality, [] AS source_datarepo_row_ids, 0 AS record_set_priority FROM (SELECT 1) WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU') IS NOT NULL)) WHERE rn = 1   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                    SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY diagnosis_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value(`dsp-data-ingest.transform_resources`.prefix_value('diagnosis', t0.donor_id)) AS diagnosis_id, `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id) AS donor_id, CASE WHEN vm_disease.mapped_value IS NOT NULL THEN [vm_disease.mapped_value] ELSE [] END AS disease, NULL AS diagnosis_age_unit, NULL AS diagnosis_age_lower_bound, NULL AS diagnosis_age_upper_bound, NULL AS onset_age_unit, NULL AS onset_age_lower_bound, NULL AS onset_age_upper_bound, [] AS phenotype, [] AS phenopacket, ARRAY_CONCAT(['biosample:'||t0.datarepo_row_id]) AS source_datarepo_row_ids, 0 AS record_set_priority FROM `datarepo-d344de2a.datarepo_ANVIL_HudsonAlpha_LR_v1_GRU_20241018.biosample` t0 LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = 'disease') vm_disease ON TRIM(vm_disease.source_value) = TRIM(CAST(t0.disease AS STRING)) WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value(`dsp-data-ingest.transform_resources`.prefix_value('diagnosis', t0.donor_id)) IS NOT NULL AND `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id) IS NOT NULL AND ARRAY_LENGTH(CASE WHEN vm_disease.mapped_value IS NOT NULL THEN [vm_disease.mapped_value] ELSE [] END) > 0)) WHERE rn = 1   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY donor_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id) AS donor_id, vm_organism_type.mapped_value AS organism_type, `dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU') AS part_of_dataset_id, vm_phenotypic_sex.mapped_value AS phenotypic_sex, [] AS reported_ethnicity, [] AS genetic_ancestry, ARRAY_CONCAT(['donor:'||t0.datarepo_row_id]) AS source_datarepo_row_ids, 0 AS record_set_priority FROM `datarepo-d344de2a.datarepo_ANVIL_HudsonAlpha_LR_v1_GRU_20241018.donor` t0 LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = 'organism_type') vm_organism_type ON TRIM(vm_organism_type.source_value) = TRIM(CAST(t0.organism_type AS STRING)) LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = 'phenotypic_sex') vm_phenotypic_sex ON TRIM(vm_phenotypic_sex.source_value) = TRIM(CAST(t0.phenotypic_sex AS STRING)) WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value(t0.donor_id) IS NOT NULL)) WHERE rn = 1   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY file_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT t0.file_ref AS file_id, CASE WHEN t0.data_modality IS NOT NULL THEN [t0.data_modality] ELSE [] END AS data_modality, CASE WHEN CASE WHEN t0.file_format NOT LIKE '.%' THEN '.'||LOWER(t0.file_format) ELSE t0.file_format END IN (SELECT file_extension FROM `dsp-data-ingest.transform_resources.acceptable_file_extensions`) THEN CASE WHEN t0.file_format NOT LIKE '.%' THEN '.'||LOWER(t0.file_format) ELSE t0.file_format END ELSE 'Other' END AS file_format, t0.file_size AS file_size, t0.file_md5sum AS file_md5sum, CASE WHEN t0.reference_assembly IS NOT NULL THEN [t0.reference_assembly] ELSE [] END AS reference_assembly, t0.file_name AS file_name, t0.file_ref AS file_ref, NULL AS is_supplementary, ARRAY_CONCAT(['file:'||t0.datarepo_row_id]) AS source_datarepo_row_ids, 0 AS record_set_priority FROM `datarepo-d344de2a.datarepo_ANVIL_HudsonAlpha_LR_v1_GRU_20241018.file` t0 WHERE 1=1 AND t0.file_ref IS NOT NULL)) WHERE rn = 1   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY project_id ORDER BY record_set_priority, 1) AS rn FROM (SELECT `dsp-data-ingest.transform_resources`.uuid_hash_value('HudsonAlpha Long Read Sequencing Data of Individuals with Rare Suspected Genetic Conditions') AS project_id, [] AS funded_by, CASE WHEN `dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU') IS NOT NULL THEN [`dsp-data-ingest.transform_resources`.uuid_hash_value('ANVIL_HudsonAlpha_LR_v1_GRU')] ELSE [] END AS generated_dataset_id, [] AS principal_investigator, 'HudsonAlpha Long Read Sequencing Data of Individuals with Rare Suspected Genetic Conditions' AS title, CASE WHEN 'phs003537' IS NOT NULL THEN ['phs003537'] ELSE [] END AS registered_identifier, [] AS source_datarepo_row_ids, 0 AS record_set_priority FROM (SELECT 1) WHERE 1=1 AND `dsp-data-ingest.transform_resources`.uuid_hash_value('HudsonAlpha Long Read Sequencing Data of Individuals with Rare Suspected Genetic Conditions') IS NOT NULL)) WHERE rn = 1   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            None   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            None   \n",
       "\n",
       "   syntax_check  \n",
       "0     Passed     \n",
       "1       None     \n",
       "2       None     \n",
       "3       None     \n",
       "4     Passed     \n",
       "5     Passed     \n",
       "6     Passed     \n",
       "7     Passed     \n",
       "8     Passed     \n",
       "9     Passed     \n",
       "10      None     \n",
       "11      None     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>attribute</th>\n",
       "      <th>source_value</th>\n",
       "      <th>mapped_value</th>\n",
       "      <th>record_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anatomical_site</td>\n",
       "      <td>blood</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>biosample_type</td>\n",
       "      <td>Blood</td>\n",
       "      <td>Blood</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disease</td>\n",
       "      <td>neurodevelopmental disease</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disease</td>\n",
       "      <td>neurodevelopmental disease, myopathy</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disease</td>\n",
       "      <td>multiple congenital anomalies</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>disease</td>\n",
       "      <td>myopathy</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>organism_type</td>\n",
       "      <td>homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phenotypic_sex</td>\n",
       "      <td>male</td>\n",
       "      <td>Male</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phenotypic_sex</td>\n",
       "      <td>female</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     attribute                  source_value             mapped_value record_count\n",
       "0  anatomical_site                                 blood    Unknown        100    \n",
       "1   biosample_type                                 Blood      Blood        100    \n",
       "2          disease            neurodevelopmental disease       None         40    \n",
       "3          disease  neurodevelopmental disease, myopathy       None         20    \n",
       "4          disease         multiple congenital anomalies       None         18    \n",
       "5          disease                              myopathy       None          4    \n",
       "6    organism_type                          homo sapiens      Human        100    \n",
       "7   phenotypic_sex                                  male       Male         59    \n",
       "8   phenotypic_sex                                female     Female         41    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # TESTING\n",
    "\n",
    "# # Additional imports\n",
    "# import data_repo_client\n",
    "# from google.cloud import storage\n",
    "# import google.auth\n",
    "# import google.auth.transport.requests\n",
    "# import pandas as pd\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.colheader_justify', 'center')\n",
    "# pd.set_option('display.precision', 3)\n",
    "\n",
    "# # Setup Google creds and establish TDR clients\n",
    "# creds, project = google.auth.default()\n",
    "# auth_req = google.auth.transport.requests.Request()\n",
    "# creds.refresh(auth_req)\n",
    "# config = data_repo_client.Configuration()\n",
    "# config.host = \"https://data.terra.bio\"\n",
    "# config.access_token = creds.token\n",
    "# api_client = data_repo_client.ApiClient(configuration=config)\n",
    "# api_client.client_side_validation = False\n",
    "# datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# # Set the desired mapping specification and retrieve artifacts needed for query construction\n",
    "# dataset_id = \"ac48514d-0b01-4a92-b164-821fa3e05d7a\" #params[\"dataset_id\"]\n",
    "# mapping_target = \"anvil\"\n",
    "# mapping_target_spec = \"hudsonalpha_1\" \n",
    "\n",
    "# # Retrieve source schema\n",
    "# src_schema_dict = {}\n",
    "# try:\n",
    "#     datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "#     response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "#     src_schema_dict[\"name\"] = response[\"name\"]\n",
    "#     src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "#     src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "#     bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "#     bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "#     phs_id = response[\"phs_id\"]\n",
    "# except Exception as e:\n",
    "#     print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "# #print(src_schema_dict)\n",
    "\n",
    "#  # Set dataset name and project name parameters to substitute into transform queries\n",
    "# dataset_name_value = re.sub(\"(_[0-9]+$)\", \"\", src_schema_dict[\"name\"])\n",
    "# project_name_value = re.sub(\"'\", \"\", utils.derive_project_name(dataset_id, phs_id, dataset_name_value))\n",
    "\n",
    "# # Retrieve target schema and mapping specification\n",
    "# target_schema_dict = {}\n",
    "# mapping_spec = {}\n",
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "# try:\n",
    "#     blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "#     target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "# except Exception as e:\n",
    "#     print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "# #print(target_schema_dict)\n",
    "# try:\n",
    "#     blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "#     blob_string = blob.download_as_text(client=None)\n",
    "#     blob_string = blob_string.replace(\"$DATASET_NAME\", dataset_name_value)\n",
    "#     blob_string = blob_string.replace(\"$PROJECT_NAME\", project_name_value)\n",
    "#     blob_string = blob_string.replace(\"$BQ_DATASET\", bq_project + \".\" + bq_schema)\n",
    "#     mapping_spec = json.loads(blob_string)\n",
    "# except Exception as e:\n",
    "#     print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "# #print(mapping_spec)\n",
    "\n",
    "# # Update aliases in mapping specification\n",
    "# mapping_spec = update_mapping_spec_aliases(mapping_spec, src_schema_dict)\n",
    "\n",
    "# # Build queries from mapping specification\n",
    "# query_dict = {}\n",
    "# if target_schema_dict:\n",
    "#     for target_table in target_schema_dict[\"tables\"]:\n",
    "#         table_name = target_table[\"name\"]\n",
    "#         missing_artifacts = False\n",
    "#         if src_schema_dict and mapping_spec:\n",
    "#             query_dict[table_name] = build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema)\n",
    "#         else:\n",
    "#             missing_artifacts = True\n",
    "#             query_dict[table_name] = {\"query\": \"\", \"syntax_check\": \"\"} \n",
    "#     if missing_artifacts == True:\n",
    "#         print(\"Source schema dictionary and/or mapping specification missing. Unable to generate queries.\")\n",
    "# else:\n",
    "#     print(\"Target schema dictionary missing. Unable to generate queries.\")\n",
    "# query_df = pd.DataFrame.from_dict(query_dict, orient=\"index\")\n",
    "# query_df.index.name = \"target_table\"\n",
    "# query_df.reset_index(inplace=True)\n",
    "# display(query_df)\n",
    "\n",
    "# # Evaluate vocab maps\n",
    "# df = evaluate_vocab_mapping(mapping_spec, src_schema_dict, target_schema_dict, bq_project, bq_schema)\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
