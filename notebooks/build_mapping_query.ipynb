{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print('Version 1.0.0: 09/13/2022 03:31pm - Nate Calvanese - Initial Version')\n",
    "#print('Version 1.0.1: 09/14/2022 09:04pm - Nate Calvanese - Additional functionality to cover remainder of CMG')\n",
    "#print('Version 1.0.2: 09/16/2022 11:37am - Nate Calvanese - Fixed bug in multi-column array agg when source column was an array itself')\n",
    "#print('Version 1.0.3: 09/19/2022 9:40am - Nate Calvanese - Added in vocab map evaluation query construction')\n",
    "#print('Version 1.0.4: 09/20/2022 4:29pm - Nate Calvanese - Changed record set validation logic')\n",
    "#print('Version 1.0.5: 09/21/2022 11:58am - Nate Calvanese - Made multi-column array agg return array with distinct values')\n",
    "#print('Version 1.0.6: 10/6/2022 9:17pm - Nate Calvanese - Added support for multiple queries for the same table')\n",
    "#print('Version 1.0.7: 10/15/2022 3:08pm - Nate Calvanese - Added support for column aggregation logic')\n",
    "print('Version 1.0.8: 10/24/2022 4:24pm - Nate Calvanese - Added UUID validation transform function')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from itertools import combinations\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "# Workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support Functions\n",
    "\n",
    "# Function to validate the syntax of a BQ SQL query\n",
    "def run_syntax_check(query):\n",
    "    try:\n",
    "        client = bigquery.Client()\n",
    "        client.query(query).result()\n",
    "        return \"Passed\"\n",
    "    except Exception as e:\n",
    "        return \"Failed: {}\".format(e)\n",
    "\n",
    "# Function to identify if a particular table and column is present in the source\n",
    "def confirm_column_exists(schema, table, column):\n",
    "    column_exists = False\n",
    "    for table_entry in schema[\"tables\"]:\n",
    "        if table_entry[\"name\"] == table:\n",
    "            for column_entry in table_entry[\"columns\"]:\n",
    "                if column_entry[\"name\"] == column:\n",
    "                    column_exists = True\n",
    "    return column_exists\n",
    "\n",
    "# Function to apply table alias and qualified name for readability and testing\n",
    "def apply_table_alias(query_str, lookup_dict):\n",
    "    for key in lookup_dict:\n",
    "        query_str = query_str.replace(\" \" + key + \".\", \" \" + lookup_dict[key][\"alias\"] + \".\")\n",
    "        query_str = query_str.replace(\"(\" + key + \".\", \"(\" + lookup_dict[key][\"alias\"] + \".\")\n",
    "        query_str = query_str.replace(\"[\" + key + \".\", \"[\" + lookup_dict[key][\"alias\"] + \".\")\n",
    "        query_str = query_str.replace(\"FROM \" + key, \"FROM \" + lookup_dict[key][\"qualified_name\"] + \" \" + lookup_dict[key][\"alias\"])\n",
    "        query_str = query_str.replace(\"JOIN \" + key, \"JOIN \" + lookup_dict[key][\"qualified_name\"] + \" \" + lookup_dict[key][\"alias\"])\n",
    "    return query_str\n",
    "\n",
    "# Function to return table name from fully qualified field name\n",
    "def split_field_name(field):\n",
    "    field_split = field.split(\".\")\n",
    "    return field_split[0], \".\".join(field_split[1:])\n",
    "\n",
    "# Function to validate whether an attribute should be processed -- Exists in target schema and at least one source field exists in the source schema\n",
    "def validate_attribute(attr_dict, src_schema, tar_schema):\n",
    "    valid_target = False\n",
    "    valid_src = False\n",
    "    # Confirm attribute exists in target schema\n",
    "    for col in tar_schema[\"columns\"]:\n",
    "        if attr_dict[\"name\"] == col[\"name\"]:\n",
    "            valid_target = True\n",
    "    # Confirm source field exists in source schema (single source field)\n",
    "    if len(attr_dict[\"source\"][\"fields\"]) == 1:\n",
    "        tab_name, col_name = split_field_name(attr_dict[\"source\"][\"fields\"][0])\n",
    "        valid_src = confirm_column_exists(src_schema, tab_name, col_name)\n",
    "        if valid_target and valid_src:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    # Confirm source fields exist in source schema, if marked as required or used in condition/transform/aggregation (multiple source fields)\n",
    "    elif len(attr_dict[\"source\"][\"fields\"]) > 1:\n",
    "        if attr_dict[\"source\"][\"all_fields_required\"] == True:\n",
    "            found_counter = 0\n",
    "            for fld in attr_dict[\"source\"][\"fields\"]:\n",
    "                tab_name, col_name = split_field_name(fld)\n",
    "                valid_src = confirm_column_exists(src_schema, tab_name, col_name)\n",
    "                if not valid_src:\n",
    "                    return False\n",
    "                else:\n",
    "                    found_counter += 1\n",
    "            if valid_target and found_counter == len(attr_dict[\"source\"][\"fields\"]):\n",
    "                return True\n",
    "        else:\n",
    "            field_counter = 0\n",
    "            for fld in attr_dict[\"source\"][\"fields\"]:\n",
    "                tab_name, col_name = split_field_name(fld)\n",
    "                valid_src = confirm_column_exists(src_schema, tab_name, col_name)\n",
    "                if not valid_src:\n",
    "                    if attr_dict[\"source\"][\"under_condition\"] != None:\n",
    "                        if fld in attr_dict[\"source\"][\"under_condition\"]:\n",
    "                            return False\n",
    "                    for transform in attr_dict[\"source\"][\"with_transformation\"]:\n",
    "                        for parameter in transform[\"parameters\"]:\n",
    "                            if fld in parameter:\n",
    "                                return False\n",
    "                    if attr_dict[\"source\"][\"with_row_aggregation\"] != None:\n",
    "                        if fld in attr_dict[\"source\"][\"with_row_aggregation\"][\"partition_by\"] or fld in attr_dict[\"source\"][\"with_row_aggregation\"][\"order_by\"]:\n",
    "                            return False\n",
    "                else:\n",
    "                    field_counter += 1\n",
    "            if valid_target and field_counter > 0:\n",
    "                return True\n",
    "    else:\n",
    "        if valid_target:\n",
    "            return True\n",
    "\n",
    "# Function to validate whether a query should be built for a record set -- Primary key attributes are all found and have valid specifications\n",
    "def validate_record_set(map_set, src_schema_dict, target_table):\n",
    "    target_pk_cols = target_table[\"primaryKey\"]\n",
    "    map_attr_list = [attr[\"name\"] for attr in map_set[\"attributes\"]]\n",
    "    # Confirm all pk_cols are present and have valid specifications for the record set\n",
    "    for pk in target_pk_cols:\n",
    "        if pk not in map_attr_list:\n",
    "            return False\n",
    "        else:\n",
    "            for attr in map_set[\"attributes\"]:\n",
    "                if attr[\"name\"] == pk and not validate_attribute(attr, src_schema_dict, target_table):\n",
    "                    return False\n",
    "    # Confirm any other columns marked as required have valid specifications for the record set\n",
    "    for attr in map_set[\"attributes\"]:\n",
    "        if attr[\"required\"] and not validate_attribute(attr, src_schema_dict, target_table):\n",
    "            return False\n",
    "    return True\n",
    "        \n",
    "# Function to find and return a direct relationship between two tables, if one exists\n",
    "def confirm_relationship(rel_list, tab1, tab2):\n",
    "    relationship_exists = False\n",
    "    relationship = {}\n",
    "    for rel in rel_list:\n",
    "        if (rel[\"_from\"][\"table\"] == tab1 and rel[\"to\"][\"table\"] == tab2) or (rel[\"_from\"][\"table\"] == tab2 and rel[\"to\"][\"table\"] == tab1):\n",
    "            relationship_exists = True\n",
    "            relationship = rel\n",
    "            break\n",
    "    return relationship_exists, relationship\n",
    "\n",
    "# Function to find the shortest path between two tables, if one exists\n",
    "def shortest_path(rel_list, tab1, tab2):\n",
    "    rels_to_use_list = []\n",
    "    # Check that relationship records exist for both tables\n",
    "    rel_table_set = set()\n",
    "    for rel in rel_list:\n",
    "        rel_table_set.add(rel[\"_from\"][\"table\"])\n",
    "        rel_table_set.add(rel[\"to\"][\"table\"])\n",
    "    if tab1 not in rel_table_set or tab2 not in rel_table_set:\n",
    "        return rels_to_use_list\n",
    "    # Check whether direct relationship exists between tables\n",
    "    direct_exists, direct_rel = confirm_relationship(rel_list, tab1, tab2)\n",
    "    if direct_exists:\n",
    "        rels_to_use_list.append(direct_rel)\n",
    "        return rels_to_use_list\n",
    "    # Check whether indirect relationship exists between tables\n",
    "    rel_table_list = list(rel_table_set)\n",
    "    if len(rel_table_list) > 0:\n",
    "        set_found = False\n",
    "        for i in range(3, len(rel_table_list)+1):\n",
    "            combination_list = list(combinations(rel_table_list, r=i))\n",
    "            for combo in combination_list:\n",
    "                rels_to_use_set = set()\n",
    "                rels_to_use_list = []\n",
    "                if tab1 in combo and tab2 in combo:\n",
    "                    path_exists = False\n",
    "                    for i2 in range(0, i):\n",
    "                        for i3 in range(0, i):\n",
    "                            rel_exists, rel = confirm_relationship(rel_list, combo[i2], combo[i3]) \n",
    "                            if i2 != i3 and rel_exists:\n",
    "                                rel_combo = min(combo[i2], combo[i3]) + \":\" + max(combo[i2], combo[i3])\n",
    "                                if rel_combo not in rels_to_use_set:\n",
    "                                    rels_to_use_set.add(rel_combo)\n",
    "                                    rels_to_use_list.append(rel)\n",
    "                                break\n",
    "                if len(rels_to_use_set) == i-1:\n",
    "                    set_found = True\n",
    "                    break\n",
    "            if set_found == True:\n",
    "                break\n",
    "    # Re-order relationships from tab2 to tab1\n",
    "    rels_to_use_list_ordered = []\n",
    "    tables_referenced_set = set()\n",
    "    tables_referenced_set.add(tab2)\n",
    "    while rels_to_use_list:\n",
    "        for rel_idx, rel in enumerate(rels_to_use_list):\n",
    "            if rel[\"_from\"][\"table\"] in tables_referenced_set and rel[\"to\"][\"table\"] in tables_referenced_set:\n",
    "                pass\n",
    "            elif rel[\"_from\"][\"table\"] in tables_referenced_set:\n",
    "                rels_to_use_list_ordered.append(rel)\n",
    "                tables_referenced_set.add(rel[\"to\"][\"table\"])\n",
    "                rels_to_use_list.pop(rel_idx)\n",
    "            elif rel[\"to\"][\"table\"] in tables_referenced_set:\n",
    "                rels_to_use_list_ordered.append(rel)\n",
    "                tables_referenced_set.add(rel[\"_from\"][\"table\"])\n",
    "                rels_to_use_list.pop(rel_idx)\n",
    "    return rels_to_use_list_ordered\n",
    "\n",
    "# Function to apply row aggregation logic\n",
    "def apply_row_agg_logic(stmt, rowagg_obj):\n",
    "    # Collect partition by parameter - If not present, exit function\n",
    "    if rowagg_obj[\"partition_by\"] == None or rowagg_obj[\"partition_by\"] == \"\":\n",
    "        return stmt\n",
    "    else:\n",
    "        partition_parameter = rowagg_obj[\"partition_by\"]\n",
    "    # Collect order by parameter\n",
    "    if rowagg_obj[\"order_by\"] == None or rowagg_obj[\"partition_by\"] == \"\":\n",
    "        order_parameter = \"1\"\n",
    "    else:\n",
    "        order_parameter = rowagg_obj[\"order_by\"]\n",
    "    # Apply aggregation function \n",
    "    if rowagg_obj[\"function\"] in (\"MAX\", \"MIN\"):\n",
    "        stmt = \"{function}({stmt}) OVER (PARTITION BY {part})\".format(function= rowagg_obj[\"function\"], stmt=stmt, part=partition_parameter, order=order_parameter)\n",
    "    elif rowagg_obj[\"function\"] in (\"FIRST_VALUE\", \"LAST_VALUE\"):\n",
    "        stmt = \"{function}({stmt} IGNORE NULLS) OVER (PARTITION BY {part} ORDER BY {order})\".format(function= rowagg_obj[\"function\"], stmt=stmt, part=partition_parameter, order=order_parameter)\n",
    "    return stmt\n",
    "\n",
    "# Function to parse a VOCAB_MAP select statement into its input expressions\n",
    "def parse_vocab_map_select(stmt):\n",
    "    tar_expr = re.search(\"COALESCE\\((.*?)\\,\", stmt).group(1)\n",
    "    src_expr = re.search(\"COALESCE\\(.*\\, (.*)\\)\", stmt).group(1)\n",
    "    return src_expr, tar_expr\n",
    "    \n",
    "# Function to apply transformation logic\n",
    "def apply_transformation_logic(stmt, attr_name, transform_obj, made_array, from_clause):\n",
    "    for transform in transform_obj:\n",
    "        if transform[\"function\"] == \"UUID\":\n",
    "            if made_array == False:\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.uuid_hash_value({})\".format(stmt)\n",
    "            else:\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.uuid_hash_array_values({})\".format(stmt)\n",
    "        elif transform[\"function\"] == \"HARDCODE\":\n",
    "            hardcode_param = transform[\"parameters\"][0].format(previous=stmt)\n",
    "            stmt = hardcode_param\n",
    "        elif transform[\"function\"] == \"CUSTOM\":\n",
    "            custom_logic_param = transform[\"parameters\"][0].format(previous=stmt)\n",
    "            array_param = transform[\"parameters\"][1]\n",
    "            stmt = custom_logic_param\n",
    "            if array_param == \"array\":\n",
    "                made_array = True\n",
    "        elif transform[\"function\"] == \"VOCAB_MAP\":\n",
    "            attribute_param = transform[\"parameters\"][0]\n",
    "            from_clause += \" LEFT JOIN (SELECT * FROM `dsp-data-ingest.transform_resources.vocab_map` WHERE attribute = '{param}') vm_{param} ON vm_{param}.source_value = {stmt}\".format(param=attribute_param, stmt=stmt)\n",
    "            stmt = \"COALESCE(vm_{param}.mapped_value, {stmt})\".format(param=attribute_param, stmt=stmt)\n",
    "        elif transform[\"function\"] == \"PREFIX\":\n",
    "            array_param = transform[\"parameters\"][0]\n",
    "            prefix_param = transform[\"parameters\"][1].format(previous=stmt)\n",
    "            input_param = transform[\"parameters\"][2].format(previous=stmt)\n",
    "            if array_param == \"array\":\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.prefix_array_values(\" + prefix_param + \", \" + input_param + \")\"\n",
    "            elif array_param == \"non-array\":\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.prefix_value(\" + prefix_param + \", \" + input_param + \")\"\n",
    "        elif transform[\"function\"] == \"VALIDATE_UUID\":\n",
    "            if made_array == False:\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.validate_uuid_value({})\".format(stmt)\n",
    "            else:\n",
    "                stmt = \"`dsp-data-ingest.transform_resources`.validate_uuid_array_values({})\".format(stmt)\n",
    "        elif transform[\"function\"] == \"EXPLODE\":\n",
    "            from_clause += \" CROSS JOIN UNNEST({}) {}\".format(stmt, attr_name)\n",
    "            stmt = attr_name\n",
    "            made_array = False\n",
    "    return stmt, made_array, from_clause\n",
    "\n",
    "# Function to construct select statement for a specific field\n",
    "def construct_select(attr_dict, tar_schema, src_schema, from_clause, where_clause):\n",
    "    made_array = False\n",
    "    # Add target field attributes to attribute mapping\n",
    "    for col in tar_schema[\"columns\"]:\n",
    "        if attr_dict[\"name\"] == col[\"name\"]:\n",
    "            attr_dict[\"datatype\"] = col[\"datatype\"]\n",
    "            attr_dict[\"array_of\"] = col[\"array_of\"]\n",
    "            if attr_dict[\"required\"] == False:\n",
    "                if col.get(\"required\") and col[\"required\"] == True:\n",
    "                    attr_dict[\"required\"] = True\n",
    "    for pk_col in tar_schema[\"primaryKey\"]:\n",
    "        if attr_dict[\"name\"] == pk_col:\n",
    "            attr_dict[\"required\"] = True\n",
    "    \n",
    "    # Set base attribute based on number of source fields, target field type, and specified column aggregation\n",
    "    if len(attr_dict[\"source\"][\"fields\"]) == 0:\n",
    "        attr_stmt = \"\"\n",
    "    elif len(attr_dict[\"source\"][\"fields\"]) == 1:\n",
    "        attr_stmt = attr_dict[\"source\"][\"fields\"][0]\n",
    "    else:\n",
    "        if attr_dict[\"source\"][\"with_column_aggregation\"] == \"FIELD_COALESCE\": ## Add support for regular coalesce, but that is complicated with arrays/non-arrays\n",
    "            attr_stmt = \"\"\n",
    "            for field in attr_dict[\"source\"][\"fields\"]:\n",
    "                tab_name, col_name = split_field_name(field)\n",
    "                if confirm_column_exists(src_schema, tab_name, col_name):\n",
    "                    attr_stmt = field\n",
    "                    break\n",
    "        else:\n",
    "            filtered_field_list = []\n",
    "            for field in attr_dict[\"source\"][\"fields\"]:\n",
    "                tab_name, col_name = split_field_name(field)\n",
    "                if confirm_column_exists(src_schema, tab_name, col_name):\n",
    "                    for src_tab in src_schema[\"tables\"]:\n",
    "                        if src_tab[\"name\"] == tab_name:\n",
    "                            for src_col in src_tab[\"columns\"]:\n",
    "                                if src_col[\"name\"] == col_name:\n",
    "                                    if src_col[\"array_of\"] == True:\n",
    "                                        array_field = \"NULLIF(ARRAY_TO_STRING({}, ','), '')\".format(field)\n",
    "                                        filtered_field_list.append(array_field)\n",
    "                                    else:\n",
    "                                        non_array_field = \"NULLIF({}, '')\".format(field)\n",
    "                                        filtered_field_list.append(non_array_field)\n",
    "            field_list_str = ', '.join(filtered_field_list)\n",
    "            attr_stmt = \"SPLIT((SELECT STRING_AGG(DISTINCT col, ',') FROM UNNEST(SPLIT(TRIM(FORMAT('%t', (SELECT AS STRUCT {})), '()'), ', ')) AS col WHERE NOT UPPER(col) = 'NULL'), ',')\".format(field_list_str)\n",
    "            made_array = True\n",
    "            if attr_dict[\"array_of\"] == False:\n",
    "                attr_stmt = \"ARRAY_TO_STRING({}, ', ')\".format(attr_stmt)\n",
    "                made_array = False\n",
    "    \n",
    "    # Apply transformations\n",
    "    if len(attr_dict[\"source\"][\"with_transformation\"]) > 0:\n",
    "        attr_stmt, made_array, from_clause = apply_transformation_logic(attr_stmt, attr_dict[\"name\"], attr_dict[\"source\"][\"with_transformation\"], made_array, from_clause)\n",
    "    \n",
    "    # Apply case logic    \n",
    "    if attr_dict[\"source\"][\"under_condition\"] != None:\n",
    "        attr_stmt = \"CASE WHEN \" + attr_dict[\"source\"][\"under_condition\"] + \" THEN {} END\".format(attr_stmt)\n",
    "    \n",
    "    # Apply row aggregation logic\n",
    "    if attr_dict[\"source\"][\"with_row_aggregation\"] != None:\n",
    "        attr_stmt = apply_row_agg_logic(attr_stmt, attr_dict[\"source\"][\"with_row_aggregation\"])\n",
    "    \n",
    "    # Construct final select clauses\n",
    "    if attr_dict[\"array_of\"] == True and made_array == False:\n",
    "        attr_stmt = \"CASE WHEN {attr} IS NOT NULL THEN [{attr}] ELSE [] END\".format(attr=attr_stmt)\n",
    "    elif attr_dict[\"array_of\"] == False and made_array == True:\n",
    "        attr_stmt = \"ARRAY_TO_STRING({}, ', ')\".format(attr_stmt)\n",
    "    \n",
    "    # Extend the where_clause for required fields\n",
    "    if attr_dict[\"required\"] == True:\n",
    "        if attr_dict[\"array_of\"] == False:\n",
    "            where_clause += \" AND {} IS NOT NULL\".format(attr_stmt)\n",
    "        else:\n",
    "            where_clause += \" AND ARRAY_LENGTH({}) > 0\".format(attr_stmt)\n",
    "    \n",
    "    # Returned the altered clauses\n",
    "    return attr_stmt, from_clause, where_clause\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main Function - Building mapping transformation queries\n",
    "def build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema):\n",
    "\n",
    "    # Initialize function variables\n",
    "    dataset = bq_project + \".\" + bq_schema\n",
    "    query_dict = {\"query\": None, \"syntax_check\": None} \n",
    "\n",
    "    # Identify the entity of interest and it's place in the mapping specification\n",
    "    entity_map_idx = None\n",
    "    entity = target_table[\"name\"]\n",
    "    for idx, entry in enumerate(mapping_spec[\"entities\"]):\n",
    "        if entry[\"name\"] == entity:\n",
    "            entity_map_idx = idx\n",
    "    if entity_map_idx == None:\n",
    "        return query_dict\n",
    "\n",
    "    # Start with mapping_specification, locate the entity of interest, and loop through the record sets associated with it\n",
    "    final_query_set = set()\n",
    "    for set_idx, map_set in enumerate(mapping_spec[\"entities\"][entity_map_idx][\"record_sets\"]):\n",
    "        \n",
    "        # Validate whether record set should be processed or skipped\n",
    "        if not validate_record_set(map_set, src_schema_dict, target_table):\n",
    "            continue\n",
    "\n",
    "        # Initialize query variables\n",
    "        select_clause = \"SELECT \"\n",
    "        source_row_agg = \"\"\n",
    "        from_clause = \"\"\n",
    "        where_clause = \"WHERE 1=1\"\n",
    "        \n",
    "        # Loop through and identify valid attributes in the record set\n",
    "        attribute_set = set()\n",
    "        for attr_idx, attr in enumerate(map_set[\"attributes\"]):\n",
    "            if validate_attribute(attr, src_schema_dict, target_table):\n",
    "                attribute_set.add(attr[\"name\"])\n",
    "        \n",
    "        # Loop through columns in target schema and build attribute, either from record set (if valid) or default logic\n",
    "        for column_entry in target_table[\"columns\"]:\n",
    "            if column_entry[\"name\"] in [\"source_datarepo_row_ids\"]:\n",
    "                continue\n",
    "            elif column_entry[\"name\"] not in attribute_set:\n",
    "                if column_entry[\"array_of\"] == True:\n",
    "                    select_clause += \"[] AS {}, \".format(column_entry[\"name\"])\n",
    "                else:\n",
    "                    select_clause += \"NULL AS {}, \".format(column_entry[\"name\"])\n",
    "            else:\n",
    "                for attr_idx, attr in enumerate(map_set[\"attributes\"]):\n",
    "                    if attr[\"name\"] == column_entry[\"name\"]:\n",
    "                        # Build/extend from clause based on source fields\n",
    "                        for fld_idx, fld in enumerate(attr[\"source\"][\"fields\"]):\n",
    "                            tab_name, col_name = split_field_name(fld)\n",
    "                            if confirm_column_exists(src_schema_dict, tab_name, col_name):\n",
    "                                if from_clause == \"\":\n",
    "                                    base_table, base_field = split_field_name(attr[\"source\"][\"fields\"][0])\n",
    "                                    table_iter = 0\n",
    "                                    table_alias_lookup = {base_table: {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + base_table + \"`\"}}\n",
    "                                    from_clause = \"FROM \" + base_table\n",
    "                                    source_row_agg = \"ARRAY_AGG(DISTINCT '{}:'||t0.datarepo_row_id)\".format(base_table)\n",
    "                                    source_row_agg = \"['{}:'||t{}.datarepo_row_id]\".format(base_table, str(table_iter))\n",
    "                                elif tab_name not in table_alias_lookup:\n",
    "                                    join_clause = \"\"\n",
    "                                    include_join = False\n",
    "                                    relationships = shortest_path(src_schema_dict[\"relationships\"], tab_name, base_table)\n",
    "                                    for rel in relationships:\n",
    "                                        if rel[\"_from\"][\"table\"] not in table_alias_lookup:\n",
    "                                            join_clause = \" LEFT JOIN \" + rel[\"_from\"][\"table\"] + \" ON \" + rel[\"_from\"][\"table\"] + \".\" + rel[\"_from\"][\"column\"] + \" = \" + rel[\"to\"][\"table\"] + \".\" + rel[\"to\"][\"column\"]\n",
    "                                            from_clause += join_clause\n",
    "                                            table_iter += 1\n",
    "                                            table_alias_lookup[rel[\"_from\"][\"table\"]] = {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + rel[\"_from\"][\"table\"] + \"`\"}\n",
    "                                            source_row_agg += \", ['{}:'||t{}.datarepo_row_id]\".format(rel[\"_from\"][\"table\"], str(table_iter))\n",
    "                                        elif rel[\"to\"][\"table\"] not in table_alias_lookup:\n",
    "                                            join_clause = \" LEFT JOIN \" + rel[\"to\"][\"table\"] + \" ON \" + rel[\"to\"][\"table\"] + \".\" + rel[\"to\"][\"column\"] + \" = \" + rel[\"_from\"][\"table\"] + \".\" + rel[\"_from\"][\"column\"]\n",
    "                                            from_clause += join_clause\n",
    "                                            table_iter += 1\n",
    "                                            table_alias_lookup[rel[\"to\"][\"table\"]] = {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + rel[\"to\"][\"table\"] + \"`\"}\n",
    "                                            source_row_agg += \", ['{}:'||t{}.datarepo_row_id]\".format(rel[\"to\"][\"table\"], str(table_iter))\n",
    "\n",
    "                        # Build select clause for attribute \n",
    "                        attr_select, from_clause, where_clause = construct_select(attr, target_table, src_schema_dict, from_clause, where_clause)\n",
    "                        select_clause += \"{stmt} AS {name}, \".format(stmt=attr_select, name=attr[\"name\"])\n",
    "             \n",
    "        # Build final queries and add to final_query_set\n",
    "        select_clause += \"ARRAY_CONCAT({}) AS source_datarepo_row_ids, \".format(source_row_agg)\n",
    "        select_clause += \"{} AS record_set_priority\".format(set_idx)\n",
    "        initial_root_query = select_clause + \" \" + from_clause + \" \" + where_clause\n",
    "        final_root_query = apply_table_alias(initial_root_query, table_alias_lookup)\n",
    "        final_query_set.add(final_root_query)\n",
    "\n",
    "    # Convert final_query_set into a union of queries if multiple record sets are present\n",
    "    final_entity_query = \"\"\n",
    "    if len(final_query_set) == 0:\n",
    "        return query_dict\n",
    "    else:\n",
    "        for idx, entry in enumerate(final_query_set):\n",
    "            if idx == 0:\n",
    "                final_entity_query += entry\n",
    "            else:\n",
    "                final_entity_query += \" UNION ALL \" + entry\n",
    "\n",
    "    # Add deduplication logic \n",
    "    pk_col_string = \", \".join(target_table[\"primaryKey\"])\n",
    "    deduped_entity_query = \"SELECT * EXCEPT(record_set_priority, rn) FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY {pk_cols} ORDER BY record_set_priority, 1) AS rn FROM ({query})) WHERE rn = 1\".format(pk_cols=pk_col_string, query=final_entity_query)\n",
    "    query_dict[\"query\"] = deduped_entity_query\n",
    "    #print(deduped_entity_query)\n",
    "\n",
    "    # Run syntax check and return query\n",
    "    smoke_test_query = deduped_entity_query.replace(\"WHERE 1=1\", \"WHERE 1=0\")\n",
    "    query_dict[\"syntax_check\"] = run_syntax_check(smoke_test_query)\n",
    "    return query_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Secondary function -- Building and executing vocab mapping validation\n",
    "def evaluate_vocab_mapping(mapping_spec, src_schema_dict, target_schema_dict, bq_project, bq_schema):\n",
    "    \n",
    "    # Initialize function variables\n",
    "    dataset = bq_project + \".\" + bq_schema\n",
    "    \n",
    "    # Determine valid vocab mapping attributes\n",
    "    vocab_map_valid_attr_set = set()\n",
    "    for entity in mapping_spec[\"entities\"]:\n",
    "        target_table = {}\n",
    "        for table_entry in target_schema_dict[\"tables\"]:\n",
    "            if table_entry[\"name\"] == entity[\"name\"]:\n",
    "                target_table = table_entry\n",
    "        for record_set in entity[\"record_sets\"]:\n",
    "            if validate_record_set(record_set, src_schema_dict, target_table): \n",
    "                for attribute in record_set[\"attributes\"]:\n",
    "                    for transform in attribute[\"source\"][\"with_transformation\"]:\n",
    "                        if transform[\"function\"] == \"VOCAB_MAP\":\n",
    "                            if validate_attribute(attribute, src_schema_dict, target_table):\n",
    "                                vocab_map_valid_attr_set.add(entity[\"name\"] + \".\" + attribute[\"name\"])\n",
    "    \n",
    "    # Loop through valid vocab mapping attributes and build mapping queries\n",
    "    eval_query_set = set()\n",
    "    for entity in mapping_spec[\"entities\"]:\n",
    "        target_table = {}\n",
    "        for table_entry in target_schema_dict[\"tables\"]:\n",
    "            if table_entry[\"name\"] == entity[\"name\"]:\n",
    "                target_table = table_entry\n",
    "        for record_set in entity[\"record_sets\"]:\n",
    "            for attr in record_set[\"attributes\"]:\n",
    "                if (entity[\"name\"] + \".\" + attr[\"name\"]) in vocab_map_valid_attr_set:\n",
    "        \n",
    "                    # Initialize query variables\n",
    "                    select_clause = \"SELECT \"\n",
    "                    from_clause = \"\"\n",
    "                    where_clause = \"WHERE 1=1\"\n",
    "                    group_by_clause = \"GROUP BY \"\n",
    "\n",
    "                    # Build/extend from clause based on source fields\n",
    "                    for fld_idx, fld in enumerate(attr[\"source\"][\"fields\"]):\n",
    "                        tab_name, col_name = split_field_name(fld)\n",
    "                        if confirm_column_exists(src_schema_dict, tab_name, col_name):\n",
    "                            if from_clause == \"\":\n",
    "                                base_table, base_field = split_field_name(attr[\"source\"][\"fields\"][0])\n",
    "                                table_iter = 0\n",
    "                                table_alias_lookup = {base_table: {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + base_table + \"`\"}}\n",
    "                                from_clause = \"FROM \" + base_table\n",
    "                            elif tab_name not in table_alias_lookup:\n",
    "                                join_clause = \"\"\n",
    "                                include_join = False\n",
    "                                relationships = shortest_path(src_schema_dict[\"relationships\"], tab_name, base_table)\n",
    "                                for rel in relationships:\n",
    "                                    if rel[\"_from\"][\"table\"] not in table_alias_lookup:\n",
    "                                        join_clause = \" LEFT JOIN \" + rel[\"_from\"][\"table\"] + \" ON \" + rel[\"_from\"][\"table\"] + \".\" + rel[\"_from\"][\"column\"] + \" = \" + rel[\"to\"][\"table\"] + \".\" + rel[\"to\"][\"column\"]\n",
    "                                        from_clause += join_clause\n",
    "                                        table_iter += 1\n",
    "                                        table_alias_lookup[rel[\"_from\"][\"table\"]] = {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + rel[\"_from\"][\"table\"] + \"`\"}\n",
    "                                    elif rel[\"to\"][\"table\"] not in table_alias_lookup:\n",
    "                                        join_clause = \" LEFT JOIN \" + rel[\"to\"][\"table\"] + \" ON \" + rel[\"to\"][\"table\"] + \".\" + rel[\"to\"][\"column\"] + \" = \" + rel[\"_from\"][\"table\"] + \".\" + rel[\"_from\"][\"column\"]\n",
    "                                        from_clause += join_clause\n",
    "                                        table_iter += 1\n",
    "                                        table_alias_lookup[rel[\"to\"][\"table\"]] = {\"alias\": \"t\" + str(table_iter), \"qualified_name\": \"`\" + dataset + \".\" + rel[\"to\"][\"table\"] + \"`\"}\n",
    "\n",
    "                    # Build select clause for attribute \n",
    "                    attr_select, from_clause, where_clause = construct_select(attr, target_table, src_schema_dict, from_clause, where_clause)\n",
    "                    src_expr, map_expr = parse_vocab_map_select(attr_select)\n",
    "                    select_clause += \"'{attr_name}' AS attribute, {src_expr} AS source_value, {map_expr} AS mapped_value, COUNT(*) as record_count\".format(attr_name=attr[\"name\"], src_expr=src_expr, map_expr=map_expr)\n",
    "                    group_by_clause += src_expr + \", \" + map_expr\n",
    "                    \n",
    "                    # Build final queries and add to final_query_set\n",
    "                    initial_root_query = select_clause + \" \" + from_clause + \" \" + where_clause + \" \" + group_by_clause\n",
    "                    final_root_query = apply_table_alias(initial_root_query, table_alias_lookup)\n",
    "                    eval_query_set.add(final_root_query)\n",
    "    \n",
    "    # Convert eval_query_set into a union of queries if vocab map fields are present\n",
    "    client = bigquery.Client()\n",
    "    df = pd.DataFrame(columns = [\"attribute\", \"source_value\", \"mapped_value\", \"record_count\"])\n",
    "    final_eval_query = \"\"\n",
    "    if len(eval_query_set) == 0:\n",
    "        return df\n",
    "    else:\n",
    "        for idx, entry in enumerate(eval_query_set):\n",
    "            if idx == 0:\n",
    "                final_eval_query += entry\n",
    "            else:\n",
    "                final_eval_query += \" UNION ALL \" + entry \n",
    "        final_eval_query += \" ORDER BY attribute, record_count DESC\"\n",
    "    \n",
    "    # Execute query and return dataframe with results\n",
    "    try:\n",
    "        df = df.append(client.query(final_eval_query).result().to_dataframe())\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error during query execution: {}\".format(str(e)))\n",
    "    return df\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TESTING\n",
    "\n",
    "# # Additional imports\n",
    "# import data_repo_client\n",
    "# from google.cloud import storage\n",
    "# import google.auth\n",
    "# import google.auth.transport.requests\n",
    "# import pandas as pd\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.colheader_justify', 'center')\n",
    "# pd.set_option('display.precision', 3)\n",
    "\n",
    "# # Setup Google creds and establish TDR clients\n",
    "# creds, project = google.auth.default()\n",
    "# auth_req = google.auth.transport.requests.Request()\n",
    "# creds.refresh(auth_req)\n",
    "# config = data_repo_client.Configuration()\n",
    "# config.host = \"https://data.terra.bio\"\n",
    "# config.access_token = creds.token\n",
    "# api_client = data_repo_client.ApiClient(configuration=config)\n",
    "# api_client.client_side_validation = False\n",
    "# datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "\n",
    "# # Set the desired mapping specification and retrieve artifacts needed for query construction\n",
    "# dataset_id = \"82bf23c1-fe49-48fe-a19b-cad536f67d41\" #params[\"dataset_id\"]\n",
    "# mapping_target = \"anvil\"\n",
    "# mapping_target_spec = \"cmg_ext_2\" \n",
    "\n",
    "# # Retrieve source schema\n",
    "# src_schema_dict = {}\n",
    "# try:\n",
    "#     datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "#     response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "#     src_schema_dict[\"tables\"] = response[\"schema\"][\"tables\"]\n",
    "#     src_schema_dict[\"relationships\"] = response[\"schema\"][\"relationships\"]\n",
    "#     bq_project = response[\"access_information\"][\"big_query\"][\"project_id\"]\n",
    "#     bq_schema = response[\"access_information\"][\"big_query\"][\"dataset_name\"]\n",
    "# except Exception as e:\n",
    "#     print(\"Error retrieving source schema from TDR. Error: {}\".format(e))\n",
    "# #print(src_schema_dict)\n",
    "\n",
    "# # Retrieve target schema and mapping specification\n",
    "# target_schema_dict = {}\n",
    "# mapping_spec = {}\n",
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.get_bucket(ws_bucket_name)\n",
    "# try:\n",
    "#     blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/mapping_schema_object.json\")\n",
    "#     target_schema_dict = json.loads(blob.download_as_string(client=None))\n",
    "# except Exception as e:\n",
    "#     print(\"Error retrieving target schema for specified mapping_target. Error: {}\".format(e))\n",
    "# #print(target_schema_dict)\n",
    "# try:\n",
    "#     blob = bucket.blob(f\"ingest_pipeline/mapping/{mapping_target}/{mapping_target_spec}/mapping_specification.json\")\n",
    "#     blob_string = blob.download_as_text(client=None)\n",
    "#     blob_string = blob_string.replace(\"$DATASET_NAME\", \"Dataset\").replace(\"$PROJECT_NAME\", \"Project\") #UPDATE WITH REAL PARAMETERS\n",
    "#     mapping_spec = json.loads(blob_string)\n",
    "# except Exception as e:\n",
    "#     print(\"Error retrieving mapping specification for specified mapping_target and mapping_target_spec. Error: {}\".format(e))\n",
    "# #print(mapping_spec)\n",
    "\n",
    "# # # Evaluate vocab maps\n",
    "# # df = evaluate_vocab_mapping(mapping_spec, src_schema_dict, target_schema_dict, bq_project, bq_schema)\n",
    "# # display(df)\n",
    "\n",
    "# # Build queries from mapping specification\n",
    "# query_dict = {}\n",
    "# if target_schema_dict:\n",
    "#     for target_table in target_schema_dict[\"tables\"]:\n",
    "#         table_name = target_table[\"name\"]\n",
    "#         missing_artifacts = False\n",
    "#         if src_schema_dict and mapping_spec:\n",
    "#             query_dict[table_name] = build_mapping_query(target_table, src_schema_dict, mapping_spec, bq_project, bq_schema)\n",
    "#         else:\n",
    "#             missing_artifacts = True\n",
    "#             query_dict[table_name] = {\"query\": \"\", \"syntax_check\": \"\"} \n",
    "#     if missing_artifacts == True:\n",
    "#         print(\"Source schema dictionary and/or mapping specification missing. Unable to generate queries.\")\n",
    "# else:\n",
    "#     print(\"Target schema dictionary missing. Unable to generate queries.\")\n",
    "# query_df = pd.DataFrame.from_dict(query_dict, orient=\"index\")\n",
    "# query_df.index.name = \"target_table\"\n",
    "# query_df.reset_index(inplace=True)\n",
    "# display(query_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
