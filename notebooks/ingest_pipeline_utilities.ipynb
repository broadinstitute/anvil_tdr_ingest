{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print('Version 1.0.0: 08/26/2022 3:48pm - Nate Calvanese - Moving transformation functions to a shared utility notebook')\n",
    "#print('Version 1.0.1: 08/30/2022 11:00am - Nate Calvanese - Moving in additional utility functions')\n",
    "#print('Version 1.0.2: 08/30/2022 11:00am - Nate Calvanese - Moving in FAPI workspace functions')\n",
    "#print('Version 1.0.3: 09/16/2022 10:46am - Nate Calvanese - Update to find_file_in_inventory function to deal with NaN')\n",
    "#print('Version 1.0.4: 09/19/2022 5:06pm - Nate Calvanese - Added find_fileref_fields function')\n",
    "print('Version 1.0.5: 09/23/2022 4:29pm - Nate Calvanese - Moved in ELT pipeline functions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from source_files_creation.ipynb\n",
      "Version 1.0.3: 09/23/2022 11:53am - Nate Calvanese - Made source workspace configurable\n",
      "importing Jupyter notebook from ingest_pipeline_utilities.ipynb\n",
      "Version 1.0.5: 09/23/2022 11:13am - Nate Calvanese - Moved in ELT pipeline functions\n",
      "importing Jupyter notebook from build_file_inventory.ipynb\n",
      "Version 1.0.0: 09/08/2022 07:56pm - Nate Calvanese - Initial Version\n",
      "importing Jupyter notebook from process_table_data.ipynb\n",
      "Version: 1.0.1: 9/16/2022 10:57am - Nate Calvanese - Fixed bug in file_inventory table creation\n",
      "importing Jupyter notebook from build_mapping_query.ipynb\n",
      "Version 1.0.5: 09/21/2022 11:58am - Nate Calvanese - Made multi-column array agg return array with distinct values\n",
      "importing Jupyter notebook from output_data_validation.ipynb\n",
      "Version 2.0.2: 09/14/2022 10:23am -- Made output directory and validation schema more configurable\n",
      "phs1234\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from firecloud import api as fapi\n",
    "import data_repo_client\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import datetime\n",
    "from time import sleep\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import source_files_creation as sfc\n",
    "import build_file_inventory as bfi\n",
    "import process_table_data as ptd\n",
    "import build_mapping_query as bmq\n",
    "import output_data_validation as odv\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert list represented as string to a list data type\n",
    "def str_list_to_list(in_str, list_delim):\n",
    "    out_list = []\n",
    "    out_list = in_str.split(sep=list_delim)\n",
    "    return out_list\n",
    "\n",
    "# Function to concatenate a string value to each entry in a list (either 'prefix' or 'suffix')\n",
    "def concat_str_to_list(in_str, in_list, delim='_', mode='prefix'):\n",
    "    out_list = []\n",
    "    for item in in_list:\n",
    "        if mode == 'prefix':\n",
    "            out_list.append(in_str + delim + item)\n",
    "        elif mode == 'suffix':\n",
    "            out_list.append(item + delim + instr)\n",
    "        else:\n",
    "            out_list.append(item)\n",
    "    return out_list\n",
    "\n",
    "# Function to convert non-null values from a list of columns into a single list\n",
    "def df_cols_to_list(in_list):\n",
    "    out_list = []\n",
    "    for entry in in_list:\n",
    "        if isinstance(entry, list):\n",
    "            for item in entry:\n",
    "                if pd.notnull(item):\n",
    "                    out_list.append(str(item))\n",
    "        else:\n",
    "            if pd.notnull(entry):\n",
    "                out_list.append(str(entry))\n",
    "    return out_list\n",
    "\n",
    "# Function to add value to existing list (or create new list)\n",
    "def add_to_list(curr_value, new_value):\n",
    "    new_list = []\n",
    "    if new_value == None:\n",
    "        if isinstance(curr_value, list):\n",
    "            new_list = curr_value\n",
    "        else:\n",
    "            new_list.append(curr_value)\n",
    "    elif isinstance(new_value, list):\n",
    "        if curr_value == None:\n",
    "            new_list = new_value\n",
    "        elif isinstance(curr_value, list):\n",
    "            new_list = curr_value\n",
    "            for item in new_value:\n",
    "                if item not in curr_value:\n",
    "                    new_list.append(item)      \n",
    "        elif not isinstance(curr_value, list):\n",
    "            new_list.append(curr_value)\n",
    "            for item in new_value:\n",
    "                if item != curr_value:\n",
    "                    new_list.append(item) \n",
    "    elif not isinstance(new_value, list):\n",
    "        if curr_value == None:\n",
    "            new_list.append(new_value)\n",
    "        elif isinstance(curr_value, list):\n",
    "            new_list = curr_value\n",
    "            if new_value not in curr_value:\n",
    "                new_list.append(new_value)         \n",
    "        elif not isinstance(curr_value, list):\n",
    "            new_list.append(curr_value)\n",
    "            if new_value != curr_value:\n",
    "                new_list.append(new_value)\n",
    "    return_list = [str(x) for x in new_list]\n",
    "    return return_list \n",
    "\n",
    "# Function to apply uuid_v5 hash to input\n",
    "def uuid_hash(value):\n",
    "    try:\n",
    "        if isinstance(value, list):\n",
    "            out_list = []\n",
    "            for item in value:\n",
    "                out_list.append(str(uuid.uuid5(uuid.NAMESPACE_OID, str(item))))\n",
    "            return out_list\n",
    "        else:\n",
    "            out_string = str(uuid.uuid5(uuid.NAMESPACE_OID, str(value)))\n",
    "            return out_string\n",
    "    except:\n",
    "        return\n",
    "\n",
    "# Function to retrieve and apply vocabulary mapping\n",
    "def map_column_values(col, map_file_path, null_unmatched=False):\n",
    "    try:\n",
    "        # Read file into dataframe and convert to dictionary\n",
    "        df_map_file = pd.read_csv(map_file_path, delimiter = ',')\n",
    "        src_col_name = df_map_file.columns[0]\n",
    "        tar_col_name = df_map_file.columns[1]\n",
    "        map_dict = df_map_file.set_index(src_col_name).to_dict()[tar_col_name]\n",
    "\n",
    "        # Iterate through series items and map\n",
    "        for idx, val in col.iteritems():\n",
    "            if not isinstance(val, (list, dict, set, tuple)):\n",
    "                if null_unmatched == False:\n",
    "                    col.at[idx] = map_dict.get(val, val)\n",
    "                else:\n",
    "                    col.at[idx] = map_dict.get(val, None)\n",
    "            elif isinstance(val, list):\n",
    "                new_list = []\n",
    "                for entry in val:\n",
    "                    if null_unmatched == False:\n",
    "                        new_list.append(map_dict.get(entry, entry))\n",
    "                    else:\n",
    "                        new_list.append(map_dict.get(entry, None))\n",
    "                col.at[idx] = list(filter(None, new_list))\n",
    "            else:\n",
    "                if null_unmatched == True:\n",
    "                    col.at[idx] = None\n",
    "        return col\n",
    "    except FileNotFoundError:\n",
    "        print('Optional map file {path} not found. File will not be used.'.format(path = map_file_path))\n",
    "        return col\n",
    "    except IndexError:\n",
    "        print('Optional map file {path} is malformed (two columns expected: source and target). File will not be used.'.format(path = map_file_path))\n",
    "        return col\n",
    "    except:\n",
    "        print('Unknown error occurred while applying map file {path}. Mapping may not be fully applied.'.format(path = map_file_path))\n",
    "        return col\n",
    "\n",
    "# Function to return first value from list that is a subset of the search term\n",
    "def get_match_val_in_list(search_val, search_list):\n",
    "    if len(search_list) == 0:\n",
    "        return None\n",
    "    elif len(search_list) == 1:\n",
    "        return search_list[0]\n",
    "    else:\n",
    "        return_str = ''\n",
    "        for item in search_list:\n",
    "            if item in search_val:\n",
    "                return_str = item\n",
    "                break\n",
    "        return return_str \n",
    "\n",
    "# Function to search for file in file inventory and return the associated fileref object\n",
    "def find_file_in_inventory(search_string, file_inventory, return_field=\"file_id\", match_multi=True, match_regex=\"\"):\n",
    "    # Intialize variables\n",
    "    fileref_obj = []\n",
    "    if match_regex == \"\" or match_regex == None:\n",
    "        match_regex = \"\"\n",
    "    \n",
    "    # Loop through file inventory, record fileref_obj entry where matches are found, and return matches\n",
    "    match_cnt = 0\n",
    "    if not (search_string == None or pd.isna(search_string)):\n",
    "        for entry in file_inventory:\n",
    "            if search_string in entry['uri'] and re.search(match_regex, entry['uri']):\n",
    "                match_cnt += 1\n",
    "                if return_field == 'file_id':\n",
    "                    fileref_obj.append(entry['file_id'])\n",
    "                else:\n",
    "                    fileref_obj.append(entry['file_ref'])\n",
    "                if match_multi == False:\n",
    "                    break\n",
    "    if match_multi == True:\n",
    "        if match_cnt == 0:\n",
    "            return []\n",
    "        else:\n",
    "            return fileref_obj\n",
    "    else:\n",
    "        if match_cnt == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return fileref_obj[0]\n",
    "\n",
    "# Function to encode table/field names (removing unwanted characters, setting to lower case, etc.)\n",
    "def encode_name(string):\n",
    "    out_str = string.strip()\n",
    "    out_str = re.sub('^entity:', '', out_str) # Specific to the workspace case, should prob be configurable\n",
    "    out_str = re.sub('[\\-\\.\\-:]', '_', out_str)\n",
    "    out_str = re.sub('[^a-z0-9_]', '', out_str)\n",
    "    if re.match('^[_0-9]+', out_str):\n",
    "        out_str = 't_' + out_str\n",
    "    return out_str\n",
    "\n",
    "# Function to map data types to TDR types\n",
    "def map_datatype(string):\n",
    "    lookup = {\n",
    "        'string': 'string',\n",
    "        'int': 'integer',\n",
    "        'int8': 'integer',\n",
    "        'int16': 'integer', \n",
    "        'int32': 'integer', \n",
    "        'int64': 'integer', \n",
    "        'boolean': 'boolean',\n",
    "        'float': 'float',\n",
    "        'float32': 'float',\n",
    "        'float64': 'float'\n",
    "    }\n",
    "    return lookup.get(string.lower(), 'other')\n",
    "\n",
    "# Function to construct relationship object between two fields\n",
    "def construct_relationship(src_table, src_column, tar_table, tar_column):\n",
    "    rel_name = src_table + '_' + src_column + '__to__' + tar_table + '_' + tar_column\n",
    "    rel_dict = {'name': rel_name, 'from': {'table': src_table, 'column': src_column}, 'to': {'table': tar_table, 'column': tar_column}}\n",
    "    return rel_dict\n",
    "\n",
    "# Function to properly format a PHS id\n",
    "def format_phs_id(input_str):\n",
    "    try:\n",
    "        num = re.search(\"phs0*([1-9]+)\", input_str, re.IGNORECASE).group(1)\n",
    "    except:\n",
    "        num = \"\"\n",
    "    if num:\n",
    "        output_str = \"phs\" + str(num).zfill(6)\n",
    "    else:\n",
    "        output_str = \"\"\n",
    "    return output_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firecloud Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pull and format workspace attributes\n",
    "def get_workspace_attributes(ws_project, ws_name):\n",
    "    # Pull workspace attributes\n",
    "    ws_attributes = fapi.get_workspace(ws_project, ws_name, fields=\"workspace.attributes, workspace.authorizationDomain, workspace.googleProject, workspace.bucketName\").json()\n",
    "    # Format workspace attributes (replace nested dictionaries with the lists they represent, for example)\n",
    "    for key, val in ws_attributes[\"workspace\"][\"attributes\"].items():\n",
    "        if isinstance(val, dict):\n",
    "            if val.get(\"items\") != None:\n",
    "                ws_attributes[\"workspace\"][\"attributes\"][key] = val[\"items\"]\n",
    "    # Add additional attributes for PHS ID and Workspace Name\n",
    "    ws_attributes[\"workspace\"][\"project\"] = ws_project\n",
    "    ws_attributes[\"workspace\"][\"name\"] = ws_name\n",
    "    ws_attributes[\"workspace\"][\"attributes\"][\"phs_id\"] = \"\"\n",
    "    if ws_attributes[\"workspace\"][\"attributes\"].get(\"tag:tags\"):\n",
    "        for item in ws_attributes[\"workspace\"][\"attributes\"][\"tag:tags\"]:\n",
    "            if 'dbgap' in item.lower(): \n",
    "                ws_attributes[\"workspace\"][\"attributes\"][\"phs_id\"] = item.lower().split(\":\", 1)[1].strip()\n",
    "    return ws_attributes[\"workspace\"]\n",
    "\n",
    "# Function to scan entity fields for potential file references\n",
    "def find_and_add_fileref_fields(ws_project, ws_name, data_files_bucket, data_file_refs_dict, rows_to_scan=100):\n",
    "    # Pre-process data_file_refs_dict\n",
    "    data_file_refs_set = set()\n",
    "    for key, val in data_file_refs_dict.items():\n",
    "        entity_name = re.sub(\"^ws_\", \"\", key.split(\".\")[0])\n",
    "        for column_entry in val:\n",
    "            data_file_refs_set.add(entity_name + \".\" + column_entry[\"column\"])\n",
    "    \n",
    "    # Get list of entity types\n",
    "    response_etypes = fapi.list_entity_types(ws_project, ws_name)\n",
    "    dict_all_etypes = json.loads(response_etypes.text)\n",
    "    etypes_list = [key for key in dict_all_etypes.keys()]\n",
    "    \n",
    "    # Loop through entity types\n",
    "    file_path = \"gs://\" + data_files_bucket\n",
    "    fileref_attr_set = set()\n",
    "    for etype in etypes_list:\n",
    "        entities_resp = fapi.get_entities(ws_project, ws_name, etype)\n",
    "        entities_dict = json.loads(entities_resp.text)\n",
    "        for i in range(0, rows_to_scan):\n",
    "            try:\n",
    "                for key, val in entities_dict[i][\"attributes\"].items():\n",
    "                    if file_path in val:\n",
    "                        fileref_attr_set.add(etype + \".\" + key)\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "    # Determine if field is already in data_file_refs dictionary and add if not\n",
    "    final_fileref_attr_set = set()\n",
    "    if fileref_attr_set:\n",
    "        for fileref in fileref_attr_set:\n",
    "            if fileref not in data_file_refs_set:\n",
    "                final_fileref_attr_set.add(fileref)\n",
    "                file = \"ws_\" + fileref.split(\".\")[0] + \".tsv\"\n",
    "                column = fileref.split(\".\")[1]\n",
    "                column_dict = {\n",
    "                    \"column\": column,\n",
    "                    \"method\": \"file_path_match\",\n",
    "                    \"match_multiple_files\": True, \n",
    "                    \"match_regex\": None,\n",
    "                    \"create_new_field\": True,\n",
    "                    \"new_field_name\": column + \"_file_id\"\n",
    "                }\n",
    "                if data_file_refs_dict.get(file):\n",
    "                    data_file_refs_dict.append(column_dict)\n",
    "                else:\n",
    "                    data_file_refs_dict[file] = [column_dict]\n",
    "    else:\n",
    "         final_fileref_attr_set = fileref_attr_set\n",
    "    return list(final_fileref_attr_set), data_file_refs_dict   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDR Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class containing TDR functions\n",
    "class TdrUtils:\n",
    "    \n",
    "    # Initialization function\n",
    "    def __init__(self, jobs_api):\n",
    "        self.jobs_api = jobs_api\n",
    "\n",
    "    # Function to wait for TDR job completion and return results\n",
    "    def wait_for_job(self, job_model):\n",
    "        result = job_model\n",
    "        print(\"TDR Job ID: \" + job_model.id)\n",
    "        while True:\n",
    "            if result == None or result.job_status == \"running\":\n",
    "                sleep(10)\n",
    "                result = self.jobs_api.retrieve_job(job_model.id)\n",
    "            elif result.job_status == 'failed':\n",
    "                return self.jobs_api.retrieve_job_result(job_model.id), job_model.id\n",
    "            elif result.job_status == \"succeeded\":\n",
    "                return self.jobs_api.retrieve_job_result(job_model.id), job_model.id\n",
    "            else:\n",
    "                raise \"Unrecognized job state: {}\".format(result.job_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"EL\" Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine whether a specified file exists in cloud storage\n",
    "def file_exists(key, params):\n",
    "    logging.info(f\"Checking for file {key}\")\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(params[\"ws_bucket_name\"])\n",
    "    return storage.Blob(bucket=bucket, name=key).exists(storage_client)\n",
    "\n",
    "# Function to find and execute ingests\n",
    "def run_el_ingests(dataset_id, params, ingest_list):\n",
    "    for table in ingest_list:\n",
    "        target_table = table\n",
    "        source_file_name = table + \".json\"\n",
    "        source_rel_file_path = \"{}/{}\".format(params[\"el_output_dir\"], source_file_name)\n",
    "        source_full_file_path = \"{}/{}/{}\".format(params[\"ws_bucket\"], params[\"el_output_dir\"], source_file_name)\n",
    "        if file_exists(source_rel_file_path, params):\n",
    "            logging.info(f\"Running ingest from {source_file_name} to table {target_table}\")\n",
    "            ingest_request = {\n",
    "                \"table\": target_table,\n",
    "                \"profile_id\": params[\"profile_id\"],\n",
    "                \"ignore_unknown_values\": True,\n",
    "                \"resolve_existing_files\": True,\n",
    "                \"updateStrategy\": \"replace\",\n",
    "                \"format\": \"json\",\n",
    "                \"load_tag\": \"Ingest for {}\".format(params[\"src_ws_name\"]),\n",
    "                \"path\": source_full_file_path\n",
    "            }\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    ingest_request_result, job_id = tdr_utils.wait_for_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "                    logging.info(\"Ingest from file {} succeeded: {}\".format(source_file_name, str(ingest_request_result)[0:1000]))\n",
    "                    params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"File: {source_file_name}\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(ingest_request_result)[0:1000])])\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logging.error(\"Error on Dataset Ingest: {}\".format(str(e)))\n",
    "                    attempt_counter += 1\n",
    "                    if attempt_counter < 2:\n",
    "                        logging.info(\"Retrying Dataset Ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                        sleep(10)\n",
    "                        continue\n",
    "                    else:\n",
    "                        logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"File: {source_file_name}\", \"Error\", str(e)])\n",
    "                        break\n",
    "        else:\n",
    "            logging.warning(f\"Source table data file does not exist.  Skipping: {source_file_name}\")\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"File: {source_file_name}\", \"Warning\", f\"Source table data file does not exist.  Skipping: {source_file_name}\"])\n",
    "            continue\n",
    "\n",
    "# Function to add TDR dataset SA to the appropriate Terra groups and workspace ACLs\n",
    "def set_up_dataset_ingest_sa(dataset_id, params, new_dataset):\n",
    "    logging.info(\"Setting up dataset ingest service account (SA)\")\n",
    "    # Collect dataset-specific SA from TDR\n",
    "    dataset_details = datasets_api.retrieve_dataset(id=dataset_id)\n",
    "    service_account = dataset_details.ingest_service_account\n",
    "    if not service_account:\n",
    "        error_message = \"No dataset ingest service account found. Ensure the dedicatedIngestServiceAccount parameter has been set to True on dataset creation.\"\n",
    "        logging.error(error_message)\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Anvil Ingest Group\", \"Error\", error_message])\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Workspace\", \"Error\", error_message])\n",
    "    else:\n",
    "        # Add SA user to the Anvil Ingest Terra Group\n",
    "        anvil_ingest_sa_group = \"anvil_tdr_ingest\" # full email: anvil_tdr_ingest@firecloud.org\n",
    "        res = fapi.add_user_to_group(anvil_ingest_sa_group, \"member\", service_account)\n",
    "        if res.status_code != 204: \n",
    "            if new_dataset:\n",
    "                error_message = f\"Error adding dataset ingest SA to {anvil_ingest_sa_group} group: {res.text}\"\n",
    "                logging.error(error_message)\n",
    "                params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Anvil Ingest Group\", \"Error\", error_message])\n",
    "            else:\n",
    "                error_message = f\"Error adding dataset ingest SA to {anvil_ingest_sa_group} group (Note that it may have already been added when the dataset SA was first created): {res.text}\"\n",
    "                logging.warning(error_message)\n",
    "                params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Anvil Ingest Group\", \"Warning\", error_message])\n",
    "        else:\n",
    "            logging.info(f\"Dataset service account {service_account} added to {anvil_ingest_sa_group}\")\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Anvil Ingest Group\", \"Success\", service_account])\n",
    "\n",
    "        # Add SA user to workspace as Reader\n",
    "        acl_updates=[{\"email\": service_account,\n",
    "                     \"accessLevel\":\"READER\",\n",
    "                     \"canShare\":False}]\n",
    "        res = fapi.update_workspace_acl(params[\"ws_project\"], params[\"ws_name\"], acl_updates=acl_updates)\n",
    "        if res.status_code != 200:\n",
    "            if new_dataset:\n",
    "                error_message = f\"Error adding dataset ingest SA to workspace: {res.text}\"\n",
    "                logging.error(error_message)\n",
    "                params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Workspace\", \"Error\", error_message])\n",
    "            else:\n",
    "                error_message = f\"Error adding dataset ingest SA to workspace (Note that it may have already been added when the dataset SA was first created): {res.text}\"\n",
    "                logging.warning(error_message)\n",
    "                params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Workspace\", \"Warning\", error_message])\n",
    "        else:\n",
    "            logging.info(f\"Dataset service account {service_account} added to workspace as READER (without SHARE) permissions successfully.\")\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Workspace\", \"Success\", service_account])\n",
    "\n",
    "# Function to create TDR dataset (with retry logic)\n",
    "def create_tdr_dataset(dataset_id, params) -> str:\n",
    "    logging.info(\"Creating new dataset: {}\".format(params[\"dataset_name\"]))\n",
    "    # Pull down TDR schema from cloud storage\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(params[\"ws_bucket_name\"])\n",
    "        schema_blob = bucket.blob(params[\"el_schema_file\"])\n",
    "        schema = json.loads(schema_blob.download_as_string(client=None))\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving TDR schema object: {}\".format(str(e)))\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Create New Dataset\", \"Error\", \"Error retrieving TDR schema object: {}\".format(str(e))])\n",
    "        return\n",
    "    \n",
    "    # Build and execute dataset creation request\n",
    "    properties_dict = {\n",
    "        \"consent_name\": params[\"consent_name\"],\n",
    "        \"auth_domains\": params[\"auth_domains\"],\n",
    "        \"source_workspaces\": [params[\"src_ws_name\"]]\n",
    "    }\n",
    "    dataset_request = {\n",
    "        \"name\": params[\"dataset_name\"],\n",
    "        \"description\": \"TDR Dataset for workspace \".format(params[\"src_ws_name\"]),\n",
    "        \"defaultProfileId\": params[\"profile_id\"],\n",
    "        \"region\": \"us-central1\",\n",
    "        \"cloudPlatform\": \"gcp\",\n",
    "        \"phsId\": params[\"phs_id\"],\n",
    "        \"experimentalSelfHosted\": True,\n",
    "        \"dedicatedIngestServiceAccount\": True,\n",
    "        \"properties\": properties_dict,\n",
    "        \"schema\": schema\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            create_dataset_result, job_id = tdr_utils.wait_for_job(datasets_api.create_dataset(dataset=dataset_request))\n",
    "            logging.info(\"Dataset Creation succeeded: {}\".format(create_dataset_result))\n",
    "            dataset_id = create_dataset_result[\"id\"]\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Create New Dataset\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(create_dataset_result)[0:1000])])\n",
    "            try:\n",
    "                resp = datasets_api.add_dataset_policy_member(id=dataset_id, policy_name=\"steward\", policy_member={\"email\": \"anvil_tdr_ingest@firecloud.org\"})\n",
    "            except:\n",
    "                logging.warning(\"Error on adding additional policy members to dataset: {}\".format(resp))\n",
    "            return dataset_id\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Dataset Creation: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Dataset Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Create New Dataset\", \"Error\", str(e)])\n",
    "                return dataset_id\n",
    "\n",
    "# Function to patch TDR dataset (with retry logic)\n",
    "def patch_tdr_dataset(dataset_id, params) -> str:\n",
    "    logging.info(\"Patching properties for existing dataset: {}\".format(params[\"dataset_name\"]))\n",
    "    # Retrieve existing data set properties\n",
    "    try:\n",
    "        dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"PROPERTIES\"]).to_dict()\n",
    "        auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "        src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "        for ad in params[\"auth_domains\"]:\n",
    "            if ad not in auth_domains:\n",
    "                auth_domains.append(ad)\n",
    "        if params[\"src_ws_name\"] not in src_workspaces:\n",
    "            src_workspaces.append(params[\"src_ws_name\"])\n",
    "        properties_dict = {\n",
    "            \"consent_name\": params[\"consent_name\"],\n",
    "            \"auth_domains\": auth_domains,\n",
    "            \"source_workspaces\": src_workspaces\n",
    "        }\n",
    "    except:\n",
    "        properties_dict = {\n",
    "            \"consent_name\": params[\"consent_name\"],\n",
    "            \"auth_domains\": params[\"auth_domains\"],\n",
    "            \"source_workspaces\": [params[\"src_ws_name\"]]\n",
    "        }\n",
    "    # Execute dataset patch request\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            phs_id = params[\"phs_id\"]\n",
    "            resp = datasets_api.patch_dataset(id=dataset_id, dataset_patch_request_model={\"phsId\": phs_id, \"properties\": properties_dict})\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Patch Existing Dataset\", \"Success\", resp])\n",
    "            return dataset_id\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Dataset Patch: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Dataset Patch (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Patch Existing Dataset\", \"Error\", str(e)])\n",
    "                return dataset_id\n",
    "\n",
    "# Function to create or retrieve specified TDR dataset\n",
    "def create_or_retrieve_dataset(params):\n",
    "    logging.info(\"Attempting to create or retrieve the specified TDR dataset\")\n",
    "    # Pull down TDR schema from cloud storage\n",
    "    dataset_id = \"\"\n",
    "    new_dataset = False\n",
    "    try:\n",
    "        dataset_list = datasets_api.enumerate_datasets(filter=params[\"dataset_name\"])\n",
    "        if dataset_list.items:\n",
    "            for dataset in dataset_list.items:\n",
    "                if dataset.name == params[\"dataset_name\"]:\n",
    "                    dataset_id = str(dataset.id)\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Enumerate Datasets\", \"Success\", \"{} datasets found. Matching dataset_id = {}\".format(len(dataset_list.items), dataset_id)])\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error on Dataset Enumeration: {}\".format(str(e)))\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Enumerate Datasets\", \"Error\", str(e)])\n",
    "        return\n",
    "    if dataset_id == \"\":\n",
    "        dataset_id = create_tdr_dataset(dataset_id, params)\n",
    "        new_dataset = True\n",
    "        return dataset_id, new_dataset\n",
    "    else:\n",
    "        logging.info(\"Dataset already exists! ID = {}\".format(dataset_id))\n",
    "        patch_tdr_dataset(dataset_id, params)\n",
    "        return dataset_id, new_dataset\n",
    "\n",
    "# Function to run file inventory build step, with retry logic\n",
    "def run_build_file_inventory(params):\n",
    "    inventory = []\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            inventory = bfi.build_inventory(params)\n",
    "            logging.info(\"File Inventory build succeeded. {} files found.\".format(len(inventory)))\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"File Inventory Creation\", \"Build File Inventory\", \"Success\", \"{} files found\".format(len(inventory))])\n",
    "            return inventory\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on File Inventory Creation: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying File Inventory Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"File Inventory Creation\", \"Build File Inventory\", \"Error\", str(e)])\n",
    "                return inventory\n",
    "    return inventory  \n",
    "\n",
    "# Function to orchestrate the various components of the ingest pipeline\n",
    "def run_el_pipeline(workspace, params):\n",
    "    \n",
    "    # Setup Google creds and establish TDR clients\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    config = data_repo_client.Configuration()\n",
    "    config.host = \"https://data.terra.bio\"\n",
    "    config.access_token = creds.token\n",
    "    api_client = data_repo_client.ApiClient(configuration=config)\n",
    "    api_client.client_side_validation = False\n",
    "    global datasets_api\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    global jobs_api\n",
    "    jobs_api = data_repo_client.JobsApi(api_client=api_client)\n",
    "    global tdr_utils\n",
    "    tdr_utils = TdrUtils(jobs_api)\n",
    "    \n",
    "    # Step 1: Set Variables for Pipeline\n",
    "    workspace_name = workspace[0] \n",
    "    logging.info(\"Starting Extract and Load (EL) Pipeline for {}.\".format(workspace_name))\n",
    "    params[\"src_ws_name\"] = workspace_name\n",
    "    params[\"src_ws_project\"] = workspace[1]\n",
    "    params[\"dataset_name\"] = workspace[2]\n",
    "    params[\"input_dir\"] = \"ingest_pipeline/input/{}/table_data\".format(workspace_name)\n",
    "    params[\"file_inventory_dir\"] = \"ingest_pipeline/input/{}/data_files/file_inventory\".format(workspace_name)\n",
    "    params[\"el_output_dir\"] = \"ingest_pipeline/output/source/{}/table_data\".format(workspace_name)\n",
    "    params[\"el_schema_file\"] = \"ingest_pipeline/output/source/{}/schema/tdr_schema_object.json\".format(workspace_name)\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "    params[\"pipeline_results\"] = []\n",
    "\n",
    "    # Step 2: Create Source Files\n",
    "    if params[\"skip_source_files_creation\"] == True:\n",
    "        logging.info(\"Skipping source files creation.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Source Files Creation\", \"Create Source Files\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running source files creation.\")\n",
    "        try:\n",
    "            log_status, log_string = sfc.create_source_table_data_files(params)\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Source Files Creation\", \"Create Source Files\", log_status, log_string])\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error creating source files: {}\".format(str(e)))\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Source Files Creation\", \"Create Source Files\", \"Error\", str(e)])\n",
    "    \n",
    "    # Step 3: Build File Inventory\n",
    "    if params[\"skip_file_inventory_creation\"] == True:\n",
    "        logging.info(\"Skipping file inventory creation.\")\n",
    "        file_inventory = {}\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"File Inventory Creation\", \"Build File Inventory\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Building file inventory.\")\n",
    "        file_inventory = run_build_file_inventory(params)\n",
    "    params[\"file_inventory\"] = file_inventory\n",
    "    \n",
    "    # Step 4: Process Table Data\n",
    "    if params[\"skip_table_data_processing\"] == True:\n",
    "        logging.info(\"Skipping table data processing.\")\n",
    "        target_tables = {}\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Table Data Processing\", \"Ingest Pre-Processing\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Processing table data for ingest.\")\n",
    "        fileref_list, params[\"data_file_refs\"] = find_and_add_fileref_fields(params[\"src_ws_project\"], params[\"src_ws_name\"], params[\"data_files_src_bucket\"], params[\"data_file_refs\"])\n",
    "        logging.info(\"Additional file reference fields found and marked for processing: \" + \", \".join(fileref_list))\n",
    "        target_tables, log_status, log_string = ptd.process_table_data(params)\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Table Data Processing\", \"Ingest Pre-Processing\", log_status, log_string])\n",
    "    \n",
    "    # Step 5: Create or Retrieve Dataset\n",
    "    dataset_id, new_dataset = create_or_retrieve_dataset(params)\n",
    "    if len(dataset_id) == 0:\n",
    "        logging.error(\"No TDR dataset created or retrieved. Exiting pipeline.\")\n",
    "        return dataset_id, params[\"pipeline_results\"]\n",
    "    else:\n",
    "        params[\"dataset_id\"] = dataset_id\n",
    "    if new_dataset == True:\n",
    "        logging.info(\"Sleeping for a few minutes to let policy/permission changes propogate for new dataset...\")\n",
    "        sleep(60)\n",
    "    \n",
    "    # Step 6: Set up Dataset-specific Service Account\n",
    "    set_up_dataset_ingest_sa(dataset_id, params, new_dataset)\n",
    "    logging.info(\"Sleeping for a few minutes to let policy/permission changes propogate...\")\n",
    "    if new_dataset == True:\n",
    "        sleep(300)\n",
    "    else:\n",
    "        sleep(60)\n",
    "    \n",
    "    # Step 7: Ingest Data to Dataset\n",
    "    if params[\"skip_ingests\"] == True:\n",
    "        logging.info(\"Skipping dataset ingests\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", \"File: All\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running dataset ingests\")\n",
    "        if len(params[\"ingest_list_override\"]) > 0:\n",
    "            target_tables = params[\"ingest_list_override\"]\n",
    "        run_el_ingests(dataset_id, params, target_tables)   \n",
    "    \n",
    "    # Aggregate and Write Out Pipeline Results\n",
    "    df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Workspace\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    output_file_path = \"pipeline_results_log_\" + current_datetime_string + \".tsv\"\n",
    "    destination_dir = params[\"ws_bucket\"] + \"/ingest_pipeline/output/source/{}/logs\".format(workspace_name)\n",
    "    df_results.to_csv(output_file_path, index=False, sep=\"\\t\")\n",
    "    !gsutil cp $output_file_path $destination_dir/ 2> stdout\n",
    "    !rm $output_file_path\n",
    "    \n",
    "    # Display Pipeline Results\n",
    "    logging.info(\"The ingest pipeline has completed for {}.\".format(workspace_name))\n",
    "    logging.info(\"Pipeline Results:\")\n",
    "    display(df_results)\n",
    "    return dataset_id, params[\"pipeline_results\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"T\" Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to look up dbgap consent codes\n",
    "def lookup_consent_code(params, phs_id, consent_name):\n",
    "    logging.info(\"Attempting to lookup consent code using PHS: {} and Consent Name: {}\".format(phs_id, consent_name))\n",
    "    # Attempt to read source files into data frame, checking for missing file\n",
    "    src_file_path = params[\"ws_bucket\"] + \"/ingest_pipeline/resources/consent_code_lookup/dbgap_consents.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(src_file_path, delimiter = ',')\n",
    "    except:\n",
    "        err = \"Consent code lookup file not found at {}\".format(src_file_path)\n",
    "        logging.error(err)\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Lookup Consent Code\", \"Warning\", err])\n",
    "        return consent_name\n",
    "    # Map consent name to consent code and return string\n",
    "    df2 = df[(df.phs == phs_id) & (df.consent_short_name == consent_name)]\n",
    "    if len(df2) > 0:\n",
    "        consent_code = \"c\" + str(df2[\"consent_code\"].values[0])\n",
    "        return consent_code\n",
    "    else:\n",
    "        return consent_name\n",
    "            \n",
    "# Function to create and share a snapshot            \n",
    "def create_and_share_snapshot(params):\n",
    "    logging.info(\"Creating full-view snapshot\")\n",
    "    # Attempt to collect and map consent code\n",
    "    try:\n",
    "        phs_short_id = re.search('([0-9]{4,6})', params[\"phs_id\"]).group(1)\n",
    "    except:\n",
    "        phs_short_id = \"\"\n",
    "    if len(params[\"consent_name\"]) > 0 and len(phs_short_id) > 0:\n",
    "        consent_code = lookup_consent_code(params, int(phs_short_id), params[\"consent_name\"])\n",
    "    else:\n",
    "        logging.warning(\"Unable to lookup consent code. Consent name and/or PHS ID missing for lookup.\")\n",
    "        consent_code = params[\"consent_name\"]\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Lookup Consent Code\", \"Warning\", \"Consent name and/or PHS ID missing for lookup\"])\n",
    "    \n",
    "    # Create reader list for snapshot, including auth domain(s) on WS\n",
    "    reader_list = params[\"snapshot_readers_list\"]\n",
    "    auth_domain_list = params[\"auth_domains\"] \n",
    "    for ad_entry in auth_domain_list:\n",
    "        reader_list.append(ad_entry + \"@firecloud.org\")\n",
    "    \n",
    "    # Create and submit snapshot creation request\n",
    "    logging.info(\"Submitting snapshot request\")\n",
    "    snapshot_req = {\n",
    "        \"name\": params[\"snapshot_name\"],\n",
    "        \"description\": \"Full view snapshot of \" + params[\"dataset_name\"],\n",
    "        \"consentCode\": consent_code,\n",
    "        \"contents\": [{\n",
    "            \"datasetName\": params[\"dataset_name\"],\n",
    "            \"mode\": \"byFullView\"\n",
    "        }],\n",
    "        \"readers\": reader_list,\n",
    "        \"profileId\": params[\"profile_id\"]\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            create_snapshot_result, job_id = tdr_utils.wait_for_job(snapshots_api.create_snapshot(snapshot=snapshot_req))\n",
    "            logging.info(\"Snapshot Creation succeeded: {}\".format(create_snapshot_result))\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(create_snapshot_result)[0:1000])])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Snapshot Creation: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Snapshot Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Error\", str(e)])\n",
    "                break\n",
    "\n",
    "# Function to find and execute ingests\n",
    "def run_t_ingests(params):\n",
    "    dataset_id = params[\"dataset_id\"]\n",
    "    query_set = params[\"query_set\"]\n",
    "    override_list = params[\"ingest_list_override\"]\n",
    "    # Determine set of ingests to run\n",
    "    ingest_list = query_set[\"transforms\"].keys()\n",
    "    filtered_ingest_list = []\n",
    "    if override_list:\n",
    "        for item in ingest_list:\n",
    "            if item in override_list:\n",
    "                filtered_ingest_list.append(item)\n",
    "    else:\n",
    "        filtered_ingest_list = ingest_list\n",
    "    # Loop through and ingest files\n",
    "    for item in filtered_ingest_list:\n",
    "        target_table = item\n",
    "        source_file_name = target_table + \".json\" \n",
    "        source_rel_file_path = \"{}/{}\".format(params[\"t_output_dir\"], source_file_name)\n",
    "        source_full_file_path = \"{}/{}/{}\".format(params[\"ws_bucket\"], params[\"t_output_dir\"], source_file_name)\n",
    "        if file_exists(source_rel_file_path, params):\n",
    "            logging.info(f\"Running ingest from {source_file_name} to table {target_table}\")\n",
    "            ingest_request = {\n",
    "                \"table\": target_table,\n",
    "                \"profile_id\": params[\"profile_id\"],\n",
    "                \"ignore_unknown_values\": True,\n",
    "                \"resolve_existing_files\": True,\n",
    "                \"updateStrategy\": \"replace\",\n",
    "                \"format\": \"json\",\n",
    "                \"load_tag\": \"Ingest for {}\".format(params[\"dataset_id\"]),\n",
    "                \"path\": source_full_file_path\n",
    "            }\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    ingest_request_result, job_id = tdr_utils.wait_for_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "                    logging.info(\"Ingest from file {} succeeded: {}\".format(source_file_name, str(ingest_request_result)[0:1000]))\n",
    "                    params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"File: {source_file_name}\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(ingest_request_result)[0:1000])])\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logging.error(\"Error on Dataset Ingest: {}\".format(str(e)))\n",
    "                    attempt_counter += 1\n",
    "                    if attempt_counter < 2:\n",
    "                        logging.info(\"Retrying Dataset Ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                        sleep(10)\n",
    "                        continue\n",
    "                    else:\n",
    "                        logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"File: {source_file_name}\", \"Error\", str(e)])\n",
    "                        break\n",
    "        else:\n",
    "            logging.warning(f\"Metadata file does not exist.  Skipping: {source_file_name}\")\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"File: {source_file_name}\", \"Warning\", f\"Metadata file does not exist.  Skipping: {source_file_name}\"])\n",
    "            continue              \n",
    "                \n",
    "# Function to extend schema of existing TDR dataset\n",
    "def run_schema_extension(params):\n",
    "    dataset_id = params[\"dataset_id\"]\n",
    "    target_schema = params[\"target_schema\"]\n",
    "    mapping_target = params[\"mapping_target\"]\n",
    "    # Add source_datarepo_row_id column to all tables\n",
    "    source_row_ids_col_def = {\"name\": \"source_datarepo_row_ids\", \"datatype\": \"string\", \"array_of\": True}\n",
    "    for table_entry in target_schema[\"tables\"]:\n",
    "        table_entry[\"columns\"].append(source_row_ids_col_def)\n",
    "    # Retrieve current TDR schema and diff with target_schema\n",
    "    logging.info(\"Retrieving current TDR schema to determine new tables and relationships to add.\")\n",
    "    additional_tables = []\n",
    "    additional_relationships = []\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        current_table_list = [table[\"name\"] for table in response[\"schema\"][\"tables\"]]\n",
    "        current_rel_list = [rel[\"name\"] for rel in response[\"schema\"][\"relationships\"]]\n",
    "        for table_entry in target_schema[\"tables\"]:\n",
    "            if table_entry[\"name\"] not in current_table_list:\n",
    "                additional_tables.append(table_entry)\n",
    "        for rel_entry in target_schema[\"relationships\"]:\n",
    "            if rel_entry[\"name\"] not in current_rel_list:\n",
    "                additional_relationships.append(rel_entry)\n",
    "    except Exception as e:\n",
    "        logging.warning(\"Error retrieving source schema from TDR. Will attempt to add all tables and relationships to schema. Error: {}\".format(e))\n",
    "        additional_tables = target_schema[\"tables\"]\n",
    "        additional_relationships = target_schema[\"relationships\"]\n",
    "    # Update TDR dataset schema to include additional tables and relationships\n",
    "    add_table_list = [table[\"name\"] for table in additional_tables]\n",
    "    add_rel_list = [rel[\"name\"] for rel in additional_relationships]\n",
    "    if add_table_list or add_rel_list:\n",
    "        logging.info(\"Submitting TDR schema extension request.\\n\\tTables to add: {tabs}\\n\\tRelationships to add: {rels}\".format(tabs=add_table_list, rels=add_rel_list))\n",
    "        schema_update_request = {\n",
    "            \"description\": \"Adding tables and relationships for mapping target: {}.\".format(params[\"mapping_target\"]),\n",
    "            \"changes\": {\n",
    "                \"addTables\": additional_tables,\n",
    "                \"addRelationships\": additional_relationships\n",
    "            }\n",
    "        }\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                schema_update_result, job_id = tdr_utils.wait_for_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "                logging.info(\"TDR schema extension succeeded: {}\".format(str(schema_update_result)[0:1000]))\n",
    "                params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"TDR Schema Extension\", \"Extend TDR Schema\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(schema_update_result)[0:1000])])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error on TDR schema extension: {}\".format(str(e)))\n",
    "                attempt_counter += 1\n",
    "                if attempt_counter < 2:\n",
    "                    logging.info(\"Retrying TDR schema extension (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(10)\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                    params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"TDR Schema Extension\", \"Extend TDR Schema\", \"Error\", str(e)])\n",
    "                    break\n",
    "    else:\n",
    "        logging.info(\"No new tables or relationships to add to the TDR schema.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"TDR Schema Extension\", \"Extend TDR Schema\", \"Success\", \"No new tables or relationships to add to the TDR schema.\"])\n",
    "                \n",
    "# Function to loop through and execute transform queries\n",
    "def run_transform_queries(params):\n",
    "    # Determine set of transforms to run\n",
    "    query_set = params[\"query_set\"][\"transforms\"]\n",
    "    override_list = params[\"transform_list_override\"]\n",
    "    filtered_query_set = {}\n",
    "    if override_list:\n",
    "        for key, val in query_set.items():\n",
    "            if key in override_list:\n",
    "                filtered_query_set[key] = val\n",
    "    else:\n",
    "        filtered_query_set = query_set\n",
    "    # Read in and execute queries\n",
    "    client = bigquery.Client()\n",
    "    for key, val in filtered_query_set.items():\n",
    "        target_table = key\n",
    "        target_file = target_table + \".json\"\n",
    "        destination_dir = params[\"t_output_dir\"]\n",
    "        logging.info(\"Running transform for target table: {}\".format(target_table))\n",
    "        query = val[\"query\"]\n",
    "        try:\n",
    "            df = client.query(query).result().to_dataframe()\n",
    "            records_json = df.to_json(orient='records') \n",
    "            records_list = json.loads(records_json)\n",
    "            records_cnt = len(records_list)\n",
    "            with open(target_file, 'w') as outfile:\n",
    "                for idx, val in enumerate(records_list):\n",
    "                    json.dump(val, outfile)\n",
    "                    if idx < (records_cnt - 1):\n",
    "                        outfile.write('\\n')\n",
    "            !gsutil cp $target_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "            !rm $target_file\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transformed Files Creation\", \"File: {}\".format(target_file), \"Success\", \"\"])\n",
    "        except Exception as e:\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transformed Files Creation\", \"File: {}\".format(target_file), \"Error\", str(e)])\n",
    "                \n",
    "# Function to orchestrate the various components of the transformation ingest pipeline\n",
    "def run_t_pipeline(params):\n",
    "    \n",
    "    # Setup Google creds and establish TDR clients\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    config = data_repo_client.Configuration()\n",
    "    config.host = \"https://data.terra.bio\"\n",
    "    config.access_token = creds.token\n",
    "    api_client = data_repo_client.ApiClient(configuration=config)\n",
    "    api_client.client_side_validation = False\n",
    "    global datasets_api\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    global snapshots_api\n",
    "    snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "    global jobs_api\n",
    "    jobs_api = data_repo_client.JobsApi(api_client=api_client)\n",
    "    global tdr_utils\n",
    "    tdr_utils = TdrUtils(jobs_api)\n",
    "    \n",
    "    # Step 1: Set Variables for Pipeline\n",
    "    src_dataset = params[\"dataset_name\"] + \" ({})\".format(params[\"dataset_id\"])\n",
    "    logging.info(\"Starting Transformation (T) Pipeline for {}.\".format(src_dataset))\n",
    "    params[\"src_dataset\"] = src_dataset\n",
    "    params[\"target_schema\"] = {}\n",
    "    params[\"query_set\"] = {}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(params[\"ws_bucket_name\"])\n",
    "    blob = bucket.blob(\"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(params[\"mapping_target\"], params[\"dataset_id\"]))\n",
    "    params[\"target_schema\"] = json.loads(blob.download_as_string(client=None))\n",
    "    blob = bucket.blob(\"ingest_pipeline/output/transformed/{}/{}/queries/transform_query_set.json\".format(params[\"mapping_target\"], params[\"dataset_id\"]))\n",
    "    params[\"query_set\"] = json.loads(blob.download_as_string(client=None))\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "    params[\"snapshot_name\"] = params[\"dataset_name\"] + \"_\" + current_datetime_string  \n",
    "    params[\"t_output_dir\"] = \"ingest_pipeline/output/transformed/{}/{}/table_data\".format(params[\"mapping_target\"], params[\"dataset_id\"])\n",
    "    params[\"t_val_output_dir\"] = \"ingest_pipeline/output/transformed/{}/{}/validation\".format(params[\"mapping_target\"], params[\"dataset_id\"])\n",
    "    params[\"validation_schema_file\"] = \"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(params[\"mapping_target\"])\n",
    "    params[\"pipeline_results\"] = []\n",
    "    \n",
    "    # Step 2: Confirm Transformation Artifacts\n",
    "    logging.info(\"Attempting to confirm transform artifact retrieval (target schema and transform queries).\")\n",
    "    if params[\"target_schema\"] and params[\"query_set\"]:\n",
    "        logging.info(\"Transform artifact retrieval confirmed.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transform Artifact Retrieval\", \"Confirm Transform Artifact Retrieval\", \"Success\", \"\"])\n",
    "    else:\n",
    "        if not params[\"target_schema\"] and not params[\"query_set\"]:\n",
    "            error_message = \"Target schema and transform queries both empty.\"\n",
    "            logging.info(error_message)\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transform Artifact Retrieval\", \"Retrieve Target Schema and Transform Queries\", \"Error\", error_message])\n",
    "            return params[\"pipeline_results\"]\n",
    "        elif not params[\"target_schema\"]:\n",
    "            error_message = \"Target schema empty.\"\n",
    "            logging.info(error_message)\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transform Artifact Retrieval\", \"Retrieve Target Schema and Transform Queries\", \"Error\", error_message])\n",
    "            return params[\"pipeline_results\"]\n",
    "        else:\n",
    "            error_message = \"Transform queries empty.\"\n",
    "            logging.info(error_message)\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transform Artifact Retrieval\", \"Retrieve Target Schema and Transform Queries\", \"Error\", error_message])\n",
    "            return params[\"pipeline_results\"]\n",
    "                 \n",
    "    # Step 3: Create Transformed Files\n",
    "    if params[\"skip_transforms\"] == True:\n",
    "        logging.info(\"Skipping transformed files creation.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transformed Files Creation\", \"File: All\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running transformed files creation.\")\n",
    "        run_transform_queries(params)\n",
    "        \n",
    "    # Step 4: Identify TDR Dataset and Extend Schema \n",
    "    if params[\"skip_schema_extension\"] == True:\n",
    "        logging.info(\"Skipping TDR schema extension.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"TDR Schema Extension\", \"Extend TDR Schema\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        if len(params[\"dataset_id\"]) == 0:\n",
    "            logging.error(\"No TDR dataset specified. Exiting pipeline.\")\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"TDR Schema Extension\", \"Extend TDR Schema\", \"Error\", \"No TDR dataset specified.\"])\n",
    "            return params[\"pipeline_results\"]\n",
    "        else:\n",
    "            logging.info(\"Running TDR schema extension.\")\n",
    "            run_schema_extension(params)\n",
    "            \n",
    "    # Step 5: Ingest Transformed Files\n",
    "    if params[\"skip_ingests\"] == True:\n",
    "        logging.info(\"Skipping dataset ingests\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", \"File: All\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running dataset ingests\")\n",
    "        run_t_ingests(params)\n",
    "    \n",
    "    # Step 6: Create and Share Snapshot\n",
    "    int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping snapshot creation due to upstream errors\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Skipped\", \"Errors found upstream in ingest pipeline\"])\n",
    "    elif params[\"skip_snapshot_creation\"] == True:\n",
    "        logging.info(\"Skipping snapshot creation on user request\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running snapshot creation\")\n",
    "        try:\n",
    "            create_and_share_snapshot(params)\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error running snapshot creation: {}\".format(str(e)))\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Error\", str(e)])\n",
    "    \n",
    "    # Step 7: Data Profiling and Validation\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping output data validation due to upstream errors\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Output Data Validation\", \"Profile and Validate Data\", \"Skipped\", \"Errors found upstream in ingest pipeline\"])\n",
    "    elif params[\"skip_data_validation\"] == True:\n",
    "        logging.info(\"Skipping output data validation on user request\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Output Data Validation\", \"Profile and Validate Data\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running output data validation\")\n",
    "        try:\n",
    "            odv.profile_data(params[\"dataset_id\"], \"dataset\", params[\"t_val_output_dir\"], params[\"validation_schema_file\"])\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Output Data Validation\", \"Profile and Validate Data\", \"Success\", \"\"])\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error running output data validation: {}\".format(str(e)))\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Output Data Validation\", \"Profile and Validate Data\", \"Error\", str(e)])   \n",
    "    \n",
    "    # Aggregate and Write Out Pipeline Results\n",
    "    df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    output_file_path = \"pipeline_results_log_\" + current_datetime_string + \".tsv\"\n",
    "    destination_dir = \"ingest_pipeline/output/transformed/{}/{}/logs\".format(params[\"mapping_target\"], params[\"dataset_id\"])\n",
    "    df_results.to_csv(output_file_path, index=False, sep=\"\\t\")\n",
    "    !gsutil cp $output_file_path $ws_bucket/$destination_dir/ 2> stdout\n",
    "    !rm $output_file_path\n",
    "    \n",
    "    # Display Pipeline Results\n",
    "    logging.info(\"The ingest pipeline has completed for {}.\".format(params[\"src_dataset\"]))\n",
    "    logging.info(\"Pipeline Results:\")\n",
    "    display(df_results)\n",
    "    return params[\"pipeline_results\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
