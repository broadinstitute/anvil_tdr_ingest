{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print('Version 1.0.0: 08/26/2022 3:48pm - Nate Calvanese - Moving transformation functions to a shared utility notebook')\n",
    "#print('Version 1.0.1: 08/30/2022 11:00am - Nate Calvanese - Moving in additional utility functions')\n",
    "#print('Version 1.0.2: 08/30/2022 11:00am - Nate Calvanese - Moving in FAPI workspace functions')\n",
    "#print('Version 1.0.3: 09/16/2022 10:46am - Nate Calvanese - Update to find_file_in_inventory function to deal with NaN')\n",
    "#print('Version 1.0.4: 09/19/2022 5:06pm - Nate Calvanese - Added find_fileref_fields function')\n",
    "#print('Version 1.0.5: 09/23/2022 4:29pm - Nate Calvanese - Moved in ELT pipeline functions')\n",
    "#print('Version 1.0.6: 09/26/2022 3:22pm - Nate Calvanese - Added PHS and dataset formatting functions')\n",
    "#print('Version 1.0.7: 09/26/2022 4:36pm - Nate Calvanese - Added schema reconciliation functionality to TDR dataset patching')\n",
    "#print('Version 1.0.8: 10/5/2022 2:58pm - Nate Calvanese - Added support for chunking ingest requests')\n",
    "#print('Version 1.0.9: 10/6/2022 10:35am - Nate Calvanese - Updated TDR utility functions')\n",
    "#print('Version 1.0.10: 10/12/2022 10:55am - Nate Calvanese - Added support for snapshotting in EL pipeline')\n",
    "#print('Version 1.0.11: 10/18/2022 9:33am - Nate Calvanese - Fixed a few bugs in various functions')\n",
    "#print('Version 1.0.12: 10/21/2022 9:56am - Nate Calvanese - Added dangling foreign key resolution and additional logging')\n",
    "#print('Version 1.0.13: 10/24/2022 1:29pm - Nate Calvanese - Added function to derive project entity name')\n",
    "#print('Version 1.0.14: 10/26/2022 1:00pm - Nate Calvanese - Updated encode name function to only return lowercase')\n",
    "#print('Version 1.0.15: 11/4/2022 9:35am - Nate Calvanese - Added file relationship inference to T pipeline')\n",
    "#print('Version 1.0.16: 11/14/2022 2:37pm - Nate Calvanese - Fixed bug in snapshot readers')\n",
    "#print('Version 1.0.17: 1/31/2023 11:01am - Nate Calvanese - Updated logging')\n",
    "#print('Version 1.0.19: 2/23/2023 11:02am - Nate Calvanese - Made addition of auth domains as snapshot readers configurable')\n",
    "#print('Version 1.0.20: 2/25/2023 2:16pm - Nate Calvanese - Replaced FAPI with Requests to make porting to dev easier')\n",
    "#print('Version 1.0.21: 2/28/2023 11:38am - Nate Calvanese - Turned on bulkMode for ingest')\n",
    "#print('Version 1.0.22: 3/3/2023 1:01pm - Nate Calvanese - Tweaks to the TDR job monitoring function')\n",
    "#print('Version 1.0.23: 3/6/2023 2:24pm - Nate Calvanese - Updates to support remote file references')\n",
    "#print('Version 1.0.24: 3/8/2023 8:46am - Nate Calvanese - Performance updates to file ref lookup')\n",
    "print('Version 1.0.25: 3/10/2023 8:46am - Nate Calvanese - Turned on the predictable file IDs dataset creation parameter')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade import_ipynb data_repo_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import requests\n",
    "import data_repo_client\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import datetime\n",
    "import sys\n",
    "import urllib3\n",
    "import xmltodict\n",
    "from time import sleep\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import source_files_creation as sfc\n",
    "import build_file_inventory as bfi\n",
    "import process_table_data as ptd\n",
    "import build_mapping_query as bmq\n",
    "import output_data_validation as odv\n",
    "import resolve_dangling_foreign_keys as rdfk\n",
    "import infer_file_relationships as ifr\n",
    "import identify_supplementary_files as isf\n",
    "\n",
    "# Configure logging format\n",
    "while logging.root.handlers:\n",
    "    logging.root.removeHandler(logging.root.handlers[-1])\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.StreamHandler(sys.stdout)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert values to list\n",
    "def convert_to_list(input_str):\n",
    "    try:\n",
    "        output = json.loads(input_str)\n",
    "        if not isinstance(output, list):\n",
    "            raise Exception\n",
    "    except:\n",
    "        output = [str(input_str)]\n",
    "    return output\n",
    "\n",
    "# Function to search for file in file inventory and return the associated fileref object\n",
    "def find_file_in_inventory(search_input, file_lookup, return_field=\"file_id\", match_multi=True, match_regex=\"\", match_type=\"partial\"):\n",
    "    # Intialize variables\n",
    "    fileref_obj = []\n",
    "    if match_regex == \"\" or match_regex == None:\n",
    "        match_regex = \"\"\n",
    "    \n",
    "    # If search_input is a list, attempt to match files for every entry in the list\n",
    "    if isinstance(search_input, list):\n",
    "        for search_string in search_input:\n",
    "            # If exact match, attempt to get from dict\n",
    "            if match_type == \"exact\":\n",
    "                try:\n",
    "                    if return_field == \"file_id\":\n",
    "                        file_id = file_lookup.get(search_string)[\"file_id\"]\n",
    "                        fileref_obj.append(file_id)\n",
    "                    else:\n",
    "                        file_ref = file_lookup.get(search_string)[\"file_ref\"]\n",
    "                        fileref_obj.append(file_ref)\n",
    "                except:\n",
    "                    pass\n",
    "           \n",
    "            # Else, loop through file lookup, record fileref_obj entry where matches are found, and return matches\n",
    "            else:\n",
    "                if not (search_string == None or pd.isna(search_string)):\n",
    "                    for entry in file_lookup.keys():\n",
    "                        if str(search_string) in entry and re.search(match_regex, entry):\n",
    "                            if return_field == \"file_id\":\n",
    "                                fileref_obj.append(file_lookup[entry][\"file_id\"])\n",
    "                            else:\n",
    "                                fileref_obj.append(file_lookup[entry][\"file_ref\"])\n",
    "                            if match_multi == False:\n",
    "                                break\n",
    "        return fileref_obj\n",
    "    else:\n",
    "        search_string = search_input\n",
    "        # If exact match, attempt to get from dict\n",
    "        if match_type == \"exact\":\n",
    "            try:\n",
    "                if return_field == \"file_id\":\n",
    "                    file_id = file_lookup.get(search_string)[\"file_id\"]\n",
    "                    fileref_obj.append(file_id)\n",
    "                else:\n",
    "                    file_ref = file_lookup.get(search_string)[\"file_ref\"]\n",
    "                    fileref_obj.append(file_ref)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Else, loop through file inventory, record fileref_obj entry where matches are found, and return matches\n",
    "        else:\n",
    "            if not (search_string == None or pd.isna(search_string)):\n",
    "                for entry in file_lookup.keys():\n",
    "                    if str(search_string) in entry and re.search(match_regex, entry):\n",
    "                        if return_field == \"file_id\":\n",
    "                            fileref_obj.append(file_lookup[entry][\"file_id\"])\n",
    "                        else:\n",
    "                            fileref_obj.append(file_lookup[entry][\"file_ref\"])\n",
    "                        if match_multi == False:\n",
    "                            break       \n",
    "        if match_multi == True:\n",
    "            return fileref_obj\n",
    "        else:\n",
    "            if not fileref_obj:\n",
    "                return None\n",
    "            else:\n",
    "                return fileref_obj[0]\n",
    "\n",
    "# Function to encode table/field names (removing unwanted characters, setting to lower case, etc.)\n",
    "def encode_name(string):\n",
    "    reserved_words= [\"all\", \"and\",\"any\",\"array\",\"as\",\"asc\",\"assert_rows_modified\",\"at\",\"between\",\"by\",\"case\",\"cast\",\"collate\",\"contains\",\"create\",\"cross\",\"cube\",\"current\",\"default\",\"define\",\"desc\",\"distinct\",\"else\",\"end\",\"enum\",\"escape\",\"except\",\"exclude\",\"exists\",\"extract\",\"false\",\"fetch\",\"following\",\"for\",\"from\",\"full\",\"group\",\"grouping\",\"groups\",\"hash\",\"having\",\"if\",\"ignore\",\"in\",\"inner\",\"intersect\",\"interval\",\"into\",\"is\",\"join\",\"lateral\",\"left\",\"like\",\"limit\",\"lookup\",\"merge\",\"natural\",\"new\",\"no\",\"not\",\"null\",\"nulls\",\"of\",\"on\",\"or\",\"order\",\"outer\",\"over\",\"partition\",\"preceding\",\"proto\",\"qualify\",\"range\",\"recursive\",\"respect\",\"right\",\"rollup\",\"rows\",\"select\",\"set\",\"some\",\"struct\",\"tablesample\",\"then\",\"to\",\"treat\",\"true\",\"unbounded\",\"union\",\"unnest\",\"using\",\"when\",\"where\",\"window\",\"with\",\"within\"]\n",
    "    out_str = string.strip().lower()\n",
    "    out_str = re.sub('^entity:', '', out_str) # Specific to the workspace case, should prob be configurable\n",
    "    out_str = re.sub('[\\-\\.\\-:]', '_', out_str)\n",
    "    out_str = re.sub('[^a-zA-Z0-9_]', '', out_str)\n",
    "    if re.match('^[_0-9]+', out_str):\n",
    "        out_str = 't_' + out_str\n",
    "    if out_str in reserved_words:\n",
    "        out_str = 't_' + out_str\n",
    "    return out_str\n",
    "\n",
    "# Function to map data types to TDR types\n",
    "def map_datatype(string):\n",
    "    lookup = {\n",
    "        'string': 'string',\n",
    "        'int': 'integer',\n",
    "        'int8': 'integer',\n",
    "        'int16': 'integer', \n",
    "        'int32': 'integer', \n",
    "        'int64': 'integer', \n",
    "        'boolean': 'boolean',\n",
    "        'float': 'float',\n",
    "        'float32': 'float',\n",
    "        'float64': 'float'\n",
    "    }\n",
    "    return lookup.get(string.lower(), 'other')\n",
    "\n",
    "# Function to construct relationship object between two fields\n",
    "def construct_relationship(src_table, src_column, tar_table, tar_column):\n",
    "    rel_name = src_table + '_' + src_column + '__to__' + tar_table + '_' + tar_column\n",
    "    rel_dict = {'name': rel_name, 'from': {'table': src_table, 'column': src_column}, 'to': {'table': tar_table, 'column': tar_column}}\n",
    "    return rel_dict\n",
    "\n",
    "# Function to properly format a PHS id\n",
    "def format_phs_id(input_str):\n",
    "    try:\n",
    "        num = re.search(\"phs0*([0-9]+)\", input_str, re.IGNORECASE).group(1)\n",
    "    except:\n",
    "        num = \"\"\n",
    "    if num:\n",
    "        output_str = \"phs\" + str(num).zfill(6)\n",
    "    else:\n",
    "        output_str = \"\"\n",
    "    return output_str\n",
    "\n",
    "# Function to build default TDR dataset name (Anvil Specific)\n",
    "def format_dataset_name(input_str):\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_date_string = current_datetime.strftime(\"%Y%m%d\")\n",
    "    output_str = \"ANVIL_\" + re.sub(\"^ANVIL[_]?\", \"\", input_str, flags=re.IGNORECASE) + \"_\" + current_date_string\n",
    "    output_str = re.sub(\"[^a-zA-Z0-9_]\", \"_\", output_str)\n",
    "    return output_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firecloud Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pull and format workspace attributes\n",
    "def get_workspace_attributes(ws_project, ws_name):\n",
    "    # Establish credentials\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    \n",
    "    # Pull workspace attributes\n",
    "    ws_attributes = requests.get(\n",
    "        url=f\"https://api.firecloud.org/api/workspaces/{ws_project}/{ws_name}?fields=workspace.attributes,workspace.authorizationDomain,workspace.googleProject,workspace.bucketName\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "    ).json()\n",
    "    \n",
    "    # Format workspace attributes (replace nested dictionaries with the lists they represent, for example)\n",
    "    for key, val in ws_attributes[\"workspace\"][\"attributes\"].items():\n",
    "        if isinstance(val, dict):\n",
    "            if val.get(\"items\") != None:\n",
    "                ws_attributes[\"workspace\"][\"attributes\"][key] = val[\"items\"]\n",
    "    \n",
    "    # Add additional attributes for PHS ID and Workspace Name\n",
    "    ws_attributes[\"workspace\"][\"project\"] = ws_project\n",
    "    ws_attributes[\"workspace\"][\"name\"] = ws_name\n",
    "    ws_attributes[\"workspace\"][\"attributes\"][\"phs_id\"] = \"\"\n",
    "    if ws_attributes[\"workspace\"][\"attributes\"].get(\"tag:tags\"):\n",
    "        for item in ws_attributes[\"workspace\"][\"attributes\"][\"tag:tags\"]:\n",
    "            if 'dbgap' in item.lower(): \n",
    "                ws_attributes[\"workspace\"][\"attributes\"][\"phs_id\"] = item.lower().split(\":\", 1)[1].strip()\n",
    "    return ws_attributes[\"workspace\"]\n",
    "\n",
    "# Function to get list of entity types in a workspace\n",
    "def list_entity_types(ws_project, ws_name):\n",
    "    # Establish credentials\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    \n",
    "    # Get and return list of entity types\n",
    "    response = requests.get(\n",
    "        url=f\"https://api.firecloud.org/api/workspaces/{ws_project}/{ws_name}/entities\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Function to return entities of an entity type in a workspace\n",
    "def get_entities(ws_project, ws_name, etype):\n",
    "    # Establish credentials\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "\n",
    "    # Get and return entities of specified type\n",
    "    response = requests.get(\n",
    "            url=f\"https://api.firecloud.org/api/workspaces/{ws_project}/{ws_name}/entities/{etype}\",\n",
    "            headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "        )\n",
    "    return response\n",
    "\n",
    "# Function to collect remote file references from a file ref field\n",
    "def collect_remote_filerefs(ws_project, ws_name, entity, field, data_files_src_bucket):\n",
    "    response_entities = get_entities(ws_project, ws_name, entity)\n",
    "    entities_dict = json.loads(response_entities.text)\n",
    "    # Loop through entities and add items to the data_files_src_bucket dict as necessary\n",
    "    for row in entities_dict:\n",
    "        try:\n",
    "            for key, val in row[\"attributes\"].items():\n",
    "                if key == field:\n",
    "                    if not isinstance(val, dict):\n",
    "                        bucket = re.match('gs:\\/\\/([a-z0-9\\-]+)\\/', val).group(1)\n",
    "                        obj = re.match('gs:\\/\\/[a-z0-9\\-]+\\/([A-Za-z0-9\\-_\\/\\.]+)', val).group(1)\n",
    "                        if bucket not in (data_files_src_bucket):\n",
    "                            data_files_src_bucket[bucket] = {\"include_dirs\": [obj], \"exclude_dirs\": []}\n",
    "                        else:\n",
    "                            if obj not in data_files_src_bucket[bucket][\"include_dirs\"]:\n",
    "                                data_files_src_bucket[bucket][\"include_dirs\"].append(obj)\n",
    "                    else:\n",
    "                        if val.get(\"items\"):\n",
    "                            for entry in val[\"items\"]:\n",
    "                                bucket = re.match('gs:\\/\\/([a-z0-9\\-]+)\\/', entry).group(1)\n",
    "                                obj = re.match('gs:\\/\\/[a-z0-9\\-]+\\/([A-Za-z0-9\\-_\\/\\.]+)', entry).group(1)\n",
    "                                if bucket not in (data_files_src_bucket):\n",
    "                                    data_files_src_bucket[bucket] = {\"include_dirs\": [obj], \"exclude_dirs\": []}\n",
    "                                else:\n",
    "                                    if obj not in data_files_src_bucket[bucket][\"include_dirs\"]:\n",
    "                                        data_files_src_bucket[bucket][\"include_dirs\"].append(obj)\n",
    "        except:\n",
    "            break\n",
    "    return data_files_src_bucket\n",
    "\n",
    "# Function to scan entity fields for file references and add to the list of file ref fields to consider if found\n",
    "def find_and_add_fileref_fields(ws_project, ws_name, data_files_bucket, data_file_refs_dict, data_files_src_bucket, rows_to_scan=100):\n",
    "    # Pre-process data_file_refs_dict\n",
    "    data_file_refs_set = set()\n",
    "    for key, val in data_file_refs_dict.items():\n",
    "        for column_entry in val:\n",
    "            data_file_refs_set.add(key + \".\" + column_entry[\"column\"])\n",
    "    \n",
    "    # Establish credentials\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    \n",
    "    # Get list of entity types\n",
    "    response_etypes = list_entity_types(ws_project, ws_name)\n",
    "    dict_all_etypes = json.loads(response_etypes.text)\n",
    "    etypes_list = [key for key in dict_all_etypes.keys()]\n",
    "    \n",
    "    # Loop through entity types and check for file references (both local and remote)\n",
    "    file_path = \"gs://\" + data_files_bucket\n",
    "    fileref_attr_set = set()\n",
    "    remote_bucket_set = set()\n",
    "    for etype in etypes_list:\n",
    "        response_entities = get_entities(ws_project, ws_name, etype)\n",
    "        entities_dict = json.loads(response_entities.text) \n",
    "        for i in range(0, rows_to_scan):\n",
    "            try:\n",
    "                for key, val in entities_dict[i][\"attributes\"].items():\n",
    "                    if not isinstance(val, dict):\n",
    "                        if file_path in val:\n",
    "                            fileref_attr_set.add(etype + \".\" + key)\n",
    "                        elif re.search(\"^gs:\\/\\/([a-z0-9\\.\\-_]+)\\/\", val):\n",
    "                            remote_bucket_set.add(re.search(\"^gs:\\/\\/([a-z0-9\\.\\-_]+)\\/\", val).group(1))\n",
    "                            fileref_attr_set.add(etype + \".\" + key)\n",
    "                            data_files_src_bucket = collect_remote_filerefs(ws_project, ws_name, etype, key, data_files_src_bucket)\n",
    "                    else:\n",
    "                        if val.get(\"items\"):\n",
    "                            for entry in val[\"items\"]:\n",
    "                                if file_path in entry:\n",
    "                                    fileref_attr_set.add(etype + \".\" + key)\n",
    "                                elif re.search(\"^gs:\\/\\/([a-z0-9\\.\\-_]+)\\/\", entry):\n",
    "                                    remote_bucket_set.add(re.search(\"^gs:\\/\\/([a-z0-9\\.\\-_]+)\\/\", entry).group(1))\n",
    "                                    fileref_attr_set.add(etype + \".\" + key)\n",
    "                                    data_files_src_bucket = collect_remote_filerefs(ws_project, ws_name, etype, key, data_files_src_bucket)\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "    # Determine if field is already in data_file_refs dictionary and add if not\n",
    "    final_fileref_attr_set = set()\n",
    "    if fileref_attr_set:\n",
    "        for fileref in fileref_attr_set:\n",
    "            if fileref not in data_file_refs_set:\n",
    "                final_fileref_attr_set.add(fileref)\n",
    "                file = fileref.split(\".\")[0]\n",
    "                column = fileref.split(\".\")[1]\n",
    "                column_dict = {\n",
    "                    \"column\": column,\n",
    "                    \"method\": \"file_path_match\",\n",
    "                    \"match_multiple_files\": False, \n",
    "                    \"match_regex\": None,\n",
    "                    \"match_type\": \"exact\",\n",
    "                    \"mode\": \"fileref_in_line\",\n",
    "                    \"create_new_field\": False,\n",
    "                    \"new_field_name\": None\n",
    "                }\n",
    "                if data_file_refs_dict.get(file):\n",
    "                    data_file_refs_dict[file].append(column_dict)\n",
    "                else:\n",
    "                    data_file_refs_dict[file] = [column_dict]\n",
    "    else:\n",
    "         final_fileref_attr_set = fileref_attr_set\n",
    "    return list(final_fileref_attr_set), data_file_refs_dict, data_files_src_bucket, list(remote_bucket_set)   \n",
    "\n",
    "# Function to add user to Terra group\n",
    "def add_user_to_group(group, role, email):\n",
    "    # Establish credentials\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    \n",
    "    # Add user to group and return response\n",
    "    response = requests.put(\n",
    "        url=f\"https://api.firecloud.org/api/groups/{group}/{role}/{email}\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"}\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Function to update workspace access list\n",
    "def update_workspace_acl(ws_project, ws_name, acl_updates):\n",
    "    # Establish credentials\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    \n",
    "    # Add user to group and return response\n",
    "    response = requests.patch(\n",
    "        url=f\"https://api.firecloud.org/api/workspaces/{ws_project}/{ws_name}/acl\",\n",
    "        headers={\"Authorization\": f\"Bearer {creds.token}\"},\n",
    "        json=acl_updates\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDR Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to refresh TDR API client\n",
    "def refresh_tdr_api_client():\n",
    "    creds, project = google.auth.default()\n",
    "    auth_req = google.auth.transport.requests.Request()\n",
    "    creds.refresh(auth_req)\n",
    "    config = data_repo_client.Configuration()\n",
    "    config.host = \"https://data.terra.bio\"\n",
    "    config.access_token = creds.token\n",
    "    api_client = data_repo_client.ApiClient(configuration=config)\n",
    "    api_client.client_side_validation = False\n",
    "    return api_client\n",
    "\n",
    "# Function to wait for TDR job completion\n",
    "def wait_for_tdr_job(job_model):\n",
    "    result = job_model\n",
    "    print(\"TDR Job ID: \" + job_model.id)\n",
    "    counter = 0\n",
    "    conn_err_counter = 0\n",
    "    while True:\n",
    "        # Re-establish credentials and API clients every 30 minutes\n",
    "        if counter == 0 or counter%180 == 0:\n",
    "            api_client = refresh_tdr_api_client()\n",
    "            jobs_api = data_repo_client.JobsApi(api_client=api_client)\n",
    "        # Check for TDR connectivity issues and raise exception if issue persists\n",
    "        if result == None or result.status_code in [\"500\", \"502\", \"503\"]:\n",
    "            conn_err_counter += 1\n",
    "            if conn_err_counter < 5:\n",
    "                sleep(10)\n",
    "                counter += 1\n",
    "                attempt_counter = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        result = jobs_api.retrieve_job(job_model.id)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        sleep(10)\n",
    "                        attempt_counter += 1\n",
    "                        if attempt_counter < 5:\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise \"Error retrieving job status: {}\".format(str(e)) \n",
    "            else:\n",
    "                raise \"Error interacting with TDR: {}\".format(result.status_code)\n",
    "        # Check if job is still running, and sleep/re-check if so\n",
    "        elif result == None or result.job_status == \"running\":\n",
    "            conn_err_counter = 0\n",
    "            sleep(10)\n",
    "            counter += 1\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    result = jobs_api.retrieve_job(job_model.id)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    if attempt_counter < 5:\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise \"Error retrieving job status: {}\".format(str(e))\n",
    "        elif result.job_status == \"failed\":\n",
    "            try:\n",
    "                fail_result = jobs_api.retrieve_job_result(job_model.id)\n",
    "                raise Exception(\"Job \" + job_model.id + \" failed: \" + fail_result)\n",
    "            except Exception as e:\n",
    "                raise Exception(\"Job \" + job_model.id + \" failed: \" + str(e))\n",
    "        elif result.job_status == \"succeeded\":\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return jobs_api.retrieve_job_result(job_model.id), job_model.id\n",
    "                except Exception as e:\n",
    "                    sleep(10)\n",
    "                    attempt_counter += 1\n",
    "                    if attempt_counter < 3:\n",
    "                        continue\n",
    "                    else:\n",
    "                        return \"Job succeeded, but error retrieving job result: {}\".format(str(e)), job_model.id\n",
    "        else:\n",
    "            raise \"Unrecognized job state: {}\".format(result.job_status)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"EL\" Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to copy log results to the cloud\n",
    "def copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results):\n",
    "    # Copy stream file to cloud\n",
    "    !gsutil cp $logs_stream_file_path $logs_destination_dir/ 2> stdout\n",
    "    !rm $logs_stream_file_path\n",
    "    \n",
    "    # Copy table file to cloud\n",
    "    df_results.to_csv(logs_table_file_path, index=False, sep=\"\\t\")\n",
    "    !gsutil cp $logs_table_file_path $logs_destination_dir/ 2> stdout\n",
    "    !rm $logs_table_file_path\n",
    "\n",
    "# Function to look up dbgap consent codes\n",
    "def lookup_consent_code(params, phs_id, consent_name):\n",
    "    logging.info(\"Attempting to lookup consent code using PHS: {} and Consent Name: {}.\".format(phs_id, consent_name))\n",
    "    # Attempt to read source files into data frame, checking for missing file\n",
    "    src_file_path = params[\"ws_bucket\"] + \"/ingest_pipeline/resources/consent_code_lookup/dbgap_consents.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(src_file_path, delimiter = ',')\n",
    "    except:\n",
    "        err = \"Consent code lookup file not found at: {}\".format(src_file_path)\n",
    "        logging.error(err)\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Lookup Consent Code\", \"Warning\", err])\n",
    "        return consent_name\n",
    "    # Map consent name to consent code and return string\n",
    "    df2 = df[(df.phs == phs_id) & (df.consent_short_name == consent_name)]\n",
    "    if len(df2) > 0:\n",
    "        consent_code = \"c\" + str(df2[\"consent_code\"].values[0])\n",
    "        return consent_code\n",
    "    else:\n",
    "        return consent_name\n",
    "            \n",
    "# Function to create and share a snapshot            \n",
    "def create_and_share_snapshot(params, src=\"\"):\n",
    "    logging.info(\"Creating full-view snapshot.\")\n",
    "    if src == \"el_pipeline\":\n",
    "        log_entity = params[\"staging_area_name\"]\n",
    "    elif src == \"t_pipeline\":\n",
    "        log_entity = params[\"src_dataset\"]\n",
    "    else:\n",
    "        log_entity = \"Unknown\"\n",
    "    \n",
    "    # Attempt to collect and map consent code\n",
    "    try:\n",
    "        phs_short_id = re.search('([0-9]{4,6})', params[\"phs_id\"]).group(1)\n",
    "    except:\n",
    "        phs_short_id = \"\"\n",
    "    if len(params[\"consent_name\"]) > 0 and len(phs_short_id) > 0:\n",
    "        consent_code = lookup_consent_code(params, int(phs_short_id), params[\"consent_name\"])\n",
    "    else:\n",
    "        logging.warning(\"Unable to lookup consent code. Consent name and/or PHS ID missing for lookup.\")\n",
    "        consent_code = params[\"consent_name\"]\n",
    "        params[\"pipeline_results\"].append([log_entity, datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Lookup Consent Code\", \"Warning\", \"Consent name and/or PHS ID missing for lookup\"])\n",
    "    \n",
    "    # Create reader list for snapshot, including auth domain(s) on WS\n",
    "    reader_list = []\n",
    "    snapshot_reader_list = params[\"snapshot_readers_list\"]\n",
    "    for srl_entry in snapshot_reader_list:\n",
    "        reader_list.append(srl_entry)\n",
    "    if \"auth-domain\" in snapshot_reader_list:\n",
    "        auth_domain_list = params[\"auth_domains\"] \n",
    "        for ad_entry in auth_domain_list:\n",
    "            reader_list.append(ad_entry + \"@firecloud.org\")\n",
    "    \n",
    "    # Create and submit snapshot creation request\n",
    "    logging.info(\"Submitting snapshot request.\")\n",
    "    snapshot_req = {\n",
    "        \"name\": params[\"snapshot_name\"],\n",
    "        \"description\": \"Full view snapshot of \" + params[\"dataset_name\"],\n",
    "        \"consentCode\": consent_code,\n",
    "        \"contents\": [{\n",
    "            \"datasetName\": params[\"dataset_name\"],\n",
    "            \"mode\": \"byFullView\"\n",
    "        }],\n",
    "        \"policies\": {\n",
    "            \"stewards\": [\"anvil_tdr_ingest@firecloud.org\"],\n",
    "            \"readers\": reader_list \n",
    "        },\n",
    "        \"profileId\": params[\"profile_id\"]\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            api_client = refresh_tdr_api_client()\n",
    "            snapshots_api = data_repo_client.SnapshotsApi(api_client=api_client)\n",
    "            create_snapshot_result, job_id = wait_for_tdr_job(snapshots_api.create_snapshot(snapshot=snapshot_req))\n",
    "            logging.info(\"Snapshot Creation succeeded: {}\".format(create_snapshot_result))\n",
    "            params[\"pipeline_results\"].append([log_entity, datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(create_snapshot_result)[0:1000])])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Snapshot Creation: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Snapshot Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                params[\"pipeline_results\"].append([log_entity, datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Error\", str(e)])\n",
    "                break\n",
    "\n",
    "# Function to determine whether a specified file exists in cloud storage\n",
    "def file_exists(key, params):\n",
    "    logging.info(\"Checking for file {}...\".format(key))\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(params[\"ws_bucket_name\"])\n",
    "    return storage.Blob(bucket=bucket, name=key).exists(storage_client)\n",
    "\n",
    "# Function to find and execute ingests\n",
    "def run_el_ingests(dataset_id, params, ingest_list):\n",
    "    \n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    \n",
    "    # If no ingest list specified, create a list of all target tables and files\n",
    "    if not ingest_list:\n",
    "        logging.info(\"Ingest list from table_data_processing step or ingest_list_override parameter is empty. Attempting to build ingest list from target dataset schema.\")\n",
    "        try:\n",
    "            dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\"]).to_dict()\n",
    "            table_list = [tab[\"name\"] for tab in dataset_info[\"schema\"][\"tables\"]]\n",
    "            ingest_list = {}\n",
    "            for table in table_list:\n",
    "                ingest_list[table] = []\n",
    "        except:\n",
    "            logging.error(\"Error building ingest from target dataset schema. No ingests will be run.\")\n",
    "            params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", \"Table: All - File: All\", \"Error\", \"Ingest list from table_data_processing step or ingest_list_override parameter was empty, and attempts to build a list from the target dataset failed. Skipping ingests.\"])\n",
    "            ingest_list = {}\n",
    "            \n",
    "    # Update ingest list with files to process (if not specified)\n",
    "    for key, val in ingest_list.items():\n",
    "        if val == []:\n",
    "            directory = params[\"el_output_dir\"] + \"/\" + key + \"/\"\n",
    "            obj_list = bfi.get_objects_list(params[\"ws_bucket_name\"], params[\"google_project\"], dirs_to_include=[directory])\n",
    "            file_list = []\n",
    "            for item in obj_list:\n",
    "                path_split = item.split(\"/\")\n",
    "                file_name_idx = len(path_split)-1\n",
    "                file_name = path_split[file_name_idx]\n",
    "                file_list.append(file_name)\n",
    "            ingest_list[key] = file_list\n",
    "    \n",
    "    # Loop through target tables and run ingest\n",
    "    for key, val in ingest_list.items():\n",
    "        target_table = key\n",
    "        logging.info(\"Running ingests for target table: {}\".format(target_table))\n",
    "        if val:\n",
    "            for file_entry in val:\n",
    "                source_file_name = file_entry\n",
    "                source_rel_file_path = \"{}/{}/{}\".format(params[\"el_output_dir\"], target_table, source_file_name)\n",
    "                source_full_file_path = \"{}/{}/{}/{}\".format(params[\"ws_bucket\"], params[\"el_output_dir\"], target_table, source_file_name)\n",
    "                if file_exists(source_rel_file_path, params):\n",
    "                    logging.info(\"Running ingest from {} to table {}.\".format(source_file_name, target_table))\n",
    "                    ingest_request = {\n",
    "                        \"table\": target_table,\n",
    "                        \"profile_id\": params[\"profile_id\"],\n",
    "                        \"ignore_unknown_values\": True,\n",
    "                        \"resolve_existing_files\": True,\n",
    "                        \"updateStrategy\": \"replace\",\n",
    "                        \"format\": \"json\",\n",
    "                        \"bulkMode\": True,\n",
    "                        \"load_tag\": \"Ingest for {}\".format(params[\"staging_area_name\"]),\n",
    "                        \"path\": source_full_file_path\n",
    "                    }\n",
    "                    attempt_counter = 0\n",
    "                    while True:\n",
    "                        try:\n",
    "                            api_client = refresh_tdr_api_client()\n",
    "                            datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "                            ingest_request_result, job_id = wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "                            logging.info(\"Ingest from file {} succeeded: {}\".format(source_file_name, str(ingest_request_result)[0:1000]))\n",
    "                            params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"Table: {target_table} - File: {source_file_name}\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(ingest_request_result)[0:1000])])\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            logging.error(\"Error on Dataset Ingest: {}\".format(str(e)))\n",
    "                            attempt_counter += 1\n",
    "                            if attempt_counter < 2:\n",
    "                                logging.info(\"Retrying Dataset Ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                                sleep(10)\n",
    "                                continue\n",
    "                            else:\n",
    "                                logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                                params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"Table: {target_table} - File: {source_file_name}\", \"Error\", str(e)])\n",
    "                                break\n",
    "                else:\n",
    "                    logging.warning(\"Source table data file does not exist.  Skipping: {}.\".format(source_file_name))\n",
    "                    params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"Table: {target_table} - File: {source_file_name}\", \"Warning\", f\"Source table data file does not exist.  Skipping: {source_file_name}\"])\n",
    "                    continue\n",
    "        else:\n",
    "            logging.warning(\"No source table data files found. Skipping to next target table.\")\n",
    "\n",
    "# Function to add TDR dataset SA to the appropriate Terra groups and workspace ACLs\n",
    "def set_up_dataset_ingest_sa(dataset_id, params, new_dataset):\n",
    "    logging.info(\"Setting up dataset ingest service account (SA).\")\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    \n",
    "    # Collect dataset-specific SA from TDR\n",
    "    dataset_details = datasets_api.retrieve_dataset(id=dataset_id)\n",
    "    service_account = dataset_details.ingest_service_account\n",
    "    if not service_account:\n",
    "        error_message = \"No dataset ingest service account found. Ensure the dedicatedIngestServiceAccount parameter has been set to True on dataset creation.\"\n",
    "        logging.error(error_message)\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Anvil Ingest Group\", \"Error\", error_message])\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Workspace\", \"Error\", error_message])\n",
    "    else:\n",
    "        # Add SA user to the Anvil Ingest Terra Group\n",
    "        anvil_ingest_sa_group = \"anvil_tdr_ingest\" # full email: anvil_tdr_ingest@firecloud.org\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                res = add_user_to_group(anvil_ingest_sa_group, \"member\", service_account)\n",
    "                if res.status_code == 204:\n",
    "                    logging.info(\"Dataset service account {} added to {} sucessfully.\".format(service_account, anvil_ingest_sa_group))\n",
    "                    params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Anvil Ingest Group\", \"Success\", service_account])\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        logging.info(\"Retrying addition of dataset service account to {} (attempt #{})...\".format(anvil_ingest_sa_group, str(attempt_counter)))\n",
    "                        attempt_counter += 1\n",
    "                        sleep(10)\n",
    "                        continue\n",
    "                    else:\n",
    "                        add_text = \" (Note that it may have already been added when the dataset SA was first created)\" if not new_dataset else \"\"\n",
    "                        error_message = \"Error adding dataset ingest SA to {} group{}: {}\".format(anvil_ingest_sa_group, add_text, res.text)\n",
    "                        logging.error(error_message)\n",
    "                        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Anvil Ingest Group\", \"Error\", error_message])   \n",
    "                        break\n",
    "            except Exception as e:\n",
    "                if attempt_counter < 2:\n",
    "                        logging.info(\"Retrying addition of dataset service account to {} (attempt #{})...\".format(anvil_ingest_sa_group, str(attempt_counter)))\n",
    "                        attempt_counter += 1\n",
    "                        sleep(10)\n",
    "                        continue\n",
    "                else:\n",
    "                    add_text = \" (Note that it may have already been added when the dataset SA was first created)\" if not new_dataset else \"\"\n",
    "                    error_message = \"Error adding dataset ingest SA to {} group{}: {}\".format(anvil_ingest_sa_group, add_text, str(e))\n",
    "                    logging.error(error_message)\n",
    "                    params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Anvil Ingest Group\", \"Error\", error_message])   \n",
    "                    break \n",
    "        # Add SA user to workspace as Reader\n",
    "        acl_updates=[{\"email\": service_account,\n",
    "                     \"accessLevel\":\"READER\",\n",
    "                     \"canShare\":False}]\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                res = update_workspace_acl(params[\"ws_project\"], params[\"ws_name\"], acl_updates=acl_updates)\n",
    "                if res.status_code == 200:\n",
    "                    logging.info(\"Dataset service account {} added to workspace as READER (without SHARE) permissions successfully.\".format(service_account))\n",
    "                    params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Workspace\", \"Success\", service_account])\n",
    "                    break\n",
    "                else:\n",
    "                    if attempt_counter < 2:\n",
    "                        logging.info(\"Retrying addition of dataset service account to workspace (attempt #{})...\".format(str(attempt_counter)))\n",
    "                        attempt_counter += 1\n",
    "                        sleep(10)\n",
    "                        continue\n",
    "                    else:\n",
    "                        add_text = \" (Note that it may have already been added when the dataset SA was first created)\" if not new_dataset else \"\"\n",
    "                        error_message = \"Error adding dataset ingest SA to workspace{}: {}\".format(add_text, res.text)\n",
    "                        logging.error(error_message)\n",
    "                        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Workspace\", \"Error\", error_message])\n",
    "                        break       \n",
    "            except Exception as e:\n",
    "                if attempt_counter < 2:\n",
    "                    logging.info(\"Retrying addition of dataset service account to workspace (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    attempt_counter += 1\n",
    "                    sleep(10)\n",
    "                    continue\n",
    "                else:\n",
    "                    add_text = \" (Note that it may have already been added when the dataset SA was first created)\" if not new_dataset else \"\"\n",
    "                    error_message = \"Error adding dataset ingest SA to workspace{}: {}\".format(add_text, str(e))\n",
    "                    logging.error(error_message)\n",
    "                    params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Service Account Setup\", \"Add SA to Workspace\", \"Error\", error_message])\n",
    "                    break\n",
    "\n",
    "# Function to create TDR dataset (with retry logic)\n",
    "def create_tdr_dataset(dataset_id, params) -> str:\n",
    "    logging.info(\"Creating new dataset: {}\".format(params[\"dataset_name\"]))\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    \n",
    "    # Pull down TDR schema from cloud storage\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(params[\"ws_bucket_name\"])\n",
    "        schema_blob = bucket.blob(params[\"el_schema_file\"])\n",
    "        schema = json.loads(schema_blob.download_as_string(client=None))\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving TDR schema object: {}\".format(str(e)))\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Create New Dataset\", \"Error\", \"Error retrieving TDR schema object: {}\".format(str(e))])\n",
    "        return\n",
    "    \n",
    "    # Build and execute dataset creation request\n",
    "    properties_dict = {\n",
    "        \"consent_name\": params[\"consent_name\"],\n",
    "        \"auth_domains\": params[\"auth_domains\"],\n",
    "        \"source_workspaces\": params[\"source_workspaces\"],\n",
    "    }\n",
    "    dataset_request = {\n",
    "        \"name\": params[\"dataset_name\"],\n",
    "        \"description\": \"TDR Dataset for {}\".format(params[\"staging_area_name\"]),\n",
    "        \"defaultProfileId\": params[\"profile_id\"],\n",
    "        \"region\": \"us-central1\",\n",
    "        \"cloudPlatform\": \"gcp\",\n",
    "        \"phsId\": params[\"phs_id\"],\n",
    "        \"experimentalSelfHosted\": True,\n",
    "        \"dedicatedIngestServiceAccount\": True,\n",
    "        \"experimentalPredictableFileIds\": True,\n",
    "        \"properties\": properties_dict,\n",
    "        \"schema\": schema\n",
    "    }\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            create_dataset_result, job_id = wait_for_tdr_job(datasets_api.create_dataset(dataset=dataset_request))\n",
    "            logging.info(\"Dataset Creation succeeded: {}\".format(create_dataset_result))\n",
    "            dataset_id = create_dataset_result[\"id\"]\n",
    "            params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Create New Dataset\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(create_dataset_result)[0:1000])])\n",
    "            try:\n",
    "                resp = datasets_api.add_dataset_policy_member(id=dataset_id, policy_name=\"steward\", policy_member={\"email\": \"anvil_tdr_ingest@firecloud.org\"})\n",
    "            except:\n",
    "                logging.warning(\"Error on adding additional policy members to dataset: {}\".format(resp))\n",
    "            return dataset_id\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Dataset Creation: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Dataset Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Create New Dataset\", \"Error\", str(e)])\n",
    "                return dataset_id\n",
    "\n",
    "# Function to patch TDR dataset (with retry logic)\n",
    "def patch_tdr_dataset(dataset_id, params) -> str:\n",
    "    logging.info(\"Patching properties for existing dataset: {}\".format(params[\"dataset_name\"]))\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    \n",
    "    # Retrieve existing data set properties\n",
    "    try:\n",
    "        dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"PROPERTIES\"]).to_dict()\n",
    "        auth_domains = dataset_info[\"properties\"][\"auth_domains\"]\n",
    "        src_workspaces = dataset_info[\"properties\"][\"source_workspaces\"]\n",
    "        for ad in params[\"auth_domains\"]:\n",
    "            if ad not in auth_domains:\n",
    "                auth_domains.append(ad)\n",
    "        for sw in params[\"source_workspaces\"]:\n",
    "            if sw not in src_workspaces:\n",
    "                src_workspaces.append(sw)\n",
    "        properties_dict = {\n",
    "            \"consent_name\": params[\"consent_name\"],\n",
    "            \"auth_domains\": auth_domains,\n",
    "            \"source_workspaces\": src_workspaces\n",
    "        }\n",
    "    except:\n",
    "        properties_dict = {\n",
    "            \"consent_name\": params[\"consent_name\"],\n",
    "            \"auth_domains\": params[\"auth_domains\"],\n",
    "            \"source_workspaces\": params[\"source_workspaces\"]\n",
    "        }   \n",
    "        \n",
    "    # Execute dataset patch request (for phs_id and properties)\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            phs_id = params[\"phs_id\"]\n",
    "            resp = datasets_api.patch_dataset(id=dataset_id, dataset_patch_request_model={\"phsId\": phs_id, \"properties\": properties_dict})\n",
    "            params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Patch Existing Dataset\", \"Success\", resp])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on Dataset Patch: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying Dataset Patch (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Patch Existing Dataset\", \"Error\", str(e)])\n",
    "                return\n",
    "            \n",
    "    # Retrieve current and target TDR schemas\n",
    "    logging.info(\"Examining TDR schema to determine if new tables, columns, and/or relationships need to be added.\")\n",
    "    try:\n",
    "        dataset_info = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\"]).to_dict()\n",
    "        src_schema = {}\n",
    "        src_schema[\"tables\"] = dataset_info[\"schema\"][\"tables\"]\n",
    "        src_schema[\"relationships\"] = dataset_info[\"schema\"][\"relationships\"]\n",
    "    except: \n",
    "        logging.error(\"Error retrieving source TDR schema: {}\".format(str(e)))\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Reconcile Existing Dataset Schema\", \"Error\", \"Error retrieving source TDR schema: {}\".format(str(e))])\n",
    "        return \n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(params[\"ws_bucket_name\"])\n",
    "        schema_blob = bucket.blob(params[\"el_schema_file\"])\n",
    "        tar_schema = json.loads(schema_blob.download_as_string(client=None))\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving target TDR schema object: {}\".format(str(e)))\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Reconcile Existing Dataset Schema\", \"Error\", \"Error retrieving target TDR schema object: {}\".format(str(e))])\n",
    "        return \n",
    "    \n",
    "    # Compare current and target TDR schemas to determine if schema extension is needed (Tables)\n",
    "    additional_tables = []\n",
    "    for tar_table_entry in tar_schema[\"tables\"]:\n",
    "        add_table = True\n",
    "        for src_table_entry in src_schema[\"tables\"]:\n",
    "            if tar_table_entry[\"name\"] == src_table_entry[\"name\"]:\n",
    "                add_table = False\n",
    "        if add_table == True:\n",
    "            additional_tables.append(tar_table_entry)\n",
    "    add_table_list = [table[\"name\"] for table in additional_tables]\n",
    "    \n",
    "    # Compare current and target TDR schemas to determine if schema extension is needed (Columns)\n",
    "    additional_columns = []\n",
    "    for tar_table_entry in tar_schema[\"tables\"]:\n",
    "        if tar_table_entry[\"name\"] not in add_table_list:\n",
    "            add_col_list = []\n",
    "            for tar_column_entry in tar_table_entry[\"columns\"]:\n",
    "                add_column = True\n",
    "                for src_table_entry in src_schema[\"tables\"]: \n",
    "                    for src_column_entry in src_table_entry[\"columns\"]:\n",
    "                        if tar_table_entry[\"name\"] == src_table_entry[\"name\"] and tar_column_entry[\"name\"] == src_column_entry[\"name\"]:\n",
    "                            add_column = False\n",
    "                if add_column == True:\n",
    "                    add_col_list.append(tar_column_entry)\n",
    "            if add_col_list:\n",
    "                additional_columns.append({\"tableName\": tar_table_entry[\"name\"], \"columns\": add_col_list})\n",
    "    add_column_list = []\n",
    "    for table_entry in additional_columns:\n",
    "        for column_entry in table_entry[\"columns\"]:\n",
    "            add_column_list.append(table_entry[\"tableName\"] + \".\" + column_entry[\"name\"]) \n",
    "            \n",
    "    # Compare current and target TDR schemas to determine if schema extension is needed (Relationships)\n",
    "    additional_relationships = []\n",
    "    for tar_rel_entry in tar_schema[\"relationships\"]:\n",
    "        add_rel = True\n",
    "        for src_rel_entry in src_schema[\"relationships\"]:\n",
    "            if tar_rel_entry[\"name\"] == src_rel_entry[\"name\"]:\n",
    "                add_rel = False\n",
    "        if add_rel == True:\n",
    "            additional_relationships.append(tar_rel_entry)\n",
    "    add_rel_list = [rel[\"name\"] for rel in additional_relationships]\n",
    "    \n",
    "    # Update TDR dataset schema to include additional tables and columns\n",
    "    if not (add_table_list or add_column_list or add_rel_list):\n",
    "        logging.info(\"No new tables, columns, or relationships to add to the TDR schema.\")\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Reconcile Existing Dataset Schema\", \"Success\", \"No new tables, columns, or relationships to add to the TDR schema.\"])\n",
    "        return\n",
    "    if add_table_list or add_column_list:\n",
    "        logging.info(\"New tables and/or columns found. Building and submitting a TDR schema extension request.\\n\\tTables to add: {tabs}\\n\\tColumns to add: {cols}\".format(tabs=add_table_list, cols=add_column_list))\n",
    "        schema_update_request = {\n",
    "            \"description\": \"Adding tables and columns for additional source workspace\",\n",
    "            \"changes\": {\n",
    "                \"addTables\": additional_tables,\n",
    "                \"addColumns\": additional_columns\n",
    "            }\n",
    "        }\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                schema_update_result, job_id = wait_for_tdr_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "                logging.info(\"TDR schema tables extension succeeded: {}\".format(str(schema_update_result)[0:1000]))\n",
    "                params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Reconcile Existing Dataset Schema (Tables)\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(schema_update_result)[0:1000])])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error on TDR schema extension: {}\".format(str(e)))\n",
    "                attempt_counter += 1\n",
    "                if attempt_counter < 2:\n",
    "                    logging.info(\"Retrying TDR schema extension (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(15)\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                    params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Reconcile Existing Dataset Schema (Tables)\", \"Error\", str(e)])\n",
    "                    break\n",
    "    if add_rel_list:\n",
    "        logging.info(\"New relationships found. Building and submitting a TDR schema extension request.\\n\\tRelationships to add: {rels}\".format(rels=add_rel_list))\n",
    "        schema_update_request = {\n",
    "            \"description\": \"Adding relationships for additional source workspace\",\n",
    "            \"changes\": {\n",
    "                \"addRelationships\": additional_relationships\n",
    "            }\n",
    "        }\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                schema_update_result, job_id = wait_for_tdr_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "                logging.info(\"TDR schema relationships extension succeeded: {}\".format(str(schema_update_result)[0:1000]))\n",
    "                params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Reconcile Existing Dataset Schema (Relationships)\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(schema_update_result)[0:1000])])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error on TDR schema extension: {}\".format(str(e)))\n",
    "                attempt_counter += 1\n",
    "                if attempt_counter < 2:\n",
    "                    logging.info(\"Retrying TDR schema extension (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(15)\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                    params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Reconcile Existing Dataset Schema (Relationships)\", \"Error\", str(e)])\n",
    "                    break\n",
    "        \n",
    "# Function to create or retrieve specified TDR dataset\n",
    "def create_or_retrieve_dataset(params):\n",
    "    logging.info(\"Attempting to create or retrieve the specified TDR dataset.\")\n",
    "    # Setup/refresh TDR clients\n",
    "    api_client = refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    \n",
    "    # Attempt to identify dataset (if it already exists)\n",
    "    dataset_id = \"\"\n",
    "    new_dataset = False\n",
    "    try:\n",
    "        dataset_list = datasets_api.enumerate_datasets(filter=params[\"dataset_name\"])\n",
    "        if dataset_list.items:\n",
    "            for dataset in dataset_list.items:\n",
    "                if dataset.name == params[\"dataset_name\"]:\n",
    "                    dataset_id = str(dataset.id)\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Enumerate Datasets\", \"Success\", \"{} datasets found. Matching dataset_id = {}\".format(len(dataset_list.items), dataset_id)])\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error on Dataset Enumeration: {}\".format(str(e)))\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Creation or Retrieval\", \"Enumerate Datasets\", \"Error\", str(e)])\n",
    "        return\n",
    "    if dataset_id == \"\":\n",
    "        dataset_id = create_tdr_dataset(dataset_id, params)\n",
    "        new_dataset = True\n",
    "        return dataset_id, new_dataset\n",
    "    else:\n",
    "        logging.info(\"Dataset already exists! ID = {}\".format(dataset_id))\n",
    "        patch_tdr_dataset(dataset_id, params)\n",
    "        return dataset_id, new_dataset\n",
    "\n",
    "# Function to run file inventory build step, with retry logic\n",
    "def run_build_file_inventory(params):\n",
    "    inventory = []\n",
    "    attempt_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            inventory, retry_count = bfi.build_inventory(params)\n",
    "            logging.info(\"File Inventory build succeeded. {} distinct files found after deduplication. {} retries required.\".format(len(inventory), retry_count))\n",
    "            params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"File Inventory Creation\", \"Build File Inventory\", \"Success\", \"{} files found\".format(len(inventory))])\n",
    "            return inventory\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error on File Inventory Creation: {}\".format(str(e)))\n",
    "            attempt_counter += 1\n",
    "            if attempt_counter < 2:\n",
    "                logging.info(\"Retrying File Inventory Creation (attempt #{})...\".format(str(attempt_counter)))\n",
    "                sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"File Inventory Creation\", \"Build File Inventory\", \"Error\", str(e)])\n",
    "                return inventory\n",
    "    return inventory  \n",
    "\n",
    "# Function to orchestrate the workspace connector pipeline\n",
    "def run_ws_connector_pipeline(workspace, params):\n",
    "    \n",
    "    # Step 1: Set Variables for Pipeline\n",
    "    workspace_name = workspace[0] \n",
    "    logging.info(\"Starting Workspace Connector Pipeline for {}.\".format(workspace_name))\n",
    "    ws_attributes = get_workspace_attributes(workspace[1], workspace[0])\n",
    "    params[\"src_ws_name\"] = workspace_name\n",
    "    params[\"src_ws_project\"] = workspace[1]\n",
    "    params[\"phs_id\"] = format_phs_id(ws_attributes[\"attributes\"][\"phs_id\"]) if ws_attributes[\"attributes\"].get(\"phs_id\") else \"\"\n",
    "    auth_list = ws_attributes[\"authorizationDomain\"] if ws_attributes.get(\"authorizationDomain\") else []\n",
    "    params[\"auth_domains\"] = [x[\"membersGroupName\"] for x in auth_list]\n",
    "    params[\"consent_name\"] = ws_attributes[\"attributes\"][\"library:dataUseRestriction\"] if ws_attributes[\"attributes\"].get(\"library:dataUseRestriction\") else \"\"\n",
    "    params[\"staging_area_name\"] = workspace[2]\n",
    "    params[\"workspace_bucket\"] = ws_attributes[\"bucketName\"] if ws_attributes.get(\"bucketName\") else \"\" \n",
    "    params[\"data_files_src_bucket\"] = {}\n",
    "    if params[\"workspace_bucket\"]:\n",
    "        params[\"data_files_src_bucket\"][params[\"workspace_bucket\"]] = {\n",
    "            \"include_dirs\": [],\n",
    "            \"exclude_dirs\": []\n",
    "        }\n",
    "    params[\"provenance_file\"] = \"ingest_pipeline/input/{}/provenance.json\".format(params[\"staging_area_name\"])\n",
    "    params[\"input_dir\"] = \"ingest_pipeline/input/{}/table_data\".format(params[\"staging_area_name\"])\n",
    "    params[\"file_inventory_dir\"] = \"ingest_pipeline/input/{}/data_files/file_inventory\".format(params[\"staging_area_name\"])\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "    params[\"pipeline_results\"] = []\n",
    "    logs_destination_dir = params[\"ws_bucket\"] + \"/ingest_pipeline/input/{}/logs\".format(params[\"staging_area_name\"])\n",
    "    logs_stream_file_path = \"pipeline_results_log_stream_\" + current_datetime_string + \".txt\"\n",
    "    logs_table_file_path = \"pipeline_results_log_\" + current_datetime_string + \".tsv\"\n",
    "    while logging.root.handlers:\n",
    "        logging.root.removeHandler(logging.root.handlers[-1])\n",
    "    logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.FileHandler(logs_stream_file_path), logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "    # Step 2: Create or Update Provenance File\n",
    "    logging.info(\"Creating or updating provenance.json file for Staging Area: {}\".format(params[\"staging_area_name\"]))\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(params[\"ws_bucket_name\"])\n",
    "        provenance_blob = bucket.blob(params[\"provenance_file\"])\n",
    "        prov = json.loads(provenance_blob.download_as_string(client=None))\n",
    "        prov[\"phs_id\"] = params[\"phs_id\"]\n",
    "        prov[\"consent_name\"] = params[\"consent_name\"]\n",
    "        if workspace_name not in prov[\"source_workspaces\"]:\n",
    "            prov[\"source_workspaces\"].append(workspace_name)\n",
    "        if params[\"data_files_src_bucket\"]:\n",
    "            for key in params[\"data_files_src_bucket\"].keys():\n",
    "                if key not in prov[\"data_files_src_buckets\"].keys():\n",
    "                       prov[\"data_files_src_buckets\"][key] = params[\"data_files_src_bucket\"][key]\n",
    "        for entry in params[\"auth_domains\"]:\n",
    "            if entry not in prov[\"auth_domains\"]:\n",
    "                prov[\"auth_domains\"].append(entry)\n",
    "        logging.info(\"Existing provenance.json file found. Updating with new information.\")\n",
    "        fileref_list, prov[\"data_file_refs\"], prov[\"data_files_src_buckets\"], remote_bucket_list = find_and_add_fileref_fields(params[\"src_ws_project\"], params[\"src_ws_name\"], params[\"workspace_bucket\"], params[\"data_file_refs\"], prov[\"data_files_src_buckets\"])\n",
    "        if fileref_list:\n",
    "            logging.info(\"Additional file reference fields found and marked for processing: \" + \", \".join(fileref_list))\n",
    "        if remote_bucket_list:\n",
    "            warning_message = \"References to files in remote buckets found. An attempt was made to add these to the data_files_src_buckets parameter for file inventory building. Ingest may not work as intended if the user and/or TDR service account does not have access to these remote buckets. Remote buckets include: \" + \", \".join(remote_bucket_list)\n",
    "            logging.warning(warning_message)\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Remote File References Check\", \"Warning\", warning_message])\n",
    "    except Exception as e:\n",
    "        logging.info(\"Unable to retrieve provenance.json file. Creating new provenance.json file.\")\n",
    "        prov = {}\n",
    "        prov[\"phs_id\"] = params[\"phs_id\"]\n",
    "        prov[\"consent_name\"] = params[\"consent_name\"]\n",
    "        prov[\"source_workspaces\"] = [workspace_name]\n",
    "        prov[\"auth_domains\"] = params[\"auth_domains\"]\n",
    "        prov[\"data_files_src_buckets\"] = params[\"data_files_src_bucket\"]\n",
    "        fileref_list, prov[\"data_file_refs\"], prov[\"data_files_src_buckets\"], remote_bucket_list = find_and_add_fileref_fields(params[\"src_ws_project\"], params[\"src_ws_name\"], params[\"workspace_bucket\"], params[\"data_file_refs\"], prov[\"data_files_src_buckets\"])\n",
    "        if fileref_list:\n",
    "            logging.info(\"Additional file reference fields found and marked for processing: \" + \", \".join(fileref_list))\n",
    "        if remote_bucket_list:\n",
    "            warning_message = \"References to files in remote buckets found. An attempt was made to add these to the data_files_src_buckets parameter for file inventory building. Ingest may not work as intended if the user and/or TDR service account does not have access to these remote buckets. Remote buckets include: \" + \", \".join(remote_bucket_list)\n",
    "            logging.warning(warning_message)\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Remote File References Check\", \"Warning\", warning_message])\n",
    "    destination_file = \"provenance.json\"\n",
    "    cloud_destination = params[\"ws_bucket\"] + \"/\" + params[\"provenance_file\"]\n",
    "    with open(destination_file, \"w\") as outfile:\n",
    "        json.dump(prov, outfile)\n",
    "    !gsutil cp $destination_file $cloud_destination 2> stdout\n",
    "    params[\"pipeline_results\"].append([params[\"src_ws_name\"], params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Create or Update Staging Area Provenance\", \"Success\", json.dumps(prov)[0:1000]])\n",
    "    \n",
    "    # Step 3: Create Source Files\n",
    "    if params[\"skip_source_files_creation\"] == True:\n",
    "        logging.info(\"Skipping source files creation.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_ws_name\"], params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Create Source Files\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running source files creation.\")\n",
    "        try:\n",
    "            log_status, log_string = sfc.create_source_table_data_files(params)\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Create Source Files\", log_status, log_string])\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error creating source files: {}\".format(str(e)))\n",
    "            params[\"pipeline_results\"].append([params[\"src_ws_name\"], params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Create Source Files\", \"Error\", str(e)]) \n",
    "    \n",
    "    # Aggregate, save, and display pipeline results\n",
    "    df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Workspace\", \"Staging Area\", \"Time\", \"Step\", \"Status\", \"Message\"])\n",
    "    logging.info(\"The Workspace Connector Pipeline has completed for {}.\".format(workspace_name))\n",
    "    copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results)\n",
    "    logging.info(\"Pipeline Results:\")\n",
    "    display(df_results)\n",
    "    return \n",
    "    \n",
    "# Function to orchestrate the various components of the ingest pipeline\n",
    "def run_el_pipeline(pipeline, params): \n",
    "    \n",
    "    # Step 1: Set Variables for Pipeline\n",
    "    staging_area_name = pipeline[0] \n",
    "    logging.info(\"Starting Extract and Load (EL) Pipeline for {}.\".format(staging_area_name))\n",
    "    params[\"staging_area_name\"] = staging_area_name\n",
    "    params[\"dataset_name\"] = pipeline[1]\n",
    "    params[\"input_dir\"] = \"ingest_pipeline/input/{}/table_data\".format(params[\"staging_area_name\"])\n",
    "    params[\"file_inventory_dir\"] = \"ingest_pipeline/input/{}/data_files/file_inventory\".format(params[\"staging_area_name\"])\n",
    "    params[\"provenance_file\"] = \"ingest_pipeline/input/{}/provenance.json\".format(params[\"staging_area_name\"])\n",
    "    params[\"el_output_dir\"] = \"ingest_pipeline/output/source/{}/table_data\".format(params[\"staging_area_name\"])\n",
    "    params[\"el_schema_file\"] = \"ingest_pipeline/output/source/{}/schema/tdr_schema_object.json\".format(params[\"staging_area_name\"])\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "    params[\"snapshot_name\"] = params[\"dataset_name\"] + \"_SRC_\" + current_datetime_string\n",
    "    params[\"pipeline_results\"] = [] \n",
    "    logs_destination_dir = params[\"ws_bucket\"] + \"/ingest_pipeline/output/source/{}/logs\".format(params[\"staging_area_name\"])\n",
    "    logs_stream_file_path = \"pipeline_results_log_stream_\" + current_datetime_string + \".txt\"\n",
    "    logs_table_file_path = \"pipeline_results_log_\" + current_datetime_string + \".tsv\"\n",
    "    while logging.root.handlers:\n",
    "        logging.root.removeHandler(logging.root.handlers[-1])\n",
    "    logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.FileHandler(logs_stream_file_path), logging.StreamHandler(sys.stdout)])\n",
    "    \n",
    "    # Step 2: Retrieve Staging Area Provenance Information\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(params[\"ws_bucket_name\"])\n",
    "        provenance_blob = bucket.blob(params[\"provenance_file\"])\n",
    "        prov = json.loads(provenance_blob.download_as_string(client=None))\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Initialization\", \"Provenance File Retrieval\", \"Success\", json.dumps(prov)[0:1000]])\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error retrieving provenance.json file from staging area. Exiting pipeline.\")\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Initialization\", \"Provenance File Retrieval\", \"Error\", str(e)])\n",
    "        df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Staging Area\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "        copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results)\n",
    "        logging.info(\"Pipeline Results:\")\n",
    "        display(df_results)\n",
    "        return\n",
    "    params[\"phs_id\"] = prov[\"phs_id\"] \n",
    "    params[\"consent_name\"] = prov[\"consent_name\"]\n",
    "    params[\"source_workspaces\"] = prov[\"source_workspaces\"]\n",
    "    params[\"data_files_src_buckets\"] = prov[\"data_files_src_buckets\"]\n",
    "    if params[\"additional_file_inventory_sources\"].get(params[\"staging_area_name\"]):\n",
    "        for key in params[\"additional_file_inventory_sources\"][params[\"staging_area_name\"]].keys():\n",
    "            if key not in params[\"data_files_src_buckets\"].keys():\n",
    "                   params[\"data_files_src_buckets\"][key] = params[\"additional_file_inventory_sources\"][params[\"staging_area_name\"]][key]\n",
    "    params[\"auth_domains\"] = prov[\"auth_domains\"]\n",
    "    params[\"data_file_refs\"] = prov[\"data_file_refs\"]\n",
    "        \n",
    "    # Step 3: Build File Inventory\n",
    "    if params[\"skip_file_inventory_creation\"] == True:\n",
    "        logging.info(\"Skipping file inventory creation.\")\n",
    "        file_inventory = {}\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"File Inventory Creation\", \"Build File Inventory\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Building file inventory.\")\n",
    "        file_inventory = run_build_file_inventory(params)\n",
    "    params[\"file_inventory\"] = file_inventory\n",
    "    \n",
    "    # Step 4: Process Table Data\n",
    "    if params[\"skip_table_data_processing\"] == True:\n",
    "        logging.info(\"Skipping table data processing.\")\n",
    "        target_tables = {}\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Table Data Processing\", \"Ingest Pre-Processing\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Processing table data for ingest.\")\n",
    "        target_tables, log_status, log_string = ptd.process_table_data(params)\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Table Data Processing\", \"Ingest Pre-Processing\", log_status, log_string])\n",
    "    \n",
    "    # Step 5: Create or Retrieve Dataset\n",
    "    int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Staging Area\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping TDR dataset creation or retrieval due to upstream errors. Exiting pipeline.\")\n",
    "        df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Staging Area\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "        copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results)\n",
    "        logging.info(\"Pipeline Results:\")\n",
    "        display(df_results)\n",
    "        return \n",
    "    else:\n",
    "        dataset_id, new_dataset = create_or_retrieve_dataset(params)\n",
    "        if not dataset_id:\n",
    "            logging.error(\"No TDR dataset created or retrieved. Exiting pipeline.\")\n",
    "            df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Staging Area\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "            copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results)\n",
    "            logging.info(\"Pipeline Results:\")\n",
    "            display(df_results)\n",
    "            return \n",
    "        else:\n",
    "            params[\"dataset_id\"] = dataset_id\n",
    "        if new_dataset == True:\n",
    "            logging.info(\"Sleeping for a few minutes to let policy/permission changes propogate for new dataset...\")\n",
    "            sleep(60)\n",
    "    \n",
    "    # Step 6: Set up Dataset-specific Service Account\n",
    "    set_up_dataset_ingest_sa(dataset_id, params, new_dataset)\n",
    "    logging.info(\"Sleeping for a few minutes to let policy/permission changes propogate...\")\n",
    "    if new_dataset == True:\n",
    "        sleep(300)\n",
    "    else:\n",
    "        sleep(60)\n",
    "    \n",
    "    # Step 7: Ingest Data to Dataset\n",
    "    int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Staging Area\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping dataset ingests due to upstream errors.\")\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", \"Table: All - File: All\", \"Skipped\", \"Errors found upstream in ingest pipeline\"])\n",
    "    elif params[\"skip_ingests\"] == True:\n",
    "        logging.info(\"Skipping dataset ingests.\")\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", \"Table: All - File: All\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running dataset ingests.\")\n",
    "        if len(params[\"ingest_list_override\"]) > 0:\n",
    "            ingest_list = params[\"ingest_list_override\"]\n",
    "        else:\n",
    "            ingest_list = {}\n",
    "            for table in target_tables:\n",
    "                ingest_list[table] = []\n",
    "        run_el_ingests(dataset_id, params, ingest_list)   \n",
    "    \n",
    "    # Step 8: Create and Share Snapshot\n",
    "    int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Staging Area\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping snapshot creation due to upstream errors.\")\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Skipped\", \"Errors found upstream in ingest pipeline\"])\n",
    "    elif params[\"skip_snapshot_creation\"] == True:\n",
    "        logging.info(\"Skipping snapshot creation on user request.\")\n",
    "        params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running snapshot creation\")\n",
    "        try:\n",
    "            create_and_share_snapshot(params, \"el_pipeline\")\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error running snapshot creation: {}\".format(str(e)))\n",
    "            params[\"pipeline_results\"].append([params[\"staging_area_name\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Error\", str(e)])\n",
    "    \n",
    "    # Aggregate, save, and display pipeline results\n",
    "    df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Staging Area\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    logging.info(\"The ingest pipeline has completed for {}.\".format(params[\"staging_area_name\"]))\n",
    "    copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results)\n",
    "    logging.info(\"Pipeline Results:\")\n",
    "    display(df_results)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"T\" Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_project_name(dataset_id, phs_id, dataset_name):\n",
    "    project_name = \"\"\n",
    "    # If PHS ID is provided, attempt to scrape from dbGaP\n",
    "    if phs_id:\n",
    "        try:\n",
    "            phs_short = phs_id.replace(\"phs\", \"\")\n",
    "            dbgap_url = \"https://dbgap.ncbi.nlm.nih.gov/ss/dbgapssws.cgi?request=Study&phs=\" + phs_short\n",
    "            http = urllib3.PoolManager()\n",
    "            response = http.request('GET', dbgap_url)\n",
    "            data = xmltodict.parse(response.data)\n",
    "            if isinstance(data[\"dbgapss\"][\"Study\"], list):\n",
    "                project_name = data[\"dbgapss\"][\"Study\"][0][\"StudyInfo\"][\"StudyNameEntrez\"]\n",
    "            else:\n",
    "                project_name = data[\"dbgapss\"][\"Study\"][\"StudyInfo\"][\"StudyNameEntrez\"]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    # If PHS ID is not provided or PHS ID doesn't produce a project name\n",
    "    if not project_name:\n",
    "        # Construct a query to pull project name from BigQuery, execute, and return results\n",
    "        try:\n",
    "            client = bigquery.Client()\n",
    "            api_client = refresh_tdr_api_client()\n",
    "            full_tdr_schema, bq_project, bq_schema, skip_bq_queries = odv.retrieve_tdr_schema(dataset_id, \"dataset\", api_client)\n",
    "            proj_query = \"\"\"SELECT COALESCE(MAX(CASE WHEN attribute = 'library:projectName' THEN NULLIF(value,'NA') END), MAX(CASE WHEN attribute = 'library:datasetName' THEN NULLIF(value,'NA') END)) AS project_name\n",
    "                            FROM `{project}.{schema}.workspace_attributes`\"\"\".format(project = bq_project, schema = bq_schema)\n",
    "            df = client.query(proj_query).result().to_dataframe()\n",
    "            record_dict = df.to_dict(orient=\"records\")\n",
    "            project_name = record_dict[0][\"project_name\"]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    # If no project name has been derived, set project name to dataset name\n",
    "    if not project_name:\n",
    "        project_name = dataset_name\n",
    "    # Return project name\n",
    "    return project_name\n",
    "\n",
    "# Function to find and execute ingests\n",
    "def run_t_ingests(params):\n",
    "    dataset_id = params[\"dataset_id\"]\n",
    "    query_set = params[\"query_set\"]\n",
    "    override_list = params[\"ingest_list_override\"]\n",
    "    # Determine set of ingests to run\n",
    "    ingest_list = query_set[\"transforms\"].keys()\n",
    "    filtered_ingest_list = []\n",
    "    if override_list:\n",
    "        for item in ingest_list:\n",
    "            if item in override_list:\n",
    "                filtered_ingest_list.append(item)\n",
    "    else:\n",
    "        filtered_ingest_list = ingest_list\n",
    "    # Loop through and ingest files\n",
    "    for item in filtered_ingest_list:\n",
    "        target_table = item\n",
    "        source_file_name = target_table + \".json\" \n",
    "        source_rel_file_path = \"{}/{}\".format(params[\"t_output_dir\"], source_file_name)\n",
    "        source_full_file_path = \"{}/{}/{}\".format(params[\"ws_bucket\"], params[\"t_output_dir\"], source_file_name)\n",
    "        if file_exists(source_rel_file_path, params):\n",
    "            logging.info(f\"Running ingest from {source_file_name} to table {target_table}.\")\n",
    "            ingest_request = {\n",
    "                \"table\": target_table,\n",
    "                \"profile_id\": params[\"profile_id\"],\n",
    "                \"ignore_unknown_values\": True,\n",
    "                \"resolve_existing_files\": True,\n",
    "                \"updateStrategy\": \"replace\",\n",
    "                \"format\": \"json\",\n",
    "                \"bulkMode\": True,\n",
    "                \"load_tag\": \"Ingest for {}\".format(params[\"dataset_id\"]),\n",
    "                \"path\": source_full_file_path\n",
    "            }\n",
    "            attempt_counter = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    api_client = refresh_tdr_api_client()\n",
    "                    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "                    ingest_request_result, job_id = wait_for_tdr_job(datasets_api.ingest_dataset(id=dataset_id, ingest=ingest_request))\n",
    "                    logging.info(\"Ingest from file {} succeeded: {}\".format(source_file_name, str(ingest_request_result)[0:1000]))\n",
    "                    params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"Table: {target_table} - File: {source_file_name}\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(ingest_request_result)[0:1000])])\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logging.error(\"Error on Dataset Ingest: {}\".format(str(e)))\n",
    "                    attempt_counter += 1\n",
    "                    if attempt_counter < 2:\n",
    "                        logging.info(\"Retrying Dataset Ingest (attempt #{})...\".format(str(attempt_counter)))\n",
    "                        sleep(10)\n",
    "                        continue\n",
    "                    else:\n",
    "                        logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"Table: {target_table} - File: {source_file_name}\", \"Error\", str(e)])\n",
    "                        break\n",
    "        else:\n",
    "            logging.warning(f\"Metadata file does not exist.  Skipping: {source_file_name}\")\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", f\"Table: {target_table} - File: {source_file_name}\", \"Warning\", f\"Metadata file does not exist.  Skipping: {source_file_name}\"])\n",
    "            continue              \n",
    "                \n",
    "# Function to extend schema of existing TDR dataset\n",
    "def run_schema_extension(params):\n",
    "    dataset_id = params[\"dataset_id\"]\n",
    "    target_schema = params[\"target_schema\"]\n",
    "    mapping_target = params[\"mapping_target\"]\n",
    "    # Retrieve current TDR schema and diff with target_schema\n",
    "    logging.info(\"Retrieving current TDR schema to determine new tables and relationships to add.\")\n",
    "    api_client = refresh_tdr_api_client()\n",
    "    datasets_api = data_repo_client.DatasetsApi(api_client=api_client)\n",
    "    additional_tables = []\n",
    "    additional_relationships = []\n",
    "    try:\n",
    "        response = datasets_api.retrieve_dataset(id=dataset_id, include=[\"SCHEMA\", \"ACCESS_INFORMATION\"]).to_dict()\n",
    "        current_table_list = [table[\"name\"] for table in response[\"schema\"][\"tables\"]]\n",
    "        current_rel_list = [rel[\"name\"] for rel in response[\"schema\"][\"relationships\"]]\n",
    "        for table_entry in target_schema[\"tables\"]:\n",
    "            if table_entry[\"name\"] not in current_table_list:\n",
    "                additional_tables.append(table_entry)\n",
    "        for rel_entry in target_schema[\"relationships\"]:\n",
    "            if rel_entry[\"name\"] not in current_rel_list:\n",
    "                additional_relationships.append(rel_entry)\n",
    "    except Exception as e:\n",
    "        logging.warning(\"Error retrieving source schema from TDR. Will attempt to add all tables and relationships to schema. Error: {}\".format(e))\n",
    "        additional_tables = target_schema[\"tables\"]\n",
    "        additional_relationships = target_schema[\"relationships\"]\n",
    "    # Update TDR dataset schema to include additional tables and relationships\n",
    "    add_table_list = [table[\"name\"] for table in additional_tables]\n",
    "    add_rel_list = [rel[\"name\"] for rel in additional_relationships]\n",
    "    if add_table_list or add_rel_list:\n",
    "        logging.info(\"Submitting TDR schema extension request.\\n\\tTables to add: {tabs}\\n\\tRelationships to add: {rels}\".format(tabs=add_table_list, rels=add_rel_list))\n",
    "        schema_update_request = {\n",
    "            \"description\": \"Adding tables and relationships for mapping target: {}.\".format(params[\"mapping_target\"]),\n",
    "            \"changes\": {\n",
    "                \"addTables\": additional_tables,\n",
    "                \"addRelationships\": additional_relationships\n",
    "            }\n",
    "        }\n",
    "        attempt_counter = 0\n",
    "        while True:\n",
    "            try:\n",
    "                schema_update_result, job_id = wait_for_tdr_job(datasets_api.update_schema(id=dataset_id, dataset_schema_update_model=schema_update_request))\n",
    "                logging.info(\"TDR schema extension succeeded: {}\".format(str(schema_update_result)[0:1000]))\n",
    "                params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"TDR Schema Extension\", \"Extend TDR Schema\", \"Success\", \"Job_ID: {} - Truncated Response: {}\".format(job_id, str(schema_update_result)[0:1000])])\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(\"Error on TDR schema extension: {}\".format(str(e)))\n",
    "                attempt_counter += 1\n",
    "                if attempt_counter < 2:\n",
    "                    logging.info(\"Retrying TDR schema extension (attempt #{})...\".format(str(attempt_counter)))\n",
    "                    sleep(10)\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(\"Maximum number of retries exceeded. Recording error to pipeline results.\")\n",
    "                    params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"TDR Schema Extension\", \"Extend TDR Schema\", \"Error\", str(e)])\n",
    "                    break\n",
    "    else:\n",
    "        logging.info(\"No new tables or relationships to add to the TDR schema.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"TDR Schema Extension\", \"Extend TDR Schema\", \"Success\", \"No new tables or relationships to add to the TDR schema.\"])\n",
    "                \n",
    "# Function to loop through and execute transform queries\n",
    "def run_transform_queries(params):\n",
    "    # Determine set of transforms to run\n",
    "    query_set = params[\"query_set\"][\"transforms\"]\n",
    "    override_list = params[\"transform_list_override\"]\n",
    "    filtered_query_set = {}\n",
    "    if override_list:\n",
    "        for key, val in query_set.items():\n",
    "            if key in override_list:\n",
    "                filtered_query_set[key] = val\n",
    "    else:\n",
    "        filtered_query_set = query_set\n",
    "    # Read in and execute queries\n",
    "    client = bigquery.Client()\n",
    "    for key, val in filtered_query_set.items():\n",
    "        target_table = key\n",
    "        target_file = target_table + \".json\"\n",
    "        destination_dir = params[\"t_output_dir\"]\n",
    "        logging.info(\"Running transform for target table: {}\".format(target_table))\n",
    "        query = val[\"query\"]\n",
    "        try:\n",
    "            df = client.query(query).result().to_dataframe()\n",
    "            records_json = df.to_json(orient='records') \n",
    "            records_list = json.loads(records_json)\n",
    "            records_cnt = len(records_list)\n",
    "            with open(target_file, 'w') as outfile:\n",
    "                for idx, val in enumerate(records_list):\n",
    "                    json.dump(val, outfile)\n",
    "                    if idx < (records_cnt - 1):\n",
    "                        outfile.write('\\n')\n",
    "            !gsutil cp $target_file $ws_bucket/$destination_dir/ 2> stdout\n",
    "            !rm $target_file\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transformed Files Creation\", \"File: {}\".format(target_file), \"Success\", \"\"])\n",
    "        except Exception as e:\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transformed Files Creation\", \"File: {}\".format(target_file), \"Error\", str(e)])\n",
    "                \n",
    "# Function to orchestrate the various components of the transformation ingest pipeline\n",
    "def run_t_pipeline(params):\n",
    "    \n",
    "    # Step 1: Set Variables for Pipeline\n",
    "    src_dataset = params[\"dataset_name\"] + \" ({})\".format(params[\"dataset_id\"])\n",
    "    logging.info(\"Starting Transformation (T) Pipeline for {}.\".format(src_dataset))\n",
    "    params[\"src_dataset\"] = src_dataset\n",
    "    params[\"target_schema\"] = {}\n",
    "    params[\"query_set\"] = {}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(params[\"ws_bucket_name\"])\n",
    "    blob = bucket.blob(\"ingest_pipeline/output/transformed/{}/{}/schema/mapping_schema_object.json\".format(params[\"mapping_target\"], params[\"dataset_id\"]))\n",
    "    params[\"target_schema\"] = json.loads(blob.download_as_string(client=None))\n",
    "    blob = bucket.blob(\"ingest_pipeline/output/transformed/{}/{}/queries/transform_query_set.json\".format(params[\"mapping_target\"], params[\"dataset_id\"]))\n",
    "    params[\"query_set\"] = json.loads(blob.download_as_string(client=None))\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_string = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "    params[\"anvil_schema_version\"] = \"ANV\" + str(params[\"target_schema\"][\"version\"])\n",
    "    params[\"snapshot_name\"] = params[\"dataset_name\"] + \"_\" + params[\"anvil_schema_version\"] + \"_\" + current_datetime_string  \n",
    "    params[\"t_output_dir\"] = \"ingest_pipeline/output/transformed/{}/{}/table_data\".format(params[\"mapping_target\"], params[\"dataset_id\"])\n",
    "    params[\"t_val_output_dir\"] = \"ingest_pipeline/output/transformed/{}/{}/validation\".format(params[\"mapping_target\"], params[\"dataset_id\"])\n",
    "    params[\"validation_schema_file\"] = \"ingest_pipeline/mapping/{}/mapping_schema_object.json\".format(params[\"mapping_target\"])\n",
    "    params[\"pipeline_results\"] = []\n",
    "    logs_destination_dir = params[\"ws_bucket\"] + \"/ingest_pipeline/output/transformed/{}/{}/logs\".format(params[\"mapping_target\"], params[\"dataset_id\"])\n",
    "    logs_stream_file_path = \"pipeline_results_log_stream_\" + current_datetime_string + \".txt\"\n",
    "    logs_table_file_path = \"pipeline_results_log_\" + current_datetime_string + \".tsv\"\n",
    "    while logging.root.handlers:\n",
    "        logging.root.removeHandler(logging.root.handlers[-1])\n",
    "    logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO, handlers=[logging.FileHandler(logs_stream_file_path), logging.StreamHandler(sys.stdout)])\n",
    "\n",
    "    # Step 2: Confirm Transformation Artifacts\n",
    "    logging.info(\"Attempting to confirm transform artifact retrieval (target schema and transform queries).\")\n",
    "    if params[\"target_schema\"] and params[\"query_set\"]:\n",
    "        logging.info(\"Transform artifact retrieval confirmed.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transform Artifact Retrieval\", \"Confirm Transform Artifact Retrieval\", \"Success\", \"\"])\n",
    "    else:\n",
    "        if not params[\"target_schema\"] and not params[\"query_set\"]:\n",
    "            error_message = \"Target schema and transform queries both empty. Exiting pipeline.\"\n",
    "            logging.info(error_message)\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transform Artifact Retrieval\", \"Retrieve Target Schema and Transform Queries\", \"Error\", error_message])\n",
    "            df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "            copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results)\n",
    "            logging.info(\"Pipeline Results:\")\n",
    "            display(df_results)\n",
    "            return\n",
    "        elif not params[\"target_schema\"]:\n",
    "            error_message = \"Target schema empty. Exiting pipeline.\"\n",
    "            logging.info(error_message)\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transform Artifact Retrieval\", \"Retrieve Target Schema and Transform Queries\", \"Error\", error_message])\n",
    "            df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "            copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results)\n",
    "            logging.info(\"Pipeline Results:\")\n",
    "            display(df_results)\n",
    "            return\n",
    "        else:\n",
    "            error_message = \"Transform queries empty. Exiting pipeline.\"\n",
    "            logging.info(error_message)\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transform Artifact Retrieval\", \"Retrieve Target Schema and Transform Queries\", \"Error\", error_message])\n",
    "            df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "            copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results)\n",
    "            logging.info(\"Pipeline Results:\")\n",
    "            display(df_results)\n",
    "            return\n",
    "                 \n",
    "    # Step 3: Create Transformed Files\n",
    "    if params[\"skip_transforms\"] == True:\n",
    "        logging.info(\"Skipping transformed files creation.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Transformed Files Creation\", \"File: All\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running transformed files creation.\")\n",
    "        run_transform_queries(params)\n",
    "        \n",
    "    # Step 4: Identify TDR Dataset and Extend Schema \n",
    "    if params[\"skip_schema_extension\"] == True:\n",
    "        logging.info(\"Skipping TDR schema extension.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"TDR Schema Extension\", \"Extend TDR Schema\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        if len(params[\"dataset_id\"]) == 0:\n",
    "            logging.error(\"No TDR dataset specified. Exiting pipeline.\")\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"TDR Schema Extension\", \"Extend TDR Schema\", \"Error\", \"No TDR dataset specified.\"])\n",
    "            df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "            copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results)\n",
    "            logging.info(\"Pipeline Results:\")\n",
    "            display(df_results)\n",
    "            return\n",
    "        else:\n",
    "            logging.info(\"Running TDR schema extension.\")\n",
    "            run_schema_extension(params)\n",
    "            \n",
    "    # Step 5: Ingest Transformed Files\n",
    "    int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping dataset ingests due to upstream errors.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", \"Table: All - File: All\", \"Skipped\", \"Errors found upstream in ingest pipeline\"])\n",
    "    elif params[\"skip_ingests\"] == True:\n",
    "        logging.info(\"Skipping dataset ingests.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dataset Ingests\", \"Table: All - File: All\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running dataset ingests\")\n",
    "        run_t_ingests(params)\n",
    "    \n",
    "    # Step 6: Infer File Relationships\n",
    "    int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping file relationships inference due to upstream errors.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"File Relationship Inference\", \"Infer and Ingest File Relationships\", \"Skipped\", \"Errors found upstream in ingest pipeline\"])\n",
    "    elif params[\"skip_file_relation_inference\"] == True:\n",
    "        logging.info(\"Skipping file relationships inference.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"File Relationship Inference\", \"Infer and Ingest File Relationships\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running file relationships inference.\")\n",
    "        try:\n",
    "            output, status = ifr.infer_file_relationships(params, params[\"dataset_id\"])\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"File Relationship Inference\", \"Infer and Ingest File Relationships\", status, output])\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error running dangling foreign key resolution: {}\".format(str(e)))\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"File Relationship Inference\", \"Infer and Ingest File Relationships\", \"Error\", str(e)])   \n",
    "    \n",
    "    # Step 7: Remediate Dangling Foreign Keys\n",
    "    int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping dangling foreign key resolution due to upstream errors.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dangling Foreign Key Resolution\", \"Resolve Dangling Foreign Keys\", \"Skipped\", \"Errors found upstream in ingest pipeline\"])\n",
    "    elif params[\"skip_dangling_fk_resolution\"] == True:\n",
    "        logging.info(\"Skipping dangling foreign key resolution.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dangling Foreign Key Resolution\", \"Resolve Dangling Foreign Keys\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running dangling foreign key resolution.\")\n",
    "        try:\n",
    "            output, status = rdfk.resolve_dangling_fks(params, params[\"dataset_id\"], params[\"target_schema\"])\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dangling Foreign Key Resolution\", \"Resolve Dangling Foreign Keys\", status, output])\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error running dangling foreign key resolution: {}\".format(str(e)))\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Dangling Foreign Key Resolution\", \"Resolve Dangling Foreign Keys\", \"Error\", str(e)])   \n",
    "    \n",
    "    # Step 8: Identify supplementary data files\n",
    "    int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping supplementary file identification due to upstream errors.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Supplementary File Identification\", \"Identify and Update Supplementary Files\", \"Skipped\", \"Errors found upstream in ingest pipeline\"])\n",
    "    elif params[\"skip_supplementary_file_identification\"] == True:\n",
    "        logging.info(\"Skipping supplementary file identification.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Supplementary File Identification\", \"Identify and Update Supplementary Files\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running supplementary file identification.\")\n",
    "        try:\n",
    "            output, status = isf.identify_supplementary_files(params, params[\"dataset_id\"])\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Supplementary File Identification\", \"Identify and Update Supplementary Files\", status, output])\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error running supplementary file identification: {}\".format(str(e)))\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Supplementary File Identification\", \"Identify and Update Supplementary Files\", \"Error\", str(e)])   \n",
    "    \n",
    "    # Step 9: Create and Share Snapshot\n",
    "    int_df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    errors = int_df_results[int_df_results[\"Status\"].str.contains(\"Error\")]\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping snapshot creation due to upstream errors.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Skipped\", \"Errors found upstream in ingest pipeline\"])\n",
    "    elif params[\"skip_snapshot_creation\"] == True:\n",
    "        logging.info(\"Skipping snapshot creation on user request.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running snapshot creation\")\n",
    "        try:\n",
    "            create_and_share_snapshot(params, \"t_pipeline\")\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error running snapshot creation: {}\".format(str(e)))\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Snapshot Creation\", \"Create and Share Snapshot\", \"Error\", str(e)])\n",
    "    \n",
    "    # Step 10: Data Profiling and Validation\n",
    "    if len(errors) > 0:\n",
    "        logging.info(\"Skipping output data validation due to upstream errors.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Output Data Validation\", \"Profile and Validate Data\", \"Skipped\", \"Errors found upstream in ingest pipeline\"])\n",
    "    elif params[\"skip_data_validation\"] == True:\n",
    "        logging.info(\"Skipping output data validation on user request.\")\n",
    "        params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Output Data Validation\", \"Profile and Validate Data\", \"Skipped\", \"User request\"])\n",
    "    else:\n",
    "        logging.info(\"Running output data validation.\")\n",
    "        try:\n",
    "            results_path = odv.profile_data(params[\"dataset_id\"], \"dataset\", params[\"t_val_output_dir\"], params[\"validation_schema_file\"])\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Output Data Validation\", \"Profile and Validate Data\", \"Success\", results_path])\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error running output data validation: {}\".format(str(e)))\n",
    "            params[\"pipeline_results\"].append([params[\"src_dataset\"], datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Output Data Validation\", \"Profile and Validate Data\", \"Error\", str(e)])   \n",
    "\n",
    "    # Aggregate, save, and display pipeline results\n",
    "    df_results = pd.DataFrame(params[\"pipeline_results\"], columns = [\"Dataset\", \"Time\", \"Step\", \"Task\", \"Status\", \"Message\"])\n",
    "    logging.info(\"The ingest pipeline has completed for {}.\".format(params[\"src_dataset\"]))\n",
    "    copy_log_results(logs_destination_dir, logs_stream_file_path, logs_table_file_path, df_results)\n",
    "    logging.info(\"Pipeline Results:\")\n",
    "    display(df_results)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
