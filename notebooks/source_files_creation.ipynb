{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print(\"Version 1.0.0: 09/08/2022 7:48pm - Nate Calvanese - First version created\")\n",
    "#print(\"Version 1.0.1: 09/14/2022 3:55pm - Nate Calvanese - Added workspace_attribute table creation\")\n",
    "#print(\"Version 1.0.2: 09/16/2022 8:23am - Nate Calvanese - Prefixed source tables\")\n",
    "#print(\"Version 1.0.3: 09/23/2022 11:53am - Nate Calvanese - Made source workspace configurable\")\n",
    "#print(\"Version 1.0.5: 09/23/2022 11:53am - Nate Calvanese - Appended source workspace to file names\")\n",
    "#print(\"Version 1.0.6: 10/04/2022 3:35pm - Nate Calvanese - Added support for set tables\")\n",
    "#print(\"Version 1.0.7: 10/07/2022 4:07pm - Nate Calvanese - Switched to parsing entity model into TSV\")\n",
    "print(\"Version 1.0.8: 10/12/2022 3:50pm - Nate Calvanese - Reverted source table prefixing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade import_ipynb data_repo_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import import_ipynb\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "import ingest_pipeline_utilities as utils\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main source connector function\n",
    "def create_source_table_data_files(params):\n",
    "    \n",
    "    # Initialize parameters\n",
    "    log_status = \"Success\"\n",
    "    log_string = \"\"\n",
    "    validation_data = []\n",
    "    destination_dir = params[\"input_dir\"]\n",
    "    src_ws_project = params[\"src_ws_project\"]\n",
    "    src_ws_name = params[\"src_ws_name\"]\n",
    "    \n",
    "    # Collect and record all entity types in workspace\n",
    "    response_etypes = fapi.list_entity_types(src_ws_project, src_ws_name)\n",
    "    dict_all_etypes = json.loads(response_etypes.text)\n",
    "    etypes_list = []\n",
    "    etypes_list = [key for key in dict_all_etypes.keys()]\n",
    "    logging.info(f\"List of entity tables in current workspace: \" + ', '.join(etypes_list))\n",
    "    \n",
    "    # Loop through the entity types, pull JSON structure, convert to tsv, validate record counts, and transfer to cloud\n",
    "    if etypes_list:\n",
    "        for etype in etypes_list:\n",
    "            logging.info(f'Starting download of tsv file for {etype} table.')\n",
    "\n",
    "            # Get entity table information and convert to tsv structure\n",
    "            res_etype = fapi.get_entities(src_ws_project, src_ws_name, etype)\n",
    "            original_tsv_name = etype + \"_\" + src_ws_name + \".tsv\"\n",
    "\n",
    "            # Build tsv records from JSON entity model\n",
    "            record_list = []\n",
    "            for record in json.loads(res_etype.text):\n",
    "                temp_dict = {}\n",
    "                ordered_dict = {}\n",
    "                pk_col_name = record[\"entityType\"] + \"_id\"\n",
    "                pk_val = record[\"name\"]\n",
    "                ordered_dict[pk_col_name] = pk_val\n",
    "                try:\n",
    "                    for key, val in record[\"attributes\"].items():\n",
    "                        col_name = key\n",
    "                        # If value is a dictionary, evaluate whether it is an entity reference, a list of attributes, or a list of entity references\n",
    "                        if isinstance(val, dict):\n",
    "                            if val.get(\"entityName\"):\n",
    "                                col_val = val[\"entityName\"]\n",
    "                                temp_dict[col_name] = col_val\n",
    "                            elif val.get(\"items\"):\n",
    "                                col_val = []\n",
    "                                for entry in val[\"items\"]:\n",
    "                                    if isinstance(entry, dict):\n",
    "                                        if entry.get(\"entityName\"):\n",
    "                                            col_val.append('\"' + entry[\"entityName\"] + '\"')\n",
    "                                    else:\n",
    "                                        col_val.append('\"' + entry + '\"')\n",
    "                                temp_dict[col_name] = \"[\" + \",\".join(col_val) + \"]\"\n",
    "                        else:\n",
    "                            col_val = val\n",
    "                            temp_dict[col_name] = col_val\n",
    "                except:\n",
    "                    pass\n",
    "                # Sort the returned record alphabetically, keeping the pk column as the first entry\n",
    "                temp_dict_sorted = {key: value for key, value in sorted(temp_dict.items())}\n",
    "                for key, val in temp_dict_sorted.items():\n",
    "                    if key not in ordered_dict:\n",
    "                        ordered_dict[key] = val \n",
    "                record_list.append(ordered_dict)\n",
    "            df = pd.DataFrame.from_records(record_list)\n",
    "            df.to_csv(original_tsv_name, index=False, sep=\"\\t\")\n",
    "            \n",
    "            # Get number of rows in downloaded tsv file for given entity\n",
    "            num_tsv_entities = !tail -n +2 $original_tsv_name | wc -l\n",
    "\n",
    "            # Capture entity record counts into a validation dictionary\n",
    "            validation_dict = {}\n",
    "            validation_dict[\"entity_type\"] = etype\n",
    "            validation_dict[\"tsv_file_count\"] = num_tsv_entities[0]\n",
    "            validation_dict[\"data_model_count\"] = dict_all_etypes[etype][\"count\"]\n",
    "            if int(num_tsv_entities[0]) == int(dict_all_etypes[etype][\"count\"]):\n",
    "                validation_dict[\"record_count_validation\"] = \"Passed\"\n",
    "            else:\n",
    "                validation_dict[\"record_count_validation\"] = \"Failed\"\n",
    "                log_status = \"Warning\"\n",
    "            validation_data.append(validation_dict)\n",
    "\n",
    "            # Copy tsv file to workspace bucket\n",
    "            logging.info(f'Copying {original_tsv_name} to {ws_bucket}/{destination_dir}/{etype}')\n",
    "            !gsutil cp $original_tsv_name $ws_bucket/$destination_dir/$etype/ 2> stdout\n",
    "\n",
    "    # Grab workspace attributes, flatten, convert to tsv, and transfer to cloud\n",
    "    logging.info(f'Starting download of workspace attribute information.')\n",
    "    etype = \"workspace_attributes\"\n",
    "    ws_attributes_file = etype + \"_\" + src_ws_name + \".tsv\"\n",
    "    raw_ws_attributes = utils.get_workspace_attributes(src_ws_project, src_ws_name)\n",
    "    ws_attributes = {}\n",
    "    for key, val in raw_ws_attributes[\"attributes\"].items():\n",
    "        if isinstance(val, list):\n",
    "            val_str = \", \".join(val)\n",
    "            ws_attributes[key] = val_str.replace(\"\\t\", \"\")\n",
    "        else:\n",
    "            ws_attributes[key] = str(val).replace(\"\\t\", \"\")\n",
    "    ws_df = pd.DataFrame.from_dict(ws_attributes, orient=\"index\")\n",
    "    ws_df.reset_index(inplace=True)\n",
    "    ws_df.columns =[\"attribute\", \"value\"] \n",
    "    ws_df.to_csv(ws_attributes_file, sep=\"\\t\", index=False)\n",
    "    logging.info(f\"Copying {ws_attributes_file} to {ws_bucket}/{destination_dir}/{etype}\")\n",
    "    !gsutil cp $ws_attributes_file $ws_bucket/$destination_dir/$etype/ 2> stdout\n",
    "    \n",
    "    # Dump validation data to log string to return to caller and display results\n",
    "    log_string = json.dumps(validation_data)\n",
    "    validation_df = pd.DataFrame(validation_data)\n",
    "    logging.info(\"Download and copy of tsv files complete. Validation results: \\n\")\n",
    "    display(validation_df)\n",
    "\n",
    "    # Delete copy of tsv files from notebook env - they will persist in designated workspace bucket directory\n",
    "    !rm *.tsv\n",
    "    \n",
    "    # Return log variables\n",
    "    return log_status, log_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"input_dir\"] = \"ingest_pipeline/input/test/table_data\"\n",
    "# params[\"src_ws_project\"] = \"anvil-datastorage\"\n",
    "# params[\"src_ws_name\"] = \"AnVIL_ccdg_asc_ndd_daly_talkowski_ac-boston_asd_exome\"\n",
    "# log_status, log_string = create_source_table_data_files(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2\n",
    "# params = {}\n",
    "# params[\"input_dir\"] = \"ingest_pipeline/input/test2/table_data\"\n",
    "# params[\"src_ws_project\"] = \"gro-share-seq-computational\"\n",
    "# params[\"src_ws_name\"] = \"GRO_share_seq_computational_testing\"\n",
    "# log_status, log_string = create_source_table_data_files(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
