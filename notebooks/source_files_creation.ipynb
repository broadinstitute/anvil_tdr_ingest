{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version History\n",
    "#print(\"Version 1.0.0: 09/08/2022 7:48pm - Nate Calvanese - First version created\")\n",
    "#print(\"Version 1.0.1: 09/14/2022 3:55pm - Nate Calvanese - Added workspace_attribute table creation\")\n",
    "#print(\"Version 1.0.2: 09/16/2022 8:23am - Nate Calvanese - Prefixed source tables\")\n",
    "#print(\"Version 1.0.3: 09/23/2022 11:53am - Nate Calvanese - Made source workspace configurable\")\n",
    "print(\"Version 1.0.3: 09/23/2022 11:53am - Nate Calvanese - Appended source workspace to file names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and environment variables\n",
    "\n",
    "# Imports\n",
    "import import_ipynb\n",
    "from firecloud import api as fapi\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "import ingest_pipeline_utilities as utils\n",
    "\n",
    "# Configure logging format\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO)\n",
    "\n",
    "# workspace environment variables\n",
    "ws_name = os.environ[\"WORKSPACE_NAME\"]\n",
    "ws_project = os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "ws_bucket = os.environ[\"WORKSPACE_BUCKET\"]\n",
    "ws_bucket_name = re.sub('^gs://', '', ws_bucket)\n",
    "\n",
    "# print(f\"workspace name = {ws_name}\")\n",
    "# print(f\"workspace project = {ws_project}\")\n",
    "# print(f\"workspace bucket = {ws_bucket}\")\n",
    "# print(f\"workspace bucket name = {ws_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main source connector function\n",
    "def create_source_table_data_files(params):\n",
    "    \n",
    "    # Initialize parameters\n",
    "    log_status = \"Success\"\n",
    "    log_string = \"\"\n",
    "    validation_data = []\n",
    "    destination_dir = params[\"input_dir\"]\n",
    "    src_ws_project = params[\"src_ws_project\"]\n",
    "    src_ws_name = params[\"src_ws_name\"]\n",
    "    \n",
    "    # Collect and record all entity types in workspace\n",
    "    response_etypes = fapi.list_entity_types(src_ws_project, src_ws_name)\n",
    "    dict_all_etypes = json.loads(response_etypes.text)\n",
    "    etypes_list = []\n",
    "    etypes_list = [key for key in dict_all_etypes.keys()]\n",
    "    logging.info(f\"List of entity tables in current workspace: \" + ', '.join(etypes_list))\n",
    "    \n",
    "    # Loop through the entity types, download as a tsv, validate the tsv, and transfer to cloud\n",
    "    if etypes_list:\n",
    "        for etype in etypes_list:\n",
    "            logging.info(f'Starting download of tsv file for {etype} table.')\n",
    "\n",
    "            # Get entity table response for API call and save to tsv\n",
    "            res_etype = fapi.get_entities_tsv(src_ws_project, src_ws_name, etype, model=\"flexible\")\n",
    "            prefixed_etype = \"ws_\" + etype\n",
    "            original_tsv_name = prefixed_etype + \"_\" + src_ws_name + \".tsv\"\n",
    "            with open(original_tsv_name, \"w\") as f:\n",
    "                f.write(res_etype.text)\n",
    "\n",
    "            # Get number of rows in downloaded tsv file for given entity and update validation dict with count\n",
    "            num_tsv_entities = !tail -n +2 $original_tsv_name | wc -l\n",
    "\n",
    "            # Capture entity record counts into a validation dictionary\n",
    "            validation_dict = {}\n",
    "            validation_dict[\"entity_type\"] = etype\n",
    "            validation_dict[\"tsv_file_count\"] = num_tsv_entities[0]\n",
    "            validation_dict[\"data_model_count\"] = dict_all_etypes[etype][\"count\"]\n",
    "            if int(num_tsv_entities[0]) == int(dict_all_etypes[etype][\"count\"]):\n",
    "                validation_dict[\"record_count_validation\"] = \"Passed\"\n",
    "            else:\n",
    "                validation_dict[\"record_count_validation\"] = \"Failed\"\n",
    "                log_status = \"Warning\"\n",
    "            validation_data.append(validation_dict)\n",
    "\n",
    "            # Copy tsv file to workspace bucket\n",
    "            logging.info(f'Copying {original_tsv_name} to {ws_bucket}/{destination_dir}/{prefixed_etype}')\n",
    "            !gsutil cp $original_tsv_name $ws_bucket/$destination_dir/$prefixed_etype/ 2> stdout\n",
    "\n",
    "    # Grab workspace attributes, flatten, convert to tsv, and transfer to cloud\n",
    "    logging.info(f'Starting download of workspace attribute information.')\n",
    "    etype = \"workspace_attributes\"\n",
    "    prefixed_etype = \"ws_\" + etype\n",
    "    ws_attributes_file = prefixed_etype + \"_\" + src_ws_name + \".tsv\"\n",
    "    raw_ws_attributes = utils.get_workspace_attributes(src_ws_project, src_ws_name)\n",
    "    ws_attributes = {}\n",
    "    for key, val in raw_ws_attributes[\"attributes\"].items():\n",
    "        if isinstance(val, list):\n",
    "            val_str = \", \".join(val)\n",
    "            ws_attributes[key] = val_str.replace(\"\\t\", \"\")\n",
    "        else:\n",
    "            ws_attributes[key] = str(val).replace(\"\\t\", \"\")\n",
    "    ws_df = pd.DataFrame.from_dict(ws_attributes, orient=\"index\")\n",
    "    ws_df.reset_index(inplace=True)\n",
    "    ws_df.columns =[\"attribute\", \"value\"] \n",
    "    ws_df.to_csv(ws_attributes_file, sep=\"\\t\", index=False)\n",
    "    logging.info(f\"Copying {ws_attributes_file} to {ws_bucket}/{destination_dir}/{prefixed_etype}\")\n",
    "    !gsutil cp $ws_attributes_file $ws_bucket/$destination_dir/$prefixed_etype/ 2> stdout\n",
    "    \n",
    "    # Dump validation data to log string to return to caller and display results\n",
    "    log_string = json.dumps(validation_data)\n",
    "    validation_df = pd.DataFrame(validation_data)\n",
    "    logging.info(\"Download and copy of tsv files complete. Validation results: \\n\")\n",
    "    display(validation_df)\n",
    "\n",
    "    # Delete copy of tsv files from notebook env - they will persist in designated workspace bucket directory\n",
    "    !rm *.tsv\n",
    "    \n",
    "    # Return log variables\n",
    "    return log_status, log_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# params = {}\n",
    "# params[\"input_dir\"] = \"ingest_pipeline/input/test/table_data\"\n",
    "# params[\"src_ws_project\"] = \"anvil-datastorage\"\n",
    "# params[\"src_ws_name\"] = \"ANVIL_CMG_UWASH_DS-BDIS\"\n",
    "# log_status, log_string = create_source_table_data_files(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
